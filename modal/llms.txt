# MODAL DOCUMENTATION
Generated: 2025-06-19 21:28:26
Files processed: 28
Total characters: 368,380
Source: Auto-scraped documentation

## OVERVIEW
This file contains the complete documentation for modal, minified and optimized for LLM consumption.
All content has been cleaned, deduplicated, and formatted for maximum information density.

---


## 001_INDEX
Modal Documentation
Modal provides a serverless cloud for engineers and researchers who want
to build compute-intensive applications without thinking about
infrastructure.
Run generative AI models, large-scale batch workflows, job queues, and more,
all faster than ever before.
Try the playground
Guide
Everything you need to know to run code on Modal. Dive deep into all of our features and best practices.
Examples
Powerful applications built with Modal. Explore guided starting points for your use case.
Reference
Technical information about the Modal API. Quickly refer to basic descriptions of various programming functionalities.
Playground
Interactive tutorials to learn how to start using Modal. Run serverless cloud functions from your browser.
Guide
Everything you need to know to run code on Modal. Dive deep into all of our features and best practices.
Examples
Powerful applications built with Modal. Explore guided starting points for your use case.
Reference
Technical information about the Modal API. Quickly refer to basic descriptions of various programming functionalities.
Playground
Interactive tutorials to learn how to start using Modal. Run serverless cloud functions from your browser.
Featured Examples
View all
Deploy an OpenAI-compatible LLM service
Run large language models with a drop-in replacement for the OpenAI API.
Custom pet art from Flux with Hugging Face and Gradio
Fine-tune an image generation model on pictures of your pet.
Run llama.cpp
Run DeepSeek-R1 and Phi-4 on llama.cpp
Voice chat with LLMs
Build an interactive voice chat app.
Serve diffusion models
Serve Flux on Modal with a number of optimizations for blazingly fast inference.
Fold proteins with Chai-1
Predict molecular structures from sequences with SotA open source models.
Serverless TensorRT-LLM (LLaMA 3 8B)
Run interactive language model applications.
Star in custom music videos
Fine-tune a Wan2.1 video model on your face and run it in parallel
Create music
Turn prompts into music with MusicGen
Sandbox a LangGraph agent's code
Run an LLM coding agent that runs its own language models.
RAG Chat with PDFs
Use ColBERT-style, multimodal embeddings with a Vision-Language Model to answer questions about documents.
Bring images to life
Prompt a generative video model to animate an image.
Fast podcast transcriptions
Build an end-to-end podcast transcription app that leverages dozens of containers for super-fast processing.
Build a protein folding dashboard
Serve a web UI for a protein model with ESM3, Molstar, and Gradio
Deploy a Hacker News Slackbot
Periodically post new Hacker News posts to Slack.
Retrieval-Augmented Generation (RAG) for Q&A
Build a question-answering web endpoint that can cite its sources.
Document OCR job queue
Use Modal as an infinitely scalable job queue that can service async tasks from a web app.
Parallel processing of Parquet files on S3
Analyze data from the Taxi and Limousine Commission of NYC in parallel.

## 002_EXAMPLES_FLUX
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Run Flux fast on H100s with
torch.compile
Update: To speed up inference by another >2x, check out the additional optimization
techniques we tried in
this blog post
In this guide, we’ll run Flux as fast as possible on Modal using open source tools.
We’ll use
torch.compile
and NVIDIA H100 GPUs.
Setting up the image and dependencies
import
time
from
import
BytesIO
from
pathlib
import
Path
import
modal
Copy
We’ll make use of the full
CUDA toolkit
in this example, so we’ll build our container image off of the
nvidia/cuda
base.
cuda_version =
"12.4.0"
## should be no greater than host CUDA version
flavor =
"devel"
## includes full CUDA toolkit
operating_sys =
"ubuntu22.04"
tag =
cuda_version
flavor
operating_sys
cuda_dev_image = modal.Image.from_registry(
"nvidia/cuda:
add_python
"3.11"
).entrypoint([])
Copy
Now we install most of our dependencies with
For Hugging Face’s
Diffusers
library
we install from GitHub source and so pin to a specific commit.
PyTorch added [faster attention kernels for Hopper GPUs in version 2.5
diffusers_commit_sha =
"81cf3b2f155f1de322079af28f625349ee21ec6b"
flux_image = (
cuda_dev_image.apt_install(
"git"
"libglib2.0-0"
"libsm6"
"libxrender1"
"libxext6"
"ffmpeg"
"libgl1"
.pip_install(
"invisible_watermark==0.2.0"
"transformers==4.44.0"
"huggingface_hub[hf_transfer]==0.26.2"
"accelerate==0.33.0"
"safetensors==0.4.4"
"sentencepiece==0.2.0"
"torch==2.5.0"
"git+https://github.com/huggingface/diffusers.git@
diffusers_commit_sha
"numpy<2"
.env({
"HF_HUB_ENABLE_HF_TRANSFER"
"HF_HUB_CACHE"
"/cache"
Copy
Later, we’ll also use
torch.compile
to increase the speed further.
Torch compilation needs to be re-executed when each new container starts,
So we turn on some extra caching to reduce compile times for later containers.
flux_image = flux_image.env(
"TORCHINDUCTOR_CACHE_DIR"
"/root/.inductor-cache"
"TORCHINDUCTOR_FX_GRAPH_CACHE"
Copy
Finally, we construct our Modal
set its default image to the one we just constructed,
and import
FluxPipeline
for downloading and running Flux.1.
app = modal.App(
"example-flux"
image
=flux_image)
with
flux_image.imports():
import
torch
from
diffusers
import
FluxPipeline
Copy
Defining a parameterized
Model
inference class
Next, we map the model’s setup and inference code onto Modal.
We the model setun in the method decorated with
@modal.enter()
. This includes loading the
weights and moving them to the GPU, along with an optional
torch.compile
step (see details below).
@modal.enter()
decorator ensures that this method runs only once, when a new container starts,
instead of in the path of every call.
We run the actual inference in methods decorated with
@modal.method()
MINUTES =
## seconds
VARIANT =
"schnell"
## or "dev", but note [dev] requires you to accept terms and conditions on HF
NUM_INFERENCE_STEPS =
## use ~50 for [dev], smaller for [schnell]
@app.cls
"H100"
## fastest GPU on Modal
scaledown_window
* MINUTES,
timeout
* MINUTES,
## leave plenty of time for compilation
volumes
## add Volumes to store serializable compilation artifacts, see section on torch.compile below
"/cache"
: modal.Volume.from_name(
"hf-hub-cache"
create_if_missing
True
"/root/.nv"
: modal.Volume.from_name(
"nv-cache"
create_if_missing
True
"/root/.triton"
: modal.Volume.from_name(
"triton-cache"
create_if_missing
True
"/root/.inductor-cache"
: modal.Volume.from_name(
"inductor-cache"
create_if_missing
True
class
Model
compile
bool
## see section on torch.compile below for details
modal.parameter(
default
False
@modal.enter
enter
self
pipe = FluxPipeline.from_pretrained(
"black-forest-labs/FLUX.1-
VARIANT
torch_dtype
=torch.bfloat16
).to(
"cuda"
## move model to GPU
self
.pipe = optimize(pipe,
compile
self
.compile)
@modal.method
inference
self
prompt
) ->
bytes
print
"🎨 generating image..."
out =
self
.pipe(
prompt,
output_type
"pil"
num_inference_steps
=NUM_INFERENCE_STEPS,
).images[
byte_stream = BytesIO()
out.save(byte_stream,
format
"JPEG"
return
byte_stream.getvalue()
Copy
Calling our inference function
To generate an image we just need to call the
Model
generate
method
with
.remote
appended to it.
You can call
.generate.remote
from any Python environment that has access to your Modal credentials.
The local environment will get back the image as bytes.
Here, we wrap the call in a Modal
local_entrypoint
so that it can be run with
modal run
modal
flux.py
Copy
By default, we call
generate
twice to demonstrate how much faster
the inference is after cold start. In our tests, clients received images in about 1.2 seconds.
We save the output bytes to a temporary file.
@app.local_entrypoint
main
prompt
"a computer screen showing ASCII terminal art of the"
" word 'Modal' in neon green. two programmers are pointing excitedly"
" at the screen."
twice
bool
True
compile
bool
False
t0 = time.time()
image_bytes = Model(
compile
compile
).inference.remote(prompt)
print
"🎨 first inference latency:
time.time() - t0
:.2f}
seconds"
twice:
t0 = time.time()
image_bytes = Model(
compile
compile
).inference.remote(prompt)
print
"🎨 second inference latency:
time.time() - t0
:.2f}
seconds"
output_path = Path(
"/tmp"
"flux"
"output.jpg"
output_path.parent.mkdir(
exist_ok
True
parents
True
print
"🎨 saving output to
output_path
output_path.write_bytes(image_bytes)
Copy
Speeding up Flux with
torch.compile
By default, we do some basic optimizations, like adjusting memory layout
and re-expressing the attention head projections as a single matrix multiplication.
But there are additional speedups to be had!
PyTorch 2 added a compiler that optimizes the
compute graphs created dynamically during PyTorch execution.
This feature helps close the gap with the performance of static graph frameworks
like TensorRT and TensorFlow.
Here, we follow the suggestions from Hugging Face’s
guide to fast diffusion inference
which we verified with our own internal benchmarks.
Review that guide for detailed explanations of the choices made below.
The resulting compiled Flux
schnell
deployment returns images to the client in under a second (~700 ms), according to our testing.
Super schnell
Compilation takes up to twenty minutes on first iteration.
As of time of writing in late 2024,
the compilation artifacts cannot be fully serialized,
so some compilation work must be re-executed every time a new container is started.
That includes when scaling up an existing deployment or the first time a Function is invoked with
modal run
We cache compilation outputs from
nvcc
triton
, and
inductor
which can reduce compilation time by up to an order of magnitude.
For details see
this tutorial
You can turn on compilation with the
--compile
flag.
Try it out with:
modal
flux.py
--compile
Copy
compile
option is passed by a
modal.parameter
on our class.
Each different choice for a
parameter
creates a
separate auto-scaling deployment
That means your client can use arbitrary logic to decide whether to hit a compiled or eager endpoint.
optimize
pipe
compile
True
## fuse QKV projections in Transformer and VAE
pipe.transformer.fuse_qkv_projections()
pipe.vae.fuse_qkv_projections()
## switch memory layout to Torch's preferred, channels_last
pipe.transformer.to(
memory_format
=torch.channels_last)
pipe.vae.to(
memory_format
=torch.channels_last)
compile
return
pipe
## set torch compile flags
config = torch._inductor.config
config.disable_progress =
False
## show progress bar
config.conv_1x1_as_mm =
True
## treat 1x1 convolutions as matrix muls
## adjust autotuning algorithm
config.coordinate_descent_tuning =
True
config.coordinate_descent_check_all_directions =
True
config.epilogue_fusion =
False
## do not fuse pointwise ops into matmuls
## tag the compute-intensive modules, the Transformer and VAE decoder, for compilation
pipe.transformer = torch.compile(
pipe.transformer,
mode
"max-autotune"
fullgraph
True
pipe.vae.decode = torch.compile(
pipe.vae.decode,
mode
"max-autotune"
fullgraph
True
## trigger torch compilation
print
"🔦 running torch compilation (may take up to 20 minutes)..."
pipe(
"dummy prompt to trigger torch compilation"
output_type
"pil"
num_inference_steps
=NUM_INFERENCE_STEPS,
## use ~50 for [dev], smaller for [schnell]
).images[
print
"🔦 finished torch compilation"
return
pipe
Copy
Run Flux fast on H100s with torch.compile
Setting up the image and dependencies
Defining a parameterized Model inference class
Calling our inference function
Speeding up Flux with torch.compile
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
06_gpu_and_ml/stable_diffusion/flux.py
--no-compile
Copy

## 003_EXAMPLES_HACKERNEWS_ALERTS
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Run cron jobs in the cloud to search Hacker News
In this example, we use Modal to deploy a cron job that periodically queries Hacker News for
new posts matching a given search term, and posts the results to Slack.
Import and define the app
Let’s start off with imports, and defining a Modal app.
import
from
datetime
import
datetime, timedelta
import
modal
app = modal.App(
"example-hn-bot"
Copy
Now, let’s define an image that has the
slack-sdk
package installed, in which we can run a function
that posts a slack message.
slack_sdk_image = modal.Image.debian_slim().pip_install(
"slack-sdk"
Copy
Defining the function and importing the secret
Our Slack bot will need access to a bot token.
We can use Modal’s
Secrets
interface to accomplish this.
To quickly create a Slack bot secret, click the “Create new secret” button.
Then, select the Slack secret template from the list options,
and follow the instructions in the “Where to find the credentials?” panel.
Name your secret
hn-bot-slack.
Now, we define the function
post_to_slack
, which simply instantiates the Slack client using our token,
and then uses it to post a message to a given channel name.
@app.function
image
=slack_sdk_image,
secrets
=[modal.Secret.from_name(
"hn-bot-slack"
required_keys
"SLACK_BOT_TOKEN"
])],
async
post_to_slack
message
import
slack_sdk
client = slack_sdk.WebClient(
token
=os.environ[
"SLACK_BOT_TOKEN"
client.chat_postMessage(
channel
"hn-alerts"
text
=message)
Copy
Searching Hacker News
We are going to use Algolia’s
Hacker News Search API
to query for posts
matching a given search term in the past X days. Let’s define our search term and query period.
QUERY =
"serverless"
WINDOW_SIZE_DAYS =
Copy
Let’s also define an image that has the
requests
package installed, so we can query the API.
requests_image = modal.Image.debian_slim().pip_install(
"requests"
Copy
We can now define our main entrypoint, that queries Algolia for the term, and calls
post_to_slack
on all the results. We specify a
schedule
in the function decorator, which means that our function will run automatically at the given interval.
@app.function
image
=requests_image)
search_hackernews
import
requests
url =
"http://hn.algolia.com/api/v1/search"
threshold = datetime.utcnow() - timedelta(
days
=WINDOW_SIZE_DAYS)
params = {
"query"
: QUERY,
"numericFilters"
"created_at_i>
threshold.timestamp()
response = requests.get(url, params,
timeout
).json()
urls = [item[
"url"
item
response[
"hits"
item.get(
"url"
print
"Query returned
(urls)
items."
post_to_slack.for_each(urls)
Copy
Test running
We can now test run our scheduled function as follows:
modal run hackernews_alerts.py::app.search_hackernews
Defining the schedule and deploying
Let’s define a function that will be called by Modal every day
@app.function
schedule
=modal.Period(
days
run_daily
search_hackernews.remote()
Copy
In order to deploy this as a persistent cron job, you can run
modal deploy hackernews_alerts.py
Once the job is deployed, visit the
apps page
page to see
its execution history, logs and other stats.
Run cron jobs in the cloud to search Hacker News
Import and define the app
Defining the function and importing the secret
Searching Hacker News
Test running
Defining the schedule and deploying
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
05_scheduling/hackernews_alerts.py
Copy

## 004_EXAMPLES_MUSIC-VIDEO-GEN
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
Deploy a personalized music video generation service on Modal
Music videos are
cool
but unless you are famous or
pay a lot of money
you don’t get to star in them.
Until now!
The repo
includes all the code you need to deploy a custom
music video generator on
Modal
a serverless infrastructure platform for data, ML, and AI applications.
Below is a sample video, generated by Modal Developer Advocate
@charles_irl
And because Modal is
generic serverless infrastructure
you can customize this custom music video generator however you wish —
it’s just code and containers!
Setup
In the Python environment of your choosing,
pip install modal
If you run into trouble with Python environments,
we suggest using
this Google Colab notebook
where we’ve set the environment up for you.
It’s a bit of work to get used to running terminal commands in a notebook
if you haven’t done that before, but the Python setup works and running the notebook in Colab is free!
All you need is a Google account.
Then, if you’ve never used Modal on the computer you’re using,
modal setup
to create an account on Modal (if you don’t have one)
and set up authentication.
Data Prep
Create a folder inside
data/
, parallel to the sample data,
data/sample
You can name it whatever you want.
Place at least four images of yourself in that folder —
ideally eight or more.
Images should be in
.png
.jpg
format
and around 400 to 800 pixels on each side.
For best results, we recommend putting a variety of images,
in particular where you are wearing different clothes and making different faces,
and including some images that have other people in them.
But you can also just take a few photos of yourself right now!
Optionally, add captions in
.txt
files in that same folder.
They should look something like
"[trigger] smiling at the camera, outdoor scene, close-up, selfie"
See the sample data for more example image-caption pairs.
Training
Start up a JupyterLab server on Modal with
modal
train_from_notebook.py
Copy
Click the
modal.host
URL that appears in the output
to open Jupyter in the browser.
Open the training notebook,
training.ipynb
Read the notebook and run it, following the instructions to edit cells as needed.
In particular, change the dataset path to the folder you created —
it has been mounted on the remote cloud machine where the notebook is running.
You can also directly upload data to the
/root/data
folder on the remote machine.
You can even edit caption files inside of JupyterLab!
This data will stick around between runs, and you can find it with
modal
volume
finetune-video-data
Copy
See the help for
modal volume
and its subcommands for details.
The notebook will kick off training, which takes a few minutes.
Take note of the name given to your training run.
By default, it’s a hash like
38c67a92f6ce87882044ab53bf94cce0
but you can customize it in the notebook.
This is your
finetune-id
If you forget it, you can show all of your
finetune-id
by running
modal
volume
finetune-video-models
Copy
Inference
Test out your new fine-tuned model by running:
modal
inference.py
--finetune-id
{your-finetune-id}
--num-frames
Copy
You can also provide a
--prompt
to customize the generation.
You can deploy the video generator onto Modal with
modal
deploy
inference.py
Copy
Modal is serverless, so this won’t cost you any money when it isn’t serving any traffic.
Music video generation
Once you’ve deployed an inference endpoint,
you can generate a music video starring yourself by running
modal
music_video_generator.py
--finetune-id
{your-finetune-id}
Copy
With the default settings, this will create a thirty second video in about five minutes
by running generation in parallel on seven H100s.
The music can be changed by passing in a different song via the
--mp3-file
argument.
The default is a Modal-themed song in
data/coding-up-a-storm.mp3
This song was created with
Suno
a music generation service — that runs on Modal!
If you want to DIY music generation as well,
this example
in the Modal docs.
The generated clips can be changed by passing a different list of prompts via the
--prompt-file
argument.
The default is a set of prompts created with OpenAI’s GPT-4.5 system.
You can write your own or generate them with a language model.
If you want to serve your own language model,
this example
in the Modal docs.
Deploy a personalized music video generation service on Modal
Setup
Data Prep
Training
Inference
Music video generation

## 005_REFERENCE_MODAL_IMAGE
Changelog
API Reference
modal.App
modal.Client
modal.CloudBucketMount
modal.Cls
modal.Cron
modal.Dict
modal.Error
modal.FilePatternMatcher
modal.Function
modal.FunctionCall
modal.Image
modal.NetworkFileSystem
modal.Period
modal.Proxy
modal.Queue
modal.Retries
modal.Sandbox
modal.SandboxSnapshot
modal.Secret
modal.Tunnel
modal.Volume
modal.asgi_app
modal.batched
modal.call_graph
modal.concurrent
modal.container_process
modal.current_function_call_id
modal.current_input_id
modal.enable_output
modal.enter
modal.exit
modal.fastapi_endpoint
modal.file_io
modal.forward
modal.gpu
modal.interact
modal.io_streams
modal.is_local
modal.method
modal.parameter
modal.web_endpoint
modal.web_server
modal.wsgi_app
modal.exception
modal.config
CLI Reference
modal app
modal config
modal container
modal deploy
modal dict
modal environment
modal launch
modal nfs
modal profile
modal queue
modal run
modal secret
modal serve
modal setup
modal shell
modal token
modal volume
modal.Image
class
Image
modal
object
Object
Copy
Base class for container images to run functions in.
Do not construct this class directly; instead use one of its static factory methods,
such as
modal.Image.debian_slim
modal.Image.from_registry
, or
modal.Image.micromamba
hydrate
hydrate
self
client
: Optional[_Client] =
None
) -> Self:
Copy
Synchronize the local object with its identity on the Modal server.
It is rarely necessary to call this method explicitly, as most operations
will lazily hydrate when needed. The main use case is when you need to
access object metadata, such as its ID.
Added in v0.72.39
: This method replaces the deprecated
.resolve()
method.
add_local_file
add_local_file
self
local_path
: Union[
, Path],
remote_path
, *,
copy
bool
False
) ->
"_Image"
Copy
Adds a local file to the image at
remote_path
within the container
By default (
copy=False
), the files are added to containers on startup and are not built into the actual Image,
which speeds up deployment.
copy=True
to copy the files into an Image layer at build time instead, similar to how
COPY
works in a
Dockerfile
copy=True can slow down iteration since it requires a rebuild of the Image and any subsequent
build steps whenever the included files change, but it is required if you want to run additional
build steps after this one.
Added in v0.66.40
: This method replaces the deprecated
modal.Image.copy_local_file
method.
add_local_dir
add_local_dir
self
local_path
: Union[
, Path],
remote_path
copy
bool
False
## Predicate filter function for file exclusion, which should accept a filepath and return `True` for exclusion.
## Defaults to excluding no files. If a Sequence is provided, it will be converted to a FilePatternMatcher.
## Which follows dockerignore syntax.
ignore
: Union[Sequence[
], Callable[[Path],
bool
]] = [],
) ->
"_Image"
Copy
Adds a local directory’s content to the image at
remote_path
within the container
By default (
copy=False
), the files are added to containers on startup and are not built into the actual Image,
which speeds up deployment.
copy=True
to copy the files into an Image layer at build time instead, similar to how
COPY
works in a
Dockerfile
copy=True can slow down iteration since it requires a rebuild of the Image and any subsequent
build steps whenever the included files change, but it is required if you want to run additional
build steps after this one.
Usage:
from
modal
import
FilePatternMatcher
image = modal.Image.debian_slim().add_local_dir(
"~/assets"
remote_path
"/assets"
ignore
"*.venv"
image = modal.Image.debian_slim().add_local_dir(
"~/assets"
remote_path
"/assets"
ignore
lambda
: p.is_relative_to(
".venv"
image = modal.Image.debian_slim().add_local_dir(
"~/assets"
remote_path
"/assets"
ignore
=FilePatternMatcher(
"**/*.txt"
## When including files is simpler than excluding them, you can use the `~` operator to invert the matcher.
image = modal.Image.debian_slim().add_local_dir(
"~/assets"
remote_path
"/assets"
ignore
=~FilePatternMatcher(
"**/*.py"
## You can also read ignore patterns from a file.
image = modal.Image.debian_slim().add_local_dir(
"~/assets"
remote_path
"/assets"
ignore
=FilePatternMatcher.from_file(
"/path/to/ignorefile"
Copy
Added in v0.66.40
: This method replaces the deprecated
modal.Image.copy_local_dir
method.
add_local_python_source
add_local_python_source
self
modules
copy
bool
False
ignore
: Union[Sequence[
], Callable[[Path],
bool
]] = NON_PYTHON_FILES
) ->
"_Image"
Copy
Adds locally available Python packages/modules to containers
Adds all files from the specified Python package or module to containers running the Image.
Packages are added to the
/root
directory of containers, which is on the
PYTHONPATH
of any executed Modal Functions, enabling import of the module by that name.
By default (
copy=False
), the files are added to containers on startup and are not built into the actual Image,
which speeds up deployment.
copy=True
to copy the files into an Image layer at build time instead. This can slow down iteration since
it requires a rebuild of the Image and any subsequent build steps whenever the included files change, but it is
required if you want to run additional build steps after this one.
Note:
This excludes all dot-prefixed subdirectories or files and all
.pyc
__pycache__
files.
To add full directories with finer control, use
.add_local_dir()
instead and specify
/root
the destination directory.
By default only includes
-files in the source modules. Set the
ignore
argument to a list of patterns
or a callable to override this behavior, e.g.:
## includes everything except data.json
modal.Image.debian_slim().add_local_python_source(
"mymodule"
ignore
"data.json"
## exclude large files
modal.Image.debian_slim().add_local_python_source(
"mymodule"
ignore
lambda
: p.stat().st_size >
Copy
Added in v0.67.28
: This method replaces the deprecated
modal.Mount.from_local_python_packages
pattern.
from_id
staticmethod
from_id
image_id
client
: Optional[_Client] =
None
) ->
"_Image"
Copy
Construct an Image from an id and look up the Image result.
The ID of an Image object can be accessed using
.object_id
pip_install
pip_install
self
packages
: Union[
, list[
## A list of Python packages, eg. ["numpy", "matplotlib>=3.5.0"]
find_links
: Optional[
None
## Passes -f (--find-links) pip install
index_url
: Optional[
None
## Passes -i (--index-url) to pip install
extra_index_url
: Optional[
None
## Passes --extra-index-url to pip install
bool
False
## Passes --pre (allow pre-releases) to pip install
extra_options
## Additional options to pass to pip install, e.g. "--no-build-isolation --no-clean"
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
secrets
: Sequence[_Secret] = [],
: GPU_T =
None
) ->
"_Image"
Copy
Install a list of Python packages using pip.
Examples
Simple installation:
image = modal.Image.debian_slim().pip_install(
"click"
"httpx~=0.23.3"
Copy
More complex installation:
image = (
modal.Image.from_registry(
"nvidia/cuda:12.2.0-devel-ubuntu22.04"
add_python
"3.11"
.pip_install(
"ninja"
"packaging"
"wheel"
"transformers==4.40.2"
.pip_install(
"flash-attn==2.5.8"
extra_options
"--no-build-isolation"
Copy
pip_install_private_repos
pip_install_private_repos
self
repositories
git_user
find_links
: Optional[
None
## Passes -f (--find-links) pip install
index_url
: Optional[
None
## Passes -i (--index-url) to pip install
extra_index_url
: Optional[
None
## Passes --extra-index-url to pip install
bool
False
## Passes --pre (allow pre-releases) to pip install
extra_options
## Additional options to pass to pip install, e.g. "--no-build-isolation --no-clean"
: GPU_T =
None
secrets
: Sequence[_Secret] = [],
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
) ->
"_Image"
Copy
Install a list of Python packages from private git repositories using pip.
This method currently supports Github and Gitlab only.
Github:
Provide a
modal.Secret
that contains a
GITHUB_TOKEN
key-value pair
Gitlab:
Provide a
modal.Secret
that contains a
GITLAB_TOKEN
key-value pair
These API tokens should have permissions to read the list of private repositories provided as arguments.
We recommend using Github’s
‘fine-grained’ access tokens
These tokens are repo-scoped, and avoid granting read permission across all of a user’s private repos.
Example
image = (
modal.Image
.debian_slim()
.pip_install_private_repos(
"github.com/ecorp/private-one@1.0.0"
"github.com/ecorp/private-two@main"
"github.com/ecorp/private-three@d4776502"
## install from 'inner' directory on default branch.
"github.com/ecorp/private-four#subdirectory=inner"
git_user
"erikbern"
secrets
=[modal.Secret.from_name(
"github-read-private"
Copy
pip_install_from_requirements
pip_install_from_requirements
self
requirements_txt
## Path to a requirements.txt file.
find_links
: Optional[
None
## Passes -f (--find-links) pip install
index_url
: Optional[
None
## Passes -i (--index-url) to pip install
extra_index_url
: Optional[
None
## Passes --extra-index-url to pip install
bool
False
## Passes --pre (allow pre-releases) to pip install
extra_options
## Additional options to pass to pip install, e.g. "--no-build-isolation --no-clean"
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
secrets
: Sequence[_Secret] = [],
: GPU_T =
None
) ->
"_Image"
Copy
Install a list of Python packages from a local
requirements.txt
file.
pip_install_from_pyproject
pip_install_from_pyproject
self
pyproject_toml
optional_dependencies
: list[
] = [],
find_links
: Optional[
None
## Passes -f (--find-links) pip install
index_url
: Optional[
None
## Passes -i (--index-url) to pip install
extra_index_url
: Optional[
None
## Passes --extra-index-url to pip install
bool
False
## Passes --pre (allow pre-releases) to pip install
extra_options
## Additional options to pass to pip install, e.g. "--no-build-isolation --no-clean"
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
secrets
: Sequence[_Secret] = [],
: GPU_T =
None
) ->
"_Image"
Copy
Install dependencies specified by a local
pyproject.toml
file.
optional_dependencies
is a list of the keys of the
optional-dependencies section(s) of the
pyproject.toml
file
(e.g. test, doc, experiment, etc). When provided,
all of the packages in each listed section are installed as well.
poetry_install_from_file
poetry_install_from_file
self
poetry_pyproject_toml
poetry_lockfile
: Optional[
None
## Path to lockfile. If not provided, uses poetry.lock in same directory.
ignore_lockfile
bool
False
## If set to True, do not use poetry.lock, even when present
## If set to True, use old installer. See https://github.com/python-poetry/poetry/issues/3336
old_installer
bool
False
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
## Selected optional dependency groups to install (See https://python-poetry.org/docs/cli/#install)
with_
: list[
] = [],
## Selected optional dependency groups to exclude (See https://python-poetry.org/docs/cli/#install)
without
: list[
] = [],
only
: list[
] = [],
## Only install dependency groups specifed in this list.
secrets
: Sequence[_Secret] = [],
: GPU_T =
None
) ->
"_Image"
Copy
Install poetry
dependencies
specified by a local
pyproject.toml
file.
If not provided as argument the path to the lockfile is inferred. However, the
file has to exist, unless
ignore_lockfile
is set to
True
Note that the root project of the poetry project is not installed, only the dependencies.
For including local python source files see
add_local_python_source
dockerfile_commands
dockerfile_commands
self
dockerfile_commands
: Union[
, list[
context_files
: dict[
] = {},
secrets
: Sequence[_Secret] = [],
: GPU_T =
None
context_mount
: Optional[_Mount] =
None
## Deprecated: the context is now inferred
context_dir
: Optional[Union[Path,
]] =
None
## Context for relative COPY commands
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
ignore
: Union[Sequence[
], Callable[[Path],
bool
]] = AUTO_DOCKERIGNORE,
) ->
"_Image"
Copy
Extend an image with arbitrary Dockerfile-like commands.
Usage:
from
modal
import
FilePatternMatcher
## By default a .dockerignore file is used if present in the current working directory
image = modal.Image.debian_slim().dockerfile_commands(
"COPY data /data"
image = modal.Image.debian_slim().dockerfile_commands(
"COPY data /data"
ignore
"*.venv"
image = modal.Image.debian_slim().dockerfile_commands(
"COPY data /data"
ignore
lambda
: p.is_relative_to(
".venv"
image = modal.Image.debian_slim().dockerfile_commands(
"COPY data /data"
ignore
=FilePatternMatcher(
"**/*.txt"
## When including files is simpler than excluding them, you can use the `~` operator to invert the matcher.
image = modal.Image.debian_slim().dockerfile_commands(
"COPY data /data"
ignore
=~FilePatternMatcher(
"**/*.py"
## You can also read ignore patterns from a file.
image = modal.Image.debian_slim().dockerfile_commands(
"COPY data /data"
ignore
=FilePatternMatcher.from_file(
"/path/to/dockerignore"
Copy
entrypoint
entrypoint
self
entrypoint_commands
: list[
) ->
"_Image"
Copy
Set the entrypoint for the image.
shell
shell
self
shell_commands
: list[
) ->
"_Image"
Copy
Overwrite default shell for the image.
run_commands
run_commands
self
commands
: Union[
, list[
secrets
: Sequence[_Secret] = [],
: GPU_T =
None
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
) ->
"_Image"
Copy
Extend an image with a list of shell commands to run.
micromamba
staticmethod
micromamba
python_version
: Optional[
None
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
) ->
"_Image"
Copy
A Micromamba base image. Micromamba allows for fast building of small Conda-based containers.
micromamba_install
micromamba_install
self
## A list of Python packages, eg. ["numpy", "matplotlib>=3.5.0"]
packages
: Union[
, list[
## A local path to a file containing package specifications
spec_file
: Optional[
None
## A list of Conda channels, eg. ["conda-forge", "nvidia"].
channels
: list[
] = [],
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
secrets
: Sequence[_Secret] = [],
: GPU_T =
None
) ->
"_Image"
Copy
Install a list of additional packages using micromamba.
from_registry
staticmethod
from_registry
secret
: Optional[_Secret] =
None
setup_dockerfile_commands
: list[
] = [],
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
add_python
: Optional[
None
kwargs
) ->
"_Image"
Copy
Build a Modal Image from a public or private image registry, such as Docker Hub.
The image must be built for the
linux/amd64
platform.
If your image does not come with Python installed, you can use the
add_python
parameter
to specify a version of Python to add to the image. Otherwise, the image is expected to
have Python on PATH as
python
, along with
You may also use
setup_dockerfile_commands
to run Dockerfile commands before the
remaining commands run. This might be useful if you want a custom Python installation or to
set a
SHELL
. Prefer
run_commands()
when possible though.
To authenticate against a private registry with static credentials, you must set the
secret
parameter to
modal.Secret
containing a username (
REGISTRY_USERNAME
) and
an access token or password (
REGISTRY_PASSWORD
To authenticate against private registries with credentials from a cloud provider,
Image.from_gcp_artifact_registry()
Image.from_aws_ecr()
Examples
modal.Image.from_registry(
"python:3.11-slim-bookworm"
modal.Image.from_registry(
"ubuntu:22.04"
add_python
"3.11"
modal.Image.from_registry(
"nvcr.io/nvidia/pytorch:22.12-py3"
Copy
from_gcp_artifact_registry
staticmethod
from_gcp_artifact_registry
secret
: Optional[_Secret] =
None
setup_dockerfile_commands
: list[
] = [],
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
add_python
: Optional[
None
kwargs
) ->
"_Image"
Copy
Build a Modal image from a private image in Google Cloud Platform (GCP) Artifact Registry.
You will need to pass a
modal.Secret
containing
your GCP service account key data
SERVICE_ACCOUNT_JSON
. This can be done from the
Secrets
page.
Your service account should be granted a specific role depending on the GCP registry used:
For Artifact Registry images (
pkg.dev
domains) use
“Artifact Registry Reader”
role
For Container Registry images (
gcr.io
domains) use
“Storage Object Viewer”
role
Note:
This method does not use
GOOGLE_APPLICATION_CREDENTIALS
as that
variable accepts a path to a JSON file, not the actual JSON string.
Image.from_registry()
for information about the other parameters.
Example
modal.Image.from_gcp_artifact_registry(
"us-east1-docker.pkg.dev/my-project-1234/my-repo/my-image:my-version"
secret
=modal.Secret.from_name(
"my-gcp-secret"
required_keys
"SERVICE_ACCOUNT_JSON"
add_python
"3.11"
Copy
from_aws_ecr
staticmethod
from_aws_ecr
secret
: Optional[_Secret] =
None
setup_dockerfile_commands
: list[
] = [],
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
add_python
: Optional[
None
kwargs
) ->
"_Image"
Copy
Build a Modal image from a private image in AWS Elastic Container Registry (ECR).
You will need to pass a
modal.Secret
containing
AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY
, and
AWS_REGION
to access the target ECR registry.
IAM configuration details can be found in the AWS documentation for
“Private repository policies”
Image.from_registry()
for information about the other parameters.
Example
modal.Image.from_aws_ecr(
"000000000000.dkr.ecr.us-east-1.amazonaws.com/my-private-registry:my-version"
secret
=modal.Secret.from_name(
"aws"
required_keys
"AWS_ACCESS_KEY_ID"
"AWS_SECRET_ACCESS_KEY"
"AWS_REGION"
add_python
"3.11"
Copy
from_dockerfile
staticmethod
from_dockerfile
path
: Union[
, Path],
## Filepath to Dockerfile.
context_mount
: Optional[_Mount] =
None
## Deprecated: the context is now inferred
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
context_dir
: Optional[Union[Path,
]] =
None
## Context for relative COPY commands
secrets
: Sequence[_Secret] = [],
: GPU_T =
None
add_python
: Optional[
None
ignore
: Union[Sequence[
], Callable[[Path],
bool
]] = AUTO_DOCKERIGNORE,
) ->
"_Image"
Copy
Build a Modal image from a local Dockerfile.
If your Dockerfile does not have Python installed, you can use the
add_python
parameter
to specify a version of Python to add to the image.
Usage:
from
modal
import
FilePatternMatcher
## By default a .dockerignore file is used if present in the current working directory
image = modal.Image.from_dockerfile(
"./Dockerfile"
add_python
"3.12"
image = modal.Image.from_dockerfile(
"./Dockerfile"
add_python
"3.12"
ignore
"*.venv"
image = modal.Image.from_dockerfile(
"./Dockerfile"
add_python
"3.12"
ignore
lambda
: p.is_relative_to(
".venv"
image = modal.Image.from_dockerfile(
"./Dockerfile"
add_python
"3.12"
ignore
=FilePatternMatcher(
"**/*.txt"
## When including files is simpler than excluding them, you can use the `~` operator to invert the matcher.
image = modal.Image.from_dockerfile(
"./Dockerfile"
add_python
"3.12"
ignore
=~FilePatternMatcher(
"**/*.py"
## You can also read ignore patterns from a file.
image = modal.Image.from_dockerfile(
"./Dockerfile"
add_python
"3.12"
ignore
=FilePatternMatcher.from_file(
"/path/to/dockerignore"
Copy
debian_slim
staticmethod
debian_slim
python_version
: Optional[
None
force_build
bool
False
) ->
"_Image"
Copy
Default image, based on the official
python
Docker images.
apt_install
apt_install
self
packages
: Union[
, list[
## A list of packages, e.g. ["ssh", "libpq-dev"]
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
secrets
: Sequence[_Secret] = [],
: GPU_T =
None
) ->
"_Image"
Copy
Install a list of Debian packages using
Example
image = modal.Image.debian_slim().apt_install(
"git"
Copy
run_function
run_function
self
raw_f
: Callable[..., Any],
secrets
: Sequence[_Secret] = (),
## Optional Modal Secret objects with environment variables for the container
: Union[GPU_T, list[GPU_T]] =
None
## Requested GPU or or list of acceptable GPUs( e.g. ["A10", "A100"])
volumes
: dict[Union[
, PurePosixPath], Union[_Volume, _CloudBucketMount]] = {},
## Volume mount paths
network_file_systems
: dict[Union[
, PurePosixPath], _NetworkFileSystem] = {},
## NFS mount paths
: Optional[
float
None
## How many CPU cores to request. This is a soft limit.
memory
: Optional[
None
## How much memory to request, in MiB. This is a soft limit.
timeout
: Optional[
## Maximum execution time of the function in seconds.
force_build
bool
False
## Ignore cached builds, similar to 'docker build --no-cache'
cloud
: Optional[
None
## Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
region
: Optional[Union[
, Sequence[
]]] =
None
## Region or regions to run the function on.
args
: Sequence[Any] = (),
## Positional arguments to the function.
kwargs
: dict[
, Any] = {},
## Keyword arguments to the function.
include_source
: Optional[
bool
None
) ->
"_Image"
Copy
Run user-defined function
raw_f
as an image build step. The function runs just like an ordinary Modal
function, and any kwargs accepted by
@app.function
(such as
Mount
NetworkFileSystem
and resource requests) can be supplied to it.
After it finishes execution, a snapshot of the resulting container file system is saved as an image.
Note
Only the source code of
raw_f
, the contents of
**kwargs
, and any referenced
global
variables
are used to determine whether the image has changed and needs to be rebuilt.
If this function references other functions or variables, the image will not be rebuilt if you
make changes to them. You can force a rebuild by changing the function’s source code itself.
Example
my_build_function
open
"model.pt"
).write(
"parameters!"
image = (
modal.Image
.debian_slim()
.pip_install(
"torch"
.run_function(my_build_function,
secrets
=[...],
mounts
=[...])
Copy
self
vars
: dict[
]) ->
"_Image"
Copy
Sets the environment variables in an Image.
Example
image = (
modal.Image.debian_slim()
.env({
"HF_HUB_ENABLE_HF_TRANSFER"
Copy
workdir
workdir
self
path
: Union[
, PurePosixPath]) ->
"_Image"
Copy
Set the working directory for subsequent image build steps and function execution.
Example
image = (
modal.Image.debian_slim()
.run_commands(
"git clone https://xyz app"
.workdir(
"/app"
.run_commands(
"yarn install"
Copy
self
: list[
]) ->
"_Image"
Copy
Set the default entrypoint argument (
) for the image.
Example
image = (
modal.Image.debian_slim().cmd([
"python"
"app.py"
Copy
imports
@contextlib.contextmanager
imports
self
Copy
Used to import packages in global scope that are only available when running remotely.
By using this context manager you can avoid an
ImportError
due to not having certain
packages installed locally.
Usage:
with
image.imports():
import
torch
Copy
modal.Image
hydrate
add_local_file
add_local_dir
add_local_python_source
from_id
pip_install
pip_install_private_repos
pip_install_from_requirements
pip_install_from_pyproject
poetry_install_from_file
dockerfile_commands
entrypoint
shell
run_commands
micromamba
micromamba_install
from_registry
from_gcp_artifact_registry
from_aws_ecr
from_dockerfile
debian_slim
apt_install
run_function
workdir
imports

## 006_EXAMPLES
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
Featured Examples
Featured
Images, video & 3D
Fine-tuning
Language modeling
Batch processing
Audio
Sandboxed code execution
Computational biology
Deploy an OpenAI-compatible LLM service
Run large language models with a drop-in replacement for the OpenAI API.
Custom pet art from Flux with Hugging Face and Gradio
Fine-tune an image generation model on pictures of your pet.
Run llama.cpp
Run DeepSeek-R1 and Phi-4 on llama.cpp
Voice chat with LLMs
Build an interactive voice chat app.
Serve diffusion models
Serve Flux on Modal with a number of optimizations for blazingly fast inference.
Fold proteins with Chai-1
Predict molecular structures from sequences with SotA open source models.
Serverless TensorRT-LLM (LLaMA 3 8B)
Run interactive language model applications.
Star in custom music videos
Fine-tune a Wan2.1 video model on your face and run it in parallel
Create music
Turn prompts into music with MusicGen
Sandbox a LangGraph agent's code
Run an LLM coding agent that runs its own language models.
RAG Chat with PDFs
Use ColBERT-style, multimodal embeddings with a Vision-Language Model to answer questions about documents.
Bring images to life
Prompt a generative video model to animate an image.
Fast podcast transcriptions
Build an end-to-end podcast transcription app that leverages dozens of containers for super-fast processing.
Build a protein folding dashboard
Serve a web UI for a protein model with ESM3, Molstar, and Gradio
Deploy a Hacker News Slackbot
Periodically post new Hacker News posts to Slack.
Retrieval-Augmented Generation (RAG) for Q&A
Build a question-answering web endpoint that can cite its sources.
Document OCR job queue
Use Modal as an infinitely scalable job queue that can service async tasks from a web app.
Parallel processing of Parquet files on S3
Analyze data from the Taxi and Limousine Commission of NYC in parallel.

## 007_REFERENCE
Changelog
API Reference
modal.App
modal.Client
modal.CloudBucketMount
modal.Cls
modal.Cron
modal.Dict
modal.Error
modal.FilePatternMatcher
modal.Function
modal.FunctionCall
modal.Image
modal.NetworkFileSystem
modal.Period
modal.Proxy
modal.Queue
modal.Retries
modal.Sandbox
modal.SandboxSnapshot
modal.Secret
modal.Tunnel
modal.Volume
modal.asgi_app
modal.batched
modal.call_graph
modal.concurrent
modal.container_process
modal.current_function_call_id
modal.current_input_id
modal.enable_output
modal.enter
modal.exit
modal.fastapi_endpoint
modal.file_io
modal.forward
modal.gpu
modal.interact
modal.io_streams
modal.is_local
modal.method
modal.parameter
modal.web_endpoint
modal.web_server
modal.wsgi_app
modal.exception
modal.config
CLI Reference
modal app
modal config
modal container
modal deploy
modal dict
modal environment
modal launch
modal nfs
modal profile
modal queue
modal run
modal secret
modal serve
modal setup
modal shell
modal token
modal volume
API Reference
This is the API reference for the
modal
Python package, which allows you to run distributed applications on Modal.
The reference is intended to be limited to low-level descriptions of various
programmatic functionality. If you’re just getting started with Modal, we would
instead recommend looking at the
guide
first
or to get started quickly with an
example
Application construction
The main unit of deployment for code on Modal
App.function
Decorator for registering a function with an App
App.cls
Decorator for registering a class with an App
Serverless execution
Function
A serverless function backed by an autoscaling container pool
A serverless class supporting parametrization and lifecycle hooks
Extended Function configuration
Class parametrization
parameter
Used to define class parameters, akin to a Dataclass field
Lifecycle hooks
enter
Decorator for a method that will be executed during container startup
exit
Decorator for a method that will be executed during container shutdown
method
Decorator for exposing a method as an invokable function
Web integrations
fastapi_endpoint
Decorator for exposing a simple FastAPI-based endpoint
asgi_app
Decorator for functions that construct an ASGI web application
wsgi_app
Decorator for functions that construct a WSGI web application
web_server
Decorator for functions that construct an HTTP web server
Function semantics
batched
Decorator that enables
dynamic input batching
concurrent
Decorator that enables
input concurrency
Scheduling
Cron
A schedule that runs based on cron syntax
Period
A schedule that runs at a fixed interval
Exception handling
Retries
Function retry policy for input failures
Sandboxed execution
Sandbox
An interface for restricted code execution
ContainerProcess
An object representing a sandboxed process
FileIO
A handle for a file in the Sandbox filesystem
Container configuration
Image
An API for specifying container images
Secret
A pointer to secrets that will be exposed as environment variables
Data primitives
Persistent storage
Volume
Distributed storage supporting highly performant parallel reads
CloudBucketMount
Storage backed by a third-party cloud bucket (S3, etc.)
NetworkFileSystem
Shared, writeable cloud storage (superseded by
modal.Volume
In-memory storage
Dict
A distributed key-value store
Queue
A distributed FIFO queue
Networking
Proxy
An object that provides a static outbound IP address for containers
forward
A context manager for publicly exposing a port from a container
API Reference
Application construction
Serverless execution
Extended Function configuration
Class parametrization
Lifecycle hooks
Web integrations
Function semantics
Scheduling
Exception handling
Sandboxed execution
Container configuration
Data primitives
Persistent storage
In-memory storage
Networking

## 008_EXAMPLES_DOC_OCR_JOBS
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Run a job queue for GOT-OCR
This tutorial shows you how to use Modal as an infinitely scalable job queue
that can service async tasks from a web app. For the purpose of this tutorial,
we’ve also built a
React + FastAPI web app on Modal
that works together with it, but note that you don’t need a web app running on Modal
to use this pattern. You can submit async tasks to Modal from any Python
application (for example, a regular Django app running on Kubernetes).
Our job queue will handle a single task: running OCR transcription for images of receipts.
We’ll make use of a pre-trained model:
General OCR Theory (GOT) 2.0 model
Try it out for yourself
here
Define an App
Let’s first import
modal
and define an
Later, we’ll use the name provided for our
to find it from our web app and submit tasks to it.
from
typing
import
Optional
import
modal
app = modal.App(
"example-doc-ocr-jobs"
Copy
We also define the dependencies for our Function by specifying an
Image
inference_image = modal.Image.debian_slim(
python_version
"3.12"
).pip_install(
"accelerate==0.28.0"
"huggingface_hub[hf_transfer]==0.27.1"
"numpy<2"
"tiktoken==0.6.0"
"torch==2.5.1"
"torchvision==0.20.1"
"transformers==4.48.0"
"verovio==4.3.1"
Copy
Cache the pre-trained model on a Modal Volume
We can obtain the pre-trained model we want to run from Hugging Face
using its name and a revision identifier.
MODEL_NAME =
"ucaslcl/GOT-OCR2_0"
MODEL_REVISION =
"cf6b7386bc89a54f09785612ba74cb12de6fa17c"
Copy
The logic for loading the model based on this information
is encapsulated in the
setup
function below.
setup
import
warnings
from
transformers
import
AutoModel, AutoTokenizer
with
warnings.catch_warnings():
## filter noisy warnings from GOT modeling code
warnings.simplefilter(
"ignore"
tokenizer = AutoTokenizer.from_pretrained(
MODEL_NAME,
revision
=MODEL_REVISION,
trust_remote_code
True
model = AutoModel.from_pretrained(
MODEL_NAME,
revision
=MODEL_REVISION,
trust_remote_code
True
device_map
"cuda"
use_safetensors
True
pad_token_id
=tokenizer.eos_token_id,
return
tokenizer, model
Copy
.from_pretrained
methods from Hugging Face are smart enough
to only download models if they haven’t been downloaded before.
But in Modal’s serverless environment, filesystems are ephemeral,
and so using this code alone would mean that models need to get downloaded
on every request.
So instead, we create a Modal
Volume
to store the model — a durable filesystem that any Modal Function can access.
model_cache = modal.Volume.from_name(
"hf-hub-cache"
create_if_missing
True
Copy
We also update the environment variables for our Function
to include this new path for the model cache —
and to enable fast downloads with the
hf_transfer
library.
MODEL_CACHE_PATH =
"/root/models"
inference_image = inference_image.env(
"HF_HUB_CACHE"
: MODEL_CACHE_PATH,
"HF_HUB_ENABLE_HF_TRANSFER"
Copy
Run OCR inference on Modal by wrapping with
app.function
Now let’s set up the actual OCR inference.
Using the
@app.function
decorator, we set up a Modal
Function
We provide arguments to that decorator to customize the hardware, scaling, and other features
of the Function.
Here, we say that this Function should use NVIDIA L40S
GPUs
automatically
retry
failures up to 3 times,
and have access to our
shared model cache
@app.function
"l40s"
retries
volumes
={MODEL_CACHE_PATH: model_cache},
image
=inference_image,
parse_receipt
image
bytes
) ->
from
tempfile
import
NamedTemporaryFile
tokenizer, model = setup()
with
NamedTemporaryFile(
delete
False
mode
"wb+"
temp_img_file:
temp_img_file.write(image)
output = model.chat(tokenizer, temp_img_file.name,
ocr_type
"format"
print
"Result: "
, output)
return
output
Copy
Deploy
Now that we have a function, we can publish it by deploying the app:
modal
deploy
doc_ocr_jobs.py
Copy
Once it’s published, we can
look up
this Function
from another Python process and submit tasks to it:
fn = modal.Function.from_name(
"example-doc-ocr-jobs"
"parse_receipt"
fn.spawn(my_image)
Copy
Modal will auto-scale to handle all the tasks queued, and
then scale back down to 0 when there’s no work left. To see how you could use this from a Python web
app, take a look at the
receipt parser frontend
tutorial.
Run manually
We can also trigger
parse_receipt
manually for easier debugging:
modal
doc_ocr_jobs
Copy
To try it out, you can find some
example receipts
here
@app.local_entrypoint
main
receipt_filename
: Optional[
None
import
urllib.request
from
pathlib
import
Path
receipt_filename
None
receipt_filename = Path(
__file__
).parent /
"receipt.png"
else
receipt_filename = Path(receipt_filename)
receipt_filename.exists():
image = receipt_filename.read_bytes()
print
"running OCR on
receipt_filename
else
receipt_url =
"https://modal-cdn.com/cdnbot/Brandys-walmart-receipt-8g68_a_hk_f9c25fce.webp"
request = urllib.request.Request(receipt_url)
with
urllib.request.urlopen(request)
response:
image = response.read()
print
"running OCR on sample from URL
receipt_url
print
(parse_receipt.remote(image))
Copy
Run a job queue for GOT-OCR
Define an App
Cache the pre-trained model on a Modal Volume
Run OCR inference on Modal by wrapping with app.function
Deploy
Run manually
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
09_job_queues/doc_ocr_jobs.py
Copy

## 009_EXAMPLES_DREAMBOOTH_APP
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Fine-tune Flux on your pet using LoRA
This example finetunes the
Flux.1-dev model
on images of a pet (by default, a puppy named Qwerty)
using a technique called textual inversion from
the “Dreambooth” paper
Effectively, it teaches a general image generation model a new “proper noun”,
allowing for the personalized generation of art and photos.
We supplement textual inversion with low-rank adaptation (LoRA)
for increased efficiency during training.
It then makes the model shareable with others — without costing $25/day for a GPU server—
by hosting a
Gradio app
on Modal.
It demonstrates a simple, productive, and cost-effective pathway
to building on large pretrained models using Modal’s building blocks, like
GPU-accelerated
Modal Functions and Clses for compute-intensive work,
Volumes
for storage,
web endpoints
for serving.
And with some light customization, you can use it to generate images of your pet!
You can find a video walkthrough of this example on the Modal YouTube channel
here
Imports and setup
We start by importing the necessary libraries and setting up the environment.
from
dataclasses
import
dataclass
from
pathlib
import
Path
import
modal
Copy
Building up the environment
Machine learning environments are complex, and the dependencies can be hard to manage.
Modal makes creating and working with environments easy via
containers and container images
We start from a base image and specify all of our dependencies.
We’ll call out the interesting ones as they come up below.
Note that these dependencies are not installed locally
— they are only installed in the remote environment where our Modal App runs.
app = modal.App(
name
"example-lora-flux"
image = modal.Image.debian_slim(
python_version
"3.10"
).pip_install(
"accelerate==0.31.0"
"datasets~=2.13.0"
"fastapi[standard]==0.115.4"
"ftfy~=6.1.0"
"gradio~=5.5.0"
"huggingface-hub==0.26.2"
"hf_transfer==0.1.8"
"numpy<2"
"peft==0.11.1"
"pydantic==2.9.2"
"sentencepiece>=0.1.91,!=0.1.92"
"smart_open~=6.4.0"
"starlette==0.41.2"
"transformers~=4.41.2"
"torch~=2.2.0"
"torchvision~=0.16"
"triton~=2.2.0"
"wandb==0.17.6"
Copy
Downloading scripts and installing a git repo with
run_commands
We’ll use an example script from the
diffusers
library to train the model.
We acquire it from GitHub and install it in our environment with a series of commands.
The container environments Modal Functions run in are highly flexible —
the docs
for more details.
GIT_SHA =
"e649678bf55aeaa4b60bd1f68b1ee726278c0304"
## specify the commit to fetch
image = (
image.apt_install(
"git"
## Perform a shallow fetch of just the target `diffusers` commit, checking out
## the commit in the container's home directory, /root. Then install `diffusers`
.run_commands(
"cd /root && git init ."
"cd /root && git remote add origin https://github.com/huggingface/diffusers"
"cd /root && git fetch --depth=1 origin
GIT_SHA
&& git checkout
GIT_SHA
"cd /root && pip install -e ."
Copy
Configuration with
dataclass
Machine learning apps often have a lot of configuration information.
We collect up all of our configuration into dataclasses to avoid scattering special/magic values throughout code.
@dataclass
class
SharedConfig
"""Configuration information shared across project components."""
## The instance name is the "proper noun" we're teaching the model
instance_name:
"Qwerty"
## That proper noun is usually a member of some class (person, bird),
## and sharing that information with the model helps it generalize better.
class_name:
"Golden Retriever"
## identifier for pretrained models on Hugging Face
model_name:
"black-forest-labs/FLUX.1-dev"
Copy
Storing data created by our app with
modal.Volume
The tools we’ve used so far work well for fetching external information,
which defines the environment our app runs in,
but what about data that we create or modify during the app’s execution?
A persisted
modal.Volume
can store and share data across Modal Apps and Functions.
We’ll use one to store both the original and fine-tuned weights we create during training
and then load them back in for inference.
volume = modal.Volume.from_name(
"dreambooth-finetuning-volume-flux"
create_if_missing
True
MODEL_DIR =
"/model"
Copy
Note that access to the Flux.1-dev model on Hugging Face is
gated by a license agreement
which
you must agree to
here
After you have accepted the license,
create a Modal Secret
with the name
huggingface-secret
following the instructions in the template.
huggingface_secret = modal.Secret.from_name(
"huggingface-secret"
required_keys
"HF_TOKEN"
image = image.env(
"HF_HUB_ENABLE_HF_TRANSFER"
## turn on faster downloads from HF
@app.function
volumes
={MODEL_DIR: volume},
image
=image,
secrets
=[huggingface_secret],
timeout
## 10 minutes
download_models
config
import
torch
from
diffusers
import
DiffusionPipeline
from
huggingface_hub
import
snapshot_download
snapshot_download(
config.model_name,
local_dir
=MODEL_DIR,
ignore_patterns
"*.pt"
"*.bin"
## using safetensors
DiffusionPipeline.from_pretrained(MODEL_DIR,
torch_dtype
=torch.bfloat16)
Copy
Load fine-tuning dataset
Part of the magic of the low-rank fine-tuning is that we only need 3-10 images for fine-tuning.
So we can fetch just a few images, stored on consumer platforms like Imgur or Google Drive,
whenever we need them — no need for expensive, hard-to-maintain data pipelines.
load_images
image_urls
: list[
]) -> Path:
import
PIL.Image
from
smart_open
import
open
img_path = Path(
"/img"
img_path.mkdir(
parents
True
exist_ok
True
ii, url
enumerate
(image_urls):
with
open
(url,
"rb"
image = PIL.Image.open(f)
image.save(img_path /
.png"
print
ii +
images loaded"
return
img_path
Copy
Low-Rank Adapation (LoRA) fine-tuning for a text-to-image model
The base model we start from is trained to do a sort of “reverse
ekphrasis
it attempts to recreate a visual work of art or image from only its description.
We can use the model to synthesize wholly new images
by combining the concepts it has learned from the training data.
We use a pretrained model, the Flux model from Black Forest Labs.
In this example, we “finetune” Flux, making only small adjustments to the weights.
Furthermore, we don’t change all the weights in the model.
Instead, using a technique called
low-rank adaptation
we change a much smaller matrix that works “alongside” the existing weights, nudging the model in the direction we want.
We can get away with such a small and simple training process because we’re just teach the model the meaning of a single new word: the name of our pet.
The result is a model that can generate novel images of our pet:
as an astronaut in space, as painted by Van Gogh or Bastiat, etc.
Finetuning with Hugging Face 🧨 Diffusers and Accelerate
The model weights, training libraries, and training script are all provided by
🤗 Hugging Face
You can kick off a training job with the command
modal run dreambooth_app.py::app.train
It should take about ten minutes.
Training machine learning models takes time and produces a lot of metadata —
metrics for performance and resource utilization,
metrics for model quality and training stability,
and model inputs and outputs like images and text.
This is especially important if you’re fiddling around with the configuration parameters.
This example can optionally use
Weights & Biases
to track all of this training information.
Just sign up for an account, switch the flag below, and add your API key as a
Modal Secret
USE_WANDB =
False
Copy
You can see an example W&B dashboard
here
Check out
this run
which
despite having high GPU utilization
suffered from numerical instability during training and produced only black images — hard to debug without experiment management logs!
You can read more about how the values in
TrainConfig
are chosen and adjusted
in this blog post on Hugging Face
To run training on images of your own pet, upload the images to separate URLs and edit the contents of the file at
TrainConfig.instance_example_urls_file
to point to them.
Tip: if the results you’re seeing don’t match the prompt too well, and instead produce an image
of your subject without taking the prompt into account, the model has likely overfit. In this case, repeat training with a lower
value of
max_train_steps
. If you used W&B, look back at results earlier in training to determine where to stop.
On the other hand, if the results don’t look like your subject, you might need to increase
max_train_steps
@dataclass
class
TrainConfig
SharedConfig
"""Configuration for the finetuning step."""
## training prompt looks like `{PREFIX} {INSTANCE_NAME} the {CLASS_NAME} {POSTFIX}`
prefix:
"a photo of"
postfix:
## locator for plaintext file with urls for images of target instance
instance_example_urls_file:
Path(
__file__
).parent /
"instance_example_urls.txt"
## Hyperparameters/constants from the huggingface training example
resolution:
train_batch_size:
rank:
## lora rank
gradient_accumulation_steps:
learning_rate:
float
4e-4
lr_scheduler:
"constant"
lr_warmup_steps:
max_train_steps:
checkpointing_steps:
1000
seed:
@app.function
image
=image,
"A100-80GB"
## fine-tuning is VRAM-heavy and requires a high-VRAM GPU
volumes
={MODEL_DIR: volume},
## stores fine-tuned model
timeout
1800
## 30 minutes
secrets
=[huggingface_secret]
[modal.Secret.from_name(
"wandb-secret"
required_keys
"WANDB_API_KEY"
USE_WANDB
else
train
instance_example_urls
config
import
subprocess
from
accelerate.utils
import
write_basic_config
## load data locally
img_path = load_images(instance_example_urls)
## set up hugging face accelerate library for fast training
write_basic_config(
mixed_precision
"bf16"
## define the training prompt
instance_phrase =
config.instance_name
config.class_name
prompt =
config.prefix
instance_phrase
config.postfix
.strip()
## the model training is packaged as a script, so we have to execute it as a subprocess, which adds some boilerplate
_exec_subprocess
: list[
"""Executes subprocess and prints log to terminal while subprocess is running."""
process = subprocess.Popen(
cmd,
stdout
=subprocess.PIPE,
stderr
=subprocess.STDOUT,
with
process.stdout
pipe:
line
iter
(pipe.readline,
line_str = line.decode()
print
line_str
exitcode := process.wait() !=
raise
subprocess.CalledProcessError(exitcode,
.join(cmd))
## run training -- see huggingface accelerate docs for details
print
"launching dreambooth training script"
_exec_subprocess(
"accelerate"
"launch"
"examples/dreambooth/train_dreambooth_lora_flux.py"
"--mixed_precision=bf16"
## half-precision floats most of the time for faster training
"--pretrained_model_name_or_path=
MODEL_DIR
"--instance_data_dir=
img_path
"--output_dir=
MODEL_DIR
"--instance_prompt=
prompt
"--resolution=
config.resolution
"--train_batch_size=
config.train_batch_size
"--gradient_accumulation_steps=
config.gradient_accumulation_steps
"--learning_rate=
config.learning_rate
"--lr_scheduler=
config.lr_scheduler
"--lr_warmup_steps=
config.lr_warmup_steps
"--max_train_steps=
config.max_train_steps
"--checkpointing_steps=
config.checkpointing_steps
"--seed=
config.seed
## increased reproducibility by seeding the RNG
"--report_to=wandb"
## validation output tracking is useful, but currently broken for Flux LoRA training
## f"--validation_prompt={prompt} in space", # simple test prompt
## f"--validation_epochs={config.max_train_steps // 5}",
USE_WANDB
else
## The trained model information has been output to the volume mounted at `MODEL_DIR`.
## To persist this data for use in our web app, we 'commit' the changes
## to the volume.
volume.commit()
Copy
Running our model
To generate images from prompts using our fine-tuned model, we define a Modal Function called
inference
Naively, this would seem to be a bad fit for the flexible, serverless infrastructure of Modal:
wouldn’t you need to include the steps to load the model and spin it up in every function call?
In order to initialize the model just once on container startup,
we use Modal’s
container lifecycle
features, which require the function to be part
of a class. Note that the
modal.Volume
we saved the model to is mounted here as well,
so that the fine-tuned model created by
train
is available to us.
@app.cls
image
=image,
"A100"
volumes
={MODEL_DIR: volume})
class
Model
@modal.enter
load_model
self
import
torch
from
diffusers
import
DiffusionPipeline
## Reload the modal.Volume to ensure the latest state is accessible.
volume.reload()
## set up a hugging face inference pipeline using our model
pipe = DiffusionPipeline.from_pretrained(
MODEL_DIR,
torch_dtype
=torch.bfloat16,
).to(
"cuda"
pipe.load_lora_weights(MODEL_DIR)
self
.pipe = pipe
@modal.method
inference
self
text
config
image =
self
.pipe(
text,
num_inference_steps
=config.num_inference_steps,
guidance_scale
=config.guidance_scale,
).images[
return
image
Copy
Wrap the trained model in a Gradio web UI
Gradio
makes it super easy to expose a model’s functionality
in an easy-to-use, responsive web interface.
This model is a text-to-image generator,
so we set up an interface that includes a user-entry text box
and a frame for displaying images.
We also provide some example text inputs to help
guide users and to kick-start their creative juices.
And we couldn’t resist adding some Modal style to it as well!
You can deploy the app on Modal with the command
modal deploy dreambooth_app.py
You’ll be able to come back days, weeks, or months later and find it still ready to go,
even though you don’t have to pay for a server to run while you’re not using it.
@dataclass
class
AppConfig
SharedConfig
"""Configuration information for inference."""
num_inference_steps:
guidance_scale:
float
web_image = image.add_local_dir(
## Add local web assets to the image
Path(
__file__
).parent /
"assets"
remote_path
"/assets"
@app.function
image
=web_image,
max_containers
@modal.concurrent
max_inputs
1000
@modal.asgi_app
fastapi_app
import
gradio
from
fastapi
import
FastAPI
from
fastapi.responses
import
FileResponse
from
gradio.routes
import
mount_gradio_app
web_app = FastAPI()
## Call out to the inference in a separate Modal environment with a GPU
text
text:
text = example_prompts[
return
Model().inference.remote(text, config)
## set up AppConfig
config = AppConfig()
instance_phrase =
config.instance_name
config.class_name
example_prompts = [
instance_phrase
"a painting of
instance_phrase.title()
With A Pearl Earring, by Vermeer"
"oil painting of
instance_phrase
flying through space as an astronaut"
"a painting of
instance_phrase
in cyberpunk city. character design by cory loftis. volumetric light, detailed, rendered in octane"
"drawing of
instance_phrase
high quality, cartoon, path traced, by studio ghibli and don bluth"
modal_docs_url =
"https://modal.com/docs"
modal_example_url =
modal_docs_url
/examples/dreambooth_app"
description =
"""Describe what they are doing or how a particular artist or style would depict them. Be fantastical! Try the examples below for inspiration.
## Learn how to make a "Dreambooth" for your own pet [here](
modal_example_url
## custom styles: an icon, a background, and a theme
@web_app.get
"/favicon.ico"
include_in_schema
False
async
favicon
return
FileResponse(
"/assets/favicon.svg"
@web_app.get
"/assets/background.svg"
include_in_schema
False
async
background
return
FileResponse(
"/assets/background.svg"
with
open
"/assets/index.css"
css = f.read()
theme = gr.themes.Default(
primary_hue
"green"
secondary_hue
"emerald"
neutral_hue
"neutral"
## add a gradio UI around inference
with
gr.Blocks(
theme
=theme,
=css,
title
"Generate images of
config.instance_name
on Modal"
interface:
gr.Markdown(
"# Generate images of
instance_phrase
\n\n
description
with
gr.Row():
inp = gr.Textbox(
## input text component
label
placeholder
"Describe the version of
instance_phrase
you'd like to see"
lines
out = gr.Image(
## output image component
height
width
label
min_width
elem_id
"output"
with
gr.Row():
btn = gr.Button(
"Dream"
variant
"primary"
scale
btn.click(
=go,
inputs
=inp,
outputs
=out
## connect inputs and outputs with inference function
gr.Button(
## shameless plug
"⚡️ Powered by Modal"
variant
"secondary"
link
"https://modal.com"
with
gr.Column(
variant
"compact"
## add in a few examples to inspire users
ii, prompt
enumerate
(example_prompts):
btn = gr.Button(prompt,
variant
"secondary"
btn.click(
lambda
=ii: example_prompts[idx],
outputs
=inp)
## mount for execution on Modal
return
mount_gradio_app(
=web_app,
blocks
=interface,
path
Copy
Running your fine-tuned model from the command line
You can use the
modal
command-line interface to set up, customize, and deploy this app:
modal run diffusers_lora_finetune.py
will train the model. Change the
instance_example_urls_file
to point to your own pet’s images.
modal serve diffusers_lora_finetune.py
will
serve
the Gradio interface at a temporary location. Great for iterating on code!
modal shell diffusers_lora_finetune.py
is a convenient helper to open a bash
shell
in our image. Great for debugging environment issues.
Remember, once you’ve trained your own fine-tuned model, you can deploy it permanently — for no cost when it is not being used! —
using
modal deploy diffusers_lora_finetune.py
If you just want to try the app out, you can find our deployment
here
@app.local_entrypoint
## add more config params here to make training configurable
max_train_steps
print
"🎨 loading model"
download_models.remote(SharedConfig())
print
"🎨 setting up training"
config = TrainConfig(
max_train_steps
=max_train_steps)
instance_example_urls = (
Path(TrainConfig.instance_example_urls_file).read_text().splitlines()
train.remote(instance_example_urls, config)
print
"🎨 training finished"
Copy
Fine-tune Flux on your pet using LoRA
Imports and setup
Building up the environment
Downloading scripts and installing a git repo with run_commands
Configuration with dataclasses
Storing data created by our app with modal.Volume
Load fine-tuning dataset
Low-Rank Adapation (LoRA) fine-tuning for a text-to-image model
Finetuning with Hugging Face 🧨 Diffusers and Accelerate
Running our model
Wrap the trained model in a Gradio web UI
Running your fine-tuned model from the command line
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
06_gpu_and_ml/dreambooth/diffusers_lora_finetune.py
Copy

## 010_EXAMPLES_LLM-VOICE-CHAT
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
QuiLLMan: Voice Chat with Moshi
QuiLLMan
is a complete voice chat application built on Modal: you speak and the chatbot speaks back!
At the core is Kyutai Lab’s
Moshi
model, a speech-to-speech language model that will continuously listen, plan, and respond to the user.
Thanks to bidirectional websocket streaming and
Opus audio compression
, response times on good internet can be nearly instantaneous, closely matching the cadence of human speech.
You can find the demo live
here
Everything — from the React frontend to the model backend — is deployed serverlessly on Modal, allowing it to automatically scale and ensuring you only pay for the compute you use.
This page provides a high-level walkthrough of the
GitHub repo
Code overview
Traditionally, building a bidirectional streaming web application as compute-heavy as QuiLLMan would take a lot of work, and it’s especially difficult to make it robust and scale to handle many concurrent users.
But with Modal, it’s as simple as writing two different classes and running a CLI command.
Our project structure looks like this:
Moshi Websocket Server
: loads an instance of the Moshi model and maintains a bidirectional websocket connection with the client.
React Frontend
: runs client-side interaction logic.
Let’s go through each of these components in more detail.
FastAPI Server
Both frontend and backend are served via a
FastAPI Server
, which is a popular Python web framework for building REST APIs.
On Modal, a function or class method can be exposed as a web endpoint by decorating it with
@app.asgi_app()
and returning a FastAPI app. You’re then free to configure the FastAPI server however you like, including adding middleware, serving static files, and running websockets.
Moshi Websocket Server
Traditionally, a speech-to-speech chat app requires three distinct modules: speech-to-text, text-to-text, and text-to-speech. Passing data between these modules introduces bottlenecks, and can limit the speed of the app and forces a turn-by-turn conversation which can feel unnatural.
Kyutai Lab’s
Moshi
bundles all modalities into one model, which decreases latency and makes for a much simpler app.
Under the hood, Moshi uses the
Mimi
streaming encoder/decoder model to maintain an unbroken stream of audio in and out. The encoded audio is processed by a
speech-text foundation model
, which uses an internal monologue to determine when and how to respond.
Using a streaming model introduces a few challenges not normally seen in inference backends:
The model is
stateful
, meaning it maintains context of the conversation so far. This means a model instance cannot be shared between user conversations, so we must run a unique GPU per user session, which is normally not an easy feat!
The model is
streaming
, so the interface around it is not as simple as a POST request. We must find a way to stream audio data in and out, and do it fast enough for seamless playback.
We solve both of these in
src/moshi.py
, using a few Modal features.
To solve statefulness, we just spin up a new GPU per concurrent user.
That’s easy with Modal!
@app.cls
image
=image,
"A10G"
scaledown_window
class
Moshi
## ...
Copy
With this setting, if a new user connects, a new GPU instance is created! When any user disconnects, the state of their model is reset and that GPU instance is returned to the warm pool for re-use (for up to 300 seconds). Be aware that a GPU per user is not going to be cheap, but it’s the simplest way to ensure user sessions are isolated.
For streaming, we use FastAPI’s support for bidirectional websockets. This allows clients to establish a single connection at the start of their session, and stream audio data both ways.
Just as a FastAPI server can run from a Modal function, it can also be attached to a Modal class method, allowing us to couple a prewarmed Moshi model to a websocket session.
@modal.asgi_app
self
from
fastapi
import
FastAPI, Response, WebSocket, WebSocketDisconnect
web_app = FastAPI()
@web_app.websocket
"/ws"
async
websocket
: WebSocket):
with
torch.no_grad():
await
ws.accept()
## handle user session
## spawn loops for async IO
async
recv_loop
while
True
data =
await
ws.receive_bytes()
## send data into inference stream...
async
send_loop
while
True
await
asyncio.sleep(
0.001
msg =
self
.opus_stream_outbound.read_bytes()
## send inference output to user ...
Copy
To run a
development server
for the Moshi module, run this command from the root of the repo.
modal
serve
src.moshi
Copy
In the terminal output, you’ll find a URL for creating a websocket connection.
React Frontend
The frontend is a static React app, found in the
src/frontend
directory and served by
src/app.py
We use the
Web Audio API
to record audio from the user’s microphone and playback audio responses from the model.
For efficient audio transmission, we use the
Opus codec
to compress audio across the network. Opus recording and playback are supported by the
opus-recorder
ogg-opus-decoder
libraries.
To serve the frontend assets, run this command from the root of the repo.
modal
serve
src.app
Copy
Since
src/app.py
imports the
src/moshi.py
module, this
serve
command also serves the Moshi websocket server as its own endpoint.
Deploy
When you’re ready to go live, use the
deploy
command to deploy the app to Modal.
modal
deploy
src.app
Copy
Steal this example
The code for this entire example is
available on GitHub
, so feel free to fork it and make it your own!
QuiLLMan: Voice Chat with Moshi
Code overview
FastAPI Server
Moshi Websocket Server
React Frontend
Deploy
Steal this example

## 011_EXAMPLES_MUSICGEN
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Create your own music samples with MusicGen
MusicGen is a popular open-source music-generation model family from Meta.
In this example, we show you how you can run MusicGen models on Modal GPUs,
along with a Gradio UI for playing around with the model.
We use
Audiocraft
the inference library released by Meta
for MusicGen and its kin, like AudioGen.
Setting up dependencies
from
pathlib
import
Path
from
typing
import
Optional
from
uuid
import
uuid4
import
modal
Copy
We start by defining the environment our generation runs in.
This takes some explaining since, like most cutting-edge ML environments, it is a bit fiddly.
This environment is captured by a
container image
which we build step-by-step by calling methods to add dependencies,
like
apt_install
to add system packages and
pip_install
to add
Python packages.
Note that we don’t have to install anything with “CUDA”
in the name — the drivers come for free with the Modal environment
and the rest gets installed
. That makes our life a lot easier!
If you want to see the details, check out
this guide
in our docs.
image = (
modal.Image.debian_slim(
python_version
"3.11"
.apt_install(
"git"
"ffmpeg"
.pip_install(
"huggingface_hub[hf_transfer]==0.27.1"
## speed up model downloads
"torch==2.1.0"
## version pinned by audiocraft
"numpy<2"
## defensively cap the numpy version
"git+https://github.com/facebookresearch/audiocraft.git@v1.3.0"
## we can install directly from GitHub!
Copy
In addition to source code, we’ll also need the model weights.
Audiocraft integrates with the Hugging Face ecosystem, so setting up the models
is straightforward — the same
get_pretrained
method we use to load the weights for execution
will also download them if they aren’t present.
load_model
and_return
False
from
audiocraft.models
import
MusicGen
model_large = MusicGen.get_pretrained(
"facebook/musicgen-large"
and_return:
return
model_large
Copy
But Modal Functions are serverless: instances spin down when they aren’t being used.
If we want to avoid downloading the weights every time we start a new instance,
we need to store the weights somewhere besides our local filesystem.
So we add a Modal
Volume
to store the weights in the cloud.
cache_dir =
"/cache"
model_cache = modal.Volume.from_name(
"audiocraft-model-cache"
create_if_missing
True
Copy
We don’t need to change any of the model loading code —
we just need to make sure the model gets stored in the right directory.
To do that, we set an environment variable that Hugging Face expects
(and another one that speeds up downloads, for good measure)
and then run the
load_model
Python function.
image = image.env(
"HF_HUB_CACHE"
: cache_dir,
"HF_HUB_ENABLE_HF_TRANSER"
).run_function(load_model,
volumes
={cache_dir: model_cache})
Copy
While we’re at it, let’s also define the environment for our UI.
We’ll stick with Python and so use FastAPI and Gradio.
web_image = modal.Image.debian_slim(
python_version
"3.11"
).pip_install(
"fastapi[standard]==0.115.4"
"gradio==4.44.1"
Copy
This is a totally different environment from the one we run our model in.
Say goodbye to Python dependency conflict hell!
Running music generation on Modal
Now, we write our music generation logic.
This is bit complicated because we want to support generating long samples,
but the model has a maximum context length of thirty seconds.
We can get longer clips by feeding the model’s output back as input,
auto-regressively, but we have to write that ourselves.
There are also a few bits to make this work well with Modal:
We make an
to organize our deployment.
We load the model at start, instead of during inference, with
modal.enter
which requires that we use a Modal
In the
app.cls
decorator, we specify the Image we built and attach the Volume.
We also pick a GPU to run on — here, an NVIDIA L40S.
app = modal.App(
"example-musicgen"
MAX_SEGMENT_DURATION =
## maximum context window size
@app.cls
"l40s"
image
=image,
volumes
={cache_dir: model_cache})
class
MusicGen
@modal.enter
init
self
self
.model = load_model(
and_return
True
@modal.method
generate
self
prompt
duration
overlap
format
"wav"
## or mp3
) ->
bytes
"""Generate a music clip based on the prompt.
Clips longer than the MAX_SEGMENT_DURATION of
MAX_SEGMENT_DURATION
are generated by clipping all but `overlap` seconds and running inference again."""
context =
None
overlap =
(overlap, MAX_SEGMENT_DURATION -
remaining_duration = duration
remaining_duration <
return
bytes
while
remaining_duration >
## calculate duration of the next segment
segment_duration = remaining_duration
context
None
segment_duration += overlap
segment_duration =
(segment_duration, MAX_SEGMENT_DURATION)
## generate next segment
generated_duration = (
segment_duration
context
None
else
(segment_duration - overlap)
print
"🎼 generating
generated_duration
seconds of music"
self
.model.set_generation_params(
duration
=segment_duration)
next_segment =
self
._generate_next_segment(prompt, context, overlap)
## update remaining duration
remaining_duration -= generated_duration
## combine with previous segments
context =
self
._combine_segments(context, next_segment, overlap)
output = context.detach().cpu().float()[
return
to_audio_bytes(
output,
self
.model.sample_rate,
format
format
## for more on audio encoding parameters, see the docs for audiocraft
strategy
"loudness"
loudness_compressor
True
_generate_next_segment
self
prompt
context
overlap
"""Generate the next audio segment, either fresh or as continuation of a context."""
context
None
return
self
.model.generate(
descriptions
=[prompt])
else
overlap_samples = overlap *
self
.model.sample_rate
last_chunk = context[:, :, -overlap_samples:]
## B, C, T
return
self
.model.generate_continuation(
last_chunk,
self
.model.sample_rate,
descriptions
=[prompt]
_combine_segments
self
context
next_segment
overlap
"""Combine context with next segment, handling overlap."""
import
torch
context
None
return
next_segment
## Calculate where to trim the context (removing overlap)
overlap_samples = overlap *
self
.model.sample_rate
context_trimmed = context[:, :, :-overlap_samples]
## B, C, T
return
torch.cat([context_trimmed, next_segment],
Copy
We can then generate music from anywhere by running code like what we have in the
local_entrypoint
below.
@app.local_entrypoint
main
prompt
: Optional[
None
duration
overlap
format
"wav"
## or mp3
prompt
None
prompt =
"Amapiano polka, klezmers, log drum bassline, 112 BPM"
print
"🎼 generating
duration
seconds of music from prompt '
prompt[:
] + (
'...'
(prompt) >
else
audiocraft = MusicGen()
clip = audiocraft.generate.remote(prompt,
duration
=duration,
format
format
= Path(
"/tmp/audiocraft"
.mkdir(
exist_ok
True
parents
True
output_path =
slugify(prompt)[:
format
print
"🎼 Saving to
output_path
output_path.write_bytes(clip)
Copy
You can execute it with a command like:
modal
musicgen.py
--prompt=
"Baroque boy band, Bachstreet Boys, basso continuo, Top 40 pop music"
--duration=60
Copy
Hosting a web UI for the music generator
With the Gradio library, we can create a simple web UI in Python
that calls out to our music generator,
then host it on Modal for anyone to try out.
To deploy both the music generator and the UI, run
modal
deploy
musicgen.py
Copy
Share the URL with your friends and they can generate their own songs!
@app.function
image
=web_image,
## Gradio requires sticky sessions
## so we limit the number of concurrent containers to 1
## and allow it to scale to 1000 concurrent inputs
max_containers
@modal.concurrent
max_inputs
1000
@modal.asgi_app
import
gradio
from
fastapi
import
FastAPI
from
gradio.routes
import
mount_gradio_app
api = FastAPI()
## Since this Gradio app is running from its own container,
## we make a `.remote` call to the music generator
model = MusicGen()
generate = model.generate.remote
temp_dir = Path(
"/dev/shm"
async
generate_music
prompt
duration
format
"wav"
audio_bytes =
await
generate.aio(prompt,
duration
=duration,
format
format
audio_path = temp_dir /
uuid4()
format
audio_path.write_bytes(audio_bytes)
return
audio_path
with
gr.Blocks(
theme
"soft"
demo:
gr.Markdown(
"# MusicGen"
with
gr.Row():
with
gr.Column():
prompt = gr.Textbox(
label
"Prompt"
duration = gr.Number(
label
"Duration (seconds)"
value
minimum
maximum
format
= gr.Radio([
"wav"
"mp3"
label
"Format"
value
"wav"
btn = gr.Button(
"Generate"
with
gr.Column():
clip_output = gr.Audio(
label
"Generated Music"
autoplay
True
btn.click(
generate_music,
inputs
=[prompt, duration,
format
outputs
=[clip_output],
return
mount_gradio_app(
=api,
blocks
=demo,
path
Copy
Addenda
The remainder of the code here is not directly related to Modal
or to music generation, but is used in the example above.
to_audio_bytes
sample_rate
, **
kwargs
) ->
bytes
from
audiocraft.data.audio
import
audio_write
## audiocraft provides a nice utility for converting waveform tensors to audio,
## but it saves to a file path. here, we create a file path that is actually
## just backed by memory, instead of disk, to save on some latency
shm = Path(
"/dev/shm"
## /dev/shm is a memory-backed filesystem
stem_name = shm /
(uuid4())
output_path = audio_write(stem_name, wav, sample_rate, **kwargs)
return
output_path.read_bytes()
slugify
string
return
string.lower()
.replace(
.replace(
.replace(
.replace(
Copy
Create your own music samples with MusicGen
Setting up dependencies
Running music generation on Modal
Hosting a web UI for the music generator
Addenda
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
06_gpu_and_ml/text-to-audio/musicgen.py
Copy

## 012_GUIDE_GETTING-STARTED
Not Found
Sorry, this page doesn't exist! We weren't able to find the documentation
that you were looking for. Did you type the correct URL?
You can find the rest of the Modal documentation
here

## 013_EXAMPLES_CHAI1
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Fold proteins with Chai-1
In biology, function follows form quite literally:
the physical shapes of proteins dictate their behavior.
Measuring those shapes directly is difficult
and first-principles physical simulation prohibitively expensive.
And so predicting protein shape from content —
determining how the one-dimensional chain of amino acids encoded by DNA
folds
into a 3D object —
has emerged as a key application for machine learning and neural networks in biology.
In this example, we demonstrate how to run the open source
Chai-1
protein structure prediction model on Modal’s flexible serverless infrastructure.
For details on how the Chai-1 model works and what it can be used for,
see the authors’
technical report on bioRxiv
This simple script is meant as a starting point showing how to handle fiddly bits
like installing dependencies, loading weights, and formatting outputs so that you can get on with the fun stuff.
To experience the full power of Modal, try scaling inference up and running on hundreds or thousands of structures!
Setup
import
hashlib
import
json
from
pathlib
import
Path
from
typing
import
Optional
from
uuid
import
uuid4
import
modal
here = Path(
__file__
).parent
## the directory of this file
MINUTES =
## seconds
app = modal.App(
name
"example-chai1-inference"
Copy
Fold a protein from the command line
The logic for running Chai-1 is encapsulated in the function below,
which you can trigger from the command line by running
modal
chai1
Copy
This will set up the environment for running Chai-1 inference in Modal’s cloud,
run it, and then save the results remotely and locally. The results are returned in the
Crystallographic Information File
format,
which you can render with the online
Molstar Viewer
To see more options, run the command with the
--help
flag.
To learn how it works, read on!
@app.local_entrypoint
main
force_redownload
bool
False
fasta_file
: Optional[
None
inference_config_file
: Optional[
None
output_dir
: Optional[
None
run_id
: Optional[
None
print
"🧬 checking inference dependencies"
download_inference_dependencies.remote(
force
=force_redownload)
fasta_file
None
fasta_file = here /
"data"
"chai1_default_input.fasta"
print
"🧬 running Chai inference on
fasta_file
fasta_content = Path(fasta_file).read_text()
inference_config_file
None
inference_config_file = here /
"data"
"chai1_default_inference.json"
print
"🧬 loading Chai inference config from
inference_config_file
inference_config = json.loads(Path(inference_config_file).read_text())
run_id
None
run_id = hashlib.sha256(uuid4().bytes).hexdigest()[:
## short id
print
"🧬 running inference with
run_id
results = chai1_inference.remote(fasta_content, inference_config, run_id)
output_dir
None
output_dir = Path(
"/tmp/chai1"
output_dir.mkdir(
parents
True
exist_ok
True
print
"🧬 saving results to disk locally in
output_dir
ii, (scores, cif)
enumerate
(results):
(Path(output_dir) /
run_id
-scores.model_idx_
.npz"
).write_bytes(scores)
(Path(output_dir) /
run_id
-preds.model_idx_
.cif"
).write_text(cif)
Copy
Installing Chai-1 Python dependencies on Modal
Code running on Modal runs inside containers built from
container images
that include that code’s dependencies.
Because Modal images include
GPU drivers
by default,
installation of higher-level packages like
chai_lab
that require GPUs is painless.
Here, we do it with one line, using the
package manager for extra speed.
image = modal.Image.debian_slim(
python_version
"3.12"
).run_commands(
"uv pip install --system --compile-bytecode chai_lab==0.5.0 hf_transfer==0.1.8"
Copy
Storing Chai-1 model weights on Modal with Volumes
Not all “dependencies” belong in a container image. Chai-1, for example, depends on
the weights of several models.
Rather than loading them dynamically at run-time (which would add several minutes of GPU time to each inference),
or installing them into the image (which would require they be re-downloaded any time the other dependencies changed),
we load them onto a
Modal Volume
A Modal Volume is a file system that all of your code running on Modal (or elsewhere!) can access.
For more on storing model weights on Modal, see
this guide
chai_model_volume = (
modal.Volume.from_name(
## create distributed filesystem for model weights
"chai1-models"
create_if_missing
True
models_dir = Path(
"/models/chai1"
Copy
The details of how we handle the download here (e.g. running concurrently for extra speed)
are in the
Addenda
image = image.env(
## update the environment variables in the image to...
"CHAI_DOWNLOADS_DIR"
(models_dir),
## point the chai code to it
"HF_HUB_ENABLE_HF_TRANSFER"
## speed up downloads
Copy
Storing Chai-1 outputs on Modal Volumes
Chai-1 produces its outputs by writing to disk —
the model’s scores for the structure and the structure itself along with rich metadata.
But Modal is a
serverless
platform, and the filesystem your Modal Functions write to
is not persistent. Any file can be converted into bytes and sent back from a Modal Function
— and we mean any! You can send files that are gigabytes in size that way.
So we do that below.
But for larger jobs, like folding every protein in the PDB, storing bytes on a local client
like a laptop won’t cut it.
So we again lean on Modal Volumes, which can store thousands of files each.
We attach a Volume to a Modal Function that runs Chai-1 and the inference code
saves the results to distributed storage, without any fuss or source code changes.
chai_preds_volume = modal.Volume.from_name(
"chai1-preds"
create_if_missing
True
preds_dir = Path(
"/preds"
Copy
Running Chai-1 on Modal
Now we’re ready to define a Modal Function that runs Chai-1.
We put our function on Modal by wrapping it in a decorator,
@app.function
We provide that decorator with some arguments that describe the infrastructure our code needs to run:
the Volumes we created, the Image we defined, and of course a fast GPU!
Note that Chai-1 takes a file path as input —
specifically, a path to a file in the
FASTA format
We pass the file contents to the function as a string and save them to disk so they can be picked up by the inference code.
Because Modal is serverless, we don’t need to worry about cleaning up these resources:
the disk is ephemeral and the GPU only costs you money when you’re using it.
@app.function
timeout
* MINUTES,
"H100"
volumes
={models_dir: chai_model_volume, preds_dir: chai_preds_volume},
image
=image,
chai1_inference
fasta_content
inference_config
dict
run_id
) -> list[(
bytes
from
pathlib
import
Path
import
torch
from
chai_lab
import
chai1
N_DIFFUSION_SAMPLES =
## hard-coded in chai-1
fasta_file = Path(
"/tmp/inputs.fasta"
fasta_file.write_text(fasta_content.strip())
output_dir = Path(
"/preds"
) / run_id
chai1.run_inference(
fasta_file
=fasta_file,
output_dir
=output_dir,
device
=torch.device(
"cuda"
**inference_config,
print
"🧬 done, results written to /
output_dir.relative_to(
'/preds'
on remote volume"
results = []
range
(N_DIFFUSION_SAMPLES):
scores = (output_dir /
"scores.model_idx_
.npz"
).read_bytes()
cif = (output_dir /
"pred.model_idx_
.cif"
).read_text()
results.append((scores, cif))
return
results
Copy
Addenda
Above, we glossed over just how we got hold of the model weights —
local_entrypoint
just called a function named
download_inference_dependencies
Here’s that function’s implementation.
A few highlights:
This Modal Function can access the model weights Volume, like the inference Function,
but it can’t access the model predictions Volume.
This Modal Function has a different Image (the default!) and doesn’t use a GPU. Modal helps you
separate the concerns, and the costs, of your infrastructure’s components.
We use the
async
keyword here so that we can run the download for each model file
as a separate task, concurrently. We don’t need to worry about this use of
async
spreading to the rest of our code — Modal launches just this Function in an async runtime.
@app.function
volumes
={models_dir: chai_model_volume})
async
download_inference_dependencies
force
False
import
asyncio
import
aiohttp
base_url =
"https://chaiassets.com/chai1-inference-depencencies/"
## sic
inference_dependencies = [
"conformers_v1.apkl"
"models_v2/trunk.pt"
"models_v2/token_embedder.pt"
"models_v2/feature_embedding.pt"
"models_v2/diffusion_module.pt"
"models_v2/confidence_head.pt"
headers = {
"User-Agent"
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
## launch downloads concurrently
async
with
aiohttp.ClientSession(
headers
=headers)
session:
tasks = []
inference_dependencies:
local_path = models_dir / dep
force
local_path.exists():
url = base_url + dep
print
"🧬 downloading
tasks.append(download_file(session, url, local_path))
## run all of the downloads and await their completion
await
asyncio.gather(*tasks)
chai_model_volume.commit()
## ensures models are visible on remote filesystem before exiting, otherwise takes a few seconds, racing with inference
async
download_file
session
local_path
: Path):
async
with
session.get(url)
response:
response.raise_for_status()
local_path.parent.mkdir(
parents
True
exist_ok
True
with
open
(local_path,
"wb"
while
chunk :=
await
response.content.read(
8192
f.write(chunk)
Copy
Fold proteins with Chai-1
Setup
Fold a protein from the command line
Installing Chai-1 Python dependencies on Modal
Storing Chai-1 model weights on Modal with Volumes
Storing Chai-1 outputs on Modal Volumes
Running Chai-1 on Modal
Addenda
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
06_gpu_and_ml/protein-folding/chai1.py
Copy

## 014_EXAMPLES_LLAMA_CPP
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Run large and small language models with llama.cpp (DeepSeek-R1, Phi-4)
This example demonstrate how to run small (Phi-4) and large (DeepSeek-R1)
language models on Modal with
llama.cpp
By default, this example uses DeepSeek-R1 to produce a “Flappy Bird” game in Python —
see the video below. The code used in the video is
here
along with the model’s raw outputs.
Note that getting the game to run required a small bugfix from a human —
our jobs are still safe, for now.
from
pathlib
import
Path
from
typing
import
Optional
import
modal
Copy
What GPU can run DeepSeek-R1? What GPU can run Phi-4?
Our large model is a real whale:
DeepSeek-R1
which has 671B total parameters and so consumes over 100GB of storage,
even when
quantized down to one ternary digit (1.58 bits)
per parameter.
To make sure we have enough room for it and its activations/KV cache,
we select four L40S GPUs, which together have 192 GB of memory.
Phi-4
on the other hand, is a svelte 14B total parameters,
or roughly 5 GB when quantized down to
two bits per parameter
That’s small enough that it can be comfortably run on a CPU,
especially for a single-user setup like the one we’ll build here.
GPU_CONFIG =
"L40S:4"
## for DeepSeek-R1, literal `None` for phi-4
Copy
Calling a Modal Function from the command line
To start, we define our
main
function —
the Python function that we’ll run locally to
trigger our inference to run on Modal’s cloud infrastructure.
This function, like the others that form our inference service
running on Modal, is part of a Modal
Specifically, it is a
local_entrypoint
Any Python code can call Modal Functions remotely,
but local entrypoints get a command-line interface for free.
app = modal.App(
"example-llama-cpp"
@app.local_entrypoint
main
prompt
: Optional[
None
model
"DeepSeek-R1"
## or "phi-4"
n_predict
## max number of tokens to predict, -1 is infinite
args
: Optional[
None
## string of arguments to pass to llama.cpp's cli
"""Run llama.cpp inference on Modal for phi-4 or deepseek r1."""
import
shlex
org_name =
"unsloth"
## two sample models: the diminuitive phi-4 and the chonky deepseek r1
model.lower() ==
"phi-4"
model_name =
"phi-4-GGUF"
quant =
"Q2_K"
model_entrypoint_file =
"phi-4-
quant
.gguf"
model_pattern =
quant
revision =
None
parsed_args = DEFAULT_PHI_ARGS
args
None
else
shlex.split(args)
elif
model.lower() ==
"deepseek-r1"
model_name =
"DeepSeek-R1-GGUF"
quant =
"UD-IQ1_S"
model_entrypoint_file = (
model
quant
/DeepSeek-R1-
quant
-00001-of-00003.gguf"
model_pattern =
quant
revision =
"02656f62d2aa9da4d3f0cdb34c341d30dd87c3b6"
parsed_args = DEFAULT_DEEPSEEK_R1_ARGS
args
None
else
shlex.split(args)
else
raise
ValueError
"Unknown model
model
repo_id =
org_name
model_name
download_model.remote(repo_id, [model_pattern], revision)
## call out to a `.remote` Function on Modal for inference
result = llama_cpp_inference.remote(
model_entrypoint_file,
prompt,
n_predict,
parsed_args,
store_output
=model.lower() ==
"deepseek-r1"
output_path = Path(
"/tmp"
"llama-cpp-
model
.txt"
output_path.parent.mkdir(
parents
True
exist_ok
True
print
"🦙 writing response to
output_path
output_path.write_text(result)
Copy
You can trigger inference from the command line with
modal
llama_cpp.py
Copy
To try out Phi-4 instead, use the
--model
argument:
modal
llama_cpp.py
--model=
"phi-4"
Copy
Note that this will run for up to 30 minutes, which costs ~$5.
To allow it to proceed even if your local terminal fails,
add the
--detach
flag after
modal run
See below for details on getting the outputs.
You can pass prompts with the
--prompt
argument and set the maximum number of tokens
with the
--n-predict
argument.
Additional arguments for
llama-cli
are passed as a string like
--args="--foo 1 --bar"
For convenience, we set a number of sensible defaults for DeepSeek-R1,
following the suggestions by the team at unsloth,
quantized the model to 1.58 bit
DEFAULT_DEEPSEEK_R1_ARGS = [
## good default llama.cpp cli args for deepseek-r1
"--cache-type-k"
"q4_0"
"--threads"
"12"
"-no-cnv"
"--prio"
"--temp"
"0.6"
"--ctx-size"
"8192"
DEFAULT_PHI_ARGS = [
## good default llama.cpp cli args for phi-4
"--threads"
"16"
"-no-cnv"
"--ctx-size"
"16384"
Copy
Compiling llama.cpp with CUDA support
In order to run inference, we need the model’s weights
and we need code to run inference with those weights.
llama.cpp
is a no-frills C++ library for running large language models.
It supports highly-quantized versions of models ideal for running
single-user language modeling services on CPU or GPU.
We compile it, with CUDA support, and add it to a Modal
container image
using the code below.
For more details on using CUDA on Modal, including why
we need to use the
nvidia/cuda
registry image in this case
(hint: it’s for the
nvcc
compiler
see the
Modal guide to using CUDA
LLAMA_CPP_RELEASE =
"b4568"
MINUTES =
cuda_version =
"12.4.0"
## should be no greater than host CUDA version
flavor =
"devel"
## includes full CUDA toolkit
operating_sys =
"ubuntu22.04"
tag =
cuda_version
flavor
operating_sys
image = (
modal.Image.from_registry(
"nvidia/cuda:
add_python
"3.12"
.apt_install(
"git"
"build-essential"
"cmake"
"curl"
"libcurl4-openssl-dev"
.run_commands(
"git clone https://github.com/ggerganov/llama.cpp"
.run_commands(
"cmake llama.cpp -B llama.cpp/build "
"-DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON "
.run_commands(
## this one takes a few minutes!
"cmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli"
.run_commands(
"cp llama.cpp/build/bin/llama-* llama.cpp"
.entrypoint([])
## remove NVIDIA base container entrypoint
Copy
Storing models on Modal
To make the model weights available on Modal,
we download them from Hugging Face.
Modal is serverless, so disks are by default ephemeral.
To make sure our weights don’t disappear between runs,
which would trigger a long download, we store them in a
Modal
Volume
For more on how to use Modal Volumes to store model weights,
this guide
model_cache = modal.Volume.from_name(
"llamacpp-cache"
create_if_missing
True
cache_dir =
"/root/.cache/llama.cpp"
download_image = (
modal.Image.debian_slim(
python_version
"3.11"
.pip_install(
"huggingface_hub[hf_transfer]==0.26.2"
.env({
"HF_HUB_ENABLE_HF_TRANSFER"
@app.function
image
=download_image,
volumes
={cache_dir: model_cache},
timeout
* MINUTES
download_model
repo_id
allow_patterns
revision
: Optional[
None
from
huggingface_hub
import
snapshot_download
print
"🦙 downloading model from
repo_id
if not present"
snapshot_download(
repo_id
=repo_id,
revision
=revision,
local_dir
=cache_dir,
allow_patterns
=allow_patterns,
model_cache.commit()
## ensure other Modal Functions can see our writes before we quit
print
"🦙 model loaded"
Copy
Storing model outputs on Modal
Contemporary large reasoning models are slow —
for the sample “flappy bird” prompt we provide,
results are sometimes produced only after several (or even tens of) minutes.
That makes their outputs worth storing.
In addition to sending them back to clients,
like our local command line,
we’ll store the results on a Modal Volume for safe-keeping.
results = modal.Volume.from_name(
"llamacpp-results"
create_if_missing
True
results_dir =
"/root/results"
Copy
You can retrieve the results later in a number of ways.
You can use the Volume CLI:
modal
volume
llamacpp-results
Copy
You can attach the Volume to a Modal
shell
to poke around in a familiar terminal environment:
modal
shell
--volume
llamacpp-results
## then cd into /mnt
Copy
Or you can access it from any other Python environment
by using the same
modal.Volume
call as above to instantiate it:
results = modal.Volume.from_name(
"llamacpp-results"
print
(results))
## show methods
Copy
Running llama.cpp as a Modal Function
Now, let’s put it all together.
At the top of our
llama_cpp_inference
function,
we add an
app.function
decorator to attach all of our infrastructure:
image
with the dependencies
volumes
with the weights and where we can put outputs
we want, if any
We also specify a
timeout
after which to cancel the run.
Inside the function, we call the
llama.cpp
with
subprocess.Popen
. This requires a bit of extra ceremony
because we want to both show the output as we run
and store the output to save and return to the local caller.
For details, see the
Addenda section
below.
Alternatively, you might set up an OpenAI-compatible server
using base
llama.cpp
or its
Python wrapper library
along with one of
Modal’s decorators for web hosting
@app.function
image
=image,
volumes
={cache_dir: model_cache, results_dir: results},
=GPU_CONFIG,
timeout
* MINUTES,
llama_cpp_inference
model_entrypoint_file
prompt
: Optional[
None
n_predict
args
: Optional[list[
]] =
None
store_output
bool
True
import
subprocess
from
uuid
import
uuid4
prompt
None
prompt = DEFAULT_PROMPT
## see end of file
"deepseek"
model_entrypoint_file.lower():
prompt =
"<｜User｜>"
+ prompt +
"<think>"
args
None
args = []
## set layers to "off-load to", aka run on, GPU
GPU_CONFIG
None
n_gpu_layers =
9999
## all
else
n_gpu_layers =
store_output:
result_id =
(uuid4())
print
"🦙 running inference with id:
result_id
command = [
"/llama.cpp/llama-cli"
"--model"
cache_dir
model_entrypoint_file
"--n-gpu-layers"
(n_gpu_layers),
"--prompt"
prompt,
"--n-predict"
(n_predict),
] + args
print
"🦙 running commmand:"
, command,
\n\t
p = subprocess.Popen(
command,
stdout
=subprocess.PIPE,
stderr
=subprocess.PIPE,
text
False
stdout, stderr = collect_output(p)
p.returncode !=
raise
subprocess.CalledProcessError(p.returncode, command, stdout, stderr)
store_output:
## save results to a Modal Volume if requested
print
"🦙 saving results for
result_id
result_dir = Path(results_dir) / result_id
result_dir.mkdir(
parents
True
(result_dir /
"out.txt"
).write_text(stdout)
(result_dir /
"err.txt"
).write_text(stderr)
return
stdout
Copy
Addenda
The remainder of this code is less interesting from the perspective
of running LLM inference on Modal but necessary for the code to run.
For example, it includes the default “Flappy Bird in Python” prompt included in
unsloth’s announcement
of their 1.58 bit quantization of DeepSeek-R1.
DEFAULT_PROMPT =
"""Create a Flappy Bird game in Python. You must include these things:
You must use pygame.
The background color should be randomly chosen and is a light shade. Start with a light blue color.
Pressing SPACE multiple times will accelerate the bird.
The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.
Place on the bottom some land colored as dark brown or yellow chosen randomly.
Make a score shown on the top right side. Increment if you pass pipes and don't hit them.
Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.
When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.
The final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section."""
stream_output
stream
queue
write_stream
"""Reads lines from a stream and writes to a queue and a write stream."""
line
iter
(stream.readline,
line = line.decode(
"utf-8"
errors
"replace"
write_stream.write(line)
write_stream.flush()
queue.put(line)
stream.close()
collect_output
process
"""Collect up the stdout and stderr of a process while still streaming it out."""
import
from
queue
import
Queue
from
threading
import
Thread
stdout_queue = Queue()
stderr_queue = Queue()
stdout_thread = Thread(
target
=stream_output,
args
=(process.stdout, stdout_queue, sys.stdout)
stderr_thread = Thread(
target
=stream_output,
args
=(process.stderr, stderr_queue, sys.stderr)
stdout_thread.start()
stderr_thread.start()
stdout_thread.join()
stderr_thread.join()
process.wait()
stdout_collected =
.join(stdout_queue.queue)
stderr_collected =
.join(stderr_queue.queue)
return
stdout_collected, stderr_collected
Copy
Run large and small language models with llama.cpp (DeepSeek-R1, Phi-4)
What GPU can run DeepSeek-R1? What GPU can run Phi-4?
Calling a Modal Function from the command line
Compiling llama.cpp with CUDA support
Storing models on Modal
Storing model outputs on Modal
Running llama.cpp as a Modal Function
Addenda
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
06_gpu_and_ml/llm-serving/llama_cpp.py
--n-predict
1024
Copy

## 015_EXAMPLES_ESM3
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Build a protein folding dashboard with ESM3, Molstar, and Gradio
There are perhaps a quadrillion distinct proteins on the planet Earth,
each one a marvel of nanotechnology discovered by painstaking evolution.
We know the amino acid sequence of nearly a billion but we only
know the three-dimensional structure of a few hundred thousand,
gathered by slow, difficult observational methods like X-ray crystallography.
Built upon this data are machine learning models like
EvolutionaryScale’s
ESM3
that can predict the structure of any sequence in seconds.
In this example, we’ll show how you can use Modal to not
just run the latest protein-folding model but also build tools around it for
you and your team of scientists to understand and analyze the results.
Basic Setup
import
base64
import
from
pathlib
import
Path
from
typing
import
Optional
import
modal
MINUTES =
## seconds
app = modal.App(
"example-esm3-dashboard"
Copy
Create a Volume to store ESM3 model weights and Entrez sequence data
To minimize cold start times, we’ll store the ESM3 model weights on a Modal
Volume
For patterns and best practices for storing model weights on Modal, see
this guide
We’ll use this same distributed storage primitive to store sequence data.
volume = modal.Volume.from_name(
"example-esm3-dashboard"
create_if_missing
True
VOLUME_PATH = Path(
"/vol"
MODELS_PATH = VOLUME_PATH /
"models"
DATA_PATH = VOLUME_PATH /
"data"
Copy
Define dependencies in container images
The container image for structure inference is based on Modal’s default slim Debian
Linux image with
for loading and running the model,
gemmi
managing protein structure file conversions, and
hf_transfer
for faster downloading of the model weights from Hugging Face.
esm3_image = (
modal.Image.debian_slim(
python_version
"3.11"
.pip_install(
"esm==3.1.1"
"torch==2.4.1"
"gemmi==0.7.0"
"huggingface_hub[hf_transfer]==0.26.2"
.env({
"HF_HUB_ENABLE_HF_TRANSFER"
"HF_HOME"
(MODELS_PATH)})
Copy
We’ll also define a separate image, with different dependencies,
for the part of our app that hosts the dashboard.
This helps reduce the complexity of Python dependency management
by “walling off” the different parts, e.g. separating
functions that depend on finicky ML packages
from those that depend on pedantic web packages.
Dependencies include
gradio
for building a web UI in Python and
biotite
for extracting sequences from UniProt accession numbers.
You can read more about how to configure container images on Modal in
this guide
web_app_image = (
modal.Image.debian_slim(
python_version
"3.11"
.pip_install(
"gradio~=4.44.0"
"biotite==0.41.2"
"fastapi[standard]==0.115.4"
.add_local_dir(Path(
__file__
).parent /
"frontend"
remote_path
"/assets"
Copy
Here we “pre-import” libraries that will be used by the functions we run
on Modal in a given image using the
with image.imports
context manager.
with
esm3_image.imports():
import
tempfile
import
gemmi
import
torch
from
esm.models.esm3
import
ESM3
from
esm.sdk.api
import
ESMProtein, GenerationConfig
with
web_app_image.imports():
import
biotite.database.entrez
entrez
import
biotite.sequence.io.fasta
fasta
from
fastapi
import
FastAPI
Copy
Define a
Model
inference class for ESM3
Next, we map the model’s setup and inference code onto Modal.
For setup code that only needs to run once, we put it in a method
decorated with
@enter
, which runs on container start. For details,
this guide
The rest of the inference code goes in a method decorated with
@method
We accelerate the compute-intensive inference with a GPU, specifically an A10G.
For more on using GPUs on Modal, see
this guide
@app.cls
image
=esm3_image,
volumes
={VOLUME_PATH: volume},
secrets
=[modal.Secret.from_name(
"huggingface-secret"
"A10G"
timeout
* MINUTES,
class
Model
@modal.enter
enter
self
self
.model = ESM3.from_pretrained(
"esm3_sm_open_v1"
self
.model.to(
"cuda"
print
"using half precision and tensor cores for fast ESM3 inference"
self
.model =
self
.model.half()
torch.backends.cuda.matmul.allow_tf32 =
True
self
.max_steps =
print
"setting max ESM steps to:
{self
.max_steps
convert_protein_to_MMCIF
self
esm_protein
output_path
structure = gemmi.read_pdb_string(esm_protein.to_pdb_string())
doc = structure.make_mmcif_document()
doc.write_file(
(output_path), gemmi.cif.WriteOptions())
get_generation_config
self
num_steps
return
GenerationConfig(
track
"structure"
num_steps
=num_steps)
@modal.method
inference
self
sequence
num_steps =
(sequence),
self
.max_steps)
print
"running ESM3 inference with num_steps=
num_steps
esm_protein =
self
.model.generate(
ESMProtein(
sequence
=sequence),
self
.get_generation_config(num_steps)
print
"checking for errors in output"
hasattr
(esm_protein,
"error_msg"
raise
ValueError
(esm_protein.error_msg)
print
"converting ESMProtein into MMCIF file"
save_path = Path(tempfile.mktemp() +
".mmcif"
self
.convert_protein_to_MMCIF(esm_protein, save_path)
print
"returning MMCIF bytes"
return
io.BytesIO(save_path.read_bytes())
Copy
Serve a dashboard as an
asgi_app
In this section we’ll create a web interface around the ESM3 model
that can help scientists and stakeholders understand and interrogate the results of the model.
You can deploy this UI, along with the backing inference endpoint,
with the following command:
modal
deploy
esm3.py
Copy
Integrating Modal Functions
The integration between our dashboard and our inference backend
is made simple by the Modal SDK:
because the definition of the
Model
class is available in the same Python
context as the defintion of the web UI,
we can instantiate an instance and call its methods with
.remote
The inference runs in a GPU-accelerated container with all of ESM3’s
dependencies, while this code executes in a CPU-only container
with only our web dependencies.
run_esm
sequence
) ->
sequence = sequence.strip()
print
"running ESM"
mmcif_buffer = Model().inference.remote(sequence)
print
"converting mmCIF bytes to base64 for compatibility with HTML"
mmcif_content = mmcif_buffer.read().decode()
mmcif_base64 = base64.b64encode(mmcif_content.encode()).decode()
return
get_molstar_html(mmcif_base64)
Copy
Building a UI in Python with Gradio
We’ll visualize the results using
Mol*
Mol* (pronounced “molstar”) is an open-source toolkit for
visualizing and analyzing large-scale molecular data, including secondary structures
and residue-specific positions of proteins.
Second, we’ll create links to lookup the metadata and structure of known
proteins using the
Universal Protein Resource
database from the UniProt consortium which is supported by the European
Bioinformatics Institute, the National Human Genome Research
Institute, and the Swiss Institute of Bioinformatics. UniProt
is also a hub that links to many other databases, like the RCSB Protein
Data Bank.
To pull sequence data, we’ll use the
Biotite
library to pull
FASTA
files from
UniProt which contain labelled sequences.
You should see the URL for this UI in the output of
modal deploy
or on your
Modal app dashboard
for this app.
@app.function
image
=web_app_image,
volumes
={VOLUME_PATH: volume},
max_containers
## Gradio requires sticky sessions
@modal.concurrent
max_inputs
1000
## Gradio can handle many async inputs
@modal.asgi_app
import
gradio
from
fastapi.responses
import
FileResponse
from
gradio.routes
import
mount_gradio_app
web_app = FastAPI()
## custom styles: an icon, a background, and some CSS
@web_app.get
"/favicon.ico"
include_in_schema
False
async
favicon
return
FileResponse(
"/assets/favicon.svg"
@web_app.get
"/assets/background.svg"
include_in_schema
False
async
background
return
FileResponse(
"/assets/background.svg"
css = Path(
"/assets/index.css"
).read_text()
theme = gr.themes.Default(
primary_hue
"green"
secondary_hue
"emerald"
neutral_hue
"neutral"
title =
"Predict & Visualize Protein Structures"
with
gr.Blocks(
theme
=theme,
=css,
title
=title,
=always_dark())
interface:
gr.Markdown(
title
with
gr.Row():
with
gr.Column():
gr.Markdown(
"## Enter UniProt ID "
uniprot_num_box = gr.Textbox(
label
"Enter UniProt ID or select one on the right"
placeholder
"e.g. P02768, P69905, etc."
get_sequence_button = gr.Button(
"Retrieve Sequence from UniProt ID"
variant
"primary"
uniprot_link_button = gr.Button(
value
"View protein on UniProt website"
uniprot_link_button.click(
None
inputs
=uniprot_num_box,
=get_js_for_uniprot_link(),
with
gr.Column():
example_uniprots = get_uniprot_examples()
extract_uniprot_num
example_idx
uniprot = example_uniprots[example_idx]
return
uniprot[uniprot.index(
: uniprot.index(
gr.Markdown(
"## Example UniProt Accession Numbers"
with
gr.Row():
half_len =
(example_uniprots) /
with
gr.Column():
i, uniprot
enumerate
(example_uniprots[:half_len]):
btn = gr.Button(uniprot,
variant
"secondary"
btn.click(
lambda
=i: extract_uniprot_num(j),
outputs
=uniprot_num_box,
with
gr.Column():
i, uniprot
enumerate
(example_uniprots[half_len:]):
btn = gr.Button(uniprot,
variant
"secondary"
btn.click(
lambda
=i + half_len: extract_uniprot_num(j),
outputs
=uniprot_num_box,
gr.Markdown(
"## Enter Sequence"
sequence_box = gr.Textbox(
label
"Enter a sequence or retrieve it from a UniProt ID"
placeholder
"e.g. MVTRLE..., PVTTIMHALL..., etc."
get_sequence_button.click(
=get_sequence,
inputs
=[uniprot_num_box],
outputs
=[sequence_box]
run_esm_button = gr.Button(
"Run ESM3 Folding"
variant
"primary"
gr.Markdown(
"## ESM3 Predicted Structure"
molstar_html = gr.HTML()
run_esm_button.click(
=run_esm,
inputs
=sequence_box,
outputs
=molstar_html)
## return a FastAPI app for Modal to serve
return
mount_gradio_app(
=web_app,
blocks
=interface,
path
Copy
Folding from the command line
If you want to quickly run the ESM3 model without the web interface, you can
run it from the command line like this:
modal
esm3
Copy
This will run the same inference code above on Modal. The results are
returned in the
Crystallographic Information File
format, which you can render with the online
Molstar Viewer
@app.local_entrypoint
main
sequence
: Optional[
None
output_dir
: Optional[
None
sequence
None
print
"using sequence for insulin [P01308]"
sequence =
"MRTPMLLALLALATLCLAGRADAKPGDAESGKGAAFVSKQEGSEVVKRLRRYLDHWLGAPAPYPDPLEPKREVCELNPDCDELADHIGFQEAYRRFYGPV"
output_dir
None
output_dir = Path(
"/tmp/esm3"
output_dir.mkdir(
parents
True
exist_ok
True
output_path = output_dir /
"output.mmcif"
print
"starting inference on Modal"
results_buffer = Model().inference.remote(sequence)
print
"writing results to
output_path
output_path.write_bytes(results_buffer.read())
Copy
Addenda
The remainder of this code is boilerplate.
Extracting Sequences from UniProt Accession Numbers
To retrieve sequence information we’ll utilize the
biotite
library which
will allow us to fetch
fasta
sequence files from the
National Center for Biotechnology Information (NCBI) Entrez database
get_sequence
uniprot_num
) ->
DATA_PATH.mkdir(
parents
True
exist_ok
True
uniprot_num = uniprot_num.strip()
fasta_path = DATA_PATH /
uniprot_num
.fasta"
print
"Fetching
fasta_path
from the entrez database"
entrez.fetch_single_file(
uniprot_num, fasta_path,
db_name
"protein"
ret_type
"fasta"
fasta_file = fasta.FastaFile.read(fasta_path)
protein_sequence = fasta.get_sequence(fasta_file)
return
(protein_sequence)
except
Exception
return
"Error:
Copy
Supporting functions for the Gradio app
The following Python code is used to enhance the Gradio app,
mostly by generating some extra HTML & JS and handling styling.
get_js_for_uniprot_link
url =
"https://www.uniprot.org/uniprotkb/"
end =
"/entry#structure"
return
"""(uni_id) =>
if (!uni_id) return; window.open("
" + uni_id + "
get_molstar_html
mmcif_base64
return
<iframe
id="molstar_frame"
style="width: 100%; height: 600px; border: none;"
srcdoc='
<!DOCTYPE html>
<html>
<head>
<script src="https://cdn.jsdelivr.net/npm/@rcsb/rcsb-molstar/build/dist/viewer/rcsb-molstar.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@rcsb/rcsb-molstar/build/dist/viewer/rcsb-molstar.css">
</head>
<body>
<div id="protein-viewer" style="width: 1200px; height: 400px; position: center"></div>
<script>
console.log("Initializing viewer...");
(async function()
// Create plugin instance
const viewer = new rcsbMolstar.Viewer("protein-viewer");
// CIF data in base64
const mmcifData = "
mmcif_base64
// Convert base64 to blob
const blob = new Blob(
[atob(mmcifData)],
type: "text/plain"
// Create object URL
const url = URL.createObjectURL(blob);
// Load structure
await viewer.loadStructureFromUrl(url, "mmcif");
catch (error)
console.error("Error loading structure:", error);
)();
</script>
</body>
</html>
</iframe>"""
get_uniprot_examples
return
"Albumin [P02768]"
"Insulin [P01308]"
"Hemoglobin [P69905]"
"Lysozyme [P61626]"
"BRCA1 [P38398]"
"Immunoglobulin [P01857]"
"Actin [P60709]"
"Ribonuclease [P07998]"
always_dark
return
function refresh() {
const url = new URL(window.location);
if (url.searchParams.get('__theme') !== 'dark') {
url.searchParams.set('__theme', 'dark');
window.location.href = url.href;
Copy
Build a protein folding dashboard with ESM3, Molstar, and Gradio
Basic Setup
Create a Volume to store ESM3 model weights and Entrez sequence data
Define dependencies in container images
Define a Model inference class for ESM3
Serve a dashboard as an asgi_app
Integrating Modal Functions
Building a UI in Python with Gradio
Folding from the command line
Addenda
Extracting Sequences from UniProt Accession Numbers
Supporting functions for the Gradio app
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
06_gpu_and_ml/protein-folding/esm3.py
Copy

## 016_REFERENCE_CHANGELOG
Changelog
API Reference
modal.App
modal.Client
modal.CloudBucketMount
modal.Cls
modal.Cron
modal.Dict
modal.Error
modal.FilePatternMatcher
modal.Function
modal.FunctionCall
modal.Image
modal.NetworkFileSystem
modal.Period
modal.Proxy
modal.Queue
modal.Retries
modal.Sandbox
modal.SandboxSnapshot
modal.Secret
modal.Tunnel
modal.Volume
modal.asgi_app
modal.batched
modal.call_graph
modal.concurrent
modal.container_process
modal.current_function_call_id
modal.current_input_id
modal.enable_output
modal.enter
modal.exit
modal.fastapi_endpoint
modal.file_io
modal.forward
modal.gpu
modal.interact
modal.io_streams
modal.is_local
modal.method
modal.parameter
modal.web_endpoint
modal.web_server
modal.wsgi_app
modal.exception
modal.config
CLI Reference
modal app
modal config
modal container
modal deploy
modal dict
modal environment
modal launch
modal nfs
modal profile
modal queue
modal run
modal secret
modal serve
modal setup
modal shell
modal token
modal volume
Changelog
This changelog documents user-facing updates (features, enhancements, fixes, and deprecations) to the
modal
client library.
Latest
1.0.4 (2025-06-13)
When
modal.Cls.with_options
is called multiple times on the same instance, the overrides will now be merged. For example, the following configuration will use an H100 GPU and request 16 CPU cores:
Model.with_options(
"A100"
).with_options(
"H100"
Copy
Added a
--secret
option to
modal shell
for including environment variables defined by named Secret(s) in the shell session:
modal shell --secret huggingface --secret wandb
Copy
Added a
verbose: bool
option to
modal.Sandbox.create()
. When this is set to
True
, execs and file system operations will appear in the Sandbox logs.
Updated
modal.Sandbox.watch()
so that exceptions are now raised in (and can be caught by) the calling task.
1.0.3 (2025-06-05)
Added support for specifying a timezone on
Cron
schedules, which allows you to run a Function at a specific local time regardless of daylight savings:
import
modal
app = modal.App()
@app.function
schedule
=modal.Cron(
"* 6 * * *"
timezone
"America/New_York"
## Use tz database naming conventions
print
"This function will run every day at 6am New York time."
Copy
Added an
h2_ports
parameter to
Sandbox.create
, which exposes encrypted ports using HTTP/2. The following example will create an H2 port on 5002 and a port using HTTPS over HTTP/1.1 on 5003:
sb = modal.Sandbox.create(
=app,
h2_ports
5002
encrypted_ports
5003
Copy
Added
--from-dotenv
--from-json
options to
modal secret create
, which will read from local files to populate Secret contents.
Sandbox.terminate
no longer waits for container shutdown to complete before returning. It still ensures that a terminated container will shutdown imminently. To restore the previous behavior (i.e., to wait until the Sandbox is actually terminated), call
sb.wait(raise_on_termination=False)
after calling
sb.terminate()
Improved performance and stability for
modal volume get
Fixed a rare race condition that could sometimes make
Function.map
and similar calls deadlock.
Fixed an issue where
Function.map()
and similar methods would stall for 55 seconds when passed an empty iterator as input instead of completing immediately.
We now raise an error during App setup when using interactive mode without the
modal.enable_output
context manager. Previously, this would run the App but raise when
modal.interact()
was called.
1.0.2 (2025-05-26)
Fixed an incompatibility with breaking changes in
aiohttp
v3.12.0, which caused issues with Volume and large input uploads. The issues typically manifest as
Local data and remote data checksum mismatch
'_io.BufferedReader' object has no attribute 'getbuffer'
errors.
1.0.1 (2025-05-19)
Added a
--timestamps
flag to
modal app logs
that prepends a timestamp to each log line.
Fixed a bug where objects returned by
Sandbox.list
returncode == 0
running
Sandboxes. Now the return code for running Sandboxes will be
None
Fixed a bug affecting systems where the
sys.platform.node
name includes unicode characters.
1.0.0 (2025-05-16)
With this release, we’re beginning to enforce the deprecations discussed in the
1.0 migration guide
. Going forward, we’ll include breaking changes for outstanding deprecations in
1.Y.0
releases, so we recommend pinning Modal on a minor version (
modal~=1.0.0
) if you have not addressed the existing warnings. While we’ll continue to make improvements to the Modal API, new deprecations will be introduced at a substantially reduced rate, and support windows for older client versions will lengthen.
⚠️ In this release, we’ve made some breaking changes to Modal’s “automounting” behavior.️ If you’ve not already adapted your source code in response to warnings about automounting, Apps built with 1.0+ will have different files included and may not run as expected:
Previously, Modal containers would automatically include the source for local Python packages that were imported by your Modal App. Going forward, it will be necessary to explicitly include such packages in the Image (i.e., with
modal.Image.add_local_python_source
Support for the
automount
configuration (
MODAL_AUTOMOUNT
) has been removed; this environment variable will no longer have any effect.
Modal will continue to automatically include the Python module or package where the Function is defined. This is narrower in scope than the old automounting behavior: it’s limited to at most a single package, and it includes only
files. The limited automounting can also be disabled in cases where your Image definition already includes the package defining the App: set
include_source=False
in the
modal.App
constructor or
@app.function
decorator.
Additionally, we have enforced a number of previously-introduced deprecations:
Removed
modal.Mount
as a public object, along with various
mount=
parameters where Mounts could be passed into the Modal API. Usage can be replaced with
modal.Image
methods, e.g.:
@app.function
image
=image,
mounts
=[modal.Mount.from_local_dir(
"data"
"/root/data"
## This is now an error!
@app.function
image
=image.add_local_dir(
"data"
"/root/data"
## Correct spelling
Copy
Removed the
show_progress
parameter from
modal.App.run
. This parameter has been replaced by the
modal.enable_output
context manager:
with
modal.enable_output(), app.run():
## Will produce verbose Modal output
Copy
Passing flagged options to the
Image.pip_install
package list will now raise an error. Use the
extra_options
parameter to specify options that aren’t exposed through the
Image.pip_install
signature:
image.pip_install(
"flash-attn"
"--no-build-isolation"
## This is now an error!
image.pip_install(
"flash-attn"
extra_options
"--no-build-isolation"
## Correct spelling
Copy
Removed backwards compatibility for using
label=
tag=
keywords in object lookup methods. We standardized these methods to use
name=
as the parameter name, but we recommend using positional arguments:
f = modal.Function.from_name(
"my-app"
## No longer supported! Will raise an error!
f = modal.Function.from_name(
"my-app"
## Preferred spelling
Copy
It’s no longer possible to invoke a generator Function with
Function.spawn
; previously this warned, now it raises an
InvalidError
. Additionally, the
FunctionCall.get_gen
method has been removed, and it’s no longer possible to set
is_generator
when using
FunctionCall.from_id
Removed the
.resolve()
method on Modal objects. This method had not been publicly documented, but where used it can be replaced straightforwardly with
.hydrate()
. Note that explicit hydration should rarely be necessary: in most cases you can rely on lazy hydration semantics (i.e., objects will be hydrated when the first method that requires server metadata is called).
Functions decorated with
@modal.asgi_app
@modal.wsgi_app
are now required to be nullary. Previously, we warned in the case where a function was defined with parameters that all had default arguments.
Referencing the deprecated
modal.Stub
object will now raise an
AttributeError
, whereas previously it was an alias for
modal.App
. This is a simple name change.
0.77
0.77.0 (2025-05-13)
This is the final pre-1.0 release of the Modal client. The next release will be version 1.0. While we do not plan to enforce most major deprecations until later in the 1.0 cycle, there will be some breaking changes introduced in the next release.
0.76
0.76.3 (2025-05-12)
Fixed the behavior of
modal app history --json
when the history contains versions with and without commit information or “tag” metadata. Commit information is now always included (with a
null
placeholder when absent), while tag metadata is included only when there is at least one tagged release (other releases will have a
null
placeholder).
0.76.0 (2025-05-12)
Fixed the behavior of
ignore=
modal.Image
methods, including when
.dockerignore
files are implicitly used in docker-oriented methods. This may result in Image rebuilds with different final inventories:
When using
modal.Image.add_local_dir
, exclusion patterns are now correctly interpreted as relative to the directory being added (e.g.,
*.json
will now ignore all json files in the top-level of the directory).
When using
modal.Image.from_dockerfile
, exclusion patterns are correctly interpreted as relative to the context directory.
As in Docker, leading or trailing path delimiters are stripped from the ignore patterns before being applied.
Breaking change
: When providing a custom function to
ignore=
, file paths passed into the function will now be
relative
, rather than absolute.
0.75
0.75.8 (2025-05-12)
Introduced
modal.Cls.with_concurrency
modal.Cls.with_batching
for runtime configuration of functionality that is exposed through the
@modal.concurrent
@modal.batched
decorators.
model = Model.with_options(
"H100"
).with_concurrency(
max_inputs
Copy
Added a deprecation warning when using
allow_concurrent_inputs
modal.Cls.with_options
Added
buffer_containers
modal.Cls.with_options
Behavior change:
when
modal.Cls.with_options
is called multiple times on the same object, the configurations will be merged rather than using the most recent.
0.75.4 (2025-05-09)
Fixed issue with .spawn_map producing wrong number of arguments
0.75.3 (2025-05-08)
modal.Dict
s (forthcoming on 2025-05-20) use a new durable storage backend with more “cache-like” behavior - items expire after 7 days of inactivity (no reads or writes). Previously created
modal.Dict
s will continue to use the old backend, but support will eventually be dropped.
modal.Dict.put
now supports an
skip_if_exists
flag that can be used to avoid overwriting the value for existing keys:
item_created = my_dict.put("foo", "bar", skip_if_exists=True)
assert item_created
new_item_created = my_dict.put("foo", "baz", skip_if_exists=True)
assert not new_item_created
Copy
Note that this flag only works for
modal.Dict
objects with the new backend (forthcoming on 2025-05-20) and will raise an error otherwise.
0.75.2 (2025-05-08)
Reverts defective changes to the interpretation of
ignore=
patterns and
.dockerignore
files that were introduced in v0.75.0.
0.75.0 (2025-05-08)
Introduced some changes to the handling of
ignore=
patterns in
modal.Image
methods. Due to a defect around the handling of leading path delimiter characters, these changes reverted in 0.75.2 and later reintroduced in 0.76.0.
0.74
0.74.63 (2025-05-08)
Deprecates
Function.web_url
in favor of a new
Function.get_web_url()
method. This also allows the url of a
Function
to be retrieved in an async manner using
Function.get_web_url.aio()
(like all other io-bearing methods in the Modal API)
0.74.61 (2025-05-07)
Adds a deprecation warning when data is passed directly to
modal.Dict.from_name
modal.Dict.ephemeral
. Going forward, it will be necessary to separate
Dict
population from creation.
0.74.60 (2025-05-07)
modal.Dict.update
now also accepts a positional Mapping, like Python’s
dict
type:
d = modal.Dict.from_name(
"some-dict"
d.update({
"a_key"
"another_key"
some_kwarg
True
Copy
0.74.56 (2025-05-06)
Experimental
modal cluster
subcommand is added.
0.74.53 (2025-05-06)
Added functionality for
.spawn_map
on a function instantiated from
Function.from_name
0.74.51 (2025-05-06)
modal
client library can now be installed with Protobuf 6.
0.74.49 (2025-05-06)
Changes the log format of the modal client’s default logger. Instead of
[%(threadName)s]
, the client now logs
[modal-client]
as the log line prefix.
Adds a configuration option (MODAL_LOG_PATTERN) to the modal config for setting the log formatting pattern, in case users want to customize the format. To get the old format, use
MODAL_LOG_PATTERN='[%(threadName)s] %(asctime)s %(message)s'
(or add this to your
.modal.toml
in the
log_pattern
field).
0.74.48 (2025-05-05)
Added a new method for spawning many function calls in parallel:
Function.spawn_map
0.74.46 (2025-05-05)
Introduces a new
.update_autoscaler()
method, which will replace the existing
.keep_warm()
method with the ability to dynamically change the entire autoscaler configuration (
min_containers
max_containers
buffer_containers
, and
scaledown_window
0.74.39 (2025-04-30)
modal
client no longer includes
fastapi
as a library dependency.
0.74.36 (2025-04-29)
A new parameter,
restrict_modal_access
, can be provided on a Function to prevent it from interacting with other resources in your Modal Workspace like Queues, Volumes, or other Functions. This can be useful for running user-provided or LLM-written code in a safe way.
0.74.35 (2025-04-29)
Fixed a bug that prevented doing
modal run
against an entrypoint defined by
Cls.with_options
0.74.32 (2025-04-29)
When setting a custom
name=
@app.function()
, an error is now raised unless
serialized=True
is also set.
0.74.25 (2025-04-25)
App.include
method now returns
self
so it’s possible to build up an App through chained calls:
app = modal.App(
"main-app"
).include(sub_app_1).include(sub_app_2)
Copy
0.74.23 (2025-04-25)
Marked some parameters in a small number of Modal functions as requiring keyword arguments (namely,
modal.App.run
modal.Cls.with_options
, all
.from_name
methods, and a few others). Code that calls these functions with positional arguments will now raise an error. This is expected to be minimally disruptive as the affected parameters are mostly “extra” options or positioned after parameters that have previously been deprecated.
0.74.22 (2025-04-24)
Added a
modal secret delete
command to the CLI.
0.74.21 (2025-04-24)
allow_cross_region_volumes
parameter of the
@app.function
@app.cls
decorators now issues a deprecation warning; the parameter is always treated as
True
on the Modal backend.
0.74.18 (2025-04-23)
Adds a
.deploy()
method to the
object. This method allows you programmatically deploy Apps from Python:
app = modal.App(
"programmatic-deploy"
app.deploy()
Copy
0.74.12 (2025-04-18)
@app.function
@app.cls
decorators now support
experimental_options
, which we’ll use going forward when testing experimental functionality that depends only on server-side configuration.
0.74.7 (2025-04-17)
Modal will now raise an error if local files included in the App are modified during the build process. This behavior can be controlled with the
MODAL_BUILD_VALIDATION
configuration, which accepts
error
(default),
warning
, or
ignore
0.74.6 (2025-04-17)
Internal change that makes containers for functions/classes with
serialized=True
start up
slightly
faster than before
0.74.0 (2025-04-15)
Introduces a deprecation warning when using explicit constructors (
__init__
methods) on
@modal.cls
-decorated classes. Class parameterization should instead be done via
dataclass-style
modal.parameter()
declarations
. Initialization logic should run in
@modal.enter()
-decorated
lifecycle methods
0.73
0.73.173 (2025-04-15)
Fix bug where containers hang with batch sizes above 100 (with
@modal.batched
Fix bug where containers can fail with large outputs and batch sizes above 49 (with
@modal.batched
0.73.170 (2025-04-14)
Fixes a bug where
modal run
didn’t recognize
modal.parameter()
class parameters
0.73.165 (2025-04-11)
Allow running new ephemeral apps from
within
Modal containers using
with app.run(): ...
. Use with care, as putting such a run block in global scope of a module could easily lead to infinite app creation recursion
0.73.160 (2025-04-10)
allow_concurrent_inputs
parameter of
@app.function
@app.cls
is now deprecated in favor of the
@modal.concurrent
decorator. See the
Modal 1.0 Migration Guide
and documentation on
input concurrency
for more information.
0.73.159 (2025-04-10)
Fixes a bug where
serialized=True
classes could not
self.
reference other methods on the class, or use
modal.parameter()
synthetic constructors
0.73.158 (2025-04-10)
Adds support for
bool
type to class parameters using
name: bool = modal.parameter()
. Note that older clients can’t instantiate classes with bool parameters unless those have default values which are not modified. Bool parameters are also not supported by web endpoints at this time.
0.73.148 (2025-04-07)
Fixes a bug introduced in 0.73.147 that broke App builds when using
@modal.batched
on a class method.
0.73.147 (2025-04-07)
Improved handling of cases where
@modal.concurrent
is stacked with other decorators.
0.73.144 (2025-04-04)
Adds a
context_dir
parameter to
modal.Image.from_dockerfile
modal.Image.dockerfile_commands
. This parameter can be used to provide a local reference for relative COPY commands.
0.73.139 (2025-04-02)
Added
modal.experimental.ipython
module, which can be loaded in Jupyter notebooks with
%load_ext modal.experimental.ipython
. Currently it provides the
%modal
line magic for looking up functions:
%modal
from
main/my-app
import
my_function, MyClass
## Now you can use my_function() and Foo in your notebook.
my_function.remote()
Foo().my_method.remote()
Copy
Removed the legacy
modal.extensions.ipython
module from 2022.
0.73.135 (2025-03-29)
Fix shutdown race bug that emitted spurious error-level logs.
0.73.132 (2025-03-28)
Adds the
@modal.concurrent
decorator, which will be replacing the beta
allow_concurrent_inputs=
parameter of
@app.function
@app.cls
for enabling input concurrency. Notably,
@modal.concurrent
introduces a distinction between
max_inputs
target_inputs
, allowing containers to burst over the concurrency level targeted by the Modal autoscaler during periods of high load.
0.73.131 (2025-03-28)
Instantiation of classes using keyword arguments that are not defined as as
modal.parameter()
will now raise an error on the calling side rather than in the receiving container. Note that this only applies if there is at least one modal.parameter() defined on the class, but this will likely apply to parameter-less classes in the future as well.
0.73.121 (2025-03-24)
Adds a new “commit info” column to the
modal app history
command. It shows the short git hash at the time of deployment, with an asterisk
if the repository had uncommitted changes.
0.73.119 (2025-03-21)
Class parameters are no longer automatically cast into their declared type. If the wrong type is provided to a class parameter, method calls to that class instance will now fail with an exception.
0.73.115 (2025-03-19)
Adds support for new strict
bytes
type for
modal.parameter
Usage:
import
typing
import
modal
app = modal.App()
@app.cls
class
bytes
= modal.parameter(
default
"hello"
@modal.method
self
return
"hello
{self
@app.local_entrypoint
main
foo = Foo(
"world"
foo.bar.remote()
Copy
Note
: For parameterized web endoints you must base64 encode the bytes before passing them in as a query parameter.
0.73.107 (2025-03-14)
Include git commit info at the time of app deployment.
0.73.105 (2025-03-14)
Added
Image.cmd()
for setting image default entrypoint args (a.k.a.
0.73.95 (2025-03-12)
Fixes a bug which could cause
Function.map
and sibling methods to stall indefinitely if there was an exception in the input iterator itself (i.e. not the mapper function)
0.73.89 (2025-03-05)
@modal.web_endpoint
decorator is now deprecated. We are replacing it with
@modal.fastapi_endpoint
. This can be a simple name substitution in your code; the two decorators have identical semantics.
0.73.84 (2025-03-04)
keep_warm=
parameter has been removed from the
@modal.method
decorator. This parameter has been nonfunctional since v0.63.0; all autoscaler configuration must be done at the level of the modal Cls.
0.73.82 (2025-03-04)
Adds
modal.fastapi_endpoint
as an alias for
modal.web_endpoint
. We will be deprecating the
modal.web_endpoint
name
(but not the functionality) as part of the Modal 1.0 release.
0.73.81 (2025-03-03)
wait_for_response
parameter of Modal’s web endpoint decorators has been removed (originally deprecated in May 2024).
0.73.78 (2025-03-01)
It is now possible to call
Cls.with_options
on an unhydrated Cls, e.g.
ModelWithGPU = modal.Cls.from_name(
"my-app"
"Model"
).with_options(
"H100"
Copy
0.73.77 (2025-03-01)
Cls.with_options()
now accept unhydated volume and secrets
0.73.76 (2025-02-28)
We’re renaming several
App.function
App.cls
parameters that configure the behavior of Modal’s autoscaler:
concurrency_limit
is now
max_containers
keep_warm
is now
min_containers
container_idle_timeout
is now
scaledown_window
The old names will continue to work, but using them will issue a deprecation warning. The aim of the renaming is to reduce some persistent confusion about what these parameters mean. Code updates should require only a simple substitution of the new name.
We’re adding a new parameter,
buffer_containers
(previously available as
_experimental_buffer_containers
). When your Function is actively handling inputs, the autoscaler will spin up additional
buffer_containers
so that subsequent inputs will not be blocked on cold starts. When the Function is idle, it will still scale down to the value given by
min_containers
0.73.75 (2025-02-28)
Adds a new config field,
ignore_cache
(also accessible via environment variables as
MODAL_IGNORE_CACHE=1
), which will force Images used by the App to rebuild but not clobber any existing cached Images. This can be useful for testing an App’s robustness to Image rebuilds without affecting other Apps that depend on the same base Image layer(s).
0.73.73 (2025-02-28)
Adds a deprecation warning to the
workspace
parameter in
modal.Cls
lookup methods. This argument is unused and will be removed in the future.
0.73.69 (2025-02-25)
We’ve moved the
modal.functions.gather
function to be a staticmethod on
modal.FunctionCall.gather
. The former spelling has been deprecated and will be removed in a future version.
0.73.68 (2025-02-25)
Fixes issue where running
modal shell
with a dot-separated module reference as input would not accept the required
flag for “module mode”, but still emitted a warning telling users to use
0.73.60 (2025-02-20)
Fixes an issue where
modal.runner.deploy_app()
didn’t work when called from within a running (remote) Modal Function
0.73.58 (2025-02-20)
Introduces an
flag to
modal run
modal shell
modal serve
modal deploy
, which indicates that the modal app/function file is specified using python “module syntax” rather than a file path. In the future this will be a required flag when using module syntax.
Old syntax:
modal
my_package/modal_main.py
modal
my_package.modal_main
Copy
New syntax (note the
on the second line):
modal
my_package/modal_main.py
modal
my_package.modal_main
Copy
0.73.54 (2025-02-18)
Passing
App.lookup
an invalid name now raises an error. App names may contain only alphanumeric characters, dashes, periods, and underscores, must be shorter than 64 characters, and cannot conflict with App ID strings.
0.73.51 (2025-02-14)
Fixes a bug where sandboxes returned from
Sandbox.list()
were not snapshottable even if they were created with
_experimental_enable_snapshot
0.73.44 (2025-02-13)
modal.FunctionCall
is now available in the top-level
modal
namespace. We recommend referencing the class this way instead of using the the fully-qualified
modal.functions.FunctionCall
name.
0.73.40 (2025-02-12)
Function.web_url
will now return None (instead of raising an error) when the Function is not a web endpoint
0.73.31 (2025-02-10)
Deprecate the GPU classes (
gpu=A100(...)
etc) in favor of just using strings (
gpu="A100"
etc)
0.73.26 (2025-02-10)
Adds a pending deprecation warning when looking up class methods using
Function.from_name
, e.g.
Function.from_name("some_app", "SomeClass.some_method")
. The recommended way to reference methods of classes is to look up the class instead:
RemoteClass = Cls.from_name("some_app", "SomeClass")
0.73.25 (2025-02-09)
Fixes an issue introduced in
0.73.19
that prevented access to GPUs during image builds
0.73.18 (2025-02-06)
When using a parameterized class (with at least one
modal.parameter()
specified), class instantiation with an incorrect construction signature (wrong arguments or types) will now fail at the
.remote()
calling site instead of container startup for the called class.
0.73.14 (2025-02-04)
Fixed the status message shown in terminal logs for ephemeral Apps to accurately report the number of active containers.
0.73.11 (2025-02-04)
Warns users if the
modal.Image
of a Function/Cls doesn’t include all the globally imported “local” modules (using
.add_local_python_source()
), and the user hasn’t explicitly set an
include_source
value of True/False. This is in preparation for an upcoming deprecation of the current “auto mount” logic.
0.73.10 (2025-02-04)
Modal functions, methods and entrypoints can now accept variable-length arguments to skip Modal’s default CLI parsing. This is useful if you want to use Modal with custom argument parsing via
argparse
HfArgumentParser
. For example, the following function can be invoked with
modal run my_file.py --foo=42 --bar="baz"
import
argparse
@app.function
train
arglist
parser = argparse.ArgumentParser()
parser.add_argument(
"--foo"
type
parser.add_argument(
"--bar"
type
args = parser.parse_args(
args
= arglist)
Copy
0.73.1 (2025-01-30)
modal run
now runs a single local entrypoints/function in the selected module. If exactly one local entrypoint or function exists in the selected module, the user doesn’t have to qualify the runnable
in the modal run command, even if some of the module’s referenced apps have additional local entrypoints or functions. This partially restores “auto-inferred function” functionality that was changed in v0.72.48.
0.73.0 (2025-01-30)
Introduces an
include_source
argument in the
App.function
App.cls
decorators that let users configure which class of python packages are automatically included as source mounts in created modal functions/classes (what we used to call “automount” behavior). This will supersede the MODAL_AUTOMOUNT configuration value which will eventually be deprecated. As a convenience, the
modal.App
constructor will also accept an
include_source
argument which serves as the default for all the app’s functions and classes.
include_source
argument accepts the following values:
True
(default in a future version of Modal) Automatically includes the Python files of the source package of the function’s own home module, but not any other local packages. Roughly equivalent ot
MODAL_AUTOMOUNT=0
in previous versions of Modal.
False
- don’t include
local source. Assumes the function’s home module is importable in the container environment through some other means (typically added to the provided
modal.Image
’s Python environment).
None
(the default) - use current soon-to-be-deprecated automounting behavior, including source of all first party packages that are not installed into site-packages locally.
Minor change to
MODAL_AUTOMOUNT=0
: When running/deploying using a module path (e.g.
modal run mypak.mymod
all non .pyc files
of the source package (
mypak
in this case) are now included in the function’s container. Previously, only the function’s home
module file + any
__init__.py
files in its package structure were included. Note that this is only for MODAL_AUTOMOUNT=0. To get full control over which source files are included with your functions, you can set
include_source=False
on your function (see above) and manually specify the files to include using the
ignore
argument to
Image.add_local_python_source
0.72
0.72.56 (2025-01-28)
Deprecated
.lookup
methods on Modal objects. Users are encouraged to use
.from_name
instead. In most cases this will be a simple name substitution. See
the 1.0 migration guide
for more information.
0.72.54 (2025-01-28)
Fixes bug introduced in v0.72.48 where
modal run
didn’t work with files having global
Function.from_name()
Function.lookup()
Cls.from_name()
Cls.lookup()
calls.
0.72.48 (2025-01-24)
Fixes a CLI bug where you couldn’t reference functions via a qualified app, e.g.
mymodule::{app_variable}.{function_name}
modal run
modal serve
modal shell
commands get more consistent error messages in cases where the passed app or function reference isn’t resolvable to something that the current command expects.
Removes the deprecated
__getattr__
__setattr__
__getitem__
__setitem__
methods from
modal.App
0.72.39 (2025-01-22)
Introduced a new public method,
.hydrate
, for on-demand hydration of Modal objects. This method replaces the existing semi-public
.resolve
method, which is now deprecated.
0.72.33 (2025-01-20)
The Image returned by
Sandbox.snapshot_filesystem
now has
object_id
and other metadata pre-assigned rather than require loading by subsequent calls to sandboxes or similar to set this data.
0.72.30 (2025-01-18)
Adds a new
oidc_auth_role_arn
field to
CloudBucketMount
for using OIDC authentication to create the mountpoint.
0.72.24 (2025-01-17)
No longer prints a warning if
app.include
re-includes an already included function (warning is still printed if
another
function with the same name is included)
0.72.22 (2025-01-17)
Internal refactor of the
modal.object
module. All entities except
Object
from that module have now been moved to the
modal._object
“private” module.
0.72.17 (2025-01-16)
@modal.build
decorator is now deprecated. For storing large assets (e.g. model weights), we now recommend using a
modal.Volume
over writing data to the
modal.Image
filesystem directly.
0.72.16 (2025-01-16)
Fixes bug introduced in v0.72.9 where
modal run SomeClass.some_method
would incorrectly print a deprecation warning.
0.72.15 (2025-01-15)
Added an
environment_name
parameter to the
App.run
context manager.
0.72.8 (2025-01-10)
Fixes a bug introduced in v0.72.2 when specifying
add_python="3.9"
Image.from_registry
0.72.0 (2025-01-09)
The default behavior
Image.from_dockerfile()
image.dockerfile_commands()
if no parameter is passed to
ignore
will be to automatically detect if there is a valid dockerignore file in the current working directory or next to the dockerfile following the same rules as
dockerignore
does using
docker
commands. Previously no patterns were ignored.
0.71
0.71.13 (2025-01-09)
FilePatternMatcher
has a new constructor
from_file
which allows you to read file matching patterns from a file instead of having to pass them in directly, this can be used for
Image
methods accepting an
ignore
parameter in order to read ignore patterns from files.
0.71.11 (2025-01-08)
Modal Volumes can now be renamed via the CLI (
modal volume rename
) or SDK (
modal.Volume.rename
0.71.7 (2025-01-08)
Adds
Image.from_id
, which returns an
Image
object from an existing image id.
0.71.1 (2025-01-06)
Sandboxes now support fsnotify-like file watching:
from
modal.file_io
import
FileWatchEventType
app = modal.App.lookup(
"file-watch"
create_if_missing
True
sb = modal.Sandbox.create(
=app)
events = sb.watch(
"/foo"
event
events:
event.type == FileWatchEventType.Modify:
print
(event.paths)
Copy
0.70
0.70.1 (2024-12-27)
The sandbox filesystem API now accepts write payloads of sizes up to 1 GiB.
0.69
0.69.0 (2024-12-21)
Image.from_dockerfile()
image.dockerfile_commands()
now auto-infer which files need to be uploaded based on COPY commands in the source if
context_mount
is omitted. The
ignore=
argument to these methods can be used to selectively omit files using a set of glob patterns.
0.68
0.68.53 (2024-12-20)
You can now point
modal launch vscode
at an arbitrary Dockerhub base image:
modal launch vscode --image=nvidia/cuda:12.4.0-devel-ubuntu22.04
0.68.44 (2024-12-19)
You can now run GPU workloads on
Nvidia L40S GPUs
@app.function
"L40S"
my_gpu_fn
Copy
0.68.43 (2024-12-19)
Fixed a bug introduced in v0.68.39 that changed the exception type raise when the target object for
.from_name
.lookup
methods was not found.
0.68.39 (2024-12-18)
Standardized terminology in
.from_name
.lookup
.delete
methods to use
name
consistently where
label
were used interchangeably before. Code that invokes these methods using
label=
as an explicit keyword argument will issue a deprecation warning and will break in a future release.
0.68.29 (2024-12-17)
The internal
deprecation_error
deprecation_warning
utilities have been moved to a private namespace
0.68.28 (2024-12-17)
Sandboxes now support additional filesystem commands
mkdir
, and
app = modal.App.lookup(
"sandbox-fs"
create_if_missing
True
sb = modal.Sandbox.create(
=app)
sb.mkdir(
"/foo"
with
sb.open(
"/foo/bar.txt"
f.write(
"baz"
print
(sb.ls(
"/foo"
Copy
0.68.27 (2024-12-17)
Two previously-introduced deprecations are now enforced and raise an error:
App.spawn_sandbox
method has been removed in favor of
Sandbox.create
Sandbox.create
now requires an
object to be passed
0.68.24 (2024-12-16)
modal run
CLI now has a
--write-result
option. When you pass a filename, Modal will write the return value of the entrypoint function to that location on your local filesystem. The return value of the function must be either
bytes
to use this option; otherwise, an error will be raised. It can be useful for exercising a remote function that returns text, image data, etc.
0.68.21 (2024-12-13)
Adds an
ignore
parameter to our
Image
add_local_dir
copy_local_dir
methods. It is similar to the
condition
method on
Mount
methods but instead operates on a
Path
object. It takes either a list of string patterns to ignore which follows the
dockerignore
syntax implemented in our
FilePatternMatcher
class, or you can pass in a callable which allows for more flexible selection of files.
Usage:
img.add_local_dir(
"./local-dir"
remote_path
"/remote-path"
ignore
=FilePatternMatcher(
"**/*"
"!*.txt"
## ignore everything except files ending with .txt
img.add_local_dir(
...,
ignore
=~FilePatternMatcher(
"**/*.py"
## can be inverted for when inclusion filters are simpler to write
img.add_local_dir(
...,
ignore
"**/*.py"
"!module/*.py"
## ignore all .py files except those in the module directory
img.add_local_dir(
...,
ignore
lambda
: fp.is_relative_to(
"somewhere"
## use a custom callable
Copy
which will add the
./local-dir
directory to the image but ignore all files except
.txt
files
0.68.15 (2024-12-13)
Adds the
requires_proxy_auth
parameter to
web_endpoint
asgi_app
wsgi_app
, and
web_server
decorators. Requests to the app will respond with 407 Proxy Authorization Required if a webhook token is not supplied in the HTTP headers. Protects against DoS attacks that will unnecessarily charge users.
0.68.11 (2024-12-13)
Cls.from_name(...)
now works as a lazy alternative to
Cls.lookup()
that doesn’t perform any IO until a method on the class is used for a .remote() call or similar
0.68.6 (2024-12-12)
Fixed a bug introduced in v0.67.47 that suppressed console output from the
modal deploy
CLI.
0.68.5 (2024-12-12)
We’re removing support for
.spawn()
ing generator functions.
0.68.2 (2024-12-11)
Sandboxes now support a new filesystem API. The
open()
method returns a
FileIO
handle for native file handling in sandboxes.
app = modal.App.lookup(
"sandbox-fs"
create_if_missing
True
sb = modal.Sandbox.create(
=app)
with
sb.open(
"test.txt"
f.write(
"Hello World
f = sb.open(
"test.txt"
"rb"
print
(f.read())
Copy
0.67
0.67.43 (2024-12-11)
modal container exec
modal shell
now work correctly even when a pseudoterminal (PTY) is not present. This means, for example, that you can pipe the output of these commands to a file:
modal shell -c
'uv pip list'
> env.txt
Copy
0.67.39 (2024-12-09)
It is now possible to delete named
NetworkFileSystem
objects via the CLI (
modal nfs delete ...
) or API
(modal.NetworkFileSystem.delete(...)
0.67.38 (2024-12-09)
Sandboxes now support filesystem snapshots. Run
Sandbox.snapshot_filesystem()
to get an Image which can be used to spawn new Sandboxes.
0.67.28 (2024-12-05)
Adds
Image.add_local_python_source
which works similarly to the old and soon-to-be-deprecated
Mount.from_local_python_packages
but for images. One notable difference is that the new
add_local_python_source
only
includes
-files by default
0.67.23 (2024-12-04)
Image build functions that use a
functools.wraps
decorator will now have their global variables included in the cache key. Previously, the cache would use global variables referenced within the wrapper itself. This will force a rebuild for Image layers defined using wrapped functions.
0.67.22 (2024-12-03)
Fixed a bug introduced in v0.67.0 where it was impossible to call
modal.Cls
methods when passing a list of requested GPUs.
0.67.12 (2024-12-02)
Fixed a bug that executes the wrong method when a Modal Cls overrides a
@modal.method
inherited from a parent.
0.67.7 (2024-11-29)
Fixed a bug where pointing
modal run
at a method on a Modal Cls would fail if the method was inherited from a parent.
0.67.0 (2024-11-27)
New minor client version
0.67.x
comes with an internal data model change for how Modal creates functions for Modal classes. There are no breaking or backwards-incompatible changes with this release. All forward lookup scenarios (
.lookup()
of a
0.67
class from a pre
0.67
client) as well as backwards lookup scenarios (
.lookup()
of a pre
0.67
class from a
0.67
client) work, except for a
0.62
client looking up a
0.67
class (this maintains our current restriction of not being able to lookup a
0.63+
class from a
0.62
client).
0.66
0.66.49 (2024-11-26)
modal config set-environment
will now raise if the requested environment does not exist.
0.66.45 (2024-11-26)
modal launch
CLI now accepts a
--detach
flag to run the App in detached mode, such that it will persist after the local client disconnects.
0.66.40 (2024-11-23)
Adds
Image.add_local_file(..., copy=False)
Image.add_local_dir(..., copy=False)
as a unified replacement for the old
Image.copy_local_*()
Mount.add_local_*
methods.
0.66.30 (2024-11-21)
Removed the
aiostream
package from the modal client library dependencies.
0.66.12 (2024-11-19)
Sandbox.exec
now accepts arguments
text
bufsize
for streaming output, which controls text output and line buffering.
0.66.0 (2024-11-15)
Modal no longer supports Python 3.8, which has reached its
official EoL
0.65
0.65.55 (2024-11-13)
Escalates stuck input cancellations to container death. This prevents unresponsive user code from holding up resources.
Input timeouts no longer kill the entire container. Instead, they just cancel the timed-out input, leaving the container and other concurrent inputs running.
0.65.49 (2024-11-12)
Fixed issue in
modal serve
where files used in
Image.copy_*
commands were not watched for changes
0.65.42 (2024-11-07)
Sandbox.exec
can now accept
timeout
workdir
, and
secrets
. See the
Sandbox.create
function for context on how to use these arguments.
0.65.33 (2024-11-06)
Removed the
interactive
parameter from
function
decorators. This parameter has been deprecated since May 2024. Instead of specifying Modal Functions as interactive, use
modal run --interactive
to activate interactive mode.
0.65.30 (2024-11-05)
checkpointing_enabled
option, deprecated in March 2024, has now been removed.
0.65.9 (2024-10-31)
Output from
Sandbox.exec
can now be directed to
/dev/null
stdout
, or stored for consumption. This behavior can be controlled via the new
StreamType
arguments.
0.65.8 (2024-10-31)
Fixed a bug where the
Image.imports
context manager would not correctly propagate ImportError when using a
modal.Cls
0.65.2 (2024-10-30)
Fixed an issue where
modal run
would pause for 10s before exiting if there was a failure during app creation.
0.64
0.64.227 (2024-10-25)
modal container list
CLI command now shows the containers within a specific environment: the active profile’s environment if there is one, otherwise the workspace’s default environment. You can pass
--env
to list containers in other environments.
0.64.223 (2024-10-24)
Fixed
modal serve
not showing progress when reloading apps on file changes since v0.63.79.
0.64.218 (2024-10-23)
Fix a regression introduced in client version 0.64.209, which affects client authentication within a container.
0.64.198 (2024-10-18)
Fixed a bug where
Queue.put
Queue.put_many
would throw
queue.Full
even if
timeout=None
0.64.194 (2024-10-18)
The previously-deprecated
--confirm
flag has been removed from the
modal volume delete
CLI. Use
--yes
to force deletion without a confirmation prompt.
0.64.193 (2024-10-18)
Passing
wait_for_response=False
in Modal webhook decorators is no longer supported. See
the docs
for alternatives.
0.64.187 (2024-10-16)
When writing to a
StreamWriter
that has already had EOF written, a
ValueError
is now raised instead of an
EOFError
0.64.185 (2024-10-15)
Memory snapshotting can now be used with parametrized functions.
0.64.184 (2024-10-15)
StreamWriters now accept strings as input.
0.64.182 (2024-10-15)
Fixed a bug where App rollbacks would not restart a schedule that had been removed in an intervening deployment.
0.64.181 (2024-10-14)
modal shell
CLI command now takes a container ID, allowing you to shell into a running container.
0.64.180 (2024-10-14)
modal shell --cmd
now can be shortened to
modal shell -c
. This means you can use it like
modal shell -c "uname -a"
to quickly run a command within the remote environment.
0.64.168 (2024-10-03)
Image.conda
Image.conda_install
, and
Image.conda_update_from_environment
methods are now fully deprecated. We recommend using
micromamba
(via
Image.micromamba
Image.micromamba_install
) instead, or manually installing and using conda with
Image.run_commands
when strictly necessary.
0.64.153 (2024-09-30)
Breaking Change:
Sandbox.tunnels()
now returns a
Dict
rather than a
List
. This dict is keyed by the container’s port, and it returns a
Tunnel
object, just like
modal.forward
does.
0.64.142 (2024-09-25)
modal.Function
modal.Cls
now support specifying a
list
of GPU configurations, allowing the Function’s container pool to scale across each GPU configuration in preference order.
0.64.139 (2024-09-25)
The deprecated
_experimental_boost
argument is now removed. (Deprecated in late July.)
0.64.123 (2024-09-18)
Sandboxes can now be created without an entrypoint command. If they are created like this, they will stay alive up until their set timeout. This is useful if you want to keep a long-lived sandbox and execute code in it later.
0.64.119 (2024-09-17)
Sandboxes now have a
cidr_allowlist
argument, enabling controlled access to certain IP ranges. When not used (and with
block_network=False
), the sandbox process will have open network access.
0.64.118 (2024-09-17)
Introduce an experimental API to allow users to set the input concurrency for a container locally.
0.64.112 (2024-09-15)
Creating sandboxes without an associated
is deprecated. If you are spawning a
Sandbox
outside a Modal container, you can lookup an
by name to attach to the
Sandbox
app = modal.App.lookup(
'my-app'
create_if_missing
True
modal.Sandbox.create(
'echo'
'hi'
=app)
Copy
0.64.109 (2024-09-13)
App handles can now be looked up by name with
modal.App.lookup(name)
. This can be useful for associating Sandboxes with Apps:
app = modal.App.lookup(
"my-app"
create_if_missing
True
modal.Sandbox.create(
"echo"
"hi"
=app)
Copy
0.64.100 (2024-09-11)
The default timeout for
modal.Image.run_function
has been lowered to 1 hour. Previously it was 24 hours.
0.64.99 (2024-09-11)
Fixes an issue that could cause containers using
enable_memory_snapshot=True
on Python 3.9 and below to shut down prematurely.
0.64.97 (2024-09-11)
Added support for
ASGI lifespan protocol
@app.function
@modal.asgi_app
func
from
fastapi
import
FastAPI, Request
lifespan
wapp
: FastAPI):
print
"Starting"
yield
"foo"
"bar"
print
"Shutting down"
web_app = FastAPI(
lifespan
=lifespan)
@web_app.get
get_state
request
: Request):
return
"message"
"This is the state:
request.state.foo
return
web_app
Copy
which enables support for
gradio>=v4
amongst other libraries using lifespans
0.64.87 (2024-09-05)
Sandboxes now support port tunneling. Ports can be exposed via the
open_ports
argument, and a list of active tunnels can be retrieved via the
.tunnels()
method.
0.64.67 (2024-08-30)
Fixed a regression in
modal launch
to resume displaying output when starting the container.
0.64.48 (2024-08-21)
Introduces new dataclass-style syntax for class parametrization (see updated
docs
@app.cls
class
MyCls
param_a:
= modal.parameter()
MyCls(
param_a
"hello"
## synthesized constructor
Copy
The new syntax enforces types (
for now) on all parameters
When the new syntax is used
, any web endpoints (
web_endpoint
asgi_app
wsgi_app
web_server
) on the app will now also support parametrization through the use of query parameters matching the parameter names, e.g.
https://myfunc.modal.run/?param_a="hello
in the above example.
The old explicit
__init__
constructor syntax is still allowed, but could be deprecated in the future and doesn’t work with web endpoint parametrization
0.64.38 (2024-08-16)
Added a
modal app rollback
CLI command for rolling back an App deployment to a previous version.
0.64.33 (2024-08-16)
Commands in the
modal app
CLI now accept an App name as a positional argument, in addition to an App ID:
modal app history my-app
Copy
Accordingly, the explicit
--name
option has been deprecated. Providing a name that can be confused with an App ID will also now raise an error.
0.64.32 (2024-08-16)
Updated type stubs using generics to allow static type inferrence for functions calls, e.g.
function.remote(...)
0.64.26 (2024-08-15)
ContainerProcess
handles now support
wait()
poll()
, like
Sandbox
objects
0.64.24 (2024-08-14)
Added support for dynamic batching. Functions or class methods decorated with
@modal.batched
will now automatically batch their invocations together, up to a specified
max_batch_size
. The batch will wait for a maximum of
wait_ms
for more invocations after the first invocation is made. See guide for more details.
@app.function
@modal.batched
max_batch_size
wait_ms
1000
async
batched_multiply
: list[
: list[
]) -> list[
return
[x * y
x, y
(xs, xs)]
@app.cls
class
BatchedClass
@modal.batched
max_batch_size
wait_ms
1000
async
batched_multiply
: list[
: list[
]) -> list[
return
[x * y
x, y
(xs, xs)]
Copy
The batched function is called with individual inputs:
await
batched_multiply.remote.aio(
Copy
0.64.18 (2024-08-12)
Sandboxes now have an
exec()
method that lets you execute a command inside the sandbox container.
exec
returns a
ContainerProcess
handle for input and output streaming.
sandbox = modal.Sandbox.create(
"sleep"
"infinity"
process = sandbox.exec(
"bash"
"-c"
"for i in $(seq 1 10); do echo foo $i; sleep 0.5; done"
line
process.stdout:
print
(line)
Copy
0.64.8 (2024-08-06)
Removed support for the undocumented
modal.apps.list_apps()
function, which was internal and not intended to be part of public API.
0.64.7 (2024-08-05)
Removed client check for CPU core request being at least 0.1, deferring to server-side enforcement.
0.64.2 (2024-08-02)
Volumes can now be mounted to an ad hoc modal shell session:
modal shell --volume my-vol-name
Copy
When the shell starts, the volume will be mounted at
/mnt/my-vol-name
. This may be helpful for shell-based exploration or manipulation of volume contents.
Note that the option can be used multiple times to mount additional models:
modal shell --volume models --volume data
Copy
0.64.0 (2024-07-29)
App deployment events are now atomic, reducing the risk that a failed deploy will leave the App in a bad state.
0.63
0.63.87 (2024-07-24)
_experimental_boost
argument can now be removed. Boost is now enabled on all modal Functions.
0.63.77 (2024-07-18)
Setting
_allow_background_volume_commits
is no longer necessary and has been deprecated. Remove this argument in your decorators.
0.63.36 (2024-07-05)
Image layers defined with a
@modal.build
method will now include the values of any
class variables
that are referenced within the method as part of the layer cache key. That means that the layer will rebuild when the class variables change or are overridden by a subclass.
0.63.22 (2024-07-01)
Fixed an error when running
@modal.build
methods that was introduced in v0.63.19
0.63.20 (2024-07-01)
Fixed bug where
self.method.local()
would re-trigger lifecycle methods in classes
0.63.14 (2024-06-28)
Adds
Cls.lookup()
backwards compatibility with classes created by clients prior to
v0.63
Important
: When updating (to >=v0.63) an app with a Modal
class
that’s accessed using
Cls.lookup()
- make sure to update the client of the app/service
using
Cls.lookup()
first, and
then
update the app containing the class being looked up.
0.63.12 (2024-06-27)
Fixed a bug introduced in 0.63.0 that broke
modal.Cls.with_options
0.63.10 (2024-06-26)
Adds warning about future deprecation of
retries
for generators. Retries are being deprecated as they can lead to nondetermistic generator behavior.
0.63.9 (2024-06-26)
Fixed a bug in
Volume.copy_files()
where some source paths may be ignored if passed as
bytes
Volume.read_file
Volume.read_file_into_fileobj
Volume.remove_file
, and
Volume.copy_files
can no longer take both string or bytes for their paths. They now only accept
0.63.2 (2024-06-25)
Fixes issue with
Cls.lookup
not working (at all) after upgrading to v0.63.0.
Note
: this doesn’t fix the cross-version lookup incompatibility introduced in 0.63.0.
0.63.0 (2024-06-24)
Changes how containers are associated with methods of
@app.cls()
-decorated Modal “classes”.
Previously each
@method
and web endpoint of a class would get its own set of isolated containers and never run in the same container as other sibling methods.
Starting in this version, all
@methods
and web endpoints will be part of the same container pool. Notably, this means all methods will scale up/down together, and options like
keep_warm
concurrency_limit
will affect the total number of containers for all methods in the class combined, rather than individually.
Version incompatibility warning:
Older clients (below 0.63) can’t use classes deployed by new clients (0.63 and above), and vice versa. Apps or standalone clients using
Cls.lookup(...)
to invoke Modal classes need to be upgraded to version
0.63
at the same time as the deployed app that’s being called into.
keep_warm
for classes is now an attribute of the
@app.cls()
decorator rather than individual methods.
0.62
0.62.236 (2024-06-21)
Added support for mounting Volume or CloudBucketMount storage in
Image.run_function
. Note that this is
typically
not necessary, as data downloaded during the Image build can be stored directly in the Image filesystem.
0.62.230 (2024-06-18)
It is now an error to create or lookup Modal objects (
Volume
Dict
Secret
, etc.) with an invalid name. Object names must be shorter than 64 characters and may contain only alphanumeric characters, dashes, periods, and underscores. The name check had inadvertently been removed for a brief time following an internal refactor and then reintroduced as a warning. It is once more a hard error. Please get in touch if this is blocking access to your data.
0.62.224 (2024-06-17)
modal app list
command now reports apps created by
modal app run
modal app serve
as being in an “ephemeral” state rather than a “running” state to reduce confusion with deployed apps that are actively processing inputs.
0.62.223 (2024-06-14)
All modal CLI commands now accept
as a short-form of
--env
0.62.220 (2024-06-12)
Added support for entrypoint and shell for custom containers:
Image.debian_slim().entrypoint([])
can be used interchangeably with
.dockerfile_commands('ENTRYPOINT []')
, and
.shell(["/bin/bash", "-c"])
can be used interchangeably with
.dockerfile_commands('SHELL ["/bin/bash", "-c"]')
0.62.219 (2024-06-12)
Fix an issue with
@web_server
decorator not working on image builder version 2023.12
0.62.208 (2024-06-08)
@web_server
endpoints can now return HTTP headers of up to 64 KiB in length. Previously, they were limited to 8 KiB due to an implementation detail.
0.62.201 (2024-06-04)
modal deploy
now accepts a
--tag
optional parameter that allows you to specify a custom tag for the deployed version, making it easier to identify and manage different deployments of your app.
0.62.199 (2024-06-04)
web_endpoint
s now have the option to include interactive SwaggerUI/redoc docs by setting
docs=True
web_endpoint
s no longer include an OpenAPI JSON spec route by default
0.62.190 (2024-05-29)
modal.Function
now supports requesting ephemeral disk (SSD) via the new
ephemeral_disk
parameter. Intended for use in doing large dataset ingestion and transform.
0.62.186 (2024-05-29)
modal.Volume
background commits are now enabled by default when using
spawn_sandbox
0.62.185 (2024-05-28)
modal app stop
CLI command now accepts a
--name
) option to stop an App by name rather than by ID.
0.62.181 (2024-05-24)
Background committing on
modal.Volume
mounts is now default behavior.
0.62.178 (2024-05-21)
Added a
modal container stop
CLI command that will kill an active container and reassign its current inputs.
0.62.175 (2024-05-17)
modal.CloudBucketMount
now supports writing to Google Cloud Storage buckets.
0.62.174 (2024-05-17)
Using
memory=
to specify the type of
modal.gpu.A100
is deprecated in favor of
size=
. Note that
size
accepts a string type (
"40GB"
"80GB"
) rather than an integer, as this is a request for a specific variant of the A100 GPU.
0.62.173 (2024-05-17)
Added a
version
flag to the
modal.Volume
API and CLI, allow opting in to a new backend implementation.
0.62.172 (2024-05-17)
Fixed a bug where other functions weren’t callable from within an
asgi_app
wsgi_app
constructor function and side effects of
@enter
methods weren’t available in that scope.
0.62.166 (2024-05-14)
Disabling background commits on
modal.Volume
volumes is now deprecated. Background commits will soon become mandatory behavior.
0.62.165 (2024-05-13)
Deprecated
wait_for_response=False
on web endpoints. See
the docs
for alternatives.
0.62.162 (2024-05-13)
A deprecation warning is now raised when using
modal.Stub
, which has been renamed to
modal.App
. Additionally, it is recommended to use
as the variable name rather than
stub
, which matters when using the automatic app discovery feature in the
modal run
CLI command.
0.62.159 (2024-05-10)
Added a
--stream-logs
flag to
modal deploy
that, if True, begins streaming the app logs once deployment is complete.
0.62.156 (2024-05-09)
Added support for looking up a deployed App by its deployment name in
modal app logs
0.62.150 (2024-05-08)
Added validation that App
name
, if provided, is a string.
0.62.149 (2024-05-08)
@app.function
decorator now raises an error when it is used to decorate a class (this was always invalid, but previously produced confusing behavior).
0.62.148 (2024-05-08)
modal app list
output has been improved in several ways:
Persistent storage objects like Volumes or Dicts are no longer included (these objects receive an app ID internally, but this is an implementation detail and subject to future change). You can use the dedicated CLI for each object (e.g.
modal volume list
) instead.
For Apps in a
stopped
state, the output is now limited to those stopped within the past 2 hours.
The number of tasks running for each App is now shown.
0.62.146 (2024-05-07)
Added the
region
parameter to the
modal.App.function
modal.App.cls
decorators. This feature allows the selection of specific regions for function execution. Note that it is available only on some plan types. See our
blog post
for more details.
0.62.144 (2024-05-06)
Added deprecation warnings when using Python 3.8 locally or in a container. Python 3.8 is nearing EOL, and Modal will be dropping support for it soon.
0.62.141 (2024-05-03)
Deprecated the
Image.conda
constructor and the
Image.conda_install
Image.conda_update_from_environment
methods. Conda-based images had a number of tricky issues and were generally slower and heavier than images based on
micromamba
, which offers a similar featureset and can install packages from the same repositories.
Added the
spec_file
parameter to allow
Image.micromamba_install
to install dependencies from a local file. Note that
micromamba
supports conda yaml syntax along with simple text files.
0.62.131 (2024-05-01)
Added a deprecation warning when object names are invalid. This applies to
Dict
NetworkFileSystem
Secret
Queue
, and
Volume
objects. Names must be shorter than 64 characters and may contain only alphanumeric characters, dashes, periods, and underscores. These rules were previously enforced, but the check had inadvertently been dropped in a recent refactor. Please update the names of your objects and transfer any data to retain access, as invalid names will become an error in a future release.
0.62.130 (2024-05-01)
Added a command-line interface for interacting with
modal.Queue
objects. Run
modal queue --help
in your terminal to see what is available.
0.62.116 (2024-04-26)
Added a command-line interface for interacting with
modal.Dict
objects. Run
modal dict --help
in your terminal to see what is available.
0.62.114 (2024-04-25)
Secret.from_dotenv
now accepts an optional filename keyword argument:
@app.function
secrets
=[modal.Secret.from_dotenv(
filename
".env-dev"
Copy
0.62.110 (2024-04-25)
Passing a glob
argument to the
modal volume get
CLI has been deprecated — instead, simply download the desired directory path, or
for the entire volume.
Volume.listdir()
no longer takes trailing glob arguments. Use
recursive=True
instead.
modal volume get
modal nfs get
performance is improved when downloading a single file. They also now work with multiple files when outputting to stdout.
Fixed a visual bug where
modal volume get
on a single file will incorrectly display the destination path.
0.62.109 (2024-04-24)
Improved feedback for deserialization failures when objects are being transferred between local / remote environments.
0.62.108 (2024-04-24)
Added
Dict.delete
Queue.delete
as API methods for deleting named storage objects:
import
modal
modal.Queue.delete(
"my-job-queue"
Copy
Deprecated invoking
Volume.delete
as an instance method; it should now be invoked as a static method with the name of the Volume to delete, as with the other methods.
0.62.98 (2024-04-21)
modal.Dict
object now implements a
keys
values
items
API. Note that there are a few differences when compared to standard Python dicts:
The return value is a simple iterator, whereas Python uses a dictionary view object with more features.
The results are unordered.
Additionally, there was no key data stored for items added to a
modal.Dict
prior to this release, so empty strings will be returned for these entries.
0.62.81 (2024-04-18)
We are introducing
modal.App
as a replacement for
modal.Stub
and encouraging the use of “app” terminology over “stub” to reduce confusion between concepts used in the SDK and the Dashboard. Support for
modal.Stub
will be gradually deprecated over the next few months.
0.62.72 (2024-04-16)
Specifying a hard memory limit for a
modal.Function
is now supported. Pass a tuple of
memory=(request, limit)
. Above the
limit
, which is specified in MiB, a Function’s container will be OOM killed.
0.62.70 (2024-04-16)
modal.CloudBucketMount
now supports read-only access to Google Cloud Storage
0.62.69 (2024-04-16)
Iterators passed to
Function.map()
and similar parallel execution primitives are now executed on the main thread, preventing blocking iterators from possibly locking up background Modal API calls, and risking task shutdowns.
0.62.67 (2024-04-15)
The return type of
Volume.listdir()
Volume.iterdir()
NetworkFileSystem.listdir()
, and
NetworkFileSystem.iterdir()
is now a
FileEntry
dataclass from the
modal.volume
module. The fields of this data class are the same as the old protobuf object returned by these methods, so it should be mostly backwards-compatible.
0.62.65 (2024-04-15)
Cloudflare R2 bucket support added to
modal.CloudBucketMount
0.62.55 (2024-04-11)
When Volume reloads fail due to an open file, we now try to identify and report the relevant path. Note that there may be some circumstances in which we are unable to identify the specific file blocking a reload and will report a generic error message in that case.
0.62.53 (2024-04-10)
Values in the
modal.toml
config file that are spelled as
false
"False"
, or
"false"
will now be coerced in Python to
False
, whereas previously only
(as a string) would have the intended effect.
0.62.25 (2024-04-01)
Fixed a recent regression that caused functions using
modal.interact()
to crash.
0.62.15 (2024-03-29)
Queue methods
put_many
get_many
now support an optional
partition
argument (must be specified as a
kwarg
). When specified, users read and write from new partitions of the queue independently.
partition=None
corresponds to the default partition of the queue.
0.62.3 (2024-03-27)
User can now mount S3 buckets using
Requester Pays
. This can be done with
CloudBucketMount(..., requester_pays=True)
0.62.1 (2024-03-27)
Raise an error on
@web_server(startup_timeout=0)
, which is an invalid configuration.
0.62.0 (2024-03-26)
.new()
method has now been deprecated on all Modal objects. It should typically be replaced with
.from_name(...)
in Modal app code, or
.ephemeral()
in scripts that use Modal
Assignment of Modal objects to a
Stub
via subscription (
stub["object"]
) or attribute (
stub.object
) syntax is now deprecated. This syntax was only necessary when using
.new()
0.61
0.61.104 (2024-03-25)
Fixed a bug where images based on
micromamba
could fail to build if requesting Python 3.12 when a different version of Python was being used locally.
0.61.76 (2024-03-19)
Sandbox
LogsReader
is now an asynchronous iterable. It supports the
async for
statement to stream data from the sandbox’s
stdout/stderr
@stub.function
async
my_fn
sandbox = stub.spawn_sandbox(
"bash"
"-c"
"while true; do echo foo; sleep 1; done"
async
message
sandbox.stdout:
print
"Message:
message
Copy
0.61.57 (2024-03-15)
Add the
@web_server
decorator, which exposes a server listening on a container port as a web endpoint.
0.61.56 (2024-03-15)
Allow users to write to the
Sandbox
stdin
with
StreamWriter
@stub.function
my_fn
sandbox = stub.spawn_sandbox(
"bash"
"-c"
"while read line; do echo $line; done"
sandbox.stdin.write(
"foo
sandbox.stdin.write(
"bar
sandbox.stdin.write_eof()
sandbox.stdin.drain()
sandbox.wait()
Copy
0.61.53 (2024-03-15)
Fixed an bug where
Mount
was failing to include symbolic links.
0.61.45 (2024-03-13)
When called from within a container,
modal.experimental.stop_fetching_inputs()
causes it to gracefully exit after the current input has been processed.
0.61.35 (2024-03-12)
@wsgi_app()
decorator now uses a different backend based on
a2wsgi
that streams requests in chunks, rather than buffering the entire request body.
0.61.32 (2024-03-11)
Stubs/apps can now be “composed” from several smaller stubs using
stub.include(...)
. This allows more ergonomic setup of multi-file Modal apps.
0.61.31 (2024-03-08)
Image.extend
method has been deprecated. This is a low-level interface and can be replaced by other
Image
methods that offer more flexibility, such as
Image.from_dockerfile
Image.dockerfile_commands
, or
Image.run_commands
0.61.24 (2024-03-06)
Fixes
modal volume put
to support uploading larger files, beyond 40 GiB.
0.61.22 (2024-03-05)
Modal containers now display a warning message if lingering threads are present at container exit, which prevents runner shutdown.
0.61.17 (2024-03-05)
Bug fix: Stopping an app while a container’s
@exit()
lifecycle methods are being run no longer interrupts the lifecycle methods.
Bug fix: Worker preemptions no longer interrupt a container’s
@exit()
lifecycle method (until 30 seconds later).
Bug fix: Async
@exit()
lifecycle methods are no longer skipped for sync functions.
Bug fix: Stopping a sync function with
allow_concurrent_inputs>1
now actually stops the container. Previously, it would not propagate the signal to worker threads, so they would continue running.
Bug fix: Input-level cancellation no longer skips the
@exit()
lifecycle method.
Improve stability of container entrypoint against race conditions in task cancellation.
0.61.9 (2024-03-05)
Fix issue with pdm where all installed packages would be automounted when using package cache (MOD-2485)
0.61.6 (2024-03-04)
For modal functions/classes with
concurrency_limit < keep_warm
, we’ll raise an exception now. Previously we (silently) respected the
concurrency_limit
parameter.
0.61.1 (2024-03-03)
modal run --interactive
modal run -i
run the app in “interactive mode”. This allows any remote code to connect to the user’s local terminal by calling
modal.interact()
@stub.function
my_fn
modal.interact()
input
print
"Your number is
Copy
This means that you can dynamically start an IPython shell if desired for debugging:
@stub.function
my_fn
modal.interact()
from
IPython
import
embed
embed()
Copy
For convenience, breakpoints automatically call
interact()
@stub.function
my_fn
breakpoint
Copy
0.60
0.60.0 (2024-02-29)
Image.run_function
now allows you to pass args and kwargs to the function. Usage:
my_build_function
name
size
, *,
variant
None
print
"Building
name
size
variant
image = modal.Image.debian_slim().run_function(
my_build_function,
args
"foo"
kwargs
"variant"
"bar"
Copy
0.59
0.59.0 (2024-02-28)
Mounted packages are now deduplicated across functions in the same stub
Mounting of local Python packages are now marked as such in the mount creation output, e.g.
PythonPackage:my_package
Automatic mounting now includes packages outside of the function file’s own directory. Mounted packages are mounted in /root/
0.58
0.58.92 (2024-02-27)
Most errors raised through usage of the CLI will now print a simple error message rather than showing a traceback from inside the
modal
library.
Tracebacks originating from user code will include fewer frames from within
modal
itself.
The new
MODAL_TRACEBACK
environment variable (and
traceback
field in the Modal config file) can override these behaviors so that full tracebacks are always shown.
0.58.90 (2024-02-27)
Fixed a bug that could cause
-based functions to to ignore timeout signals.
0.58.88 (2024-02-26)
volume get
performance is improved for large (> 100MB) files
0.58.79 (2024-02-23)
Support for function parameters in methods decorated with
@exit
has been deprecated. Previously, exit methods were required to accept three arguments containing exception information (akin to
__exit__
in the context manager protocol). However, due to a bug, these arguments were always null. Going forward,
@exit
methods are expected to have no parameters.
0.58.75 (2024-02-23)
Function calls can now be cancelled without killing the container running the inputs. This allows new inputs by different function calls to the same function to be picked up immediately without having to cold-start new containers after cancelling calls.
0.57
0.57.62 (2024-02-21)
InvalidError
is now raised when a lifecycle decorator (
@build
@enter
, or
@exit
) is used in conjunction with
@method
. Previously, this was undefined and could produce confusing failures.
0.57.61 (2024-02-21)
Reduced the amount of context for frames in modal’s CLI framework when showing a traceback.
0.57.60 (2024-02-21)
The “dunder method” approach for class lifecycle management (
__build__
__enter__
__exit__
, etc.) is now deprecated in favor of the modal
@build
@enter
, and
@exit
decorators.
0.57.52 (2024-02-17)
modal token new
modal token set
, the
--no-no-verify
flag has been removed in favor of a
--verify
flag. This remains the default behavior.
0.57.51 (2024-02-17)
Fixes a regression from 0.57.40 where
@enter
methods used a separate event loop.
0.57.42 (2024-02-14)
Adds a new environment variable/config setting,
MODAL_FORCE_BUILD
force_build
, that coerces all images to be built from scratch, rather than loaded from cache.
0.57.40 (2024-02-13)
@enter()
lifecycle method can now be used to run additional setup code prior to function checkpointing (when the class is decorated with
stub.cls(enable_checkpointing=True)
. Note that there are currently some limitations on function checkpointing:
Checkpointing only works for CPU memory; any GPUs attached to the function will not available
Networking is disabled while the checkpoint is being created
Please note that function checkpointing is still a beta feature.
0.57.31 (2024-02-12)
Fixed an issue with displaying deprecation warnings on Windows systems.
0.57.22 (2024-02-09)
Modal client deprecation warnings are now highlighted in the CLI
0.57.16 (2024-02-07)
Fixes a regression in container scheduling. Users on affected versions (
0.57.5
0.57.15
) are encouraged to upgrade immediately.
0.57.15 (2024-02-07)
The legacy
image_python_version
config option has been removed. Use the
python_version=
parameter on your image definition instead.
0.57.13 (2024-02-07)
Adds support for mounting an S3 bucket as a volume.
0.57.9 (2024-02-07)
Support for an implicit ‘default’ profile is now deprecated. If you have more than one profile in your Modal config file, one must be explicitly set to
active
(use
modal profile activate
or edit your
.modal.toml
file to resolve).
An error is now raised when more than one profile is set to
active
0.57.2 (2024-02-06)
Improve error message when generator functions are called with
.map(...)
0.57.0 (2024-02-06)
Greatly improved streaming performance of generators and WebSocket web endpoints.
Breaking change:
You cannot use
.map()
to call a generator function. (In previous versions, this merged the results onto a single stream, but the behavior was undocumented and not widely used.)
Incompatibility:
Generator outputs are now on a different internal system. Modal code on client versions before 0.57 cannot trigger
deployed functions
with
.remote_gen()
that are on client version 0.57, and vice versa.
0.56
Note that in version 0.56 and prior, Modal used a different numbering system for patch releases.
0.56.4964 (2024-02-05)
When using
modal token new
model token set
, the profile containing the new token will now be activated by default. Use the
--no-activate
switch to update the
modal.toml
file without activating the corresponding profile.
0.56.4953 (2024-02-05)
modal profile list
output now indicates when the workspace is determined by a token stored in environment variables.
0.56.4952 (2024-02-05)
Variadic parameters (e.g. *args and **kwargs) can now be used in scheduled functions as long as the function doesn’t have any other parameters without a default value
0.56.4903 (2024-02-01)
modal container exec
--no-tty
flag has been renamed to
--no-pty
0.56.4902 (2024-02-01)
The singular form of the
secret
parameter in
Stub.function
Stub.cls
, and
Image.run_function
has been deprecated. Please update your code to use the plural form instead:
secrets=[Secret(...)]
0.56.4885 (2024-02-01)
modal profile list
, the user’s GitHub username is now shown as the name for the “Personal” workspace.
0.56.4874 (2024-01-31)
modal token new
modal token set
commands now create profiles that are more closely associated with workspaces, and they have more explicit profile activation behavior:
By default, these commands will create/update a profile named after the workspace that the token points to, rather than a profile named “default”
Both commands now have an
--activate
flag that will activate the profile associated with the new token
If no other profiles exist at the time of creation, the new profile will have its
active
metadata set to True
With these changes, we are moving away from the concept of a “default” profile. Implicit usage of the “default” profile will be deprecated in a future update.
0.56.4849 (2024-01-29)
Adds tty support to
modal container exec
for fully-interactive commands. Example:
modal container exec [container-id] /bin/bash
0.56.4792 (2024-01-26)
modal profile list
command now shows the workspace associated with each profile.
0.56.4715 (2024-01-24)
Mount.from_local_python_packages
now places mounted packages at
/root
in the Modal runtime by default (used to be
/pkg
). To override this behavior, the function now takes a
remote_dir: Union[str, PurePosixPath]
argument.
0.56.4707 (2024-01-23)
The Modal client library is now compatible with Python 3.12, although there are a few limitations:
Images that use Python 3.12 without explicitly specifing it through
python_version
add_python
will not build
properly unless the modal client is also running on Python 3.12.
conda
microconda
base images currently do not support Python 3.12 because an upstream dependency is not yet compatible.
0.56.4700 (2024-01-22)
gpu.A100
class now supports specifying GiB memory configuration using a
size: str
parameter. The
memory: int
parameter is deprecated.
0.56.4693 (2024-01-22)
You can now execute commands in running containers with
modal container exec [container-id] [command]
0.56.4691 (2024-01-22)
modal
cli now works more like the
python
cli in regard to script/module loading:
Running
modal my_dir/my_script.py
now puts
my_dir
on the PYTHONPATH.
modal my_package.my_module
will now mount to /root/my_package/my_module.py in your Modal container, regardless if using automounting or not (and any intermediary
__init__.py
files will also be mounted)
0.56.4687 (2024-01-20)
Modal now uses the current profile if
MODAL_PROFILE
is set to the empty string.
0.56.4649 (2024-01-17)
Dropped support for building Python 3.7 based
modal.Image
s. Python 3.7 is end-of-life since late June 2023.
0.56.4620 (2024-01-16)
modal.Stub.function now takes a
block_network
argument.
0.56.4616 (2024-01-16)
modal.Stub now takes a
volumes
argument for setting the default volumes of all the stub’s functions, similarly to the
mounts
secrets
argument.
0.56.4590 (2024-01-13)
modal serve
: Setting MODAL_LOGLEVEL=DEBUG now displays which files cause an app reload during serve
0.56.4570 (2024-01-12)
modal run
cli command now properly propagates
--env
values to object lookups in global scope of user code
Changelog
Latest
1.0.4 (2025-06-13)
1.0.3 (2025-06-05)
1.0.2 (2025-05-26)
1.0.1 (2025-05-19)
1.0.0 (2025-05-16)
0.77
0.77.0 (2025-05-13)
0.76
0.76.3 (2025-05-12)
0.76.0 (2025-05-12)
0.75
0.75.8 (2025-05-12)
0.75.4 (2025-05-09)
0.75.3 (2025-05-08)
0.75.2 (2025-05-08)
0.75.0 (2025-05-08)
0.74
0.74.63 (2025-05-08)
0.74.61 (2025-05-07)
0.74.60 (2025-05-07)
0.74.56 (2025-05-06)
0.74.53 (2025-05-06)
0.74.51 (2025-05-06)
0.74.49 (2025-05-06)
0.74.48 (2025-05-05)
0.74.46 (2025-05-05)
0.74.39 (2025-04-30)
0.74.36 (2025-04-29)
0.74.35 (2025-04-29)
0.74.32 (2025-04-29)
0.74.25 (2025-04-25)
0.74.23 (2025-04-25)
0.74.22 (2025-04-24)
0.74.21 (2025-04-24)
0.74.18 (2025-04-23)
0.74.12 (2025-04-18)
0.74.7 (2025-04-17)
0.74.6 (2025-04-17)
0.74.0 (2025-04-15)
0.73
0.73.173 (2025-04-15)
0.73.170 (2025-04-14)
0.73.165 (2025-04-11)
0.73.160 (2025-04-10)
0.73.159 (2025-04-10)
0.73.158 (2025-04-10)
0.73.148 (2025-04-07)
0.73.147 (2025-04-07)
0.73.144 (2025-04-04)
0.73.139 (2025-04-02)
0.73.135 (2025-03-29)
0.73.132 (2025-03-28)
0.73.131 (2025-03-28)
0.73.121 (2025-03-24)
0.73.119 (2025-03-21)
0.73.115 (2025-03-19)
0.73.107 (2025-03-14)
0.73.105 (2025-03-14)
0.73.95 (2025-03-12)
0.73.89 (2025-03-05)
0.73.84 (2025-03-04)
0.73.82 (2025-03-04)
0.73.81 (2025-03-03)
0.73.78 (2025-03-01)
0.73.77 (2025-03-01)
0.73.76 (2025-02-28)
0.73.75 (2025-02-28)
0.73.73 (2025-02-28)
0.73.69 (2025-02-25)
0.73.68 (2025-02-25)
0.73.60 (2025-02-20)
0.73.58 (2025-02-20)
0.73.54 (2025-02-18)
0.73.51 (2025-02-14)
0.73.44 (2025-02-13)
0.73.40 (2025-02-12)
0.73.31 (2025-02-10)
0.73.26 (2025-02-10)
0.73.25 (2025-02-09)
0.73.18 (2025-02-06)
0.73.14 (2025-02-04)
0.73.11 (2025-02-04)
0.73.10 (2025-02-04)
0.73.1 (2025-01-30)
0.73.0 (2025-01-30)
0.72
0.72.56 (2025-01-28)
0.72.54 (2025-01-28)
0.72.48 (2025-01-24)
0.72.39 (2025-01-22)
0.72.33 (2025-01-20)
0.72.30 (2025-01-18)
0.72.24 (2025-01-17)
0.72.22 (2025-01-17)
0.72.17 (2025-01-16)
0.72.16 (2025-01-16)
0.72.15 (2025-01-15)
0.72.8 (2025-01-10)
0.72.0 (2025-01-09)
0.71
0.71.13 (2025-01-09)
0.71.11 (2025-01-08)
0.71.7 (2025-01-08)
0.71.1 (2025-01-06)
0.70
0.70.1 (2024-12-27)
0.69
0.69.0 (2024-12-21)
0.68
0.68.53 (2024-12-20)
0.68.44 (2024-12-19)
0.68.43 (2024-12-19)
0.68.39 (2024-12-18)
0.68.29 (2024-12-17)
0.68.28 (2024-12-17)
0.68.27 (2024-12-17)
0.68.24 (2024-12-16)
0.68.21 (2024-12-13)
0.68.15 (2024-12-13)
0.68.11 (2024-12-13)
0.68.6 (2024-12-12)
0.68.5 (2024-12-12)
0.68.2 (2024-12-11)
0.67
0.67.43 (2024-12-11)
0.67.39 (2024-12-09)
0.67.38 (2024-12-09)
0.67.28 (2024-12-05)
0.67.23 (2024-12-04)
0.67.22 (2024-12-03)
0.67.12 (2024-12-02)
0.67.7 (2024-11-29)
0.67.0 (2024-11-27)
0.66
0.66.49 (2024-11-26)
0.66.45 (2024-11-26)
0.66.40 (2024-11-23)
0.66.30 (2024-11-21)
0.66.12 (2024-11-19)
0.66.0 (2024-11-15)
0.65
0.65.55 (2024-11-13)
0.65.49 (2024-11-12)
0.65.42 (2024-11-07)
0.65.33 (2024-11-06)
0.65.30 (2024-11-05)
0.65.9 (2024-10-31)
0.65.8 (2024-10-31)
0.65.2 (2024-10-30)
0.64
0.64.227 (2024-10-25)
0.64.223 (2024-10-24)
0.64.218 (2024-10-23)
0.64.198 (2024-10-18)
0.64.194 (2024-10-18)
0.64.193 (2024-10-18)
0.64.187 (2024-10-16)
0.64.185 (2024-10-15)
0.64.184 (2024-10-15)
0.64.182 (2024-10-15)
0.64.181 (2024-10-14)
0.64.180 (2024-10-14)
0.64.168 (2024-10-03)
0.64.153 (2024-09-30)
0.64.142 (2024-09-25)
0.64.139 (2024-09-25)
0.64.123 (2024-09-18)
0.64.119 (2024-09-17)
0.64.118 (2024-09-17)
0.64.112 (2024-09-15)
0.64.109 (2024-09-13)
0.64.100 (2024-09-11)
0.64.99 (2024-09-11)
0.64.97 (2024-09-11)
0.64.87 (2024-09-05)
0.64.67 (2024-08-30)
0.64.48 (2024-08-21)
0.64.38 (2024-08-16)
0.64.33 (2024-08-16)
0.64.32 (2024-08-16)
0.64.26 (2024-08-15)
0.64.24 (2024-08-14)
0.64.18 (2024-08-12)
0.64.8 (2024-08-06)
0.64.7 (2024-08-05)
0.64.2 (2024-08-02)
0.64.0 (2024-07-29)
0.63
0.63.87 (2024-07-24)
0.63.77 (2024-07-18)
0.63.36 (2024-07-05)
0.63.22 (2024-07-01)
0.63.20 (2024-07-01)
0.63.14 (2024-06-28)
0.63.12 (2024-06-27)
0.63.10 (2024-06-26)
0.63.9 (2024-06-26)
0.63.2 (2024-06-25)
0.63.0 (2024-06-24)
0.62
0.62.236 (2024-06-21)
0.62.230 (2024-06-18)
0.62.224 (2024-06-17)
0.62.223 (2024-06-14)
0.62.220 (2024-06-12)
0.62.219 (2024-06-12)
0.62.208 (2024-06-08)
0.62.201 (2024-06-04)
0.62.199 (2024-06-04)
0.62.190 (2024-05-29)
0.62.186 (2024-05-29)
0.62.185 (2024-05-28)
0.62.181 (2024-05-24)
0.62.178 (2024-05-21)
0.62.175 (2024-05-17)
0.62.174 (2024-05-17)
0.62.173 (2024-05-17)
0.62.172 (2024-05-17)
0.62.166 (2024-05-14)
0.62.165 (2024-05-13)
0.62.162 (2024-05-13)
0.62.159 (2024-05-10)
0.62.156 (2024-05-09)
0.62.150 (2024-05-08)
0.62.149 (2024-05-08)
0.62.148 (2024-05-08)
0.62.146 (2024-05-07)
0.62.144 (2024-05-06)
0.62.141 (2024-05-03)
0.62.131 (2024-05-01)
0.62.130 (2024-05-01)
0.62.116 (2024-04-26)
0.62.114 (2024-04-25)
0.62.110 (2024-04-25)
0.62.109 (2024-04-24)
0.62.108 (2024-04-24)
0.62.98 (2024-04-21)
0.62.81 (2024-04-18)
0.62.72 (2024-04-16)
0.62.70 (2024-04-16)
0.62.69 (2024-04-16)
0.62.67 (2024-04-15)
0.62.65 (2024-04-15)
0.62.55 (2024-04-11)
0.62.53 (2024-04-10)
0.62.25 (2024-04-01)
0.62.15 (2024-03-29)
0.62.3 (2024-03-27)
0.62.1 (2024-03-27)
0.62.0 (2024-03-26)
0.61
0.61.104 (2024-03-25)
0.61.76 (2024-03-19)
0.61.57 (2024-03-15)
0.61.56 (2024-03-15)
0.61.53 (2024-03-15)
0.61.45 (2024-03-13)
0.61.35 (2024-03-12)
0.61.32 (2024-03-11)
0.61.31 (2024-03-08)
0.61.24 (2024-03-06)
0.61.22 (2024-03-05)
0.61.17 (2024-03-05)
0.61.9 (2024-03-05)
0.61.6 (2024-03-04)
0.61.1 (2024-03-03)
0.60
0.60.0 (2024-02-29)
0.59
0.59.0 (2024-02-28)
0.58
0.58.92 (2024-02-27)
0.58.90 (2024-02-27)
0.58.88 (2024-02-26)
0.58.79 (2024-02-23)
0.58.75 (2024-02-23)
0.57
0.57.62 (2024-02-21)
0.57.61 (2024-02-21)
0.57.60 (2024-02-21)
0.57.52 (2024-02-17)
0.57.51 (2024-02-17)
0.57.42 (2024-02-14)
0.57.40 (2024-02-13)
0.57.31 (2024-02-12)
0.57.22 (2024-02-09)
0.57.16 (2024-02-07)
0.57.15 (2024-02-07)
0.57.13 (2024-02-07)
0.57.9 (2024-02-07)
0.57.2 (2024-02-06)
0.57.0 (2024-02-06)
0.56
0.56.4964 (2024-02-05)
0.56.4953 (2024-02-05)
0.56.4952 (2024-02-05)
0.56.4903 (2024-02-01)
0.56.4902 (2024-02-01)
0.56.4885 (2024-02-01)
0.56.4874 (2024-01-31)
0.56.4849 (2024-01-29)
0.56.4792 (2024-01-26)
0.56.4715 (2024-01-24)
0.56.4707 (2024-01-23)
0.56.4700 (2024-01-22)
0.56.4693 (2024-01-22)
0.56.4691 (2024-01-22)
0.56.4687 (2024-01-20)
0.56.4649 (2024-01-17)
0.56.4620 (2024-01-16)
0.56.4616 (2024-01-16)
0.56.4590 (2024-01-13)
0.56.4570 (2024-01-12)

## 017_GUIDE_WEBHOOKS
Introduction
Custom container images
Defining Images
Private registries
Fast pull from registry
GPUs and other resources
GPU acceleration
Using CUDA on Modal
Reserving CPU and memory
Scaling out
Scaling out
Input concurrency
Batch processing
Job queues
Dynamic batching (beta)
Dicts and queues
Scheduling and cron jobs
Deployment
Apps, Functions, and entrypoints
Managing deployments
Invoking deployed functions
Continuous deployment
Running untrusted code in Functions
Secrets and environment variables
Secrets
Environment variables
Web endpoints
Web endpoints
Streaming endpoints
Web endpoint URLs
Request timeouts
Webhook tokens (beta)
Networking
Tunnels (beta)
Proxies (beta)
Cluster networking
Data sharing and storage
Passing local data
Volumes
Storing model weights
Dataset ingestion
Cloud bucket mounts
Sandboxes
Sandboxes
Running commands
Networking and security
File access
Snapshots
Performance
Cold start performance
Memory Snapshot (beta)
Geographic latency
Reliability and robustness
Failures and retries
Preemption
Timeouts
Troubleshooting
Security and privacy
Integrations
Using OIDC to authenticate with external services
Connecting Modal to your Datadog account
Connecting Modal to your OpenTelemetry provider
Okta SSO
Slack notifications (beta)
Workspace & account settings
Workspaces
Environments
Modal user account setup
Service users
Other topics
Modal 1.0 migration guide
File and project structure
Developing and debugging
Jupyter notebooks
Asynchronous API usage
Global variables
Region selection
Container lifecycle hooks
Parametrized functions
S3 Gateway endpoints
GPU Metrics
Web endpoints
This guide explains how to set up web endpoints with Modal.
All deployed Modal Functions can be
invoked from any other Python application
using the Modal client library. We additionally provide multiple ways to expose
your Functions over the web for non-Python clients.
You can
turn any Python function into a web endpoint
with a single line
of code, you can
serve a full app
using
frameworks like FastAPI, Django, or Flask, or you can
serve anything that speaks HTTP and listens on a port
Below we walk through each method, assuming you’re familiar with web applications outside of Modal.
For a detailed walkthrough of basic web endpoints on Modal aimed at developers new to web applications,
this tutorial
Simple endpoints
The easiest way to create a web endpoint from an existing Python function is to use the
@modal.fastapi_endpoint
decorator
image = modal.Image.debian_slim().pip_install(
"fastapi[standard]"
@app.function
image
=image)
@modal.fastapi_endpoint
return
"Hello world!"
Copy
This decorator wraps the Modal Function in a
FastAPI application
Note: Prior to v0.73.82, this function was named
@modal.web_endpoint
Developing with
modal serve
You can run this code as an ephemeral app, by running the command
modal
serve
server_script.py
Copy
Where
server_script.py
is the file name of your code. This will create an
ephemeral app for the duration of your script (until you hit Ctrl-C to stop it).
It creates a temporary URL that you can use like any other REST endpoint. This
URL is on the public internet.
modal serve
command will live-update an app when any of its supporting
files change.
Live updating is particularly useful when working with apps containing web
endpoints, as any changes made to web endpoint handlers will show up almost
immediately, without requiring a manual restart of the app.
Deploying with
modal deploy
You can also deploy your app and create a persistent web endpoint in the cloud
by running
modal deploy
--:--
--:--
Keyboard shortcuts (?)
Fullscreen (f)
Passing arguments to an endpoint
When using
@modal.fastapi_endpoint
, you can add
query parameters
which
will be passed to your Function as arguments. For instance
image = modal.Image.debian_slim().pip_install(
"fastapi[standard]"
@app.function
image
=image)
@modal.fastapi_endpoint
square
return
"square"
: x**
Copy
If you hit this with a URL-encoded query string with the
parameter present,
the Function will receive the value as an argument:
$ curl https://modal-labs--web-endpoint-square-dev.modal.run?x=42
{"square":1764}
Copy
If you want to use a
POST
request, you can use the
method
argument to
@modal.fastapi_endpoint
to set the HTTP verb. To accept any valid JSON object,
dict
as your type annotation
and FastAPI will handle the rest.
image = modal.Image.debian_slim().pip_install(
"fastapi[standard]"
@app.function
image
=image)
@modal.fastapi_endpoint
method
"POST"
square
item
dict
return
"square"
: item[
Copy
This now creates an endpoint that takes a JSON body:
$ curl -X POST -H 'Content-Type: application/json' --data-binary '{"x": 42}' https://modal-labs--web-endpoint-square-dev.modal.run
{"square":1764}
Copy
This is often the easiest way to get started, but note that FastAPI recommends
that you use
typed Pydantic models
in order to
get automatic validation and documentation. FastAPI also lets you pass data to
web endpoints in other ways, for instance as
form data
file uploads
How do web endpoints run in the cloud?
Note that web endpoints, like everything else on Modal, only run when they need
to. When you hit the web endpoint the first time, it will boot up the container,
which might take a few seconds. Modal keeps the container alive for a short
period in case there are subsequent requests. If there are a lot of requests,
Modal might create more containers running in parallel.
For the shortcut
@modal.fastapi_endpoint
decorator, Modal wraps your function in a
FastAPI
application. This means that the
Image
your Function uses must have FastAPI installed, and the Functions that you write
need to follow its request and response
semantics
. Web endpoint Functions can use
all of FastAPI’s powerful features, such as Pydantic models for automatic validation,
typed query and path parameters, and response types.
Here’s everything together, combining Modal’s abilities to run functions in
user-defined containers with the expressivity of FastAPI:
import
modal
from
fastapi.responses
import
HTMLResponse
from
pydantic
import
BaseModel
image = modal.Image.debian_slim().pip_install(
"fastapi[standard]"
"boto3"
app = modal.App(
image
=image)
class
Item
BaseModel
name:
qty:
@app.function
@modal.fastapi_endpoint
method
"POST"
item
: Item):
import
boto3
## do things with boto3...
return
HTMLResponse(
"<html>Hello,
item.name
!</html>"
Copy
This endpoint definition would be called like so:
curl
'{"name": "Erik", "qty": 10}'
"Content-Type: application/json"
POST
https://ecorp--web-demo-f-dev.modal.run
Copy
Or in Python with the
requests
library:
import
requests
data = {
"name"
"Erik"
"qty"
requests.post(
"https://ecorp--web-demo-f-dev.modal.run"
json
=data,
timeout
10.0
Copy
Serving ASGI and WSGI apps
You can also serve any app written in an
ASGI
WSGI
-compatible
web framework on Modal.
ASGI provides support for async web frameworks. WSGI provides support for
synchronous web frameworks.
ASGI apps - FastAPI, FastHTML, Starlette
For ASGI apps, you can create a function decorated with
@modal.asgi_app
that returns a reference to
your web app:
image = modal.Image.debian_slim().pip_install(
"fastapi[standard]"
@app.function
image
=image)
@modal.concurrent
max_inputs
@modal.asgi_app
fastapi_app
from
fastapi
import
FastAPI, Request
web_app = FastAPI()
@web_app.post
"/echo"
async
echo
request
: Request):
body =
await
request.json()
return
body
return
web_app
Copy
Now, as before, when you deploy this script as a Modal App, you get a URL for
your app that you can hit:
--:--
--:--
Keyboard shortcuts (?)
Fullscreen (f)
@modal.concurrent
decorator enables a single container
to process multiple inputs at once, taking advantage of the asynchronous
event loops in ASGI applications. See
this guide
for details.
ASGI Lifespan
While we recommend using
@modal.enter
for defining container lifecycle hooks, we also support the
ASGI lifespan protocol
. Lifespans begin when containers start, typically at the time of the first request. Here’s an example using
FastAPI
import
modal
app = modal.App(
"fastapi-lifespan-app"
image = modal.Image.debian_slim().pip_install(
"fastapi[standard]"
@app.function
image
=image)
@modal.asgi_app
fastapi_app_with_lifespan
from
fastapi
import
FastAPI, Request
lifespan
wapp
: FastAPI):
print
"Starting"
yield
print
"Shutting down"
web_app = FastAPI(
lifespan
=lifespan)
@web_app.get
async
hello
request
: Request):
return
"hello"
return
web_app
Copy
WSGI apps - Django, Flask
You can serve WSGI apps using the
@modal.wsgi_app
decorator:
image = modal.Image.debian_slim().pip_install(
"flask"
@app.function
image
=image)
@modal.concurrent
max_inputs
@modal.wsgi_app
flask_app
from
flask
import
Flask, request
web_app = Flask(
__name__
@web_app.post
"/echo"
echo
return
request.json
return
web_app
Copy
Flask’s docs
for more information on using Flask as a WSGI app.
Because WSGI apps are synchronous, concurrent inputs will be run on separate
threads. See
this guide
for details.
Non-ASGI web servers
Not all web frameworks offer an ASGI or WSGI interface. For example,
aiohttp
tornado
use their own asynchronous network binding, while others like
text-generation-inference
actually expose a Rust-based HTTP server running as a subprocess.
For these cases, you can use the
@modal.web_server
decorator to “expose” a
port on the container:
@app.function
@modal.concurrent
max_inputs
@modal.web_server
8000
my_file_server
import
subprocess
subprocess.Popen(
"python -m http.server -d / 8000"
shell
True
Copy
Just like all web endpoints on Modal, this is only run on-demand. The function
is executed on container startup, creating a file server at the root directory.
When you hit the web endpoint URL, your request will be routed to the file
server listening on port
8000
@web_server
endpoints, you need to make sure that the application binds to
the external network interface, not just localhost. This usually means binding
0.0.0.0
instead of
127.0.0.1
See our examples of how to serve
Streamlit
ComfyUI
on Modal.
Serve many configurations with parametrized functions
Python functions that launch ASGI/WSGI apps or web servers on Modal
cannot take arguments.
One simple pattern for allowing client-side configuration of these web endpoints
is to use
parametrized functions
Each different choice for the values of the parameters will create a distinct
auto-scaling container pool.
@app.cls
@modal.concurrent
max_inputs
class
Server
root:
= modal.parameter(
default
@modal.web_server
8000
files
self
import
subprocess
subprocess.Popen(
"python -m http.server -d
{self
.root
8000"
shell
True
Copy
The values are provided in URLs as query parameters:
curl
https://ecorp--server-files.modal.run
## use the default value
curl
https://ecorp--server-files.modal.run?root=.cache
## use a different value
curl
https://ecorp--server-files.modal.run?root=%2F
## don't forget to URL encode!
Copy
For details, see
this guide to parametrized functions
WebSockets
Functions annotated with
@web_server
@asgi_app
, or
@wsgi_app
also support
the WebSocket protocol. Consult your web framework for appropriate documentation
on how to use WebSockets with that library.
WebSockets on Modal maintain a single function call per connection, which can be
useful for keeping state around. Most of the time, you will want to set your
handler function to
allow concurrent inputs
which allows multiple simultaneous WebSocket connections to be handled by the
same container.
We support the full WebSocket protocol as per
RFC 6455
, but we do not yet have
support for
RFC 8441
(WebSockets over
HTTP/2) or
RFC 7692
permessage-deflate
extension). WebSocket messages can be up to 2 MiB each.
Performance and scaling
If you have no active containers when the web endpoint receives a request, it will
experience a “cold start”. Consult the guide page on
cold start performance
for more information on when
Functions will cold start and advice how to mitigate the impact.
If your Function uses
@modal.concurrent
, multiple requests to the same
endpoint may be handled by the same container. Beyond this limit, additional
containers will start up to scale your App horizontally. When you reach the
Function’s limit on containers, requests will queue for handling.
Each workspace on Modal has a rate limit on total operations. For a new account,
this is set to 200 function inputs or web endpoint requests per second, with a
burst multiplier of 5 seconds. If you reach the rate limit, excess requests to
web endpoints will return a
429 status code
and you’ll need to
get in touch
with us about
raising the limit.
Web endpoint request bodies can be up to 4 GiB, and their response bodies are
unlimited in size.
Authentication
Modal offers first-class web endpoint protection via
proxy auth tokens
Proxy auth tokens protect web endpoints by requiring a key and token combination to be passed
in the
Modal-Key
Modal-Secret
headers.
Modal works as a proxy, rejecting requests that aren’t authorized to access
your endpoint.
We also support standard techniques for securing web servers.
Token-based authentication
This is easy to implement in whichever framework you’re using. For example, if
you’re using
@modal.fastapi_endpoint
@modal.asgi_app
with FastAPI, you
can validate a Bearer token like this:
from
fastapi
import
Depends, HTTPException, status, Request
from
fastapi.security
import
HTTPBearer, HTTPAuthorizationCredentials
import
modal
image = modal.Image.debian_slim().pip_install(
"fastapi[standard]"
app = modal.App(
"auth-example"
image
=image)
auth_scheme = HTTPBearer()
@app.function
secrets
=[modal.Secret.from_name(
"my-web-auth-token"
@modal.fastapi_endpoint
async
request
: Request,
token
: HTTPAuthorizationCredentials = Depends(auth_scheme)):
import
print
(os.environ[
"AUTH_TOKEN"
token.credentials != os.environ[
"AUTH_TOKEN"
raise
HTTPException(
status_code
=status.HTTP_401_UNAUTHORIZED,
detail
"Incorrect bearer token"
headers
"WWW-Authenticate"
"Bearer"
## Function body
return
"success!"
Copy
This assumes you have a
Modal Secret
named
my-web-auth-token
created, with contents
{AUTH_TOKEN: secret-random-token}
Now, your endpoint will return a 401 status code except when you hit it with the
correct
Authorization
header set (note that you have to prefix the token with
Bearer
curl
--header
"Authorization: Bearer secret-random-token"
https://modal-labs--auth-example-f.modal.run
Copy
Client IP address
You can access the IP address of the client making the request. This can be used
for geolocation, whitelists, blacklists, and rate limits.
from
fastapi
import
Request
import
modal
image = modal.Image.debian_slim().pip_install(
"fastapi[standard]"
app = modal.App(
image
=image)
@app.function
@modal.fastapi_endpoint
get_ip_address
request
: Request):
return
"Your IP address is
request.client.host
Copy
Web endpoints
Simple endpoints
Developing with modal serve
Deploying with modal deploy
Passing arguments to an endpoint
How do web endpoints run in the cloud?
Serving ASGI and WSGI apps
ASGI apps - FastAPI, FastHTML, Starlette
ASGI Lifespan
WSGI apps - Django, Flask
Non-ASGI web servers
Serve many configurations with parametrized functions
WebSockets
Performance and scaling
Authentication
Token-based authentication
Client IP address
Fully featured web apps
LLM Voice Chat (React)
Stable Diffusion (Alpine)
Whisper Podcast Transcriber (React)

## 018_EXAMPLES_AGENT
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Build a coding agent with Modal Sandboxes and LangGraph
This example demonstrates how to build an LLM coding “agent” that can generate and execute Python code, using
documentation from the web to inform its approach.
Naturally, we use the agent to generate code that runs language models.
The agent is built with
LangGraph
, a library for building
directed graphs of computation popular with AI agent developers,
and uses models from the OpenAI API.
Setup
import
modal
from
.src
import
edges, nodes, retrieval
from
.src.common
import
COLOR, PYTHON_VERSION, image
Copy
You will need two
Modal Secrets
to run this example:
one to access the OpenAI API and another to access the LangSmith API for logging the agent’s behavior.
To create them, head to the
Secrets dashboard
, select “Create new secret”,
and use the provided templates for OpenAI and LangSmith.
app = modal.App(
"example-code-langchain"
image
=image,
secrets
modal.Secret.from_name(
"openai-secret"
required_keys
"OPENAI_API_KEY"
modal.Secret.from_name(
"langsmith-secret"
required_keys
"LANGCHAIN_API_KEY"
Copy
Creating a Sandbox
We execute the agent’s code in a Modal
Sandbox
, which allows us to
run arbitrary code in a safe environment. In this example, we will use the
transformers
library to generate text with a pre-trained model. Let’s create a Sandbox with the necessary dependencies.
create_sandbox
) -> modal.Sandbox:
## Change this image (and the retrieval logic in the retrieval module)
## if you want the agent to give coding advice on other libraries!
agent_image = modal.Image.debian_slim(
python_version
=PYTHON_VERSION).pip_install(
"torch==2.5.0"
"transformers==4.46.0"
return
modal.Sandbox.create(
image
=agent_image,
timeout
## 10 minutes
=app,
## Modal sandboxes support GPUs!
"T4"
## you can also pass secrets here -- note that the main app's secrets are not shared
Copy
We also need a way to run our code in the sandbox. For this, we’ll write a simple wrapper
around the Modal Sandox
exec
method. We use
exec
because it allows us to run code without spinning up a
new container. And we can reuse the same container for multiple runs, preserving state.
code
: modal.Sandbox) -> tuple[
print
COLOR[
'HEADER'
📦: Running in sandbox
COLOR[
'ENDC'
COLOR[
'GREEN'
code
COLOR[
'ENDC'
exc = sb.exec(
"python"
"-c"
, code)
exc.wait()
stdout = exc.stdout.read()
stderr = exc.stderr.read()
exc.returncode !=
print
COLOR[
'HEADER'
📦: Failed with exitcode
sb.returncode
COLOR[
'ENDC'
return
stdout, stderr
Copy
Constructing the agent’s graph
Now that we have the sandbox to execute code in, we can construct our agent’s graph. Our graph is
defined in the
edges
nodes
modules
associated with this example
Nodes are actions that change the state. Edges are transitions between nodes.
The idea is simple: we start at the node
generate
, which invokes the LLM to generate code based off documentation.
The generated code is executed (in the sandbox) as part of an edge called
check_code_execution
and then the outputs are passed to the LLM for evaluation (the
evaluate_execution
node).
If the LLM determines that the code has executed correctly — which might mean that the code raised an exception! —
we pass along the
decide_to_finish
edge and finish.
construct_graph
sandbox
: modal.Sandbox,
debug
bool
False
from
langgraph.graph
import
StateGraph
from
.src.common
import
GraphState
## Crawl the transformers documentation to inform our code generation
context = retrieval.retrieve_docs(
debug
=debug)
graph = StateGraph(GraphState)
## Attach our nodes to the graph
graph_nodes = nodes.Nodes(context, sandbox, run,
debug
=debug)
key, value
graph_nodes.node_map.items():
graph.add_node(key, value)
## Construct the graph by adding edges
graph = edges.enrich(graph)
## Set the starting and ending nodes of the graph
graph.set_entry_point(
"generate"
graph.set_finish_point(
"finish"
return
graph
Copy
We now set up the graph and compile it. See the
module for details
on the content of the graph and the nodes we’ve defined.
DEFAULT_QUESTION =
"How do I generate Python code using a pre-trained model from the transformers library?"
@app.function
question
= DEFAULT_QUESTION,
debug
bool
False
"""Compiles the Python code generation agent graph and runs it, returning the result."""
sb = create_sandbox(app)
graph = construct_graph(sb,
debug
=debug)
runnable = graph.compile()
result = runnable.invoke(
"keys"
"question"
: question,
"iterations"
config
"recursion_limit"
sb.terminate()
return
result[
"keys"
"response"
Copy
Running the Graph
Now let’s call the agent from the command line!
We define a
local_entrypoint
that runs locally and triggers execution on Modal.
You can invoke it by executing following command from a folder that contains the
codelangchain
directory
from our examples repo
modal
codelangchain.agent
--question
"How do I run a pre-trained model from the transformers library?"
Copy
@app.local_entrypoint
main
question
= DEFAULT_QUESTION,
debug
bool
False
"""Sends a question to the Python code generation agent.
Switch to debug mode for shorter context and smaller model."""
debug:
question == DEFAULT_QUESTION:
question =
"hi there, how are you?"
print
(go.remote(question,
debug
=debug))
Copy
If things are working properly, you should see output like the following:
modal
codelangchain.agent
--question
"generate some cool output with transformers"
---DECISION:
FINISH---
---FINISHING---
generate
some
cool
output
using
transformers,
pre-trained
language
model
from
Hugging
Face
Transformers
library.
this
example,
we'll use the GPT-2 model to generate text based on a given prompt. The GPT-2 model is a popular choice for text generation tasks due to its ability to produce coherent and contextually relevant text. We'll
pipeline
from
Transformers
library,
which
simplifies
process
using
pre-trained
models
various
tasks,
including
text
generation.
from
transformers
import
pipeline
## Initialize the text generation pipeline with the GPT-2 model
generator
pipeline
'text-generation'
model='gpt2'
## Define a prompt for the model to generate text from
prompt
"Once upon a time in a land far, far away"
## Generate text using the model
output
generator
prompt,
max_length=50,
num_return_sequences=
## Print the generated text
print
output[0][
'generated_text'
Result
code
execution:
Once
upon
time
land
far,
away,
still
inhabited
even
after
human
race,
there
would
God:
perfect
universal
always
been
will
ever
worshipped.
acts
deeds
immutable,
Copy
Build a coding agent with Modal Sandboxes and LangGraph
Setup
Creating a Sandbox
Constructing the agent’s graph
Running the Graph
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
13_sandboxes.codelangchain.agent
--question
'Use gpt2 and transformers to generate text'
Copy

## 019_GUIDE
Introduction
Custom container images
Defining Images
Private registries
Fast pull from registry
GPUs and other resources
GPU acceleration
Using CUDA on Modal
Reserving CPU and memory
Scaling out
Scaling out
Input concurrency
Batch processing
Job queues
Dynamic batching (beta)
Dicts and queues
Scheduling and cron jobs
Deployment
Apps, Functions, and entrypoints
Managing deployments
Invoking deployed functions
Continuous deployment
Running untrusted code in Functions
Secrets and environment variables
Secrets
Environment variables
Web endpoints
Web endpoints
Streaming endpoints
Web endpoint URLs
Request timeouts
Webhook tokens (beta)
Networking
Tunnels (beta)
Proxies (beta)
Cluster networking
Data sharing and storage
Passing local data
Volumes
Storing model weights
Dataset ingestion
Cloud bucket mounts
Sandboxes
Sandboxes
Running commands
Networking and security
File access
Snapshots
Performance
Cold start performance
Memory Snapshot (beta)
Geographic latency
Reliability and robustness
Failures and retries
Preemption
Timeouts
Troubleshooting
Security and privacy
Integrations
Using OIDC to authenticate with external services
Connecting Modal to your Datadog account
Connecting Modal to your OpenTelemetry provider
Okta SSO
Slack notifications (beta)
Workspace & account settings
Workspaces
Environments
Modal user account setup
Service users
Other topics
Modal 1.0 migration guide
File and project structure
Developing and debugging
Jupyter notebooks
Asynchronous API usage
Global variables
Region selection
Container lifecycle hooks
Parametrized functions
S3 Gateway endpoints
GPU Metrics
Introduction
Modal is a cloud function platform that lets you:
Run any code remotely within seconds.
Define
container environments
in code (or use one of our pre-built backends).
Scale out horizontally
to thousands of containers.
Attach
GPUs
with a single line of code.
Serve your functions as
web endpoints
Deploy and monitor
persistent scheduled jobs
Use powerful primitives like
distributed dictionaries and queues
You get
full serverless execution and pricing
, because we host everything and charge per second of usage. Notably, there’s zero configuration in Modal - everything is code. Take a breath of fresh air and feel how good it tastes with no YAML in it.
Getting started
The nicest thing about all of this is that
you don’t have to set up any
infrastructure.
Just:
Create an account at
modal.com
pip install modal
to install the
modal
Python package
modal setup
to authenticate (if this doesn’t work, try
python -m modal setup
…and you can start running jobs right away. Check out some of our simple getting started examples:
Hello, world!
A simple web scraper
You can also learn Modal interactively without installing anything through our
code playground
How does it work?
Modal takes your code, puts it in a container, and executes it in the cloud.
Where does it run? Modal runs it in its own cloud environment. The benefit is
that we solve all the hard infrastructure problems for you, so you don’t have to
do anything. You don’t need to mess with Kubernetes, Docker or even an AWS
account.
Modal is currently Python-only, but we may support other languages in the
future.
Introduction
Getting started
How does it work?
See it in action
Hello, world!
A simple web scraper

## 020_EXAMPLES_WHISPER-TRANSCRIBER
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
Parallel podcast transcription using Whisper
This example shows how to build a massively parallel application on Modal:
Modal Podcast Transcriber
This example application is more feature-packed than others, and it doesn’t fit in
a single page of code and commentary. So instead of progressing through the
example’s code linearly, this document provides a higher-level walkthrough of how
Modal is used to do fast, on-demand podcast episode transcription for whichever
podcast you’d like.
You can find the code
here
Hour-long episodes transcribed in just 1 minute
The focal point of this demonstration app is that it does serverless CPU
transcription across dozens of containers at the click of a button, completing
hour-long audio files in just 1 minute.
We use a podcast metadata API to allow users to transcribe an arbitrary episode
from whatever niche podcast they desire —
how about
The Pen Addict
, a podcast dedicated to stationery
The video below shows the 45-minute long first episode of
Serial
season 2
transcribed in 62 seconds.
Each transcription segment includes links back to the original audio.
Try it yourself
If you’re itching to see this in action, here are links to begin transcribing
three popular podcasts:
Case 63
by Gimlet Media
The Joe Rogan Experience
The Psychology of your 20s
Tech-stack overview
The entire application is hosted serverlessly on Modal and consists of these
main components:
A React +
Vite
single page application (SPA) deployed
as static files into a Modal web endpoint.
A Python backend running
FastAPI
in a Modal web endpoint.
Podchaser API
provides
podcast search and episode metadata retrieval. It’s hooked into our code with
Modal Secret
A Modal async job queue, described in more detail below.
All of this is deployed with one command and costs
$0.00
when it’s not
transcribing podcasts or serving HTTP requests.
Speed-boosting Whisper with parallelism
Modal’s dead-simple parallelism primitives are the key to doing the
transcription so quickly. Even with a GPU, transcribing a full episode serially
was taking around 10 minutes.
But by pulling in
ffmpeg
with a simple
.pip_install("ffmpeg-python")
addition to our Modal Image, we could exploit the natural silences of the
podcast medium to partition episodes into hundreds of short segments. Each
segment is transcribed by Whisper in its own container task,
and when all are done we stitch the segments back together with only a
minimal loss in transcription quality. This approach actually accords quite well
with Whisper’s model architecture:
“The Whisper architecture is a simple end-to-end approach, implemented as an
encoder-decoder Transformer. Input audio is split into 30-second chunks,
converted into a log-Mel spectrogram, and then passed into an encoder.”
Introducing Whisper
Run this app on Modal
All source code for this example can be
found on GitHub
README.md
includes instructions on setting up the frontend build and
getting authenticated with the Podchaser API. Happy transcribing!
Parallel podcast transcription using Whisper
Hour-long episodes transcribed in just 1 minute
Try it yourself
Tech-stack overview
Speed-boosting Whisper with parallelism
Run this app on Modal

## 021_EXAMPLES_POTUS_SPEECH_QANDA
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Retrieval-augmented generation (RAG) for question-answering with LangChain
In this example we create a large-language-model (LLM) powered question answering
web endpoint and CLI. Only a single document is used as the knowledge-base of the application,
the 2022 USA State of the Union address by President Joe Biden. However, this same application structure
could be extended to do question-answering over all State of the Union speeches, or other large text corpuses.
It’s the
LangChain
library that makes this all so easy.
This demo is only around 100 lines of code!
Defining dependencies
The example uses packages to implement scraping, the document parsing & LLM API interaction, and web serving.
These are installed into a Debian Slim base image using the
pip_install
method.
Because OpenAI’s API is used, we also specify the
openai-secret
Modal Secret, which contains an OpenAI API key.
retriever
global variable is also declared to facilitate caching a slow operation in the code below.
from
pathlib
import
Path
import
modal
image = modal.Image.debian_slim(
python_version
"3.11"
).pip_install(
## scraping pkgs
"beautifulsoup4~=4.11.1"
"httpx==0.23.3"
"lxml~=4.9.2"
## llm pkgs
"faiss-cpu~=1.7.3"
"langchain==0.3.7"
"langchain-community==0.3.7"
"langchain-openai==0.2.9"
"openai~=1.54.0"
"tiktoken==0.8.0"
## web app packages
"fastapi[standard]==0.115.4"
"pydantic==2.9.2"
"starlette==0.41.2"
app = modal.App(
name
"example-langchain-qanda"
image
=image,
secrets
=[modal.Secret.from_name(
"openai-secret"
required_keys
"OPENAI_API_KEY"
])],
retriever =
None
## embedding index that's relatively expensive to compute, so caching with global var.
Copy
Scraping the speech
It’s super easy to scrape the transcipt of Biden’s speech using
httpx
BeautifulSoup
This speech is just one document and it’s relatively short, but it’s enough to demonstrate
the question-answering capability of the LLM chain.
scrape_state_of_the_union
() ->
import
httpx
from
import
BeautifulSoup
url =
"https://www.presidency.ucsb.edu/documents/address-before-joint-session-the-congress-the-state-the-union-28"
## fetch article; simulate desktop browser
headers = {
"User-Agent"
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9"
response = httpx.get(url,
headers
=headers)
soup = BeautifulSoup(response.text,
"lxml"
## locate the div containing the speech
speech_div = soup.find(
"div"
class_
"field-docs-content"
speech_div:
speech_text = speech_div.get_text(
separator
strip
True
speech_text:
raise
ValueError
"error parsing speech text from HTML"
else
raise
ValueError
"error locating speech in HTML"
return
speech_text
Copy
Constructing the Q&A chain
At a high-level, this LLM chain will be able to answer questions asked about Biden’s speech and provide
references to which parts of the speech contain the evidence for given answers.
The chain combines a text-embedding index over parts of Biden’s speech with an OpenAI LLM.
The index is used to select the most likely relevant parts of the speech given the question, and these
are used to build a specialized prompt for the OpenAI language model.
qanda_langchain
query
) -> tuple[
, list[
from
langchain.chains
import
create_retrieval_chain
from
langchain.chains.combine_documents
import
create_stuff_documents_chain
from
langchain.text_splitter
import
CharacterTextSplitter
from
langchain_community.vectorstores
import
FAISS
from
langchain_core.prompts
import
ChatPromptTemplate
from
langchain_openai
import
ChatOpenAI, OpenAIEmbeddings
## Support caching speech text on disk.
speech_file_path = Path(
"state-of-the-union.txt"
speech_file_path.exists():
state_of_the_union = speech_file_path.read_text()
else
print
"scraping the 2022 State of the Union speech"
state_of_the_union = scrape_state_of_the_union()
speech_file_path.write_text(state_of_the_union)
## Questions about a document can often be answered
## using a much smaller subset, and passing more tokens
## to an LLM increases cost and latency.
## So here, we split up the speech into smaller chunks.
text_splitter = CharacterTextSplitter(
chunk_size
1000
chunk_overlap
print
"splitting speech into text chunks"
texts = text_splitter.split_text(state_of_the_union)
## Embedding-based query<->text similarity comparison is used to select
## a small subset of the speech text chunks.
## Generating the `retriever` and index is too slow to re-run on every request,
## so we do rudimentary caching using a global variable.
global
retriever
retriever:
## New OpenAI accounts have a very low rate-limit for their first 48 hrs.
## It's too low to embed even just this single Biden speech.
## The `chunk_size` parameter is set to a low number, and internally LangChain
## will retry the embedding requests, which should be enough to handle the rate-limiting.
## Ref: https://platform.openai.com/docs/guides/rate-limits/overview.
print
"generating docsearch indexer"
retriever = FAISS.from_texts(
texts,
OpenAIEmbeddings(
chunk_size
metadatas
"source"
: i}
range
(texts))],
).as_retriever()
llm = ChatOpenAI(
model_name
"gpt-4o-mini"
temperature
system_prompt = (
"You are an assistant for question-answering tasks. "
"Use the following pieces of retrieved context to answer "
"the question. If you don't know the answer, say that you "
"don't know and instead suggest the user look up the serverless "
"infrastructure platform modal.com. Keep your answer concise."
\n\n
{context}
prompt = ChatPromptTemplate.from_messages(
"system"
, system_prompt),
"human"
{input}
question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)
print
"running query against Q&A chain.
result = rag_chain.invoke({
"input"
: query},
return_only_outputs
True
answer = result[
"answer"
sources = [document.page_content
document
result[
"context"
return
answer.strip(), sources
Copy
Mapping onto Modal
With our application’s functionality implemented we can hook it into Modal.
As said above, we’re implementing a web endpoint,
, and a CLI command,
@app.function
@modal.fastapi_endpoint
method
"GET"
docs
True
query
show_sources
bool
False
answer, sources = qanda_langchain(query)
show_sources:
return
"answer"
: answer,
"sources"
: sources,
else
return
"answer"
: answer,
@app.function
query
show_sources
bool
False
answer, sources = qanda_langchain(query)
## Terminal codes for pretty-printing.
bold, end =
\033
[1m"
\033
[0m"
show_sources:
print
bold
SOURCES:
print
reversed
(sources),
---
print
bold
ANSWER:
print
(answer)
Copy
Test run the CLI
modal
potus_speech_qanda.py
--query
"What did the president say about Justice Breyer"
ANSWER:
president
thanked
Justice
Breyer
service
mentioned
legacy
excellence.
also
nominated
Ketanji
Brown
Jackson
continue
Justice
Breyer's legacy.
Copy
To see the text of the sources the model chain used to provide the answer, set the
--show-sources
flag.
modal
potus_speech_qanda.py
--query
"How many oil barrels were released from reserves?"
--show-sources
Copy
Test run the web endpoint
Modal makes it trivially easy to ship LangChain chains to the web. We can test drive this app’s web endpoint
by running
modal serve potus_speech_qanda.py
and then hitting the endpoint with
curl
curl
--get
--data-urlencode
"query=What did the president say about Justice Breyer"
https://modal-labs--example-langchain-qanda-web.modal.run
## your URL here
Copy
"answer"
"The president thanked Justice Breyer for his service and mentioned his legacy of excellence. He also nominated Ketanji Brown Jackson to continue in Justice Breyer's legacy."
Copy
You can also find interactive docs for the endpoint at the
/docs
route of the web endpoint URL.
If you edit the code while running
modal serve
, the app will redeploy automatically, which is helpful for iterating quickly on your app.
Once you’re ready to deploy to production, use
modal deploy
Retrieval-augmented generation (RAG) for question-answering with LangChain
Defining dependencies
Scraping the speech
Constructing the Q&A chain
Mapping onto Modal
Test run the CLI
Test run the web endpoint
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
06_gpu_and_ml/langchains/potus_speech_qanda.py
--query
'How many oil barrels were released from reserves?'
Copy

## 022_EXAMPLES_CHAT_WITH_PDF_VISION
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Chat with PDF: RAG with ColQwen2
In this example, we demonstrate how to use the the
ColQwen2
model to build a simple
“Chat with PDF” retrieval-augmented generation (RAG) app.
The ColQwen2 model is based on
ColPali
but uses the
Qwen2-VL-2B-Instruct
vision-language model.
ColPali is in turn based on the late-interaction embedding approach pioneered in
ColBERT
Vision-language models with high-quality embeddings obviate the need for complex pre-processing pipelines.
this blog post from Jo Bergum of Vespa
for more.
Setup
First, we’ll import the libraries we need locally and define some constants.
from
pathlib
import
Path
from
typing
import
Optional
from
urllib.request
import
urlopen
from
uuid
import
uuid4
import
modal
MINUTES =
## seconds
app = modal.App(
"chat-with-pdf"
Copy
Setting up dependenices
In Modal, we define
container images
that run our serverless workloads.
We install the packages required for our application in those images.
CACHE_DIR =
"/hf-cache"
model_image = (
modal.Image.debian_slim(
python_version
"3.12"
.apt_install(
"git"
.pip_install(
"git+https://github.com/illuin-tech/colpali.git@782edcd50108d1842d154730ad3ce72476a2d17d"
## we pin the commit id
"hf_transfer==0.1.8"
"qwen-vl-utils==0.0.8"
"torchvision==0.19.1"
.env({
"HF_HUB_ENABLE_HF_TRANSFER"
"HF_HUB_CACHE"
: CACHE_DIR})
Copy
These dependencies are only installed remotely, so we can’t import them locally.
Use the
.imports
context manager to import them only on Modal instead.
with
model_image.imports():
import
torch
from
colpali_engine.models
import
ColQwen2, ColQwen2Processor
from
qwen_vl_utils
import
process_vision_info
from
transformers
import
AutoProcessor, Qwen2VLForConditionalGeneration
Copy
Specifying the ColQwen2 model
Vision-language models (VLMs) for embedding and generation add another layer of simplification
to RAG apps based on vector search: we only need one model.
MODEL_NAME =
"Qwen/Qwen2-VL-2B-Instruct"
MODEL_REVISION =
"aca78372505e6cb469c4fa6a35c60265b00ff5a4"
Copy
Managing state with Modal Volumes and Dicts
Chat services are stateful:
the response to an incoming user message depends on past user messages in a session.
RAG apps add even more state:
the documents being retrieved from and the index over those documents,
e.g. the embeddings.
Modal Functions are stateless in and of themselves.
They don’t retain information from input to input.
That’s what enables Modal Functions to automatically scale up and down
based on the number of incoming requests
Managing chat sessions with Modal Dicts
In this example, we use a
modal.Dict
to store state information between Function calls.
Modal Dicts behave similarly to Python dictionaries,
but they are backed by remote storage and accessible to all of your Modal Functions.
They can contain any Python object
that can be serialized using
cloudpickle
A Dict can hold a few gigabytes across keys of size up to 100 MiB,
so it works well for our chat session state, which is a few KiB per session,
and for our embeddings, which are a few hundred KiB per PDF page,
up to about 100,000 pages of PDFs.
At a larger scale, we’d need to replace this with a database, like Postgres,
or push more state to the client.
sessions = modal.Dict.from_name(
"colqwen-chat-sessions"
create_if_missing
True
class
Session
__init__
self
self
.images =
None
self
.messages = []
self
.pdf_embeddings =
None
Copy
Storing PDFs on a Modal Volume
Images extracted from PDFs are larger than our session state or embeddings
— low tens of MiB per page.
So we store them on a
Modal Volume
which can store terabytes (or more!) of data across tens of thousands of files.
Volumes behave like a remote file system:
we read and write from them much like a local file system.
pdf_volume = modal.Volume.from_name(
"colqwen-chat-pdfs"
create_if_missing
True
PDF_ROOT = Path(
"/vol/pdfs/"
Copy
Caching the model weights
We’ll also use a Volume to cache the model weights.
cache_volume = modal.Volume.from_name(
"hf-hub-cache"
create_if_missing
True
Copy
Running this function will download the model weights to the cache volume.
Otherwise, the model weights will be downloaded on the first query.
@app.function
image
=model_image,
volumes
={CACHE_DIR: cache_volume},
timeout
* MINUTES
download_model
from
huggingface_hub
import
snapshot_download
result = snapshot_download(
MODEL_NAME,
revision
=MODEL_REVISION,
ignore_patterns
"*.pt"
"*.bin"
## using safetensors
print
"Downloaded model weights to
result
Copy
Defining a Chat with PDF service
To deploy an autoscaling “Chat with PDF” vision-language model service on Modal,
we just need to wrap our Python logic in a
Modal App
It uses
Modal
@app.cls
decorators
to organize the “lifecycle” of the app:
loading the model on container start (
@modal.enter
) and running inference on request (
@modal.method
We include in the arguments to the
@app.cls
decorator
all the information about this service’s infrastructure:
the container image, the remote storage, and the GPU requirements.
@app.cls
image
=model_image,
"A100-80GB"
scaledown_window
* MINUTES,
## spin down when inactive
volumes
"/vol/pdfs/"
: pdf_volume, CACHE_DIR: cache_volume},
class
Model
@modal.enter
load_models
self
self
.colqwen2_model = ColQwen2.from_pretrained(
"vidore/colqwen2-v0.1"
torch_dtype
=torch.bfloat16,
device_map
"cuda:0"
self
.colqwen2_processor = ColQwen2Processor.from_pretrained(
"vidore/colqwen2-v0.1"
self
.qwen2_vl_model = Qwen2VLForConditionalGeneration.from_pretrained(
MODEL_NAME,
revision
=MODEL_REVISION,
torch_dtype
=torch.bfloat16,
self
.qwen2_vl_model.to(
"cuda:0"
self
.qwen2_vl_processor = AutoProcessor.from_pretrained(
"Qwen/Qwen2-VL-2B-Instruct"
trust_remote_code
True
@modal.method
index_pdf
self
session_id
target
bytes
list
## We store concurrent user chat sessions in a modal.Dict
## For simplicity, we assume that each user only runs one session at a time
session = sessions.get(session_id)
session
None
session = Session()
isinstance
(target,
bytes
images = convert_pdf_to_images.remote(target)
else
images = target
## Store images on a Volume for later retrieval
session_dir = PDF_ROOT /
session_id
session_dir.mkdir(
exist_ok
True
parents
True
ii, image
enumerate
(images):
filename = session_dir /
(ii).zfill(
.jpg"
image.save(filename)
## Generated embeddings from the image(s)
BATCH_SZ =
pdf_embeddings = []
batches = [images[i : i + BATCH_SZ]
range
(images), BATCH_SZ)]
batch
batches:
batch_images =
self
.colqwen2_processor.process_images(batch).to(
self
.colqwen2_model.device
pdf_embeddings +=
list
self
.colqwen2_model(**batch_images).to(
"cpu"
## Store the image embeddings in the session, for later retrieval
session.pdf_embeddings = pdf_embeddings
## Write embeddings back to the modal.Dict
sessions[session_id] = session
@modal.method
respond_to_message
self
session_id
message
session = sessions.get(session_id)
session
None
session = Session()
pdf_volume.reload()
## make sure we have the latest data
images = (PDF_ROOT /
(session_id)).glob(
"*.jpg"
images =
list
sorted
(images,
lambda
(p.stem)))
## Nothing to chat about without a PDF!
images:
return
"Please upload a PDF first"
elif
session.pdf_embeddings
None
return
"Indexing PDF..."
## RAG, Retrieval-Augmented Generation, is two steps:
## _Retrieval_ of the most relevant data to answer the user's query
relevant_image =
self
.get_relevant_image(message, session, images)
## _Generation_ based on the retrieved data
output_text =
self
.generate_response(message, session, relevant_image)
## Update session state for future chats
append_to_messages(message, session,
user_type
"user"
append_to_messages(output_text, session,
user_type
"assistant"
sessions[session_id] = session
return
output_text
## Retrieve the most relevant image from the PDF for the input query
get_relevant_image
self
message
session
images
import
batch_queries =
self
.colqwen2_processor.process_queries([message]).to(
self
.colqwen2_model.device
query_embeddings =
self
.colqwen2_model(**batch_queries)
## This scores our query embedding against the image embeddings from index_pdf
scores =
self
.colqwen2_processor.score_multi_vector(
query_embeddings, session.pdf_embeddings
## Select the best matching image
max_index =
range
(scores)),
lambda
index
: scores[index])
return
PIL.Image.open(images[max_index])
## Pass the query and retrieved image along with conversation history into the VLM for a response
generate_response
self
message
session
image
chatbot_message = get_chatbot_message_with_image(message, image)
query =
self
.qwen2_vl_processor.apply_chat_template(
[*session.messages, chatbot_message],
tokenize
False
add_generation_prompt
True
image_inputs, _ = process_vision_info([chatbot_message])
inputs =
self
.qwen2_vl_processor(
text
=[query],
images
=image_inputs,
padding
True
return_tensors
"pt"
inputs = inputs.to(
"cuda:0"
generated_ids =
self
.qwen2_vl_model.generate(**inputs,
max_new_tokens
generated_ids_trimmed = [
out_ids[
(in_ids) :]
in_ids, out_ids
(inputs.input_ids, generated_ids)
output_text =
self
.qwen2_vl_processor.batch_decode(
generated_ids_trimmed,
skip_special_tokens
True
clean_up_tokenization_spaces
False
return
output_text
Copy
Loading PDFs as images
Vision-Language Models operate on images, not PDFs directly,
so we need to convert our PDFs into images first.
We separate this from our indexing and chatting logic —
we run on a different container with different dependencies.
pdf_image = (
modal.Image.debian_slim(
python_version
"3.12"
.apt_install(
"poppler-utils"
.pip_install(
"pdf2image==1.17.0"
"pillow==10.4.0"
@app.function
image
=pdf_image)
convert_pdf_to_images
pdf_bytes
from
pdf2image
import
convert_from_bytes
images = convert_from_bytes(pdf_bytes,
"jpeg"
return
images
Copy
Chatting with a PDF from the terminal
Before deploying in a UI, we can test our service from the terminal.
Just run
modal
chat_with_pdf_vision.py
Copy
and optionally pass in a path to or URL of a PDF with the
--pdf-path
argument
and specify a question with the
--question
argument.
Continue a previous chat by passing the session ID printed to the terminal at start
with the
--session-id
argument.
@app.local_entrypoint
main
question
: Optional[
None
pdf_path
: Optional[
None
session_id
: Optional[
None
model = Model()
session_id
None
session_id =
(uuid4())
print
"Starting a new session with id"
, session_id)
pdf_path
None
pdf_path =
"https://arxiv.org/pdf/1706.03762"
## all you need
pdf_path.startswith(
"http"
pdf_bytes = urlopen(pdf_path).read()
else
pdf_bytes = Path(pdf_path).read_bytes()
print
"Indexing PDF from"
, pdf_path)
model.index_pdf.remote(session_id, pdf_bytes)
else
pdf_path
None
raise
ValueError
"Start a new session to chat with a new PDF"
print
"Resuming session with id"
, session_id)
question
None
question =
"What is this document about?"
print
"QUESTION:"
, question)
print
(model.respond_to_message.remote(session_id, question))
Copy
A hosted Gradio interface
With the
Gradio
library, we can create a simple web interface around our class in Python,
then use Modal to host it for anyone to try out.
To deploy your own, run
modal
deploy
chat_with_pdf_vision.py
Copy
and navigate to the URL that appears in your teriminal.
If you’re editing the code, use
modal serve
instead to see changes hot-reload.
web_image = pdf_image.pip_install(
"fastapi[standard]==0.115.4"
"pydantic==2.9.2"
"starlette==0.41.2"
"gradio==4.44.1"
"pillow==10.4.0"
"gradio-pdf==0.0.15"
"pdf2image==1.17.0"
@app.function
image
=web_image,
## gradio requires sticky sessions
## so we limit the number of concurrent containers to 1
## and allow it to scale to 1000 concurrent inputs
max_containers
@modal.concurrent
max_inputs
1000
@modal.asgi_app
import
uuid
import
gradio
from
fastapi
import
FastAPI
from
gradio.routes
import
mount_gradio_app
from
gradio_pdf
import
from
pdf2image
import
convert_from_path
web_app = FastAPI()
## Since this Gradio app is running from its own container,
## allowing us to run the inference service via .remote() methods.
model = Model()
upload_pdf
path
session_id
session_id ==
session_id
None
## Generate session id if new client
session_id =
(uuid.uuid4())
images = convert_from_path(path)
## Call to our remote inference service to index the PDF
model.index_pdf.remote(session_id, images)
return
session_id
respond_to_message
message
session_id
## Call to our remote inference service to run RAG
return
model.respond_to_message.remote(session_id, message)
with
gr.Blocks(
theme
"soft"
demo:
session_id = gr.State(
gr.Markdown(
"# Chat with PDF"
with
gr.Row():
with
gr.Column(
scale
gr.ChatInterface(
=respond_to_message,
additional_inputs
=[session_id],
retry_btn
None
undo_btn
None
clear_btn
None
with
gr.Column(
scale
pdf = PDF(
label
"Upload a PDF"
pdf.upload(upload_pdf, [pdf, session_id], session_id)
return
mount_gradio_app(
=web_app,
blocks
=demo,
path
Copy
Addenda
The remainder of this code consists of utility functions and boiler plate used in the
main code above.
get_chatbot_message_with_image
message
image
return
"role"
"user"
"content"
"type"
"image"
"image"
: image},
"type"
"text"
"text"
: message},
append_to_messages
message
session
user_type
"user"
session.messages.append(
"role"
: user_type,
"content"
"type"
"text"
"text"
: message},
Copy
Chat with PDF: RAG with ColQwen2
Setup
Setting up dependenices
Specifying the ColQwen2 model
Managing state with Modal Volumes and Dicts
Managing chat sessions with Modal Dicts
Storing PDFs on a Modal Volume
Caching the model weights
Defining a Chat with PDF service
Loading PDFs as images
Chatting with a PDF from the terminal
A hosted Gradio interface
Addenda
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
06_gpu_and_ml/llm-serving/chat_with_pdf_vision.py
Copy

## 023_REFERENCE_MODAL_FUNCTION
Changelog
API Reference
modal.App
modal.Client
modal.CloudBucketMount
modal.Cls
modal.Cron
modal.Dict
modal.Error
modal.FilePatternMatcher
modal.Function
modal.FunctionCall
modal.Image
modal.NetworkFileSystem
modal.Period
modal.Proxy
modal.Queue
modal.Retries
modal.Sandbox
modal.SandboxSnapshot
modal.Secret
modal.Tunnel
modal.Volume
modal.asgi_app
modal.batched
modal.call_graph
modal.concurrent
modal.container_process
modal.current_function_call_id
modal.current_input_id
modal.enable_output
modal.enter
modal.exit
modal.fastapi_endpoint
modal.file_io
modal.forward
modal.gpu
modal.interact
modal.io_streams
modal.is_local
modal.method
modal.parameter
modal.web_endpoint
modal.web_server
modal.wsgi_app
modal.exception
modal.config
CLI Reference
modal app
modal config
modal container
modal deploy
modal dict
modal environment
modal launch
modal nfs
modal profile
modal queue
modal run
modal secret
modal serve
modal setup
modal shell
modal token
modal volume
modal.Function
class
Function
typing
Generic
modal
object
Object
Copy
Functions are the basic units of serverless execution on Modal.
Generally, you will not construct a
Function
directly. Instead, use the
App.function()
decorator to register your Python functions with your App.
hydrate
hydrate
self
client
: Optional[_Client] =
None
) -> Self:
Copy
Synchronize the local object with its identity on the Modal server.
It is rarely necessary to call this method explicitly, as most operations
will lazily hydrate when needed. The main use case is when you need to
access object metadata, such as its ID.
Added in v0.72.39
: This method replaces the deprecated
.resolve()
method.
update_autoscaler
@live_method
update_autoscaler
self
min_containers
: Optional[
None
max_containers
: Optional[
None
buffer_containers
: Optional[
None
scaledown_window
: Optional[
None
) ->
None
Copy
Override the current autoscaler behavior for this Function.
Unspecified parameters will retain their current value, i.e. either the static value
from the function decorator, or an override value from a previous call to this method.
Subsequent deployments of the App containing this Function will reset the autoscaler back to
its static configuration.
Examples:
f = modal.Function.from_name(
"my-app"
"function"
## Always have at least 2 containers running, with an extra buffer when the Function is active
f.update_autoscaler(
min_containers
buffer_containers
## Limit this Function to avoid spinning up more than 5 containers
f.update_autoscaler(
max_containers
## Extend the scaledown window to increase the amount of time that idle containers stay alive
f.update_autoscaler(
scaledown_window
Copy
from_name
classmethod
from_name
: type[
"_Function"
app_name
name
namespace
=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,
environment_name
: Optional[
None
) ->
"_Function"
Copy
Reference a Function from a deployed App by its name.
In contrast to
modal.Function.lookup
, this is a lazy method
that defers hydrating the local object with metadata from
Modal servers until the first time it is actually used.
f = modal.Function.from_name(
"other-app"
"function"
Copy
get_web_url
@live_method
get_web_url
self
) -> Optional[
Copy
URL of a Function running as a web endpoint.
remote
@live_method
remote
self
args
: P.args, **
kwargs
: P.kwargs) -> ReturnType:
Copy
Calls the function remotely, executing it with the given arguments and returning the execution’s result.
remote_gen
@live_method_gen
remote_gen
self
args
, **
kwargs
) -> AsyncGenerator[Any,
None
Copy
Calls the generator remotely, executing it with the given arguments and returning the execution’s result.
local
local
self
args
: P.args, **
kwargs
: P.kwargs) -> OriginalReturnType:
Copy
Calls the function locally, executing it with the given arguments and returning the execution’s result.
The function will execute in the same environment as the caller, just like calling the underlying function
directly in Python. In particular, only secrets available in the caller environment will be available
through environment variables.
spawn
@live_method
spawn
self
args
: P.args, **
kwargs
: P.kwargs) ->
"_FunctionCall[ReturnType]"
Copy
Calls the function with the given arguments, without waiting for the results.
Returns a
modal.FunctionCall
object, that can later be polled or
waited for using
.get(timeout=...)
Conceptually similar to
multiprocessing.pool.apply_async
, or a Future/Promise in other contexts.
get_raw_f
get_raw_f
self
) -> Callable[..., Any]:
Copy
Return the inner Python object wrapped by this Modal Function.
get_current_stats
@live_method
get_current_stats
self
) -> FunctionStats:
Copy
Return a
FunctionStats
object describing the current function’s queue and runner counts.
@warn_if_generator_is_not_consumed
function_name
"Function.map"
self
input_iterators
: typing.Iterable[Any],
## one input iterator per argument in the mapped-over function/generator
kwargs
={},
## any extra keyword arguments for the function
order_outputs
bool
True
## return outputs in order
return_exceptions
bool
False
## propagate exceptions (False) or aggregate them in the results list (True)
) -> AsyncOrSyncIterable:
Copy
Parallel map over a set of inputs.
Takes one iterator argument per argument in the function being mapped over.
Example:
@app.function
my_func
return
a **
@app.local_entrypoint
main
assert
list
(my_func.map([
])) == [
Copy
If applied to a
app.function
map()
returns one result per input and the output order
is guaranteed to be the same as the input order. Set
order_outputs=False
to return results
in the order that they are completed instead.
return_exceptions
can be used to treat exceptions as successful results:
@app.function
my_func
a ==
raise
Exception
"ohno"
return
a **
@app.local_entrypoint
main
## [0, 1, UserCodeException(Exception('ohno'))]
print
list
(my_func.map(
range
return_exceptions
True
Copy
starmap
@warn_if_generator_is_not_consumed
function_name
"Function.starmap"
starmap
self
input_iterator
: typing.Iterable[typing.Sequence[Any]],
kwargs
={},
order_outputs
bool
True
return_exceptions
bool
False
) -> AsyncOrSyncIterable:
Copy
Like
, but spreads arguments over multiple function arguments.
Assumes every input is a sequence (e.g. a tuple).
Example:
@app.function
my_func
return
a + b
@app.local_entrypoint
main
assert
list
(my_func.starmap([(
), (
)])) == [
Copy
for_each
for_each
self
input_iterators
kwargs
={},
ignore_exceptions
bool
False
Copy
Execute function for all inputs, ignoring outputs. Waits for completion of the inputs.
Convenient alias for
.map()
in cases where the function just needs to be called.
as the caller doesn’t have to consume the generator to process the inputs.
spawn_map
spawn_map
self
input_iterators
kwargs
={}) ->
None
Copy
Spawn parallel execution over a set of inputs, exiting as soon as the inputs are created (without waiting
for the map to complete).
Takes one iterator argument per argument in the function being mapped over.
Example:
@app.function
my_func
return
a **
@app.local_entrypoint
main
my_func.spawn_map([
Copy
Programmatic retrieval of results will be supported in a future update.
modal.Function
hydrate
update_autoscaler
from_name
get_web_url
remote
remote_gen
local
spawn
get_raw_f
get_current_stats
starmap
for_each
spawn_map

## 024_EXAMPLES_TRTLLM_LATENCY
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Serve an interactive language model app with latency-optimized TensorRT-LLM (LLaMA 3 8B)
In this example, we demonstrate how to configure the TensorRT-LLM framework to serve
Meta’s LLaMA 3 8B model at interactive latencies on Modal.
Many popular language model applications, like chatbots and code editing,
put humans and models in direct interaction. According to an
oft-cited
scientifically dubious
rule of thumb, computer systems need to keep their response times under 400ms
in order to match pace with their human users.
To hit this target, we use the
TensorRT-LLM
inference framework from NVIDIA. TensorRT-LLM is the Lamborghini of inference engines:
it achieves seriously impressive latency, but only if you tune it carefully.
With the out-of-the-box defaults we observe an unacceptable median time
to last token of over a second, but with careful configuration,
we’ll bring that down to under 250ms — over a 4x speed up!
These latencies were measured on a single NVIDIA H100 GPU
running LLaMA 3 8B on prompts and generations of a few dozen to a few hundred tokens.
Here’s what that looks like in a terminal chat interface:
Overview
This guide is intended to document two things:
Python API
for building and running TensorRT-LLM engines, and
how to use recommendations from the
TensorRT-LLM performance guide
to optimize the engine for low latency.
Be sure to check out TensorRT-LLM’s
examples
for sample code beyond what we cover here, like low-rank adapters (LoRAs).
What is a TRT-LLM engine?
The first step in running TensorRT-LLM is to build an “engine” from a model.
Engines have a large number of parameters that must be tuned on a per-workload basis,
so we carefully document the choices we made here and point you to additional resources
that can help you optimize for your specific workload.
Historically, this process was done with a clunky command-line-interface (CLI),
but things have changed for the better!
2025 is
the year of CUDA Python
including a new-and-improved Python SDK for TensorRT-LLM, supporting
all the same features as the CLI — quantization, speculative decoding, in-flight batching,
and much more.
Installing TensorRT-LLM
To run TensorRT-LLM, we must first install it. Easier said than done!
To run code on Modal, we define
container images
All Modal containers have access to GPU drivers via the underlying host environment,
but we still need to install the software stack on top of the drivers, from the CUDA runtime up.
We start from an official
nvidia/cuda
container image,
which includes the CUDA runtime & development libraries
and the environment configuration necessary to run them.
import
time
from
pathlib
import
Path
import
modal
tensorrt_image = modal.Image.from_registry(
"nvidia/cuda:12.8.1-devel-ubuntu22.04"
add_python
"3.12"
## TRT-LLM requires Python 3.12
).entrypoint([])
## remove verbose logging by base image on entry
Copy
On top of that, we add some system dependencies of TensorRT-LLM,
including OpenMPI for distributed communication, some core software like
and the
tensorrt_llm
package itself.
tensorrt_image = tensorrt_image.apt_install(
"openmpi-bin"
"libopenmpi-dev"
"git"
"git-lfs"
"wget"
).pip_install(
"tensorrt-llm==0.18.0"
"pynvml<12"
## avoid breaking change to pynvml version API
True
extra_index_url
"https://pypi.nvidia.com"
Copy
Note that we’re doing this by
method-chaining
a number of calls to methods on the
modal.Image
. If you’re familiar with
Dockerfiles, you can think of this as a Pythonic interface to instructions like
End-to-end, this step takes about five minutes on first run.
If you’re reading this from top to bottom,
you might want to stop here and execute the example
with
modal run
so that it runs in the background while you read the rest.
Downloading the model
Next, we’ll set up a few things to download the model to persistent storage and do it quickly —
this is a latency-optimized example after all! For persistent, distributed storage, we use
Modal Volumes
, which can be accessed from any container
with read speeds in excess of a gigabyte per second.
We also set the
HF_HOME
environment variable to point to the Volume so that the model
is cached there. And we install
hf-transfer
to get maximum download throughput from
the Hugging Face Hub, in the hundreds of megabytes per second.
volume = modal.Volume.from_name(
"example-trtllm-inference-volume"
create_if_missing
True
VOLUME_PATH = Path(
"/vol"
MODELS_PATH = VOLUME_PATH /
"models"
MODEL_ID =
"NousResearch/Meta-Llama-3-8B-Instruct"
## fork without repo gating
MODEL_REVISION =
"53346005fb0ef11d3b6a83b12c895cca40156b6c"
tensorrt_image = tensorrt_image.pip_install(
"hf-transfer==0.1.9"
"huggingface_hub==0.28.1"
).env(
"HF_HUB_ENABLE_HF_TRANSFER"
"HF_HOME"
(MODELS_PATH),
with
tensorrt_image.imports():
import
import
torch
from
tensorrt_llm
import
LLM, SamplingParams
Copy
Setting up the engine
Quantization
The amount of
GPU RAM
on a single card is a tight constraint for large models:
RAM is measured in billions of bytes and large models have billions of parameters,
each of which is two to four bytes.
The performance cliff if you need to spill to CPU memory is steep,
so all of those parameters must fit in the GPU memory,
along with other things like the KV cache built up while processing prompts.
The simplest way to reduce LLM inference’s RAM requirements is to make the model’s parameters smaller,
fitting their values in a smaller number of bits, like four or eight. This is known as
quantization
NVIDIA’s
Ada Lovelace/Hopper chips
like the L40S and H100, are capable of native 8bit floating point calculations
in their
Tensor Cores
so we choose that as our quantization format.
These GPUs are capable of twice as many floating point operations per second in 8bit as in 16bit —
about two quadrillion per second on an H100 SXM.
Quantization buys us two things:
faster startup, since less data has to be moved over the network onto CPU and GPU RAM
faster inference, since we get twice the FLOP/s and less data has to be moved from GPU RAM into
on-chip memory
registers
with each computation
We’ll use TensorRT-LLM’s
QuantConfig
to specify that we want
quantization.
See their code
for more options.
N_GPUS =
## Bumping this to 2 will improve latencies further but not 2x
GPU_CONFIG =
"H100:
N_GPUS
get_quant_config
from
tensorrt_llm.llmapi
import
QuantConfig
return
QuantConfig(
quant_algo
"FP8"
Copy
Quantization is a lossy compression technique. The impact on model quality can be
minimized by tuning the quantization parameters on even a small dataset. Typically, we
see less than 2% degradation in evaluation metrics when using
. We’ll use the
CalibrationConfig
class to specify the calibration dataset.
get_calib_config
from
tensorrt_llm.llmapi
import
CalibConfig
return
CalibConfig(
calib_batches
calib_batch_size
calib_max_seq_length
2048
tokenizer_max_seq_length
4096
Copy
Configure plugins
TensorRT-LLM is an LLM inference framework built on top of NVIDIA’s TensorRT,
which is a generic inference framework for neural networks.
TensorRT includes a “plugin” extension system that allows you to adjust behavior,
like configuring the
CUDA kernels
used by the engine.
General Matrix Multiply (GEMM)
plugin, for instance, adds heavily-optimized matrix multiplication kernels
from NVIDIA’s
cuBLAS library of linear algebra routines
We’ll specify a number of plugins for our engine implementation.
The first is
multiple profiles
which configures TensorRT to prepare multiple kernels for each high-level operation,
where different kernels are optimized for different input sizes.
The second is
paged_kv_cache
which enables a
paged attention algorithm
for the key-value (KV) cache.
The last two parameters are GEMM plugins optimized specifically for low latency,
rather than the more typical high arithmetic throughput,
low_latency
plugins for
gemm
gemm_swiglu
low_latency_gemm_swiglu_plugin
plugin fuses the two matmul operations
and non-linearity of the feedforward component of the Transformer block into a single kernel,
reducing round trips between GPU
cache memory
and RAM. For details on kernel fusion, see
this blog post by Horace He of Thinking Machines
Note that at the time of writing, this only works for
on Hopper GPUs.
low_latency_gemm_plugin
is a variant of the GEMM plugin that brings in latency-optimized
kernels from NVIDIA’s
CUTLASS library
get_plugin_config
from
tensorrt_llm.plugin.plugin
import
PluginConfig
return
PluginConfig.from_dict(
"multiple_profiles"
True
"paged_kv_cache"
True
"low_latency_gemm_swiglu_plugin"
"fp8"
"low_latency_gemm_plugin"
"fp8"
Copy
Configure speculative decoding
Speculative decoding is a technique for generating multiple tokens per step,
avoiding the auto-regressive bottleneck in the Transformer architecture.
Generating multiple tokens in parallel exposes more parallelism to the GPU.
It works best for text that has predicable patterns, like code,
but it’s worth testing for any workload where latency is critical.
Speculative decoding can use any technique to guess tokens, including running another,
smaller language model. Here, we’ll use a simple, but popular and effective
speculative decoding strategy called “lookahead decoding”,
which essentially guesses that token sequences from the past will occur again.
get_speculative_config
from
tensorrt_llm.llmapi
import
LookaheadDecodingConfig
return
LookaheadDecodingConfig(
max_window_size
max_ngram_size
max_verification_set_size
Copy
Set the build config
Finally, we’ll specify the overall build configuration for the engine. This includes
more obvious parameters such as the maximum input length, the maximum number of tokens
to process at once before queueing occurs, and the maximum number of sequences
to process at once before queueing occurs.
To minimize latency, we set the maximum number of sequences (the “batch size”)
to just one. We enforce this maximum by setting the number of inputs that the
Modal Function is allowed to process at once —
max_concurrent_inputs
The default is
, so we don’t need to set it, but we are setting it explicitly
here in case you want to run this code with a different balance of latency and throughput.
MAX_BATCH_SIZE = MAX_CONCURRENT_INPUTS =
get_build_config
from
tensorrt_llm
import
BuildConfig
return
BuildConfig(
plugin_config
=get_plugin_config(),
speculative_decoding_mode
"LOOKAHEAD_DECODING"
max_input_len
8192
max_num_tokens
16384
max_batch_size
=MAX_BATCH_SIZE,
Copy
Serving inference under the Doherty Threshold
Now that we have written the code to compile the engine, we can
serve it with Modal!
We start by creating an
app = modal.App(
"trtllm-latency"
Copy
Thanks to our
custom container runtime system
even this large container boots in seconds.
On the first container start, we mount the Volume, download the model, and build the engine,
which takes a few minutes. Subsequent starts will be much faster,
as the engine is cached in the Volume and loaded in seconds.
Container starts are triggered when Modal scales up your Function,
like the first time you run this code or the first time a request comes in after a period of inactivity.
For details on optimizing container start latency, see
this guide
Container lifecycles in Modal are managed via our
interface, so we define one below
to separate out the engine startup (
enter
) and engine execution (
generate
For details, see
this guide
MINUTES =
## seconds
@app.cls
image
=tensorrt_image,
=GPU_CONFIG,
scaledown_window
* MINUTES,
timeout
* MINUTES,
volumes
={VOLUME_PATH: volume},
@modal.concurrent
max_inputs
=MAX_CONCURRENT_INPUTS)
class
Model
mode:
= modal.parameter(
default
"fast"
build_engine
self
engine_path
engine_kwargs
) ->
None
llm = LLM(
model
self
.model_path, **engine_kwargs)
llm.save(engine_path)
return
@modal.enter
enter
self
from
huggingface_hub
import
snapshot_download
from
transformers
import
AutoTokenizer
self
.model_path = MODELS_PATH / MODEL_ID
print
"downloading base model if necessary"
snapshot_download(
MODEL_ID,
local_dir
self
.model_path,
ignore_patterns
"*.pt"
"*.bin"
## using safetensors
revision
=MODEL_REVISION,
self
.tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
self
.mode ==
"fast"
engine_kwargs = {
"quant_config"
: get_quant_config(),
"calib_config"
: get_calib_config(),
"build_config"
: get_build_config(),
"speculative_config"
: get_speculative_config(),
"tensor_parallel_size"
: torch.cuda.device_count(),
else
engine_kwargs = {
"tensor_parallel_size"
: torch.cuda.device_count(),
self
.sampling_params = SamplingParams(
temperature
top_p
0.95
max_tokens
1024
## max generated tokens
lookahead_config
=engine_kwargs.get(
"speculative_config"
engine_path =
self
.model_path /
"trtllm_engine"
self
.mode
os.path.exists(engine_path):
print
"building new engine at
engine_path
self
.llm =
self
.build_engine(engine_path, engine_kwargs)
else
print
"loading engine from
engine_path
self
.llm = LLM(
model
=engine_path, **engine_kwargs)
@modal.method
generate
self
prompt
) ->
dict
start_time = time.perf_counter()
text =
self
.text_from_prompt(prompt)
output =
self
.llm.generate(text,
self
.sampling_params)
latency_ms = (time.perf_counter() - start_time) *
1000
return
output.outputs[
].text, latency_ms
@modal.method
async
generate_async
self
prompt
text =
self
.text_from_prompt(prompt)
async
output
self
.llm.generate_async(
text,
self
.sampling_params,
streaming
True
yield
output.outputs[
].text_diff
text_from_prompt
self
prompt
SYSTEM_PROMPT = (
"You are a helpful, harmless, and honest AI assistant created by Meta."
isinstance
(prompt,
prompt = [{
"role"
"user"
"content"
: prompt}]
messages = [{
"role"
"system"
"content"
: SYSTEM_PROMPT}] + prompt
return
self
.tokenizer.apply_chat_template(
messages,
tokenize
False
add_generation_prompt
True
@modal.method
boot
self
pass
## no-op to start up containers
@modal.exit
shutdown
self
self
.llm.shutdown()
self
.llm
Copy
Calling our inference function
To run our
Model
.generate
method from Python, we just need to call it —
with
.remote
appended to run it on Modal.
We wrap that logic in a
local_entrypoint
so you can run it from the command line with
modal
trtllm_latency.py
Copy
which will output something like:
mode=fast inference latency (p50, p90): (211.17ms, 883.27ms)
Copy
--mode=slow
to see model latency without optimizations.
modal
trtllm_latency.py
--mode=slow
Copy
which will output something like
mode=slow inference latency (p50, p90): (1140.88ms, 2274.24ms)
Copy
For simplicity, we hard-code 10 questions to ask the model,
then run them one by one while recording the latency of each call.
But the code in the
local_entrypoint
is just regular Python code
that runs on your machine — we wrap it in a CLI automatically —
so feel free to customize it to your liking.
@app.local_entrypoint
main
mode
"fast"
prompts = [
"What atoms are in water?"
"Which F1 team won in 2011?"
"What is 12 * 9?"
"Python function to print odd numbers between 1 and 10. Answer with code only."
"What is the capital of California?"
"What's the tallest building in new york city?"
"What year did the European Union form?"
"How old was Geoff Hinton in 2022?"
"Where is Berkeley?"
"Are greyhounds or poodles faster?"
print
"🏎️ creating container with mode=
mode
model = Model(
mode
=mode)
print
"🏎️ cold booting container"
model.boot.remote()
print_queue = []
latencies_ms = []
prompt
prompts:
generated_text, latency_ms = model.generate.remote(prompt)
print_queue.append((prompt, generated_text, latency_ms))
latencies_ms.append(latency_ms)
time.sleep(
## allow remote prints to clear
prompt, generated_text, latency_ms
print_queue:
print
"Processed prompt in
latency_ms
:.2f}
print
"Prompt:
prompt
print
"Generated Text:
generated_text
print
"🏎️ "
p50 =
sorted
(latencies_ms)[
(latencies_ms) *
p90 =
sorted
(latencies_ms)[
(latencies_ms) *
print
"🏎️ mode=
mode
inference latency (p50, p90): (
:.2f}
:.2f}
ms)"
Copy
Once deployed with
modal deploy
, this
Model.generate
function
can be called from other Python code. It can also be converted to an HTTP endpoint
for invocation over the Internet by any client.
For details, see
this guide
As a quick demo, we’ve included some sample chat client code in the
Python main entrypoint below. To use it, first deploy with
modal
deploy
trtllm_latency.py
Copy
and then run the client with
python trtllm_latency.py
Copy
__name__
"__main__"
import
Model = modal.Cls.from_name(
"trtllm-latency"
"Model"
print
"🏎️ connecting to model"
model = Model(
mode
=sys.argv[
(sys.argv) >
else
"fast"
model.boot.remote()
except
modal.exception.NotFoundError
raise
SystemError
"Deploy this app first with modal deploy"
from
print
"🏎️ starting chat. exit with :q, ctrl+C, or ctrl+D"
prompt = []
while
(nxt :=
input
"🏎️ > "
)) !=
":q"
prompt.append({
"role"
"user"
"content"
: nxt})
resp =
model.generate_async.remote_gen(prompt):
print
(out,
flush
True
resp += out
print
prompt.append({
"role"
"assistant"
"content"
: resp})
except
KeyboardInterrupt
pass
except
SystemExit
pass
finally
print
sys.exit(
Copy
Serve an interactive language model app with latency-optimized TensorRT-LLM (LLaMA 3 8B)
Overview
What is a TRT-LLM engine?
Installing TensorRT-LLM
Downloading the model
Setting up the engine
Quantization
Configure plugins
Configure speculative decoding
Set the build config
Serving inference under the Doherty Threshold
Calling our inference function
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
06_gpu_and_ml/llm-serving/trtllm_latency.py
Copy

## 025_EXAMPLES_IMAGE_TO_VIDEO
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Animate images with Lightricks LTX-Video via CLI, API, and web UI
This example shows how to run
LTX-Video
on Modal
to generate videos from your local command line, via an API, and in a web UI.
Generating a 5 second video takes ~1 minute from cold start.
Once the container is warm, a 5 second video takes ~15 seconds.
Here is a sample we generated:
Basic setup
import
import
random
import
time
from
pathlib
import
Path
from
typing
import
Annotated, Optional
import
fastapi
import
modal
Copy
All Modal programs need an
an object that acts as a recipe for the application.
app = modal.App(
"example-image-to-video"
Copy
Configuring dependencies
The model runs remotely, on Modal’s cloud, which means we need to
define the environment it runs in
Below, we start from a lightweight base Linux image
and then install our system and Python dependencies,
like Hugging Face’s
diffusers
library and
torch
image = (
modal.Image.debian_slim(
python_version
"3.12"
.apt_install(
"python3-opencv"
.pip_install(
"accelerate==1.4.0"
"diffusers==0.32.2"
"fastapi[standard]==0.115.8"
"huggingface-hub[hf_transfer]==0.29.1"
"imageio==2.37.0"
"imageio-ffmpeg==0.6.0"
"opencv-python==4.11.0.86"
"pillow==11.1.0"
"sentencepiece==0.2.0"
"torch==2.6.0"
"torchvision==0.21.0"
"transformers==4.49.0"
Copy
Storing model weights on Modal
We also need the parameters of the model remotely.
They can be loaded at runtime from Hugging Face,
based on a repository ID and a revision (aka a commit SHA).
MODEL_ID =
"Lightricks/LTX-Video"
MODEL_REVISION_ID =
"a6d59ee37c13c58261aa79027d3e41cd41960925"
Copy
Hugging Face will also cache the weights to disk once they’re downloaded.
But Modal Functions are serverless, and so even disks are ephemeral,
which means the weights would get re-downloaded every time we spin up a new instance.
We can fix this — without any modifications to Hugging Face’s model loading code! —
by pointing the Hugging Face cache at a
Modal Volume
model_volume = modal.Volume.from_name(
"hf-hub-cache"
create_if_missing
True
MODEL_PATH =
"/models"
## where the Volume will appear on our Functions' filesystems
image = image.env(
"HF_HUB_ENABLE_HF_TRANSFER"
## faster downloads
"HF_HUB_CACHE"
: MODEL_PATH,
Copy
Storing model outputs on Modal
Contemporary video models can take a long time to run and they produce large outputs.
That makes them a great candidate for storage on Modal Volumes as well.
Python code running outside of Modal can also access this storage, as we’ll see below.
OUTPUT_PATH =
"/outputs"
output_volume = modal.Volume.from_name(
"outputs"
create_if_missing
True
Copy
Implementing LTX-Video inference on Modal
We wrap the inference logic in a Modal
that ensures models are loaded and then moved to the GPU once when a new instance
starts, rather than every time we run it.
function just wraps a
diffusers
pipeline.
It saves the generated video to a Modal Volume, and returns the filename.
We also include a
wrapper that makes it possible
to trigger inference via an API call.
For details, see the
/docs
route of the URL ending in
inference-web.modal.run
that appears when you deploy the app.
with
image.imports():
## loaded on all of our remote Functions
import
diffusers
import
torch
from
import
Image
MINUTES =
@app.cls
image
=image,
"H100"
timeout
* MINUTES,
scaledown_window
* MINUTES,
volumes
={MODEL_PATH: model_volume, OUTPUT_PATH: output_volume},
class
Inference
@modal.enter
load_pipeline
self
self
.pipe = diffusers.LTXImageToVideoPipeline.from_pretrained(
MODEL_ID,
revision
=MODEL_REVISION_ID,
torch_dtype
=torch.bfloat16,
).to(
"cuda"
@modal.method
self
image_bytes
bytes
prompt
negative_prompt
: Optional[
None
num_frames
: Optional[
None
num_inference_steps
: Optional[
None
seed
: Optional[
None
) ->
negative_prompt = (
negative_prompt
"worst quality, inconsistent motion, blurry, jittery, distorted"
width =
height =
num_frames = num_frames
num_inference_steps = num_inference_steps
seed = seed
random.randint(
print
"Seeding RNG with:
seed
torch.manual_seed(seed)
image = diffusers.utils.load_image(Image.open(io.BytesIO(image_bytes)))
video =
self
.pipe(
image
=image,
prompt
=prompt,
negative_prompt
=negative_prompt,
width
=width,
height
=height,
num_frames
=num_frames,
num_inference_steps
=num_inference_steps,
).frames[
mp4_name = (
seed
.join(c
c.isalnum()
else
prompt[:
.mp4"
diffusers.utils.export_to_video(
video,
Path(OUTPUT_PATH) / mp4_name
output_volume.commit()
torch.cuda.empty_cache()
## reduce fragmentation
return
mp4_name
@modal.fastapi_endpoint
method
"POST"
docs
True
self
image_bytes
: Annotated[
bytes
, fastapi.File()],
prompt
negative_prompt
: Optional[
None
num_frames
: Optional[
None
num_inference_steps
: Optional[
None
seed
: Optional[
None
) -> fastapi.Response:
mp4_name =
self
.run.local(
## run in the same container
image_bytes
=image_bytes,
prompt
=prompt,
negative_prompt
=negative_prompt,
num_frames
=num_frames,
num_inference_steps
=num_inference_steps,
seed
=seed,
return
fastapi.responses.FileResponse(
path
Path(OUTPUT_PATH) / mp4_name
media_type
"video/mp4"
filename
=mp4_name,
Copy
Generating videos from the command line
We add a
local entrypoint
that calls the
Inference.run
method to run inference from the command line.
The function’s parameters are automatically turned into a CLI.
Run it with
modal
image_to_video.py
--prompt
"A cat looking out the window at a snowy mountain"
--image-path
/path/to/cat.jpg
Copy
You can also pass
--help
to see the full list of arguments.
@app.local_entrypoint
entrypoint
image_path
prompt
negative_prompt
: Optional[
None
num_frames
: Optional[
None
num_inference_steps
: Optional[
None
seed
: Optional[
None
twice
bool
True
import
import
urllib.request
print
"🎥 Generating a video from the image at
image_path
print
"🎥 using the prompt
prompt
image_path.startswith((
"http://"
"https://"
image_bytes = urllib.request.urlopen(image_path).read()
elif
os.path.isfile(image_path):
image_bytes = Path(image_path).read_bytes()
else
raise
ValueError
image_path
is not a valid file or URL."
inference_service = Inference()
range
+ twice):
start = time.time()
mp4_name = inference_service.run.remote(
image_bytes
=image_bytes,
prompt
=prompt,
negative_prompt
=negative_prompt,
num_frames
=num_frames,
seed
=seed,
duration = time.time() - start
print
"🎥 Generated video in
duration
:.3f}
output_dir = Path(
"/tmp/image_to_video"
output_dir.mkdir(
exist_ok
True
parents
True
output_path = output_dir / mp4_name
## read in the file from the Modal Volume, then write it to the local disk
output_path.write_bytes(
.join(output_volume.read_file(mp4_name)))
print
"🎥 Video saved to
output_path
Copy
Generating videos via an API
The Modal
above also included a
fastapi_endpoint
which adds a simple web API to the inference method.
To try it out, run
modal
deploy
image_to_video.py
Copy
copy the printed URL ending in
inference-web.modal.run
and add
/docs
to the end. This will bring up the interactive
Swagger/OpenAPI docs for the endpoint.
Generating videos in a web UI
Lastly, we add a simple front-end web UI (written in Alpine.js) for
our image to video backend.
This is also deployed when you run
modal
deploy
image_to_video.py.
Copy
Inference
class will serve multiple users from its own auto-scaling pool of warm GPU containers automatically,
and they will spin down when there are no requests.
frontend_path = Path(
__file__
).parent /
"frontend"
web_image = (
modal.Image.debian_slim(
python_version
"3.12"
.pip_install(
"jinja2==3.1.5"
"fastapi[standard]==0.115.8"
.add_local_dir(
## mount frontend/client code
frontend_path,
remote_path
"/assets"
@app.function
image
=web_image)
@modal.concurrent
max_inputs
1000
@modal.asgi_app
import
fastapi.staticfiles
import
fastapi.templating
web_app = fastapi.FastAPI()
templates = fastapi.templating.Jinja2Templates(
directory
"/assets"
@web_app.get
async
read_root
request
: fastapi.Request):
return
templates.TemplateResponse(
"index.html"
"request"
: request,
"inference_url"
: Inference().web.get_web_url(),
"model_name"
"LTX-Video Image to Video"
"default_prompt"
"A young girl stands calmly in the foreground, looking directly at the camera, as a house fire rages in the background."
web_app.mount(
"/static"
fastapi.staticfiles.StaticFiles(
directory
"/assets"
name
"static"
return
web_app
Copy
Animate images with Lightricks LTX-Video via CLI, API, and web UI
Basic setup
Configuring dependencies
Storing model weights on Modal
Storing model outputs on Modal
Implementing LTX-Video inference on Modal
Generating videos from the command line
Generating videos via an API
Generating videos in a web UI
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
06_gpu_and_ml/image-to-video/image_to_video.py
--prompt
'A young girl stands calmly in the foreground, looking directly at the camera, as a house fire rages in the background.'
--image-path
https
//modal-cdn.com/example_image_to_video_image.png
Copy

## 026_REFERENCE_MODAL_APP
Changelog
API Reference
modal.App
modal.Client
modal.CloudBucketMount
modal.Cls
modal.Cron
modal.Dict
modal.Error
modal.FilePatternMatcher
modal.Function
modal.FunctionCall
modal.Image
modal.NetworkFileSystem
modal.Period
modal.Proxy
modal.Queue
modal.Retries
modal.Sandbox
modal.SandboxSnapshot
modal.Secret
modal.Tunnel
modal.Volume
modal.asgi_app
modal.batched
modal.call_graph
modal.concurrent
modal.container_process
modal.current_function_call_id
modal.current_input_id
modal.enable_output
modal.enter
modal.exit
modal.fastapi_endpoint
modal.file_io
modal.forward
modal.gpu
modal.interact
modal.io_streams
modal.is_local
modal.method
modal.parameter
modal.web_endpoint
modal.web_server
modal.wsgi_app
modal.exception
modal.config
CLI Reference
modal app
modal config
modal container
modal deploy
modal dict
modal environment
modal launch
modal nfs
modal profile
modal queue
modal run
modal secret
modal serve
modal setup
modal shell
modal token
modal volume
modal.App
class
object
Copy
A Modal App is a group of functions and classes that are deployed together.
The app serves at least three purposes:
A unit of deployment for functions and classes.
Syncing of identities of (primarily) functions and classes across processes
(your local Python interpreter and every Modal container active in your application).
Manage log collection for everything that happens inside your code.
Registering functions with an app
The most common way to explicitly register an Object with an app is through the
@app.function()
decorator. It both registers the annotated function itself and
other passed objects, like schedules and secrets, with the app:
import
modal
app = modal.App()
@app.function
secrets
=[modal.Secret.from_name(
"some_secret"
schedule
=modal.Period(
days
pass
Copy
In this example, the secret and schedule are registered with the app.
__init__
self
name
: Optional[
None
image
: Optional[_Image] =
None
## default image for all functions (default is `modal.Image.debian_slim()`)
secrets
: Sequence[_Secret] = [],
## default secrets for all functions
volumes
: dict[Union[
, PurePosixPath], _Volume] = {},
## default volumes for all functions
include_source
: Optional[
bool
None
) ->
None
Copy
Construct a new app, optionally with default image, mounts, secrets, or volumes.
image = modal.Image.debian_slim().pip_install(...)
secret = modal.Secret.from_name(
"my-secret"
volume = modal.Volume.from_name(
"my-data"
app = modal.App(
image
=image,
secrets
=[secret],
volumes
"/mnt/data"
: volume})
Copy
name
property
name
self
) -> Optional[
Copy
The user-provided name of the App.
is_interactive
property
is_interactive
self
) ->
bool
Copy
Whether the current app for the app is running in interactive mode.
app_id
property
app_id
self
) -> Optional[
Copy
Return the app_id of a running or stopped app.
description
property
description
self
) -> Optional[
Copy
The App’s
name
, if available, or a fallback descriptive identifier.
lookup
staticmethod
lookup
name
client
: Optional[_Client] =
None
environment_name
: Optional[
None
create_if_missing
bool
False
) ->
"_App"
Copy
Look up an App with a given name, creating a new App if necessary.
Note that Apps created through this method will be in a deployed state,
but they will not have any associated Functions or Classes. This method
is mainly useful for creating an App to associate with a Sandbox:
app = modal.App.lookup(
"my-app"
create_if_missing
True
modal.Sandbox.create(
"echo"
"hi"
=app)
Copy
set_description
set_description
self
description
Copy
image
property
image
self
) -> _Image:
Copy
@contextmanager
self
client
: Optional[_Client] =
None
detach
bool
False
interactive
bool
False
environment_name
: Optional[
None
) -> AsyncGenerator[
"_App"
None
Copy
Context manager that runs an ephemeral app on Modal.
Use this as the main entry point for your Modal application. All calls
to Modal Functions should be made within the scope of this context
manager, and they will correspond to the current App.
Example
with
app.run():
some_modal_function.remote()
Copy
To enable output printing (i.e., to see App logs), use
modal.enable_output()
with
modal.enable_output():
with
app.run():
some_modal_function.remote()
Copy
Note that you should not invoke this in global scope of a file where you have
Modal Functions or Classes defined, since that would run the block when the Function
or Cls is imported in your containers as well. If you want to run it as your entrypoint,
consider protecting it:
__name__
"__main__"
with
app.run():
some_modal_function.remote()
Copy
You can then run your script with:
python
app_module.py
Copy
deploy
deploy
self
name
: Optional[
None
## Name for the deployment, overriding any set on the App
environment_name
: Optional[
None
## Environment to deploy the App in
## Optional metadata that will be visible in the deployment history
client
: Optional[_Client] =
None
## Alternate client to use for RPCs
) -> typing_extensions.Self:
Copy
Deploy the App so that it is available persistently.
Deployed Apps will be avaible for lookup or web-based invocations until they are stopped.
Unlike with
App.run
, this method will return as soon as the deployment completes.
This method is a programmatic alternative to the
modal deploy
CLI command.
Examples:
app = App(
"my-app"
app.deploy()
Copy
To enable output printing (i.e., to see build logs), use
modal.enable_output()
app = App(
"my-app"
with
modal.enable_output():
app.deploy()
Copy
Unlike with
App.run
, Function logs will not stream back to the local client after the
App is deployed.
Note that you should not invoke this method in global scope, as that would redeploy
the App every time the file is imported. If you want to write a programmatic deployment
script, protect this call so that it only runs when the file is executed directly:
__name__
"__main__"
with
modal.enable_output():
app.deploy()
Copy
Then you can deploy your app with:
python
app_module.py
Copy
registered_functions
property
registered_functions
self
) -> dict[
, _Function]:
Copy
All modal.Function objects registered on the app.
registered_classes
property
registered_classes
self
) -> dict[
, _Cls]:
Copy
All modal.Cls objects registered on the app.
registered_entrypoints
property
registered_entrypoints
self
) -> dict[
, _LocalEntrypoint]:
Copy
All local CLI entrypoints registered on the app.
registered_web_endpoints
property
registered_web_endpoints
self
) -> list[
Copy
Names of web endpoint (ie. webhook) functions registered on the app.
local_entrypoint
local_entrypoint
self
_warn_parentheses_missing
: Any =
None
, *,
name
: Optional[
None
) -> Callable[[Callable[..., Any]], _LocalEntrypoint]:
Copy
Decorate a function to be used as a CLI entrypoint for a Modal App.
These functions can be used to define code that runs locally to set up the app,
and act as an entrypoint to start Modal functions from. Note that regular
Modal functions can also be used as CLI entrypoints, but unlike
local_entrypoint
those functions are executed remotely directly.
Example
@app.local_entrypoint
main
some_modal_function.remote()
Copy
You can call the function using
modal run
directly from the CLI:
modal
app_module.py
Copy
Note that an explicit
app.run()
is not needed, as an
is automatically created for you.
Multiple Entrypoints
If you have multiple
local_entrypoint
functions, you can qualify the name of your app and function:
modal
app_module.py::app.some_other_function
Copy
Parsing Arguments
If your entrypoint function take arguments with primitive types,
modal run
automatically parses them as
CLI options.
For example, the following function can be called with
modal run app_module.py --foo 1 --bar "hello"
@app.local_entrypoint
main
some_modal_function.call(foo, bar)
Copy
Currently,
float
bool
, and
datetime.datetime
are supported.
modal run app_module.py --help
for more information on usage.
function
@warn_on_renamed_autoscaler_settings
function
self
_warn_parentheses_missing
: Any =
None
image
: Optional[_Image] =
None
## The image to run as the container for the function
schedule
: Optional[Schedule] =
None
## An optional Modal Schedule for the function
secrets
: Sequence[_Secret] = (),
## Optional Modal Secret objects with environment variables for the container
: Union[
GPU_T, list[GPU_T]
None
## GPU request as string ("any", "T4", ...), object (`modal.GPU.A100()`, ...), or a list of either
serialized
bool
False
## Whether to send the function over using cloudpickle.
network_file_systems
: dict[
Union[
, PurePosixPath], _NetworkFileSystem
] = {},
## Mountpoints for Modal NetworkFileSystems
volumes
: dict[
Union[
, PurePosixPath], Union[_Volume, _CloudBucketMount]
] = {},
## Mount points for Modal Volumes & CloudBucketMounts
## Specify, in fractional CPU cores, how many CPU cores to request.
## Or, pass (request, limit) to additionally specify a hard limit in fractional CPU cores.
## CPU throttling will prevent a container from exceeding its specified limit.
: Optional[Union[
float
, tuple[
float
float
]]] =
None
## Specify, in MiB, a memory request which is the minimum memory required.
## Or, pass (request, limit) to additionally specify a hard limit in MiB.
memory
: Optional[Union[
, tuple[
]]] =
None
ephemeral_disk
: Optional[
None
## Specify, in MiB, the ephemeral disk size for the Function.
min_containers
: Optional[
None
## Minimum number of containers to keep warm, even when Function is idle.
max_containers
: Optional[
None
## Limit on the number of containers that can be concurrently running.
buffer_containers
: Optional[
None
## Number of additional idle containers to maintain under active load.
scaledown_window
: Optional[
None
## Max time (in seconds) a container can remain idle while scaling down.
proxy
: Optional[_Proxy] =
None
## Reference to a Modal Proxy to use in front of this function.
retries
: Optional[Union[
, Retries]] =
None
## Number of times to retry each input in case of failure.
timeout
: Optional[
None
## Maximum execution time of the function in seconds.
name
: Optional[
None
## Sets the Modal name of the function within the app
is_generator
: Optional[
bool
None
## Set this to True if it's a non-generator function returning a [sync/async] generator object
cloud
: Optional[
None
## Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
region
: Optional[Union[
, Sequence[
]]] =
None
## Region or regions to run the function on.
enable_memory_snapshot
bool
False
## Enable memory checkpointing for faster cold starts.
block_network
bool
False
## Whether to block network access
restrict_modal_access
bool
False
## Whether to allow this function access to other Modal resources
## Maximum number of inputs a container should handle before shutting down.
## With `max_inputs = 1`, containers will be single-use.
max_inputs
: Optional[
None
i6pn
: Optional[
bool
None
## Whether to enable IPv6 container networking within the region.
## Whether the function's home package should be included in the image - defaults to True
include_source
: Optional[
bool
None
## When `False`, don't automatically add the App source to the container.
experimental_options
: Optional[dict[
, Any]] =
None
## Parameters below here are experimental. Use with caution!
_experimental_scheduler_placement
: Optional[
SchedulerPlacement
None
## Experimental controls over fine-grained scheduling (alpha).
_experimental_proxy_ip
: Optional[
None
## IP address of proxy
_experimental_custom_scaling_factor
: Optional[
float
None
## Custom scaling factor
_experimental_enable_gpu_snapshot
bool
False
## Experimentally enable GPU memory snapshots.
## Parameters below here are deprecated. Please update your code as suggested
keep_warm
: Optional[
None
## Replaced with `min_containers`
concurrency_limit
: Optional[
None
## Replaced with `max_containers`
container_idle_timeout
: Optional[
None
## Replaced with `scaledown_window`
allow_concurrent_inputs
: Optional[
None
## Replaced with the `@modal.concurrent` decorator
_experimental_buffer_containers
: Optional[
None
## Now stable API with `buffer_containers`
allow_cross_region_volumes
: Optional[
bool
None
## Always True on the Modal backend now
) -> _FunctionDecoratorType:
Copy
Decorator to register a new Modal
Function
with this App.
@typing_extensions.dataclass_transform
field_specifiers
=(parameter,),
kw_only_default
True
@warn_on_renamed_autoscaler_settings
self
_warn_parentheses_missing
: Optional[
bool
None
image
: Optional[_Image] =
None
## The image to run as the container for the function
secrets
: Sequence[_Secret] = (),
## Optional Modal Secret objects with environment variables for the container
: Union[
GPU_T, list[GPU_T]
None
## GPU request as string ("any", "T4", ...), object (`modal.GPU.A100()`, ...), or a list of either
serialized
bool
False
## Whether to send the function over using cloudpickle.
network_file_systems
: dict[
Union[
, PurePosixPath], _NetworkFileSystem
] = {},
## Mountpoints for Modal NetworkFileSystems
volumes
: dict[
Union[
, PurePosixPath], Union[_Volume, _CloudBucketMount]
] = {},
## Mount points for Modal Volumes & CloudBucketMounts
## Specify, in fractional CPU cores, how many CPU cores to request.
## Or, pass (request, limit) to additionally specify a hard limit in fractional CPU cores.
## CPU throttling will prevent a container from exceeding its specified limit.
: Optional[Union[
float
, tuple[
float
float
]]] =
None
## Specify, in MiB, a memory request which is the minimum memory required.
## Or, pass (request, limit) to additionally specify a hard limit in MiB.
memory
: Optional[Union[
, tuple[
]]] =
None
ephemeral_disk
: Optional[
None
## Specify, in MiB, the ephemeral disk size for the Function.
min_containers
: Optional[
None
## Minimum number of containers to keep warm, even when Function is idle.
max_containers
: Optional[
None
## Limit on the number of containers that can be concurrently running.
buffer_containers
: Optional[
None
## Number of additional idle containers to maintain under active load.
scaledown_window
: Optional[
None
## Max time (in seconds) a container can remain idle while scaling down.
proxy
: Optional[_Proxy] =
None
## Reference to a Modal Proxy to use in front of this function.
retries
: Optional[Union[
, Retries]] =
None
## Number of times to retry each input in case of failure.
timeout
: Optional[
None
## Maximum execution time of the function in seconds.
cloud
: Optional[
None
## Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
region
: Optional[Union[
, Sequence[
]]] =
None
## Region or regions to run the function on.
enable_memory_snapshot
bool
False
## Enable memory checkpointing for faster cold starts.
block_network
bool
False
## Whether to block network access
restrict_modal_access
bool
False
## Whether to allow this class access to other Modal resources
## Limits the number of inputs a container handles before shutting down.
## Use `max_inputs = 1` for single-use containers.
max_inputs
: Optional[
None
include_source
: Optional[
bool
None
## When `False`, don't automatically add the App source to the container.
experimental_options
: Optional[dict[
, Any]] =
None
## Parameters below here are experimental. Use with caution!
_experimental_scheduler_placement
: Optional[
SchedulerPlacement
None
## Experimental controls over fine-grained scheduling (alpha).
_experimental_proxy_ip
: Optional[
None
## IP address of proxy
_experimental_custom_scaling_factor
: Optional[
float
None
## Custom scaling factor
_experimental_enable_gpu_snapshot
bool
False
## Experimentally enable GPU memory snapshots.
## Parameters below here are deprecated. Please update your code as suggested
keep_warm
: Optional[
None
## Replaced with `min_containers`
concurrency_limit
: Optional[
None
## Replaced with `max_containers`
container_idle_timeout
: Optional[
None
## Replaced with `scaledown_window`
allow_concurrent_inputs
: Optional[
None
## Replaced with the `@modal.concurrent` decorator
_experimental_buffer_containers
: Optional[
None
## Now stable API with `buffer_containers`
allow_cross_region_volumes
: Optional[
bool
None
## Always True on the Modal backend now
) -> Callable[[Union[CLS_T, _PartialFunction]], CLS_T]:
Copy
Decorator to register a new Modal
with this App.
include
include
self
, /,
other_app
"_App"
) -> typing_extensions.Self:
Copy
Include another App’s objects in this one.
Useful for splitting up Modal Apps across different self-contained files.
app_a = modal.App(
@app.function
app_b = modal.App(
@app.function
app_a.include(app_b)
@app_a.local_entrypoint
main
## use function declared on the included app
bar.remote()
Copy
modal.App
name
is_interactive
app_id
description
lookup
set_description
image
deploy
registered_functions
registered_classes
registered_entrypoints
registered_web_endpoints
local_entrypoint
function
include

## 027_EXAMPLES_S3_BUCKET_MOUNT
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Analyze NYC yellow taxi data with DuckDB on Parquet files from S3
This example shows how to use Modal for a classic data science task: loading table-structured data into cloud stores,
analyzing it, and plotting the results.
In particular, we’ll load public NYC taxi ride data into S3 as Parquet files,
then run SQL queries on it with DuckDB.
We’ll mount the S3 bucket in a Modal app with
CloudBucketMount
We will write to and then read from that bucket, in each case using
Modal’s
parallel execution features
to handle many files at once.
Basic setup
You will need to have an S3 bucket and AWS credentials to run this example. Refer to the documentation
for the exact
IAM permissions
your credentials will need.
After you are done creating a bucket and configuring IAM settings,
you now need to create a
Secret
to share
the relevant AWS credentials with your Modal apps.
from
datetime
import
datetime
from
pathlib
import
Path, PosixPath
import
modal
image = modal.Image.debian_slim(
python_version
"3.12"
).pip_install(
"requests==2.31.0"
"duckdb==0.10.0"
"matplotlib==3.8.3"
app = modal.App(
image
=image)
secret = modal.Secret.from_name(
"s3-bucket-secret"
required_keys
"AWS_ACCESS_KEY_ID"
"AWS_SECRET_ACCESS_KEY"
MOUNT_PATH = PosixPath(
"/bucket"
YELLOW_TAXI_DATA_PATH = MOUNT_PATH /
"yellow_taxi"
Copy
The dependencies installed above are not available locally. The following block instructs Modal
to only import them inside the container.
with
image.imports():
import
duckdb
import
requests
Copy
Download New York City’s taxi data
NYC makes data about taxi rides publicly available. The city’s
Taxi & Limousine Commission (TLC)
publishes files in the Parquet format. Files are organized by year and month.
We are going to download all available files and store them in an S3 bucket. We do this by
attaching a
modal.CloudBucketMount
with the S3 bucket name and its respective credentials.
The files in the bucket will then be available at
MOUNT_PATH
As we’ll see below, this operation can be massively sped up by running it in parallel on Modal.
@app.function
volumes
MOUNT_PATH: modal.CloudBucketMount(
"modal-s3mount-test-bucket"
secret
=secret),
download_data
year
month
) ->
filename =
"yellow_tripdata_
year
month
:02d}
.parquet"
url =
"https://d37ci6vzurychx.cloudfront.net/trip-data/
filename
s3_path = MOUNT_PATH / filename
## Skip downloading if file exists.
s3_path.exists():
YELLOW_TAXI_DATA_PATH.exists():
YELLOW_TAXI_DATA_PATH.mkdir(
parents
True
exist_ok
True
with
requests.get(url,
stream
True
r.raise_for_status()
print
"downloading =>
s3_path
## It looks like we writing locally, but this is actually writing to S3!
with
open
(s3_path,
"wb"
file:
chunk
r.iter_content(
chunk_size
8192
file.write(chunk)
return
s3_path.as_posix()
Copy
Analyze data with DuckDB
DuckDB
is an analytical database with rich support for Parquet files.
It is also very fast. Below, we define a Modal Function that aggregates yellow taxi trips
within a month (each file contains all the rides from a specific month).
@app.function
volumes
MOUNT_PATH: modal.CloudBucketMount(
"modal-s3mount-test-bucket"
secret
=modal.Secret.from_name(
"s3-bucket-secret"
aggregate_data
path
) -> list[tuple[datetime,
print
"processing =>
path
## Parse file.
year_month_part = path.split(
"yellow_tripdata_"
year, month = year_month_part.split(
month = month.replace(
".parquet"
## Make DuckDB query using in-memory storage.
con = duckdb.connect(
database
":memory:"
with sub as (
select tpep_pickup_datetime::date d, count(1) c
from read_parquet(?)
group by 1
select d, c from sub
where date_part('year', d) = ? -- filter out garbage
and date_part('month', d) = ? -- same
con.execute(q, (path, year, month))
return
list
(con.fetchall())
Copy
Plot daily taxi rides
Finally, we want to plot our results.
The plot created shows the number of yellow taxi rides per day in NYC.
This function runs remotely, on Modal, so we don’t need to install plotting libraries locally.
@app.function
plot
dataset
) ->
bytes
import
import
matplotlib.pyplot
## Sorting data by date
dataset.sort(
lambda
: x[
## Unpacking dates and values
dates, values =
(*dataset)
## Plotting
plt.figure(
figsize
plt.plot(dates, values)
plt.title(
"Number of NYC yellow taxi trips by weekday, 2018-2023"
plt.ylabel(
"Number of daily trips"
plt.grid(
True
plt.tight_layout()
## Saving plot as raw bytes to send back
buf = io.BytesIO()
plt.savefig(buf,
format
"png"
buf.seek(
return
buf.getvalue()
Copy
Run everything
@app.local_entrypoint()
defines what happens when we run our Modal program locally.
We invoke it from the CLI by calling
modal run s3_bucket_mount.py
We first call
download_data()
starmap
(named because it’s kind of like
map(*args)
on tuples of inputs
(year, month)
. This will download, in parallel,
all yellow taxi data files into our locally mounted S3 bucket and return a list of
Parquet file paths. Then, we call
aggregate_data()
with
on that list. These files are
also read from our S3 bucket. So one function writes files to S3 and the other
reads files from S3 in; both run across many files in parallel.
Finally, we call
plot
to generate the following figure:
This program should run in less than 30 seconds.
@app.local_entrypoint
main
## List of tuples[year, month].
inputs = [(year, month)
year
range
2018
2023
month
range
## List of file paths in S3.
parquet_files: list[
] = []
path
download_data.starmap(inputs):
print
"done =>
path
parquet_files.append(path)
## List of datetimes and number of yellow taxi trips.
dataset = []
aggregate_data.map(parquet_files):
dataset += r
= Path(
"/tmp"
"s3_bucket_mount"
.exists():
.mkdir(
exist_ok
True
parents
True
figure = plot.remote(dataset)
path =
"nyc_yellow_taxi_trips_s3_mount.png"
with
open
(path,
"wb"
file:
print
"Saving figure to
path
file.write(figure)
Copy
Analyze NYC yellow taxi data with DuckDB on Parquet files from S3
Basic setup
Download New York City’s taxi data
Analyze data with DuckDB
Plot daily taxi rides
Run everything
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
10_integrations/s3_bucket_mount.py
Copy

## 028_EXAMPLES_VLLM_INFERENCE
Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Run OpenAI-compatible LLM inference with LLaMA 3.1-8B and vLLM
LLMs do more than just model language: they chat, they produce JSON and XML, they run code, and more.
This has complicated their interface far beyond “text-in, text-out”.
OpenAI’s API has emerged as a standard for that interface,
and it is supported by open source LLM serving frameworks like
vLLM
In this example, we show how to run a vLLM server in OpenAI-compatible mode on Modal.
Our examples repository also includes scripts for running clients and load-testing for OpenAI-compatible APIs
here
You can find a (somewhat out-of-date) video walkthrough of this example and the related scripts on the Modal YouTube channel
here
Set up the container image
Our first order of business is to define the environment our server will run in:
container
Image
vLLM can be installed with
, since Modal
provides the CUDA drivers
To take advantage of optimized kernels for CUDA 12.8, we install PyTorch, flashinfer, and their dependencies
via an
extra
Python package index.
import
json
from
typing
import
import
aiohttp
import
modal
vllm_image = (
modal.Image.debian_slim(
python_version
"3.12"
.pip_install(
"vllm==0.9.1"
"huggingface_hub[hf_transfer]==0.32.0"
"flashinfer-python==0.2.6.post1"
extra_index_url
"https://download.pytorch.org/whl/cu128"
.env({
"HF_HUB_ENABLE_HF_TRANSFER"
## faster model transfers
Copy
Download the model weights
We’ll be running a pretrained foundation model — Meta’s LLaMA 3.1 8B
in the Instruct variant that’s trained to chat and follow instructions.
Model parameters are often quantized to a lower precision during training
than they are run at during inference.
We’ll use an eight bit floating point quantization from Neural Magic/Red Hat.
Native hardware support for FP8 formats in
Tensor Cores
is limited to the latest
Streaming Multiprocessor architectures
like those of Modal’s
Hopper H100/H200 and Blackwell B200 GPUs
You can swap this model out for another by changing the strings below.
A single B200 GPUs has enough VRAM to store a 70,000,000,000 parameter model,
like Llama 3.3, in eight bit precision, along with a very large KV cache.
MODEL_NAME =
"RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8"
MODEL_REVISION =
"12fd6884d2585dd4d020373e7f39f74507b31866"
## avoid nasty surprises when repos update!
Copy
Although vLLM will download weights from Hugging Face on-demand,
we want to cache them so we don’t do it every time our server starts.
We’ll use
Modal Volumes
for our cache.
Modal Volumes are essentially a “shared disk” that all Modal Functions can access like it’s a regular disk.
hf_cache_vol = modal.Volume.from_name(
"huggingface-cache"
create_if_missing
True
Copy
We’ll also cache some of vLLM’s JIT compilation artifacts in a Modal Volume.
vllm_cache_vol = modal.Volume.from_name(
"vllm-cache"
create_if_missing
True
Copy
Configuring vLLM
The V1 engine
In its 0.7 release, in early 2025, vLLM added a new version of its backend infrastructure,
V1 Engine
Using this new engine can lead to some
impressive speedups
It was made the default in version 0.8 and is
slated for complete removal by 0.11
in late summer of 2025.
A small number of features, described in the RFC above, may still require the V0 engine prior to removal.
Until deprecation, you can use it by setting the below environment variable to
vllm_image = vllm_image.env({
"VLLM_USE_V1"
Copy
Trading off fast boots and token generation performance
vLLM has embraced dynamic and just-in-time compilation to eke out additional performance without having to write too many custom kernels,
e.g. via the Torch compiler and CUDA graph capture.
These compilation features incur latency at startup in exchange for lowered latency and higher throughput during generation.
We make this trade-off controllable with the
FAST_BOOT
variable below.
FAST_BOOT =
True
Copy
If you’re running an LLM service that frequently scales from 0 (frequent
“cold starts”
then you’ll want to set this to
True
If you’re running an LLM service that usually has multiple replicas running, then set this to
False
for improved performance.
See the code below for details on the parameters that
FAST_BOOT
controls.
For more on the performance you can expect when serving your own LLMs, see
our LLM engine performance benchmarks
Build a vLLM engine and serve it
The function below spawns a vLLM instance listening at port 8000, serving requests to our model.
We wrap it in the
@modal.web_server
decorator
to connect it to the Internet.
The server runs in an independent process, via
subprocess.Popen
, and only starts accepting requests
once the model is spun up and the
serve
function returns.
app = modal.App(
"example-vllm-openai-compatible"
N_GPU =
MINUTES =
## seconds
VLLM_PORT =
8000
@app.function
image
=vllm_image,
"B200:
N_GPU
scaledown_window
* MINUTES,
## how long should we stay up with no requests?
timeout
* MINUTES,
## how long should we wait for container start?
volumes
"/root/.cache/huggingface"
: hf_cache_vol,
"/root/.cache/vllm"
: vllm_cache_vol,
@modal.concurrent
## how many requests can one replica handle? tune carefully!
max_inputs
@modal.web_server
port
=VLLM_PORT,
startup_timeout
* MINUTES)
serve
import
subprocess
cmd = [
"vllm"
"serve"
"--uvicorn-log-level=info"
MODEL_NAME,
"--revision"
MODEL_REVISION,
"--served-model-name"
MODEL_NAME,
"llm"
"--host"
"0.0.0.0"
"--port"
(VLLM_PORT),
## enforce-eager disables both Torch compilation and CUDA graph capture
## default is no-enforce-eager. see the --compilation-config flag for tighter control
cmd += [
"--enforce-eager"
FAST_BOOT
else
"--no-enforce-eager"
## assume multiple GPUs are for splitting up large matrix multiplications
cmd += [
"--tensor-parallel-size"
(N_GPU)]
print
(cmd)
subprocess.Popen(
.join(cmd),
shell
True
Copy
Deploy the server
To deploy the API on Modal, just run
modal
deploy
vllm_inference.py
Copy
This will create a new app on Modal, build the container image for it if it hasn’t been built yet,
and deploy the app.
Interact with the server
Once it is deployed, you’ll see a URL appear in the command line,
something like
https://your-workspace-name--example-vllm-openai-compatible-serve.modal.run
You can find
interactive Swagger UI docs
at the
/docs
route of that URL, i.e.
https://your-workspace-name--example-vllm-openai-compatible-serve.modal.run/docs
These docs describe each route and indicate the expected input and output
and translate requests into
curl
commands.
For simple routes like
/health
, which checks whether the server is responding,
you can even send a request directly from the docs.
To interact with the API programmatically in Python, we recommend the
openai
library.
See the
client.py
script in the examples repository
here
to take it for a spin:
## pip install openai==1.76.0
python
openai_compatible/client.py
Copy
Testing the server
To make it easier to test the server setup, we also include a
local_entrypoint
that does a healthcheck and then hits the server.
If you execute the command
modal
vllm_inference.py
Copy
a fresh replica of the server will be spun up on Modal while
the code below executes on your local machine.
Think of this like writing simple tests inside of the
if __name__ == "__main__"
block of a Python script, but for cloud deployments!
@app.local_entrypoint
async
test
test_timeout
* MINUTES,
content
None
twice
True
url = serve.get_web_url()
system_prompt = {
"role"
"system"
"content"
"You are a pirate who can't help but drop sly reminders that he went to Harvard."
content
None
content =
"Explain the singular value decomposition."
messages = [
## OpenAI chat format
system_prompt,
"role"
"user"
"content"
: content},
async
with
aiohttp.ClientSession(
base_url
=url)
session:
print
"Running health check for server at
async
with
session.get(
"/health"
timeout
=test_timeout -
* MINUTES)
resp:
up = resp.status ==
assert
"Failed health check for server at
print
"Successful health check for server at
print
"Sending messages to
, *messages,
\n\t
await
_send_request(session,
"llm"
, messages)
twice:
messages[
"content"
"You are Jar Jar Binks."
print
"Sending messages to
, *messages,
\n\t
await
_send_request(session,
"llm"
, messages)
async
_send_request
session
: aiohttp.ClientSession,
model
messages
list
) ->
None
## `stream=True` tells an OpenAI-compatible backend to stream chunks
payload: dict[
, Any] = {
"messages"
: messages,
"model"
: model,
"stream"
True
headers = {
"Content-Type"
"application/json"
"Accept"
"text/event-stream"
async
with
session.post(
"/v1/chat/completions"
json
=payload,
headers
=headers,
timeout
* MINUTES
resp:
async
resp.content:
resp.raise_for_status()
## extract new content and stream it
line = raw.decode().strip()
line
line ==
"data: [DONE]"
continue
line.startswith(
"data: "
## SSE prefix
line = line[
"data: "
) :]
chunk = json.loads(line)
assert
chunk[
"object"
] ==
"chat.completion.chunk"
## or something went horribly wrong
print
(chunk[
"choices"
"delta"
"content"
print
Copy
We also include a basic example of a load-testing setup using
locust
in the
load_test.py
script
here
modal
openai_compatible/load_test.py
Copy
Run OpenAI-compatible LLM inference with LLaMA 3.1-8B and vLLM
Set up the container image
Download the model weights
Configuring vLLM
The V1 engine
Trading off fast boots and token generation performance
Build a vLLM engine and serve it
Deploy the server
Interact with the server
Testing the server
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
06_gpu_and_ml/llm-serving/vllm_inference.py
Copy