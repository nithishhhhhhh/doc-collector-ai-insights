URL: https://modal.com/docs/examples/image_to_video
==================================================

Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Animate images with Lightricks LTX-Video via CLI, API, and web UI
This example shows how to run
LTX-Video
on Modal
to generate videos from your local command line, via an API, and in a web UI.
Generating a 5 second video takes ~1 minute from cold start.
Once the container is warm, a 5 second video takes ~15 seconds.
Here is a sample we generated:
Basic setup
import
import
random
import
time
from
pathlib
import
Path
from
typing
import
Annotated, Optional
import
fastapi
import
modal
Copy
All Modal programs need an
an object that acts as a recipe for the application.
app = modal.App(
"example-image-to-video"
Copy
Configuring dependencies
The model runs remotely, on Modalâ€™s cloud, which means we need to
define the environment it runs in
Below, we start from a lightweight base Linux image
and then install our system and Python dependencies,
like Hugging Faceâ€™s
diffusers
library and
torch
image = (
modal.Image.debian_slim(
python_version
"3.12"
.apt_install(
"python3-opencv"
.pip_install(
"accelerate==1.4.0"
"diffusers==0.32.2"
"fastapi[standard]==0.115.8"
"huggingface-hub[hf_transfer]==0.29.1"
"imageio==2.37.0"
"imageio-ffmpeg==0.6.0"
"opencv-python==4.11.0.86"
"pillow==11.1.0"
"sentencepiece==0.2.0"
"torch==2.6.0"
"torchvision==0.21.0"
"transformers==4.49.0"
Copy
Storing model weights on Modal
We also need the parameters of the model remotely.
They can be loaded at runtime from Hugging Face,
based on a repository ID and a revision (aka a commit SHA).
MODEL_ID =
"Lightricks/LTX-Video"
MODEL_REVISION_ID =
"a6d59ee37c13c58261aa79027d3e41cd41960925"
Copy
Hugging Face will also cache the weights to disk once theyâ€™re downloaded.
But Modal Functions are serverless, and so even disks are ephemeral,
which means the weights would get re-downloaded every time we spin up a new instance.
We can fix this â€” without any modifications to Hugging Faceâ€™s model loading code! â€”
by pointing the Hugging Face cache at a
Modal Volume
model_volume = modal.Volume.from_name(
"hf-hub-cache"
create_if_missing
True
MODEL_PATH =
"/models"
# where the Volume will appear on our Functions' filesystems
image = image.env(
"HF_HUB_ENABLE_HF_TRANSFER"
# faster downloads
"HF_HUB_CACHE"
: MODEL_PATH,
Copy
Storing model outputs on Modal
Contemporary video models can take a long time to run and they produce large outputs.
That makes them a great candidate for storage on Modal Volumes as well.
Python code running outside of Modal can also access this storage, as weâ€™ll see below.
OUTPUT_PATH =
"/outputs"
output_volume = modal.Volume.from_name(
"outputs"
create_if_missing
True
Copy
Implementing LTX-Video inference on Modal
We wrap the inference logic in a Modal
that ensures models are loaded and then moved to the GPU once when a new instance
starts, rather than every time we run it.
function just wraps a
diffusers
pipeline.
It saves the generated video to a Modal Volume, and returns the filename.
We also include a
wrapper that makes it possible
to trigger inference via an API call.
For details, see the
/docs
route of the URL ending in
inference-web.modal.run
that appears when you deploy the app.
with
image.imports():
# loaded on all of our remote Functions
import
diffusers
import
torch
from
import
Image
MINUTES =
@app.cls
image
=image,
"H100"
timeout
* MINUTES,
scaledown_window
* MINUTES,
volumes
={MODEL_PATH: model_volume, OUTPUT_PATH: output_volume},
class
Inference
@modal.enter
load_pipeline
self
self
.pipe = diffusers.LTXImageToVideoPipeline.from_pretrained(
MODEL_ID,
revision
=MODEL_REVISION_ID,
torch_dtype
=torch.bfloat16,
).to(
"cuda"
@modal.method
self
image_bytes
bytes
prompt
negative_prompt
: Optional[
None
num_frames
: Optional[
None
num_inference_steps
: Optional[
None
seed
: Optional[
None
) ->
negative_prompt = (
negative_prompt
"worst quality, inconsistent motion, blurry, jittery, distorted"
width =
height =
num_frames = num_frames
num_inference_steps = num_inference_steps
seed = seed
random.randint(
print
"Seeding RNG with:
seed
torch.manual_seed(seed)
image = diffusers.utils.load_image(Image.open(io.BytesIO(image_bytes)))
video =
self
.pipe(
image
=image,
prompt
=prompt,
negative_prompt
=negative_prompt,
width
=width,
height
=height,
num_frames
=num_frames,
num_inference_steps
=num_inference_steps,
).frames[
mp4_name = (
seed
.join(c
c.isalnum()
else
prompt[:
.mp4"
diffusers.utils.export_to_video(
video,
Path(OUTPUT_PATH) / mp4_name
output_volume.commit()
torch.cuda.empty_cache()
# reduce fragmentation
return
mp4_name
@modal.fastapi_endpoint
method
"POST"
docs
True
self
image_bytes
: Annotated[
bytes
, fastapi.File()],
prompt
negative_prompt
: Optional[
None
num_frames
: Optional[
None
num_inference_steps
: Optional[
None
seed
: Optional[
None
) -> fastapi.Response:
mp4_name =
self
.run.local(
# run in the same container
image_bytes
=image_bytes,
prompt
=prompt,
negative_prompt
=negative_prompt,
num_frames
=num_frames,
num_inference_steps
=num_inference_steps,
seed
=seed,
return
fastapi.responses.FileResponse(
path
Path(OUTPUT_PATH) / mp4_name
media_type
"video/mp4"
filename
=mp4_name,
Copy
Generating videos from the command line
We add a
local entrypoint
that calls the
Inference.run
method to run inference from the command line.
The functionâ€™s parameters are automatically turned into a CLI.
Run it with
modal
image_to_video.py
--prompt
"A cat looking out the window at a snowy mountain"
--image-path
/path/to/cat.jpg
Copy
You can also pass
--help
to see the full list of arguments.
@app.local_entrypoint
entrypoint
image_path
prompt
negative_prompt
: Optional[
None
num_frames
: Optional[
None
num_inference_steps
: Optional[
None
seed
: Optional[
None
twice
bool
True
import
import
urllib.request
print
"ðŸŽ¥ Generating a video from the image at
image_path
print
"ðŸŽ¥ using the prompt
prompt
image_path.startswith((
"http://"
"https://"
image_bytes = urllib.request.urlopen(image_path).read()
elif
os.path.isfile(image_path):
image_bytes = Path(image_path).read_bytes()
else
raise
ValueError
image_path
is not a valid file or URL."
inference_service = Inference()
range
+ twice):
start = time.time()
mp4_name = inference_service.run.remote(
image_bytes
=image_bytes,
prompt
=prompt,
negative_prompt
=negative_prompt,
num_frames
=num_frames,
seed
=seed,
duration = time.time() - start
print
"ðŸŽ¥ Generated video in
duration
:.3f}
output_dir = Path(
"/tmp/image_to_video"
output_dir.mkdir(
exist_ok
True
parents
True
output_path = output_dir / mp4_name
# read in the file from the Modal Volume, then write it to the local disk
output_path.write_bytes(
.join(output_volume.read_file(mp4_name)))
print
"ðŸŽ¥ Video saved to
output_path
Copy
Generating videos via an API
The Modal
above also included a
fastapi_endpoint
which adds a simple web API to the inference method.
To try it out, run
modal
deploy
image_to_video.py
Copy
copy the printed URL ending in
inference-web.modal.run
and add
/docs
to the end. This will bring up the interactive
Swagger/OpenAPI docs for the endpoint.
Generating videos in a web UI
Lastly, we add a simple front-end web UI (written in Alpine.js) for
our image to video backend.
This is also deployed when you run
modal
deploy
image_to_video.py.
Copy
Inference
class will serve multiple users from its own auto-scaling pool of warm GPU containers automatically,
and they will spin down when there are no requests.
frontend_path = Path(
__file__
).parent /
"frontend"
web_image = (
modal.Image.debian_slim(
python_version
"3.12"
.pip_install(
"jinja2==3.1.5"
"fastapi[standard]==0.115.8"
.add_local_dir(
# mount frontend/client code
frontend_path,
remote_path
"/assets"
@app.function
image
=web_image)
@modal.concurrent
max_inputs
1000
@modal.asgi_app
import
fastapi.staticfiles
import
fastapi.templating
web_app = fastapi.FastAPI()
templates = fastapi.templating.Jinja2Templates(
directory
"/assets"
@web_app.get
async
read_root
request
: fastapi.Request):
return
templates.TemplateResponse(
"index.html"
"request"
: request,
"inference_url"
: Inference().web.get_web_url(),
"model_name"
"LTX-Video Image to Video"
"default_prompt"
"A young girl stands calmly in the foreground, looking directly at the camera, as a house fire rages in the background."
web_app.mount(
"/static"
fastapi.staticfiles.StaticFiles(
directory
"/assets"
name
"static"
return
web_app
Copy
Animate images with Lightricks LTX-Video via CLI, API, and web UI
Basic setup
Configuring dependencies
Storing model weights on Modal
Storing model outputs on Modal
Implementing LTX-Video inference on Modal
Generating videos from the command line
Generating videos via an API
Generating videos in a web UI
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
06_gpu_and_ml/image-to-video/image_to_video.py
--prompt
'A young girl stands calmly in the foreground, looking directly at the camera, as a house fire rages in the background.'
--image-path
https
//modal-cdn.com/example_image_to_video_image.png
Copy