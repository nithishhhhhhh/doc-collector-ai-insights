URL: https://modal.com/docs/examples/s3_bucket_mount
==================================================

Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Analyze NYC yellow taxi data with DuckDB on Parquet files from S3
This example shows how to use Modal for a classic data science task: loading table-structured data into cloud stores,
analyzing it, and plotting the results.
In particular, we’ll load public NYC taxi ride data into S3 as Parquet files,
then run SQL queries on it with DuckDB.
We’ll mount the S3 bucket in a Modal app with
CloudBucketMount
We will write to and then read from that bucket, in each case using
Modal’s
parallel execution features
to handle many files at once.
Basic setup
You will need to have an S3 bucket and AWS credentials to run this example. Refer to the documentation
for the exact
IAM permissions
your credentials will need.
After you are done creating a bucket and configuring IAM settings,
you now need to create a
Secret
to share
the relevant AWS credentials with your Modal apps.
from
datetime
import
datetime
from
pathlib
import
Path, PosixPath
import
modal
image = modal.Image.debian_slim(
python_version
"3.12"
).pip_install(
"requests==2.31.0"
"duckdb==0.10.0"
"matplotlib==3.8.3"
app = modal.App(
image
=image)
secret = modal.Secret.from_name(
"s3-bucket-secret"
required_keys
"AWS_ACCESS_KEY_ID"
"AWS_SECRET_ACCESS_KEY"
MOUNT_PATH = PosixPath(
"/bucket"
YELLOW_TAXI_DATA_PATH = MOUNT_PATH /
"yellow_taxi"
Copy
The dependencies installed above are not available locally. The following block instructs Modal
to only import them inside the container.
with
image.imports():
import
duckdb
import
requests
Copy
Download New York City’s taxi data
NYC makes data about taxi rides publicly available. The city’s
Taxi & Limousine Commission (TLC)
publishes files in the Parquet format. Files are organized by year and month.
We are going to download all available files and store them in an S3 bucket. We do this by
attaching a
modal.CloudBucketMount
with the S3 bucket name and its respective credentials.
The files in the bucket will then be available at
MOUNT_PATH
As we’ll see below, this operation can be massively sped up by running it in parallel on Modal.
@app.function
volumes
MOUNT_PATH: modal.CloudBucketMount(
"modal-s3mount-test-bucket"
secret
=secret),
download_data
year
month
) ->
filename =
"yellow_tripdata_
year
month
:02d}
.parquet"
url =
"https://d37ci6vzurychx.cloudfront.net/trip-data/
filename
s3_path = MOUNT_PATH / filename
# Skip downloading if file exists.
s3_path.exists():
YELLOW_TAXI_DATA_PATH.exists():
YELLOW_TAXI_DATA_PATH.mkdir(
parents
True
exist_ok
True
with
requests.get(url,
stream
True
r.raise_for_status()
print
"downloading =>
s3_path
# It looks like we writing locally, but this is actually writing to S3!
with
open
(s3_path,
"wb"
file:
chunk
r.iter_content(
chunk_size
8192
file.write(chunk)
return
s3_path.as_posix()
Copy
Analyze data with DuckDB
DuckDB
is an analytical database with rich support for Parquet files.
It is also very fast. Below, we define a Modal Function that aggregates yellow taxi trips
within a month (each file contains all the rides from a specific month).
@app.function
volumes
MOUNT_PATH: modal.CloudBucketMount(
"modal-s3mount-test-bucket"
secret
=modal.Secret.from_name(
"s3-bucket-secret"
aggregate_data
path
) -> list[tuple[datetime,
print
"processing =>
path
# Parse file.
year_month_part = path.split(
"yellow_tripdata_"
year, month = year_month_part.split(
month = month.replace(
".parquet"
# Make DuckDB query using in-memory storage.
con = duckdb.connect(
database
":memory:"
with sub as (
select tpep_pickup_datetime::date d, count(1) c
from read_parquet(?)
group by 1
select d, c from sub
where date_part('year', d) = ?  -- filter out garbage
and date_part('month', d) = ?   -- same
con.execute(q, (path, year, month))
return
list
(con.fetchall())
Copy
Plot daily taxi rides
Finally, we want to plot our results.
The plot created shows the number of yellow taxi rides per day in NYC.
This function runs remotely, on Modal, so we don’t need to install plotting libraries locally.
@app.function
plot
dataset
) ->
bytes
import
import
matplotlib.pyplot
# Sorting data by date
dataset.sort(
lambda
: x[
# Unpacking dates and values
dates, values =
(*dataset)
# Plotting
plt.figure(
figsize
plt.plot(dates, values)
plt.title(
"Number of NYC yellow taxi trips by weekday, 2018-2023"
plt.ylabel(
"Number of daily trips"
plt.grid(
True
plt.tight_layout()
# Saving plot as raw bytes to send back
buf = io.BytesIO()
plt.savefig(buf,
format
"png"
buf.seek(
return
buf.getvalue()
Copy
Run everything
@app.local_entrypoint()
defines what happens when we run our Modal program locally.
We invoke it from the CLI by calling
modal run s3_bucket_mount.py
We first call
download_data()
starmap
(named because it’s kind of like
map(*args)
on tuples of inputs
(year, month)
. This will download, in parallel,
all yellow taxi data files into our locally mounted S3 bucket and return a list of
Parquet file paths. Then, we call
aggregate_data()
with
on that list. These files are
also read from our S3 bucket. So one function writes files to S3 and the other
reads files from S3 in; both run across many files in parallel.
Finally, we call
plot
to generate the following figure:
This program should run in less than 30 seconds.
@app.local_entrypoint
main
# List of tuples[year, month].
inputs = [(year, month)
year
range
2018
2023
month
range
# List of file paths in S3.
parquet_files: list[
] = []
path
download_data.starmap(inputs):
print
"done =>
path
parquet_files.append(path)
# List of datetimes and number of yellow taxi trips.
dataset = []
aggregate_data.map(parquet_files):
dataset += r
= Path(
"/tmp"
"s3_bucket_mount"
.exists():
.mkdir(
exist_ok
True
parents
True
figure = plot.remote(dataset)
path =
"nyc_yellow_taxi_trips_s3_mount.png"
with
open
(path,
"wb"
file:
print
"Saving figure to
path
file.write(figure)
Copy
Analyze NYC yellow taxi data with DuckDB on Parquet files from S3
Basic setup
Download New York City’s taxi data
Analyze data with DuckDB
Plot daily taxi rides
Run everything
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
10_integrations/s3_bucket_mount.py
Copy