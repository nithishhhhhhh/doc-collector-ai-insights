URL: https://modal.com/docs/examples/vllm_inference
==================================================

Featured
Getting started
Hello, world
Simple web scraper
Serving web endpoints
Large language models (LLMs)
Deploy an OpenAI-compatible LLM service with vLLM
Run DeepSeek-R1 and Phi-4 with llama.cpp
Low-latency, serverless TensorRT-LLM
Run Vision-Language Models with SGLang
Run a multimodal RAG chatbot to answer questions about PDFs
Fine-tune an LLM to replace your CEO
Images, video, & 3D
Fine-tune Wan2.1 video models on your face
Run Flux fast with torch.compile
Fine-tune Flux with LoRA
Animate images with LTX-Video
Generate video clips with LTX-Video
Generate video clips with Mochi
Run Stable Diffusion with a CLI, API, and web UI
Deploy ControlNet demos with Gradio
Audio
Run Text to Speech (TTS) with Chatterbox
Deploy a Moshi voice chatbot
Create music with MusicGen
Real-time communication
Serverless WebRTC
Real-time audio transcription using Parakeet
WebRTC quickstart with FastRTC
Computational biology
Fold proteins with Chai-1
Build a protein-folding dashboard
Fold proteins with Boltz-2
Sandboxed code execution
Run a LangGraph agent's code in a secure GPU sandbox
Build a stateful, sandboxed code interpreter
Run Node.js, Ruby, and more in a Sandbox
Run a sandboxed Jupyter notebook
Embeddings
Embed millions of documents with TEI
Turn satellite images into vectors and store them in MongoDB
Parallel processing and job scheduling
Transcribe podcasts with Whisper
Deploy a Hacker News Slackbot
Run a Document OCR job queue
Serve a Document OCR web app
Training models from scratch
Train an SLM with early-stopping grid search over hyperparameters
Run long, resumable training jobs
Hosting popular libraries
FastHTML: Deploy 100,000 multiplayer checkboxes
YOLO: Fine-tune and serve computer vision models
MultiOn: Create an agent for AI news
Blender: Build a 3D render farm
Streamlit: Run and deploy Streamlit apps
ComfyUI: Run Flux on ComfyUI as an API
SQLite: Publish explorable data with Datasette
Algolia: Build docsearch with a crawler
Connecting to other APIs
Discord: Deploy and run a Discord Bot
Google Sheets: Sync databases and APIs to a Google Sheet
OpenAI: Run a RAG Q&A chatbot
Tailscale: Add Modal Apps to your VPN
Prometheus: Publish custom metrics with Pushgateway
Managing data
Mount S3 buckets in Modal apps
Build your own data warehouse with DuckDB, DBT, and Modal
Create a LoRA Playground with Modal, Gradio, and S3
Miscellaneous
View on GitHub
Run OpenAI-compatible LLM inference with LLaMA 3.1-8B and vLLM
LLMs do more than just model language: they chat, they produce JSON and XML, they run code, and more.
This has complicated their interface far beyond “text-in, text-out”.
OpenAI’s API has emerged as a standard for that interface,
and it is supported by open source LLM serving frameworks like
vLLM
In this example, we show how to run a vLLM server in OpenAI-compatible mode on Modal.
Our examples repository also includes scripts for running clients and load-testing for OpenAI-compatible APIs
here
You can find a (somewhat out-of-date) video walkthrough of this example and the related scripts on the Modal YouTube channel
here
Set up the container image
Our first order of business is to define the environment our server will run in:
container
Image
vLLM can be installed with
, since Modal
provides the CUDA drivers
To take advantage of optimized kernels for CUDA 12.8, we install PyTorch, flashinfer, and their dependencies
via an
extra
Python package index.
import
json
from
typing
import
import
aiohttp
import
modal
vllm_image = (
modal.Image.debian_slim(
python_version
"3.12"
.pip_install(
"vllm==0.9.1"
"huggingface_hub[hf_transfer]==0.32.0"
"flashinfer-python==0.2.6.post1"
extra_index_url
"https://download.pytorch.org/whl/cu128"
.env({
"HF_HUB_ENABLE_HF_TRANSFER"
# faster model transfers
Copy
Download the model weights
We’ll be running a pretrained foundation model — Meta’s LLaMA 3.1 8B
in the Instruct variant that’s trained to chat and follow instructions.
Model parameters are often quantized to a lower precision during training
than they are run at during inference.
We’ll use an eight bit floating point quantization from Neural Magic/Red Hat.
Native hardware support for FP8 formats in
Tensor Cores
is limited to the latest
Streaming Multiprocessor architectures
like those of Modal’s
Hopper H100/H200 and Blackwell B200 GPUs
You can swap this model out for another by changing the strings below.
A single B200 GPUs has enough VRAM to store a 70,000,000,000 parameter model,
like Llama 3.3, in eight bit precision, along with a very large KV cache.
MODEL_NAME =
"RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8"
MODEL_REVISION =
"12fd6884d2585dd4d020373e7f39f74507b31866"
# avoid nasty surprises when repos update!
Copy
Although vLLM will download weights from Hugging Face on-demand,
we want to cache them so we don’t do it every time our server starts.
We’ll use
Modal Volumes
for our cache.
Modal Volumes are essentially a “shared disk” that all Modal Functions can access like it’s a regular disk.
hf_cache_vol = modal.Volume.from_name(
"huggingface-cache"
create_if_missing
True
Copy
We’ll also cache some of vLLM’s JIT compilation artifacts in a Modal Volume.
vllm_cache_vol = modal.Volume.from_name(
"vllm-cache"
create_if_missing
True
Copy
Configuring vLLM
The V1 engine
In its 0.7 release, in early 2025, vLLM added a new version of its backend infrastructure,
V1 Engine
Using this new engine can lead to some
impressive speedups
It was made the default in version 0.8 and is
slated for complete removal by 0.11
in late summer of 2025.
A small number of features, described in the RFC above, may still require the V0 engine prior to removal.
Until deprecation, you can use it by setting the below environment variable to
vllm_image = vllm_image.env({
"VLLM_USE_V1"
Copy
Trading off fast boots and token generation performance
vLLM has embraced dynamic and just-in-time compilation to eke out additional performance without having to write too many custom kernels,
e.g. via the Torch compiler and CUDA graph capture.
These compilation features incur latency at startup in exchange for lowered latency and higher throughput during generation.
We make this trade-off controllable with the
FAST_BOOT
variable below.
FAST_BOOT =
True
Copy
If you’re running an LLM service that frequently scales from 0 (frequent
“cold starts”
then you’ll want to set this to
True
If you’re running an LLM service that usually has multiple replicas running, then set this to
False
for improved performance.
See the code below for details on the parameters that
FAST_BOOT
controls.
For more on the performance you can expect when serving your own LLMs, see
our LLM engine performance benchmarks
Build a vLLM engine and serve it
The function below spawns a vLLM instance listening at port 8000, serving requests to our model.
We wrap it in the
@modal.web_server
decorator
to connect it to the Internet.
The server runs in an independent process, via
subprocess.Popen
, and only starts accepting requests
once the model is spun up and the
serve
function returns.
app = modal.App(
"example-vllm-openai-compatible"
N_GPU =
MINUTES =
# seconds
VLLM_PORT =
8000
@app.function
image
=vllm_image,
"B200:
N_GPU
scaledown_window
* MINUTES,
# how long should we stay up with no requests?
timeout
* MINUTES,
# how long should we wait for container start?
volumes
"/root/.cache/huggingface"
: hf_cache_vol,
"/root/.cache/vllm"
: vllm_cache_vol,
@modal.concurrent
# how many requests can one replica handle? tune carefully!
max_inputs
@modal.web_server
port
=VLLM_PORT,
startup_timeout
* MINUTES)
serve
import
subprocess
cmd = [
"vllm"
"serve"
"--uvicorn-log-level=info"
MODEL_NAME,
"--revision"
MODEL_REVISION,
"--served-model-name"
MODEL_NAME,
"llm"
"--host"
"0.0.0.0"
"--port"
(VLLM_PORT),
# enforce-eager disables both Torch compilation and CUDA graph capture
# default is no-enforce-eager. see the --compilation-config flag for tighter control
cmd += [
"--enforce-eager"
FAST_BOOT
else
"--no-enforce-eager"
# assume multiple GPUs are for splitting up large matrix multiplications
cmd += [
"--tensor-parallel-size"
(N_GPU)]
print
(cmd)
subprocess.Popen(
.join(cmd),
shell
True
Copy
Deploy the server
To deploy the API on Modal, just run
modal
deploy
vllm_inference.py
Copy
This will create a new app on Modal, build the container image for it if it hasn’t been built yet,
and deploy the app.
Interact with the server
Once it is deployed, you’ll see a URL appear in the command line,
something like
https://your-workspace-name--example-vllm-openai-compatible-serve.modal.run
You can find
interactive Swagger UI docs
at the
/docs
route of that URL, i.e.
https://your-workspace-name--example-vllm-openai-compatible-serve.modal.run/docs
These docs describe each route and indicate the expected input and output
and translate requests into
curl
commands.
For simple routes like
/health
, which checks whether the server is responding,
you can even send a request directly from the docs.
To interact with the API programmatically in Python, we recommend the
openai
library.
See the
client.py
script in the examples repository
here
to take it for a spin:
# pip install openai==1.76.0
python
openai_compatible/client.py
Copy
Testing the server
To make it easier to test the server setup, we also include a
local_entrypoint
that does a healthcheck and then hits the server.
If you execute the command
modal
vllm_inference.py
Copy
a fresh replica of the server will be spun up on Modal while
the code below executes on your local machine.
Think of this like writing simple tests inside of the
if __name__ == "__main__"
block of a Python script, but for cloud deployments!
@app.local_entrypoint
async
test
test_timeout
* MINUTES,
content
None
twice
True
url = serve.get_web_url()
system_prompt = {
"role"
"system"
"content"
"You are a pirate who can't help but drop sly reminders that he went to Harvard."
content
None
content =
"Explain the singular value decomposition."
messages = [
# OpenAI chat format
system_prompt,
"role"
"user"
"content"
: content},
async
with
aiohttp.ClientSession(
base_url
=url)
session:
print
"Running health check for server at
async
with
session.get(
"/health"
timeout
=test_timeout -
* MINUTES)
resp:
up = resp.status ==
assert
"Failed health check for server at
print
"Successful health check for server at
print
"Sending messages to
, *messages,
\n\t
await
_send_request(session,
"llm"
, messages)
twice:
messages[
"content"
"You are Jar Jar Binks."
print
"Sending messages to
, *messages,
\n\t
await
_send_request(session,
"llm"
, messages)
async
_send_request
session
: aiohttp.ClientSession,
model
messages
list
) ->
None
# `stream=True` tells an OpenAI-compatible backend to stream chunks
payload: dict[
, Any] = {
"messages"
: messages,
"model"
: model,
"stream"
True
headers = {
"Content-Type"
"application/json"
"Accept"
"text/event-stream"
async
with
session.post(
"/v1/chat/completions"
json
=payload,
headers
=headers,
timeout
* MINUTES
resp:
async
resp.content:
resp.raise_for_status()
# extract new content and stream it
line = raw.decode().strip()
line
line ==
"data: [DONE]"
continue
line.startswith(
"data: "
# SSE prefix
line = line[
"data: "
) :]
chunk = json.loads(line)
assert
chunk[
"object"
] ==
"chat.completion.chunk"
# or something went horribly wrong
print
(chunk[
"choices"
"delta"
"content"
print
Copy
We also include a basic example of a load-testing setup using
locust
in the
load_test.py
script
here
modal
openai_compatible/load_test.py
Copy
Run OpenAI-compatible LLM inference with LLaMA 3.1-8B and vLLM
Set up the container image
Download the model weights
Configuring vLLM
The V1 engine
Trading off fast boots and token generation performance
Build a vLLM engine and serve it
Deploy the server
Interact with the server
Testing the server
Try this on Modal!
You can run this example on Modal in 60 seconds.
Create account to run
After creating a free account, install the Modal Python package, and
create an API token.
install
modal
modal
setup
Copy
Clone the
modal-examples
repository and run:
clone
https://github.com/modal-labs/modal-examples
modal-examples
modal
06_gpu_and_ml/llm-serving/vllm_inference.py
Copy