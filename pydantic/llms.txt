# PYDANTIC Documentation
# Generated from 37 documentation pages
# Minified for LLM consumption

--- 035_annotated_handlers.txt ---
Annotated Handlers
Type annotations to use with
__get_pydantic_core_schema__
and
__get_pydantic_json_schema__
GetJsonSchemaHandler
Handler to call into the next JSON schema generation function.
Attributes:
Name
Type
Description
mode
JsonSchemaMode
Json schema mode, can be
validation
serialization
resolve_ref_schema
resolve_ref_schema
maybe_ref_json_schema
JsonSchemaValue
JsonSchemaValue
Get the real schema for a
{"$ref": ...}
schema.
If the schema given is not a
$ref
schema, it will be returned as is.
This means you don't have to check before calling this function.
Parameters:
Name
Type
Description
Default
maybe_ref_json_schema
JsonSchemaValue
A JsonSchemaValue which may be a
$ref
schema.
required
Raises:
Type
Description
LookupError
If the ref is not found.
Returns:
Name
Type
Description
JsonSchemaValue
JsonSchemaValue
A JsonSchemaValue that has no
$ref
Source code in
pydantic/annotated_handlers.py
def
resolve_ref_schema
self
maybe_ref_json_schema
JsonSchemaValue
JsonSchemaValue
"""Get the real schema for a `{"$ref": ...}` schema.
If the schema given is not a `$ref` schema, it will be returned as is.
This means you don't have to check before calling this function.
Args:
maybe_ref_json_schema: A JsonSchemaValue which may be a `$ref` schema.
Raises:
LookupError: If the ref is not found.
Returns:
JsonSchemaValue: A JsonSchemaValue that has no `$ref`.
"""
raise
NotImplementedError
GetCoreSchemaHandler
Handler to call into the next CoreSchema schema generation function.
field_name
property
field_name
str
None
Get the name of the closest field to this validator.
generate_schema
generate_schema
source_type
Any
CoreSchema
Generate a schema unrelated to the current context.
Use this function if e.g. you are handling schema generation for a sequence
and want to generate a schema for its items.
Otherwise, you may end up doing something like applying a
min_length
constraint
that was intended for the sequence itself to its items!
Parameters:
Name
Type
Description
Default
source_type
Any
The input type.
required
Returns:
Name
Type
Description
CoreSchema
CoreSchema
The
pydantic-core
CoreSchema generated.
Source code in
pydantic/annotated_handlers.py
def
generate_schema
self
source_type
Any
core_schema
CoreSchema
"""Generate a schema unrelated to the current context.
Use this function if e.g. you are handling schema generation for a sequence
and want to generate a schema for its items.
Otherwise, you may end up doing something like applying a `min_length` constraint
that was intended for the sequence itself to its items!
Args:
source_type: The input type.
Returns:
CoreSchema: The `pydantic-core` CoreSchema generated.
"""
raise
NotImplementedError
resolve_ref_schema
resolve_ref_schema
maybe_ref_schema
CoreSchema
CoreSchema
Get the real schema for a
definition-ref
schema.
If the schema given is not a
definition-ref
schema, it will be returned as is.
This means you don't have to check before calling this function.
Parameters:
Name
Type
Description
Default
maybe_ref_schema
CoreSchema
CoreSchema
ref
-based or not.
required
Raises:
Type
Description
LookupError
If the
ref
is not found.
Returns:
Type
Description
CoreSchema
A concrete
CoreSchema
Source code in
pydantic/annotated_handlers.py
100
101
102
103
104
105
106
107
108
109
110
111
112
113
def
resolve_ref_schema
self
maybe_ref_schema
core_schema
CoreSchema
core_schema
CoreSchema
"""Get the real schema for a `definition-ref` schema.
If the schema given is not a `definition-ref` schema, it will be returned as is.
This means you don't have to check before calling this function.
Args:
maybe_ref_schema: A `CoreSchema`, `ref`-based or not.
Raises:
LookupError: If the `ref` is not found.
Returns:
A concrete `CoreSchema`.
"""
raise
NotImplementedError
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 036_pydantic_extra_types_payment.txt ---
Payment
The
pydantic_extra_types.payment
module provides the
PaymentCardNumber
data type.
PaymentCardBrand
Bases:
str
Enum
Payment card brands supported by the
PaymentCardNumber
PaymentCardNumber
PaymentCardNumber
card_number
str
Bases:
str
payment card number
Source code in
pydantic_extra_types/payment.py
def
__init__
self
card_number
str
self
validate_digits
card_number
card_number
self
validate_luhn_check_digit
card_number
self
bin
card_number
self
last4
card_number
self
brand
self
validate_brand
card_number
strip_whitespace
class-attribute
strip_whitespace
bool
True
Whether to strip whitespace from the input value.
min_length
class-attribute
min_length
int
The minimum length of the card number.
max_length
class-attribute
max_length
int
The maximum length of the card number.
bin
instance-attribute
bin
str
card_number
The first 6 digits of the card number.
last4
instance-attribute
last4
str
card_number
The last 4 digits of the card number.
brand
instance-attribute
brand
PaymentCardBrand
validate_brand
card_number
The brand of the card.
masked
property
masked
str
The masked card number.
validate
classmethod
validate
__input_value
str
ValidationInfo
PaymentCardNumber
Validate the
PaymentCardNumber
instance.
Parameters:
Name
Type
Description
Default
__input_value
str
The input value to validate.
required
ValidationInfo
The validation info.
required
Returns:
Type
Description
PaymentCardNumber
The validated
PaymentCardNumber
instance.
Source code in
pydantic_extra_types/payment.py
@classmethod
def
validate
cls
__input_value
str
core_schema
ValidationInfo
PaymentCardNumber
"""Validate the `PaymentCardNumber` instance.
Args:
__input_value: The input value to validate.
_: The validation info.
Returns:
The validated `PaymentCardNumber` instance.
"""
return
cls
__input_value
validate_digits
classmethod
validate_digits
card_number
str
None
Validate that the card number is all digits.
Parameters:
Name
Type
Description
Default
card_number
str
The card number to validate.
required
Raises:
Type
Description
PydanticCustomError
If the card number is not all digits.
Source code in
pydantic_extra_types/payment.py
@classmethod
def
validate_digits
cls
card_number
str
None
"""Validate that the card number is all digits.
Args:
card_number: The card number to validate.
Raises:
PydanticCustomError: If the card number is not all digits.
"""
not
card_number
not
all
'0'
'9'
for
card_number
raise
PydanticCustomError
'payment_card_number_digits'
'Card number is not all digits'
validate_luhn_check_digit
classmethod
validate_luhn_check_digit
card_number
str
str
Validate the payment card number.
Based on the
Luhn algorithm
Parameters:
Name
Type
Description
Default
card_number
str
The card number to validate.
required
Returns:
Type
Description
str
The validated card number.
Raises:
Type
Description
PydanticCustomError
If the card number is not valid.
Source code in
pydantic_extra_types/payment.py
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
@classmethod
def
validate_luhn_check_digit
cls
card_number
str
str
"""Validate the payment card number.
Based on the [Luhn algorithm](https://en.wikipedia.org/wiki/Luhn_algorithm).
Args:
card_number: The card number to validate.
Returns:
The validated card number.
Raises:
PydanticCustomError: If the card number is not valid.
"""
sum_
int
card_number
length
len
card_number
parity
length
for
range
length
digit
int
card_number
parity
digit
digit
digit
sum_
digit
valid
sum_
not
valid
raise
PydanticCustomError
'payment_card_number_luhn'
'Card number is not luhn valid'
return
card_number
validate_brand
staticmethod
validate_brand
card_number
str
PaymentCardBrand
Validate length based on
BIN
for major brands.
Parameters:
Name
Type
Description
Default
card_number
str
The card number to validate.
required
Returns:
Type
Description
PaymentCardBrand
The validated card brand.
Raises:
Type
Description
PydanticCustomError
If the card number is not valid.
Source code in
pydantic_extra_types/payment.py
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
@staticmethod
def
validate_brand
card_number
str
PaymentCardBrand
"""Validate length based on
[BIN](https://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN))
for major brands.
Args:
card_number: The card number to validate.
Returns:
The validated card brand.
Raises:
PydanticCustomError: If the card number is not valid.
"""
brand
PaymentCardBrand
other
card_number
'4'
brand
PaymentCardBrand
visa
required_length
elif
int
card_number
brand
PaymentCardBrand
mastercard
required_length
elif
card_number
'34'
'37'
brand
PaymentCardBrand
amex
required_length
elif
2200
int
card_number
2204
brand
PaymentCardBrand
mir
required_length
list
range
elif
card_number
'5018'
'5020'
'5038'
'5893'
'6304'
'6759'
'6761'
'6762'
'6763'
card_number
'676770'
'676774'
brand
PaymentCardBrand
maestro
required_length
list
range
elif
card_number
startswith
'65'
644
int
card_number
649
card_number
startswith
'6011'
brand
PaymentCardBrand
discover
required_length
list
range
elif
506099
int
card_number
506198
650002
int
card_number
650027
507865
int
card_number
507964
brand
PaymentCardBrand
verve
required_length
elif
card_number
'5019'
'4571'
brand
PaymentCardBrand
dankort
required_length
elif
card_number
startswith
'9792'
brand
PaymentCardBrand
troy
required_length
elif
card_number
'62'
'81'
brand
PaymentCardBrand
unionpay
required_length
elif
3528
int
card_number
3589
brand
PaymentCardBrand
jcb
required_length
valid
len
card_number
required_length
brand
PaymentCardBrand
other
else
True
not
valid
raise
PydanticCustomError
'payment_card_number_brand'
'Length for a
brand
card must be
" or "
join
map
str
required_length
'brand'
brand
'required_length'
required_length
return
brand
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 007_functional_serializers_pydantic_functional_serializers_field_serializer.txt ---
Functional Serializers
This module contains related classes and functions for serialization.
FieldPlainSerializer
module-attribute
FieldPlainSerializer
TypeAlias
"core_schema.SerializerFunction | _Partial"
A field serializer method or function in
plain
mode.
FieldWrapSerializer
module-attribute
FieldWrapSerializer
TypeAlias
"core_schema.WrapSerializerFunction | _Partial"
A field serializer method or function in
wrap
mode.
FieldSerializer
module-attribute
FieldSerializer
TypeAlias
"FieldPlainSerializer | FieldWrapSerializer"
A field serializer method or function.
ModelPlainSerializerWithInfo
module-attribute
ModelPlainSerializerWithInfo
TypeAlias
Callable
Any
SerializationInfo
Any
A model serializer method with the
info
argument, in
plain
mode.
ModelPlainSerializerWithoutInfo
module-attribute
ModelPlainSerializerWithoutInfo
TypeAlias
Callable
Any
Any
A model serializer method without the
info
argument, in
plain
mode.
ModelPlainSerializer
module-attribute
ModelPlainSerializer
TypeAlias
"ModelPlainSerializerWithInfo | ModelPlainSerializerWithoutInfo"
A model serializer method in
plain
mode.
ModelWrapSerializerWithInfo
module-attribute
ModelWrapSerializerWithInfo
TypeAlias
Callable
Any
SerializerFunctionWrapHandler
SerializationInfo
Any
A model serializer method with the
info
argument, in
wrap
mode.
ModelWrapSerializerWithoutInfo
module-attribute
ModelWrapSerializerWithoutInfo
TypeAlias
Callable
Any
SerializerFunctionWrapHandler
Any
A model serializer method without the
info
argument, in
wrap
mode.
ModelWrapSerializer
module-attribute
ModelWrapSerializer
TypeAlias
"ModelWrapSerializerWithInfo | ModelWrapSerializerWithoutInfo"
A model serializer method in
wrap
mode.
PlainSerializer
dataclass
PlainSerializer
func
SerializerFunction
return_type
Any
PydanticUndefined
when_used
WhenUsed
"always"
Plain serializers use a function to modify the output of serialization.
This is particularly helpful when you want to customize the serialization for annotated types.
Consider an input of
list
, which will be serialized into a space-delimited string.
from
typing
import
Annotated
from
pydantic
import
BaseModel
PlainSerializer
CustomStr
Annotated
list
PlainSerializer
lambda
' '
join
return_type
str
class
StudentModel
BaseModel
courses
CustomStr
student
StudentModel
courses
'Math'
'Chemistry'
'English'
print
student
model_dump
())
#> {'courses': 'Math Chemistry English'}
Attributes:
Name
Type
Description
func
SerializerFunction
The serializer function.
return_type
Any
The return type for the function. If omitted it will be inferred from the type annotation.
when_used
WhenUsed
Determines when this serializer should be used. Accepts a string with values
'always'
'unless-none'
'json'
, and
'json-unless-none'
. Defaults to 'always'.
WrapSerializer
dataclass
WrapSerializer
func
WrapSerializerFunction
return_type
Any
PydanticUndefined
when_used
WhenUsed
"always"
Wrap serializers receive the raw inputs along with a handler function that applies the standard serialization
logic, and can modify the resulting value before returning it as the final output of serialization.
For example, here's a scenario in which a wrap serializer transforms timezones to UTC
and
utilizes the existing
datetime
serialization logic.
from
datetime
import
datetime
timezone
from
typing
import
Annotated
Any
from
pydantic
import
BaseModel
WrapSerializer
class
EventDatetime
BaseModel
start
datetime
end
datetime
def
convert_to_utc
value
Any
handler
info
dict
str
datetime
# Note that `handler` can actually help serialize the `value` for
# further custom serialization in case it's a subclass.
partial_result
handler
value
info
info
mode
'json'
return
datetime
fromisoformat
astimezone
timezone
utc
for
partial_result
items
return
astimezone
timezone
utc
for
partial_result
items
()}
UTCEventDatetime
Annotated
EventDatetime
WrapSerializer
convert_to_utc
class
EventModel
BaseModel
event_datetime
UTCEventDatetime
EventDatetime
start
'2024-01-01T07:00:00-08:00'
end
'2024-01-03T20:00:00+06:00'
event
EventModel
event_datetime
print
event
model_dump
())
'''
'event_datetime': {
'start': datetime.datetime(
2024, 1, 1, 15, 0, tzinfo=datetime.timezone.utc
'end': datetime.datetime(
2024, 1, 3, 14, 0, tzinfo=datetime.timezone.utc
'''
print
event
model_dump_json
())
'''
{"event_datetime":{"start":"2024-01-01T15:00:00Z","end":"2024-01-03T14:00:00Z"}}
'''
Attributes:
Name
Type
Description
func
WrapSerializerFunction
The serializer function to be wrapped.
return_type
Any
The return type for the function. If omitted it will be inferred from the type annotation.
when_used
WhenUsed
Determines when this serializer should be used. Accepts a string with values
'always'
'unless-none'
'json'
, and
'json-unless-none'
. Defaults to 'always'.
field_serializer
field_serializer
field
str
fields
str
mode
Literal
"wrap"
return_type
Any
...
when_used
WhenUsed
...
check_fields
bool
None
...
Callable
_FieldWrapSerializerT
_FieldWrapSerializerT
field_serializer
field
str
fields
str
mode
Literal
"plain"
...
return_type
Any
...
when_used
WhenUsed
...
check_fields
bool
None
...
Callable
_FieldPlainSerializerT
_FieldPlainSerializerT
field_serializer
fields
str
mode
Literal
"plain"
"wrap"
"plain"
return_type
Any
PydanticUndefined
when_used
WhenUsed
"always"
check_fields
bool
None
None
Callable
_FieldWrapSerializerT
_FieldWrapSerializerT
Callable
_FieldPlainSerializerT
_FieldPlainSerializerT
Decorator that enables custom field serialization.
In the below example, a field of type
set
is used to mitigate duplication. A
field_serializer
is used to serialize the data as a sorted list.
from
typing
import
Set
from
pydantic
import
BaseModel
field_serializer
class
StudentModel
BaseModel
name
str
'Jane'
courses
Set
str
@field_serializer
'courses'
when_used
'json'
def
serialize_courses_in_order
self
courses
Set
str
]):
return
sorted
courses
student
StudentModel
courses
'Math'
'Chemistry'
'English'
print
student
model_dump_json
())
#> {"name":"Jane","courses":["Chemistry","English","Math"]}
See
Custom serializers
for more information.
Four signatures are supported:
(self, value: Any, info: FieldSerializationInfo)
(self, value: Any, nxt: SerializerFunctionWrapHandler, info: FieldSerializationInfo)
(value: Any, info: SerializationInfo)
(value: Any, nxt: SerializerFunctionWrapHandler, info: SerializationInfo)
Parameters:
Name
Type
Description
Default
fields
str
Which field(s) the method should be called on.
mode
Literal
['plain', 'wrap']
The serialization mode.
plain
means the function will be called instead of the default serialization logic,
wrap
means the function will be called with an argument to optionally call the
default serialization logic.
'plain'
return_type
Any
Optional return type for the function, if omitted it will be inferred from the type annotation.
PydanticUndefined
when_used
WhenUsed
Determines the serializer will be used for serialization.
'always'
check_fields
bool
| None
Whether to check that the fields actually exist on the model.
None
Returns:
Type
Description
Callable
_FieldWrapSerializerT
_FieldWrapSerializerT
] |
Callable
_FieldPlainSerializerT
_FieldPlainSerializerT
The decorator function.
Source code in
pydantic/functional_serializers.py
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
def
field_serializer
fields
str
mode
Literal
'plain'
'wrap'
'plain'
return_type
Any
PydanticUndefined
when_used
WhenUsed
'always'
check_fields
bool
None
None
Callable
_FieldWrapSerializerT
_FieldWrapSerializerT
Callable
_FieldPlainSerializerT
_FieldPlainSerializerT
"""Decorator that enables custom field serialization.
In the below example, a field of type `set` is used to mitigate duplication. A `field_serializer` is used to serialize the data as a sorted list.
```python
from typing import Set
from pydantic import BaseModel, field_serializer
class StudentModel(BaseModel):
name: str = 'Jane'
courses: Set[str]
@field_serializer('courses', when_used='json')
def serialize_courses_in_order(self, courses: Set[str]):
return sorted(courses)
student = StudentModel(courses={'Math', 'Chemistry', 'English'})
print(student.model_dump_json())
#> {"name":"Jane","courses":["Chemistry","English","Math"]}
```
See [Custom serializers](../concepts/serialization.md#custom-serializers) for more information.
Four signatures are supported:
- `(self, value: Any, info: FieldSerializationInfo)`
- `(self, value: Any, nxt: SerializerFunctionWrapHandler, info: FieldSerializationInfo)`
- `(value: Any, info: SerializationInfo)`
- `(value: Any, nxt: SerializerFunctionWrapHandler, info: SerializationInfo)`
Args:
fields: Which field(s) the method should be called on.
mode: The serialization mode.
- `plain` means the function will be called instead of the default serialization logic,
- `wrap` means the function will be called with an argument to optionally call the
default serialization logic.
return_type: Optional return type for the function, if omitted it will be inferred from the type annotation.
when_used: Determines the serializer will be used for serialization.
check_fields: Whether to check that the fields actually exist on the model.
Returns:
The decorator function.
"""
def
dec
FieldSerializer
_decorators
PydanticDescriptorProxy
Any
dec_info
_decorators
FieldSerializerDecoratorInfo
fields
fields
mode
mode
return_type
return_type
when_used
when_used
check_fields
check_fields
return
_decorators
PydanticDescriptorProxy
dec_info
# pyright: ignore[reportArgumentType]
return
dec
# pyright: ignore[reportReturnType]
model_serializer
model_serializer
_ModelPlainSerializerT
_ModelPlainSerializerT
model_serializer
mode
Literal
"wrap"
when_used
WhenUsed
"always"
return_type
Any
...
Callable
_ModelWrapSerializerT
_ModelWrapSerializerT
model_serializer
mode
Literal
"plain"
...
when_used
WhenUsed
"always"
return_type
Any
...
Callable
_ModelPlainSerializerT
_ModelPlainSerializerT
model_serializer
_ModelPlainSerializerT
_ModelWrapSerializerT
None
None
mode
Literal
"plain"
"wrap"
"plain"
when_used
WhenUsed
"always"
return_type
Any
PydanticUndefined
_ModelPlainSerializerT
Callable
_ModelWrapSerializerT
_ModelWrapSerializerT
Callable
_ModelPlainSerializerT
_ModelPlainSerializerT
Decorator that enables custom model serialization.
This is useful when a model need to be serialized in a customized manner, allowing for flexibility beyond just specific fields.
An example would be to serialize temperature to the same temperature scale, such as degrees Celsius.
from
typing
import
Literal
from
pydantic
import
BaseModel
model_serializer
class
TemperatureModel
BaseModel
unit
Literal
'C'
'F'
value
int
@model_serializer
def
serialize_model
self
self
unit
'F'
return
'unit'
'C'
'value'
int
self
value
1.8
return
'unit'
self
unit
'value'
self
value
temperature
TemperatureModel
unit
'F'
value
212
print
temperature
model_dump
())
#> {'unit': 'C', 'value': 100}
Two signatures are supported for
mode='plain'
, which is the default:
(self)
(self, info: SerializationInfo)
And two other signatures for
mode='wrap'
(self, nxt: SerializerFunctionWrapHandler)
(self, nxt: SerializerFunctionWrapHandler, info: SerializationInfo)
See
Custom serializers
for more information.
Parameters:
Name
Type
Description
Default
_ModelPlainSerializerT
_ModelWrapSerializerT
| None
The function to be decorated.
None
mode
Literal
['plain', 'wrap']
The serialization mode.
'plain'
means the function will be called instead of the default serialization logic
'wrap'
means the function will be called with an argument to optionally call the default
serialization logic.
'plain'
when_used
WhenUsed
Determines when this serializer should be used.
'always'
return_type
Any
The return type for the function. If omitted it will be inferred from the type annotation.
PydanticUndefined
Returns:
Type
Description
_ModelPlainSerializerT
Callable
_ModelWrapSerializerT
_ModelWrapSerializerT
] |
Callable
_ModelPlainSerializerT
_ModelPlainSerializerT
The decorator function.
Source code in
pydantic/functional_serializers.py
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
def
model_serializer
_ModelPlainSerializerT
_ModelWrapSerializerT
None
None
mode
Literal
'plain'
'wrap'
'plain'
when_used
WhenUsed
'always'
return_type
Any
PydanticUndefined
_ModelPlainSerializerT
Callable
_ModelWrapSerializerT
_ModelWrapSerializerT
Callable
_ModelPlainSerializerT
_ModelPlainSerializerT
"""Decorator that enables custom model serialization.
This is useful when a model need to be serialized in a customized manner, allowing for flexibility beyond just specific fields.
An example would be to serialize temperature to the same temperature scale, such as degrees Celsius.
```python
from typing import Literal
from pydantic import BaseModel, model_serializer
class TemperatureModel(BaseModel):
unit: Literal['C', 'F']
value: int
@model_serializer()
def serialize_model(self):
if self.unit == 'F':
return {'unit': 'C', 'value': int((self.value - 32) / 1.8)}
return {'unit': self.unit, 'value': self.value}
temperature = TemperatureModel(unit='F', value=212)
print(temperature.model_dump())
#> {'unit': 'C', 'value': 100}
```
Two signatures are supported for `mode='plain'`, which is the default:
- `(self)`
- `(self, info: SerializationInfo)`
And two other signatures for `mode='wrap'`:
- `(self, nxt: SerializerFunctionWrapHandler)`
- `(self, nxt: SerializerFunctionWrapHandler, info: SerializationInfo)`
See [Custom serializers](../concepts/serialization.md#custom-serializers) for more information.
Args:
f: The function to be decorated.
mode: The serialization mode.
- `'plain'` means the function will be called instead of the default serialization logic
- `'wrap'` means the function will be called with an argument to optionally call the default
serialization logic.
when_used: Determines when this serializer should be used.
return_type: The return type for the function. If omitted it will be inferred from the type annotation.
Returns:
The decorator function.
"""
def
dec
ModelSerializer
_decorators
PydanticDescriptorProxy
Any
dec_info
_decorators
ModelSerializerDecoratorInfo
mode
mode
return_type
return_type
when_used
when_used
return
_decorators
PydanticDescriptorProxy
dec_info
None
return
dec
# pyright: ignore[reportReturnType]
else
return
dec
# pyright: ignore[reportReturnType]
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 011_pydantic_extra_types_isbn_pydantic_extra_types_isbn.txt ---
ISBN
The
pydantic_extra_types.isbn
module provides functionality to recieve and validate ISBN.
ISBN (International Standard Book Number) is a numeric commercial book identifier which is intended to be unique. This module provides a ISBN type for Pydantic models.
ISBN
Bases:
str
Represents a ISBN and provides methods for conversion, validation, and serialization.
from
pydantic
import
BaseModel
from
pydantic_extra_types.isbn
import
ISBN
class
Book
BaseModel
isbn
ISBN
book
Book
isbn
"8537809667"
print
book
#> isbn='9788537809662'
validate_isbn_format
staticmethod
validate_isbn_format
value
str
None
Validate a ISBN format from the provided str value.
Parameters:
Name
Type
Description
Default
value
str
The str value representing the ISBN in 10 or 13 digits.
required
Raises:
Type
Description
PydanticCustomError
If the ISBN is not valid.
Source code in
pydantic_extra_types/isbn.py
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
@staticmethod
def
validate_isbn_format
value
str
None
"""Validate a ISBN format from the provided str value.
Args:
value: The str value representing the ISBN in 10 or 13 digits.
Raises:
PydanticCustomError: If the ISBN is not valid.
"""
isbn_length
len
value
isbn_length
not
raise
PydanticCustomError
'isbn_length'
'Length for ISBN must be 10 or 13 digits, not
isbn_length
isbn_length
not
value
isdigit
value
'X'
and
not
value
isdigit
())):
raise
PydanticCustomError
'isbn10_invalid_characters'
'First 9 digits of ISBN-10 must be integers'
isbn10_digit_calc
value
value
raise
PydanticCustomError
'isbn_invalid_digit_check_isbn10'
'Provided digit is invalid for given ISBN'
isbn_length
not
value
isdigit
():
raise
PydanticCustomError
'isbn13_invalid_characters'
'All digits of ISBN-13 must be integers'
value
not
'978'
'979'
raise
PydanticCustomError
'isbn_invalid_early_characters'
'The first 3 digits of ISBN-13 must be 978 or 979'
isbn13_digit_calc
value
value
raise
PydanticCustomError
'isbn_invalid_digit_check_isbn13'
'Provided digit is invalid for given ISBN'
convert_isbn10_to_isbn13
staticmethod
convert_isbn10_to_isbn13
value
str
str
Convert an ISBN-10 to ISBN-13.
Parameters:
Name
Type
Description
Default
value
str
The ISBN-10 value to be converted.
required
Returns:
Type
Description
str
The converted ISBN or the original value if no conversion is necessary.
Source code in
pydantic_extra_types/isbn.py
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
@staticmethod
def
convert_isbn10_to_isbn13
value
str
str
"""Convert an ISBN-10 to ISBN-13.
Args:
value: The ISBN-10 value to be converted.
Returns:
The converted ISBN or the original value if no conversion is necessary.
"""
len
value
base_isbn
'978
value
isbn13_digit
isbn13_digit_calc
base_isbn
return
ISBN
base_isbn
isbn13_digit
return
ISBN
value
isbn10_digit_calc
isbn10_digit_calc
isbn
str
str
Calc a ISBN-10 last digit from the provided str value. More information of validation algorithm on
Wikipedia
Parameters:
Name
Type
Description
Default
isbn
str
The str value representing the ISBN in 10 digits.
required
Returns:
Type
Description
str
The calculated last digit of the ISBN-10 value.
Source code in
pydantic_extra_types/isbn.py
def
isbn10_digit_calc
isbn
str
str
"""Calc a ISBN-10 last digit from the provided str value. More information of validation algorithm on [Wikipedia](https://en.wikipedia.org/wiki/ISBN#Check_digits)
Args:
isbn: The str value representing the ISBN in 10 digits.
Returns:
The calculated last digit of the ISBN-10 value.
"""
total
sum
int
digit
idx
for
idx
digit
enumerate
isbn
]))
for
check_digit
range
total
check_digit
valid_check_digit
'X'
check_digit
else
str
check_digit
return
valid_check_digit
isbn13_digit_calc
isbn13_digit_calc
isbn
str
str
Calc a ISBN-13 last digit from the provided str value. More information of validation algorithm on
Wikipedia
Parameters:
Name
Type
Description
Default
isbn
str
The str value representing the ISBN in 13 digits.
required
Returns:
Type
Description
str
The calculated last digit of the ISBN-13 value.
Source code in
pydantic_extra_types/isbn.py
def
isbn13_digit_calc
isbn
str
str
"""Calc a ISBN-13 last digit from the provided str value. More information of validation algorithm on [Wikipedia](https://en.wikipedia.org/wiki/ISBN#Check_digits)
Args:
isbn: The str value representing the ISBN in 13 digits.
Returns:
The calculated last digit of the ISBN-13 value.
"""
total
sum
int
digit
idx
else
for
idx
digit
enumerate
isbn
]))
check_digit
total
return
str
check_digit
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 010_errors_pydantic_errors_PydanticUserError.txt ---
Errors
Pydantic-specific errors.
PydanticErrorMixin
PydanticErrorMixin
message
str
code
PydanticErrorCodes
None
A mixin class for common functionality shared by all Pydantic-specific errors.
Attributes:
Name
Type
Description
message
A message describing the error.
code
An optional error code from PydanticErrorCodes enum.
Source code in
pydantic/errors.py
def
__init__
self
message
str
code
PydanticErrorCodes
None
None
self
message
message
self
code
code
PydanticUserError
PydanticUserError
message
str
code
PydanticErrorCodes
None
Bases:
PydanticErrorMixin
TypeError
An error raised due to incorrect use of Pydantic.
Source code in
pydantic/errors.py
def
__init__
self
message
str
code
PydanticErrorCodes
None
None
self
message
message
self
code
code
PydanticUndefinedAnnotation
PydanticUndefinedAnnotation
name
str
message
str
Bases:
PydanticErrorMixin
NameError
A subclass of
NameError
raised when handling undefined annotations during
CoreSchema
generation.
Attributes:
Name
Type
Description
name
Name of the error.
message
Description of the error.
Source code in
pydantic/errors.py
113
114
115
def
__init__
self
name
str
message
str
None
self
name
name
super
__init__
message
message
code
'undefined-annotation'
from_name_error
classmethod
from_name_error
name_error
NameError
Self
Convert a
NameError
to a
PydanticUndefinedAnnotation
error.
Parameters:
Name
Type
Description
Default
name_error
NameError
NameError
to be converted.
required
Returns:
Type
Description
Self
Converted
PydanticUndefinedAnnotation
error.
Source code in
pydantic/errors.py
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
@classmethod
def
from_name_error
cls
name_error
NameError
Self
"""Convert a `NameError` to a `PydanticUndefinedAnnotation` error.
Args:
name_error: `NameError` to be converted.
Returns:
Converted `PydanticUndefinedAnnotation` error.
"""
try
name
name_error
name
# type: ignore # python > 3.10
except
AttributeError
name
search
".*'(.+?)'"
str
name_error
group
# type: ignore[union-attr]
return
cls
name
name
message
str
name_error
PydanticImportError
PydanticImportError
message
str
Bases:
PydanticErrorMixin
ImportError
An error raised when an import fails due to module changes between V1 and V2.
Attributes:
Name
Type
Description
message
Description of the error.
Source code in
pydantic/errors.py
141
142
def
__init__
self
message
str
None
super
__init__
message
code
'import-error'
PydanticSchemaGenerationError
PydanticSchemaGenerationError
message
str
Bases:
PydanticUserError
An error raised during failures to generate a
CoreSchema
for some type.
Attributes:
Name
Type
Description
message
Description of the error.
Source code in
pydantic/errors.py
152
153
def
__init__
self
message
str
None
super
__init__
message
code
'schema-for-unknown-type'
PydanticInvalidForJsonSchema
PydanticInvalidForJsonSchema
message
str
Bases:
PydanticUserError
An error raised during failures to generate a JSON schema for some
CoreSchema
Attributes:
Name
Type
Description
message
Description of the error.
Source code in
pydantic/errors.py
163
164
def
__init__
self
message
str
None
super
__init__
message
code
'invalid-for-json-schema'
PydanticForbiddenQualifier
PydanticForbiddenQualifier
qualifier
Qualifier
annotation
Any
Bases:
PydanticUserError
An error raised if a forbidden type qualifier is found in a type annotation.
Source code in
pydantic/errors.py
179
180
181
182
183
184
185
186
def
__init__
self
qualifier
Qualifier
annotation
Any
None
super
__init__
message
'The annotation
_repr
display_as_type
annotation
!r}
contains the
self
_qualifier_repr_map
qualifier
!r}
'type qualifier, which is invalid in the context it is defined.'
code
None
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 024_pydantic_extra_types_routing_numbers.txt ---
Routing Numbers
The
pydantic_extra_types.routing_number
module provides the
ABARoutingNumber
data type.
ABARoutingNumber
ABARoutingNumber
routing_number
str
Bases:
str
The
ABARoutingNumber
data type is a string of 9 digits representing an ABA routing transit number.
The algorithm used to validate the routing number is described in the
ABA routing transit number
Wikipedia article.
from
pydantic
import
BaseModel
from
pydantic_extra_types.routing_number
import
ABARoutingNumber
class
BankAccount
BaseModel
routing_number
ABARoutingNumber
account
BankAccount
routing_number
'122105155'
print
account
#> routing_number='122105155'
Source code in
pydantic_extra_types/routing_number.py
def
__init__
self
routing_number
str
self
_validate_digits
routing_number
self
_routing_number
self
_validate_routing_number
routing_number
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 016_experimental_pydantic_experimental_pipeline__Pipeline_in_.txt ---
Pipeline APIÂ¶
=============
Pipeline API
Experimental pipeline API functionality. Be careful with this API, it's subject to change.
_Pipeline
dataclass
_Pipeline
_steps
tuple
_Step
...
Bases:
Generic
_InT
_OutT
Abstract representation of a chain of validation, transformation, and parsing steps.
transform
transform
func
Callable
_OutT
_NewOutT
_Pipeline
_InT
_NewOutT
Transform the output of the previous step.
If used as the first step in a pipeline, the type of the field is used.
That is, the transformation is applied to after the value is parsed to the field's type.
Source code in
pydantic/experimental/pipeline.py
134
135
136
137
138
139
140
141
142
143
def
transform
self
func
Callable
_OutT
_NewOutT
_Pipeline
_InT
_NewOutT
"""Transform the output of the previous step.
If used as the first step in a pipeline, the type of the field is used.
That is, the transformation is applied to after the value is parsed to the field's type.
"""
return
_Pipeline
_InT
_NewOutT
self
_steps
_Transform
func
),))
validate_as
validate_as
type
_NewOutT
strict
bool
...
_Pipeline
_InT
_NewOutT
validate_as
EllipsisType
strict
bool
...
_Pipeline
_InT
Any
validate_as
type
_NewOutT
EllipsisType
strict
bool
False
_Pipeline
_InT
Any
Validate / parse the input into a new type.
If no type is provided, the type of the field is used.
Types are parsed in Pydantic's
lax
mode by default,
but you can enable
strict
mode by passing
strict=True
Source code in
pydantic/experimental/pipeline.py
152
153
154
155
156
157
158
159
160
161
162
def
validate_as
self
type
_NewOutT
EllipsisType
strict
bool
False
_Pipeline
_InT
Any
# type: ignore
"""Validate / parse the input into a new type.
If no type is provided, the type of the field is used.
Types are parsed in Pydantic's `lax` mode by default,
but you can enable `strict` mode by passing `strict=True`.
"""
isinstance
EllipsisType
return
_Pipeline
_InT
Any
self
_steps
_ValidateAs
_FieldTypeMarker
strict
strict
),))
return
_Pipeline
_InT
_NewOutT
self
_steps
_ValidateAs
strict
strict
),))
validate_as_deferred
validate_as_deferred
func
Callable
[[],
type
_NewOutT
_Pipeline
_InT
_NewOutT
Parse the input into a new type, deferring resolution of the type until the current class
is fully defined.
This is useful when you need to reference the class in it's own type annotations.
Source code in
pydantic/experimental/pipeline.py
164
165
166
167
168
169
170
def
validate_as_deferred
self
func
Callable
[[],
type
_NewOutT
]])
_Pipeline
_InT
_NewOutT
"""Parse the input into a new type, deferring resolution of the type until the current class
is fully defined.
This is useful when you need to reference the class in it's own type annotations.
"""
return
_Pipeline
_InT
_NewOutT
self
_steps
_ValidateAsDefer
func
),))
constrain
constrain
constraint
_Pipeline
_InT
_NewOutGe
constrain
constraint
_Pipeline
_InT
_NewOutGt
constrain
constraint
_Pipeline
_InT
_NewOutLe
constrain
constraint
_Pipeline
_InT
_NewOutLt
constrain
constraint
Len
_Pipeline
_InT
_NewOutLen
constrain
constraint
MultipleOf
_Pipeline
_InT
_NewOutT
constrain
constraint
Timezone
_Pipeline
_InT
_NewOutDatetime
constrain
constraint
Predicate
_Pipeline
_InT
_OutT
constrain
constraint
Interval
_Pipeline
_InT
_NewOutInterval
constrain
constraint
_Eq
_Pipeline
_InT
_OutT
constrain
constraint
_NotEq
_Pipeline
_InT
_OutT
constrain
constraint
_In
_Pipeline
_InT
_OutT
constrain
constraint
_NotIn
_Pipeline
_InT
_OutT
constrain
constraint
Pattern
str
_Pipeline
_InT
_NewOutT
constrain
constraint
_ConstraintAnnotation
Any
Constrain a value to meet a certain condition.
We support most conditions from
annotated_types
, as well as regular expressions.
Most of the time you'll be calling a shortcut method like
len
, etc
so you don't need to call this directly.
Source code in
pydantic/experimental/pipeline.py
223
224
225
226
227
228
229
230
231
def
constrain
self
constraint
_ConstraintAnnotation
Any
"""Constrain a value to meet a certain condition.
We support most conditions from `annotated_types`, as well as regular expressions.
Most of the time you'll be calling a shortcut method like `gt`, `lt`, `len`, etc
so you don't need to call this directly.
"""
return
_Pipeline
_InT
_OutT
self
_steps
_Constraint
constraint
),))
predicate
predicate
func
Callable
_NewOutT
bool
_Pipeline
_InT
_NewOutT
Constrain a value to meet a certain predicate.
Source code in
pydantic/experimental/pipeline.py
233
234
235
def
predicate
self
_Pipeline
_InT
_NewOutT
func
Callable
_NewOutT
bool
_Pipeline
_InT
_NewOutT
"""Constrain a value to meet a certain predicate."""
return
self
constrain
annotated_types
Predicate
func
_NewOutGt
_Pipeline
_InT
_NewOutGt
Constrain a value to be greater than a certain value.
Source code in
pydantic/experimental/pipeline.py
237
238
239
def
self
_Pipeline
_InT
_NewOutGt
_NewOutGt
_Pipeline
_InT
_NewOutGt
"""Constrain a value to be greater than a certain value."""
return
self
constrain
annotated_types
_NewOutLt
_Pipeline
_InT
_NewOutLt
Constrain a value to be less than a certain value.
Source code in
pydantic/experimental/pipeline.py
241
242
243
def
self
_Pipeline
_InT
_NewOutLt
_NewOutLt
_Pipeline
_InT
_NewOutLt
"""Constrain a value to be less than a certain value."""
return
self
constrain
annotated_types
_NewOutGe
_Pipeline
_InT
_NewOutGe
Constrain a value to be greater than or equal to a certain value.
Source code in
pydantic/experimental/pipeline.py
245
246
247
def
self
_Pipeline
_InT
_NewOutGe
_NewOutGe
_Pipeline
_InT
_NewOutGe
"""Constrain a value to be greater than or equal to a certain value."""
return
self
constrain
annotated_types
_NewOutLe
_Pipeline
_InT
_NewOutLe
Constrain a value to be less than or equal to a certain value.
Source code in
pydantic/experimental/pipeline.py
249
250
251
def
self
_Pipeline
_InT
_NewOutLe
_NewOutLe
_Pipeline
_InT
_NewOutLe
"""Constrain a value to be less than or equal to a certain value."""
return
self
constrain
annotated_types
len
len
min_len
int
max_len
int
None
None
_Pipeline
_InT
_NewOutLen
Constrain a value to have a certain length.
Source code in
pydantic/experimental/pipeline.py
253
254
255
def
len
self
_Pipeline
_InT
_NewOutLen
min_len
int
max_len
int
None
None
_Pipeline
_InT
_NewOutLen
"""Constrain a value to have a certain length."""
return
self
constrain
annotated_types
Len
min_len
max_len
multiple_of
multiple_of
multiple_of
_NewOutDiv
_Pipeline
_InT
_NewOutDiv
multiple_of
multiple_of
_NewOutMod
_Pipeline
_InT
_NewOutMod
multiple_of
multiple_of
Any
_Pipeline
_InT
Any
Constrain a value to be a multiple of a certain number.
Source code in
pydantic/experimental/pipeline.py
263
264
265
def
multiple_of
self
_Pipeline
_InT
Any
multiple_of
Any
_Pipeline
_InT
Any
"""Constrain a value to be a multiple of a certain number."""
return
self
constrain
annotated_types
MultipleOf
multiple_of
value
_OutT
_Pipeline
_InT
_OutT
Constrain a value to be equal to a certain value.
Source code in
pydantic/experimental/pipeline.py
267
268
269
def
self
_Pipeline
_InT
_OutT
value
_OutT
_Pipeline
_InT
_OutT
"""Constrain a value to be equal to a certain value."""
return
self
constrain
_Eq
value
not_eq
not_eq
value
_OutT
_Pipeline
_InT
_OutT
Constrain a value to not be equal to a certain value.
Source code in
pydantic/experimental/pipeline.py
271
272
273
def
not_eq
self
_Pipeline
_InT
_OutT
value
_OutT
_Pipeline
_InT
_OutT
"""Constrain a value to not be equal to a certain value."""
return
self
constrain
_NotEq
value
in_
in_
values
Container
_OutT
_Pipeline
_InT
_OutT
Constrain a value to be in a certain set.
Source code in
pydantic/experimental/pipeline.py
275
276
277
def
in_
self
_Pipeline
_InT
_OutT
values
Container
_OutT
_Pipeline
_InT
_OutT
"""Constrain a value to be in a certain set."""
return
self
constrain
_In
values
not_in
not_in
values
Container
_OutT
_Pipeline
_InT
_OutT
Constrain a value to not be in a certain set.
Source code in
pydantic/experimental/pipeline.py
279
280
281
def
not_in
self
_Pipeline
_InT
_OutT
values
Container
_OutT
_Pipeline
_InT
_OutT
"""Constrain a value to not be in a certain set."""
return
self
constrain
_NotIn
values
otherwise
otherwise
other
_Pipeline
_OtherIn
_OtherOut
_Pipeline
_InT
_OtherIn
_OutT
_OtherOut
Combine two validation chains, returning the result of the first chain if it succeeds, and the second chain if it fails.
Source code in
pydantic/experimental/pipeline.py
326
327
328
def
otherwise
self
other
_Pipeline
_OtherIn
_OtherOut
_Pipeline
_InT
_OtherIn
_OutT
_OtherOut
"""Combine two validation chains, returning the result of the first chain if it succeeds, and the second chain if it fails."""
return
_Pipeline
_PipelineOr
self
other
),))
then
then
other
_Pipeline
_OutT
_OtherOut
_Pipeline
_InT
_OtherOut
Pipe the result of one validation chain into another.
Source code in
pydantic/experimental/pipeline.py
332
333
334
def
then
self
other
_Pipeline
_OutT
_OtherOut
_Pipeline
_InT
_OtherOut
"""Pipe the result of one validation chain into another."""
return
_Pipeline
_PipelineAnd
self
other
),))
Arguments schema API
Experimental module exposing a function to generate a core schema that validates callable arguments.
generate_arguments_schema
generate_arguments_schema
func
Callable
...
Any
schema_type
Literal
"arguments"
"arguments-v3"
"arguments-v3"
parameters_callback
Callable
int
str
Any
Literal
"skip"
None
None
None
config
ConfigDict
None
None
CoreSchema
Generate the schema for the arguments of a function.
Parameters:
Name
Type
Description
Default
func
Callable
[...,
Any
The function to generate the schema for.
required
schema_type
Literal
['arguments', 'arguments-v3']
The type of schema to generate.
'arguments-v3'
parameters_callback
Callable
int
str
Any
Literal
['skip'] | None] | None
A callable that will be invoked for each parameter. The callback
should take three required arguments: the index, the name and the type annotation
(or
Parameter.empty
if not annotated) of the parameter.
The callback can optionally return
'skip'
, so that the parameter gets excluded
from the resulting schema.
None
config
ConfigDict
| None
The configuration to use.
None
Returns:
Type
Description
CoreSchema
The generated schema.
Source code in
pydantic/experimental/arguments_schema.py
def
generate_arguments_schema
func
Callable
...
Any
schema_type
Literal
'arguments'
'arguments-v3'
'arguments-v3'
parameters_callback
Callable
int
str
Any
Literal
'skip'
None
None
None
config
ConfigDict
None
None
CoreSchema
"""Generate the schema for the arguments of a function.
Args:
func: The function to generate the schema for.
schema_type: The type of schema to generate.
parameters_callback: A callable that will be invoked for each parameter. The callback
should take three required arguments: the index, the name and the type annotation
(or [`Parameter.empty`][inspect.Parameter.empty] if not annotated) of the parameter.
The callback can optionally return `'skip'`, so that the parameter gets excluded
from the resulting schema.
config: The configuration to use.
Returns:
The generated schema.
"""
generate_schema
_generate_schema
GenerateSchema
_config
ConfigWrapper
config
ns_resolver
_namespace_utils
NsResolver
namespaces_tuple
_namespace_utils
ns_for_function
func
)),
schema_type
'arguments'
schema
generate_schema
_arguments_schema
func
parameters_callback
# pyright: ignore[reportArgumentType]
else
schema
generate_schema
_arguments_v3_schema
func
parameters_callback
# pyright: ignore[reportArgumentType]
return
generate_schema
clean_schema
schema
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 017_config.txt ---
Configuration
Configuration for Pydantic models.
ConfigDict
Bases:
TypedDict
A TypedDict for configuring Pydantic behaviour.
title
instance-attribute
title
str
None
The title for the generated JSON schema, defaults to the model's name
model_title_generator
instance-attribute
model_title_generator
Callable
type
str
None
A callable that takes a model class and returns the title for it. Defaults to
None
field_title_generator
instance-attribute
field_title_generator
Callable
str
FieldInfo
ComputedFieldInfo
str
None
A callable that takes a field's name and info and returns title for it. Defaults to
None
str_to_lower
instance-attribute
str_to_lower
bool
Whether to convert all characters to lowercase for str types. Defaults to
False
str_to_upper
instance-attribute
str_to_upper
bool
Whether to convert all characters to uppercase for str types. Defaults to
False
str_strip_whitespace
instance-attribute
str_strip_whitespace
bool
Whether to strip leading and trailing whitespace for str types.
str_min_length
instance-attribute
str_min_length
int
The minimum length for str types. Defaults to
None
str_max_length
instance-attribute
str_max_length
int
None
The maximum length for str types. Defaults to
None
extra
instance-attribute
extra
ExtraValues
None
Whether to ignore, allow, or forbid extra data during model initialization. Defaults to
'ignore'
Three configuration values are available:
'ignore'
: Providing extra data is ignored (the default):
from
pydantic
import
BaseModel
ConfigDict
class
User
BaseModel
model_config
ConfigDict
extra
'ignore'
name
str
user
User
name
'John Doe'
age
print
user
#> name='John Doe'
'forbid'
: Providing extra data is not permitted, and a
ValidationError
will be raised if this is the case:
from
pydantic
import
BaseModel
ConfigDict
ValidationError
class
Model
BaseModel
int
model_config
ConfigDict
extra
'forbid'
try
Model
'a'
except
ValidationError
exc
print
exc
"""
1 validation error for Model
Extra inputs are not permitted [type=extra_forbidden, input_value='a', input_type=str]
"""
'allow'
: Providing extra data is allowed and stored in the
__pydantic_extra__
dictionary attribute:
from
pydantic
import
BaseModel
ConfigDict
class
Model
BaseModel
int
model_config
ConfigDict
extra
'allow'
Model
'a'
assert
__pydantic_extra__
'y'
'a'
By default, no validation will be applied to these extra items, but you can set a type for the values by overriding
the type annotation for
__pydantic_extra__
from
pydantic
import
BaseModel
ConfigDict
Field
ValidationError
class
Model
BaseModel
__pydantic_extra__
dict
str
int
Field
init
False
int
model_config
ConfigDict
extra
'allow'
try
Model
'a'
except
ValidationError
exc
print
exc
"""
1 validation error for Model
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]
"""
Model
'2'
assert
assert
assert
model_dump
'x'
'y'
assert
__pydantic_extra__
'y'
frozen
instance-attribute
frozen
bool
Whether models are faux-immutable, i.e. whether
__setattr__
is allowed, and also generates
__hash__()
method for the model. This makes instances of the model potentially hashable if all the
attributes are hashable. Defaults to
False
Note
On V1, the inverse of this setting was called
allow_mutation
, and was
True
by default.
populate_by_name
instance-attribute
populate_by_name
bool
Whether an aliased field may be populated by its name as given by the model
attribute, as well as the alias. Defaults to
False
Warning
populate_by_name
usage is not recommended in v2.11+ and will be deprecated in v3.
Instead, you should use the
validate_by_name
configuration setting.
When
validate_by_name=True
and
validate_by_alias=True
, this is strictly equivalent to the
previous behavior of
populate_by_name=True
In v2.11, we also introduced a
validate_by_alias
setting that introduces more fine grained
control for validation behavior.
Here's how you might go about using the new settings to achieve the same behavior:
from
pydantic
import
BaseModel
ConfigDict
Field
class
Model
BaseModel
model_config
ConfigDict
validate_by_name
True
validate_by_alias
True
my_field
str
Field
alias
'my_alias'
Model
my_alias
'foo'
print
#> my_field='foo'
Model
my_alias
'foo'
print
#> my_field='foo'
use_enum_values
instance-attribute
use_enum_values
bool
Whether to populate models with the
value
property of enums, rather than the raw enum.
This may be useful if you want to serialize
model.model_dump()
later. Defaults to
False
Note
If you have an
Optional[Enum]
value that you set a default for, you need to use
validate_default=True
for said Field to ensure that the
use_enum_values
flag takes effect on the default, as extracting an
enum's value occurs during validation, not serialization.
from
enum
import
Enum
from
typing
import
Optional
from
pydantic
import
BaseModel
ConfigDict
Field
class
SomeEnum
Enum
FOO
'foo'
BAR
'bar'
BAZ
'baz'
class
SomeModel
BaseModel
model_config
ConfigDict
use_enum_values
True
some_enum
SomeEnum
another_enum
Optional
SomeEnum
Field
default
SomeEnum
FOO
validate_default
True
model1
SomeModel
some_enum
SomeEnum
BAR
print
model1
model_dump
())
#> {'some_enum': 'bar', 'another_enum': 'foo'}
model2
SomeModel
some_enum
SomeEnum
BAR
another_enum
SomeEnum
BAZ
print
model2
model_dump
())
#> {'some_enum': 'bar', 'another_enum': 'baz'}
validate_assignment
instance-attribute
validate_assignment
bool
Whether to validate the data when the model is changed. Defaults to
False
not
revalidated.
from
pydantic
import
BaseModel
class
User
BaseModel
name
str
user
User
name
'John Doe'
print
user
#> name='John Doe'
user
name
123
print
user
#> name=123
The validation does not happen when the data is changed.
In case you want to revalidate the model when the data is changed, you can use
validate_assignment=True
from
pydantic
import
BaseModel
ValidationError
class
User
BaseModel
validate_assignment
True
name
str
user
User
name
'John Doe'
print
user
#> name='John Doe'
try
user
name
123
except
ValidationError
print
'''
1 validation error for User
name
Input should be a valid string [type=string_type, input_value=123, input_type=int]
'''
arbitrary_types_allowed
instance-attribute
arbitrary_types_allowed
bool
Whether arbitrary types are allowed for field types. Defaults to
False
from
pydantic
import
BaseModel
ConfigDict
ValidationError
# This is not a pydantic model, it's an arbitrary class
class
Pet
def
__init__
self
name
str
self
name
name
class
Model
BaseModel
model_config
ConfigDict
arbitrary_types_allowed
True
pet
Pet
owner
str
pet
Pet
name
'Hedwig'
# A simple check of instance type is used to validate the data
model
Model
owner
'Harry'
pet
pet
print
model
#> pet=<__main__.Pet object at 0x0123456789ab> owner='Harry'
print
model
pet
#> <__main__.Pet object at 0x0123456789ab>
print
model
pet
name
#> Hedwig
print
type
model
pet
#> <class '__main__.Pet'>
try
# If the value is not an instance of the type, it's invalid
Model
owner
'Harry'
pet
'Hedwig'
except
ValidationError
print
'''
1 validation error for Model
pet
Input should be an instance of Pet [type=is_instance_of, input_value='Hedwig', input_type=str]
'''
# Nothing in the instance of the arbitrary type is checked
# Here name probably should have been a str, but it's not validated
pet2
Pet
name
model2
Model
owner
'Harry'
pet
pet2
print
model2
#> pet=<__main__.Pet object at 0x0123456789ab> owner='Harry'
print
model2
pet
#> <__main__.Pet object at 0x0123456789ab>
print
model2
pet
name
#> 42
print
type
model2
pet
#> <class '__main__.Pet'>
from_attributes
instance-attribute
from_attributes
bool
Whether to build models and look up discriminators of tagged unions using python object attributes.
loc_by_alias
instance-attribute
loc_by_alias
bool
Whether to use the actual key provided in the data (e.g. alias) for error
loc
s rather than the field's name. Defaults to
True
alias_generator
instance-attribute
alias_generator
Callable
str
str
AliasGenerator
None
A callable that takes a field name and returns an alias for it
or an instance of
AliasGenerator
. Defaults to
None
When using a callable, the alias generator is used for both validation and serialization.
If you want to use different alias generators for validation and serialization, you can use
AliasGenerator
instead.
If data source field names do not match your code style (e. g. CamelCase fields),
you can automatically generate aliases using
alias_generator
. Here's an example with
a basic callable:
from
pydantic
import
BaseModel
ConfigDict
from
pydantic.alias_generators
import
to_pascal
class
Voice
BaseModel
model_config
ConfigDict
alias_generator
to_pascal
name
str
language_code
str
voice
Voice
Name
'Filiz'
LanguageCode
'tr-TR'
print
voice
language_code
#> tr-TR
print
voice
model_dump
by_alias
True
#> {'Name': 'Filiz', 'LanguageCode': 'tr-TR'}
If you want to use different alias generators for validation and serialization, you can use
AliasGenerator
from
pydantic
import
AliasGenerator
BaseModel
ConfigDict
from
pydantic.alias_generators
import
to_camel
to_pascal
class
Athlete
BaseModel
first_name
str
last_name
str
sport
str
model_config
ConfigDict
alias_generator
AliasGenerator
validation_alias
to_camel
serialization_alias
to_pascal
athlete
Athlete
firstName
'John'
lastName
'Doe'
sport
'track'
print
athlete
model_dump
by_alias
True
#> {'FirstName': 'John', 'LastName': 'Doe', 'Sport': 'track'}
Note
Pydantic offers three built-in alias generators:
to_pascal
to_camel
, and
to_snake
ignored_types
instance-attribute
ignored_types
tuple
type
...
A tuple of types that may occur as values of class attributes without annotations. This is
typically used for custom descriptors (classes that behave like
property
). If an attribute is set on a
class without an annotation and has a type that is not in this tuple (or otherwise recognized by
pydantic
), an error will be raised. Defaults to
allow_inf_nan
instance-attribute
allow_inf_nan
bool
Whether to allow infinity (
+inf
-inf
) and NaN values to float and decimal fields. Defaults to
True
json_schema_extra
instance-attribute
json_schema_extra
JsonDict
JsonSchemaExtraCallable
None
A dict or callable to provide extra JSON schema properties. Defaults to
None
json_encoders
instance-attribute
json_encoders
dict
type
object
JsonEncoder
None
dict
of custom JSON encoders for specific types. Defaults to
None
Deprecated
This config option is a carryover from v1.
We originally planned to remove it in v2 but didn't have a 1:1 replacement so we are keeping it for now.
It is still deprecated and will likely be removed in the future.
strict
instance-attribute
strict
bool
(new in V2)
True
, strict validation is applied to all fields on the model.
By default, Pydantic attempts to coerce values to the correct type, when possible.
There are situations in which you may want to disable this behavior, and instead raise an error if a value's type
does not match the field's type annotation.
To configure strict mode for all fields on a model, you can set
strict=True
on the model.
from
pydantic
import
BaseModel
ConfigDict
class
Model
BaseModel
model_config
ConfigDict
strict
True
name
str
age
int
See
Strict Mode
for more details.
See the
Conversion Table
for more details on how Pydantic converts data in both
strict and lax modes.
revalidate_instances
instance-attribute
revalidate_instances
Literal
"always"
"never"
"subclass-instances"
When and how to revalidate models and dataclasses during validation. Accepts the string
values of
'never'
'always'
and
'subclass-instances'
. Defaults to
'never'
'never'
will not revalidate models and dataclasses during validation
'always'
will revalidate models and dataclasses during validation
'subclass-instances'
will revalidate models and dataclasses during validation if the instance is a
subclass of the model or dataclass
By default, model and dataclass instances are not revalidated during validation.
from
pydantic
import
BaseModel
class
User
BaseModel
revalidate_instances
'never'
hobbies
list
str
class
SubUser
User
sins
list
str
class
Transaction
BaseModel
user
User
my_user
User
hobbies
'reading'
Transaction
user
my_user
print
#> user=User(hobbies=['reading'])
my_user
hobbies
Transaction
user
my_user
print
#> user=User(hobbies=[1])
my_sub_user
SubUser
hobbies
'scuba diving'
sins
'lying'
Transaction
user
my_sub_user
print
#> user=SubUser(hobbies=['scuba diving'], sins=['lying'])
If you want to revalidate instances during validation, you can set
revalidate_instances
'always'
in the model's config.
from
pydantic
import
BaseModel
ValidationError
class
User
BaseModel
revalidate_instances
'always'
hobbies
list
str
class
SubUser
User
sins
list
str
class
Transaction
BaseModel
user
User
my_user
User
hobbies
'reading'
Transaction
user
my_user
print
#> user=User(hobbies=['reading'])
my_user
hobbies
try
Transaction
user
my_user
except
ValidationError
print
'''
1 validation error for Transaction
user.hobbies.0
Input should be a valid string [type=string_type, input_value=1, input_type=int]
'''
my_sub_user
SubUser
hobbies
'scuba diving'
sins
'lying'
Transaction
user
my_sub_user
print
#> user=User(hobbies=['scuba diving'])
It's also possible to set
revalidate_instances
'subclass-instances'
to only revalidate instances
of subclasses of the model.
from
pydantic
import
BaseModel
class
User
BaseModel
revalidate_instances
'subclass-instances'
hobbies
list
str
class
SubUser
User
sins
list
str
class
Transaction
BaseModel
user
User
my_user
User
hobbies
'reading'
Transaction
user
my_user
print
#> user=User(hobbies=['reading'])
my_user
hobbies
Transaction
user
my_user
print
#> user=User(hobbies=[1])
my_sub_user
SubUser
hobbies
'scuba diving'
sins
'lying'
Transaction
user
my_sub_user
print
#> user=User(hobbies=['scuba diving'])
ser_json_timedelta
instance-attribute
ser_json_timedelta
Literal
'iso8601'
'float'
The format of JSON serialized timedeltas. Accepts the string values of
'iso8601'
and
'float'
. Defaults to
'iso8601'
'iso8601'
will serialize timedeltas to ISO 8601 durations.
'float'
will serialize timedeltas to the total number of seconds.
ser_json_bytes
instance-attribute
ser_json_bytes
Literal
'utf8'
'base64'
'hex'
The encoding of JSON serialized bytes. Defaults to
'utf8'
Set equal to
val_json_bytes
to get back an equal value after serialization round trip.
'utf8'
will serialize bytes to UTF-8 strings.
'base64'
will serialize bytes to URL safe base64 strings.
'hex'
will serialize bytes to hexadecimal strings.
val_json_bytes
instance-attribute
val_json_bytes
Literal
'utf8'
'base64'
'hex'
The encoding of JSON serialized bytes to decode. Defaults to
'utf8'
Set equal to
ser_json_bytes
to get back an equal value after serialization round trip.
'utf8'
will deserialize UTF-8 strings to bytes.
'base64'
will deserialize URL safe base64 strings to bytes.
'hex'
will deserialize hexadecimal strings to bytes.
ser_json_inf_nan
instance-attribute
ser_json_inf_nan
Literal
'null'
'constants'
'strings'
The encoding of JSON serialized infinity and NaN float values. Defaults to
'null'
'null'
will serialize infinity and NaN values as
null
'constants'
will serialize infinity and NaN values as
Infinity
and
NaN
'strings'
will serialize infinity as string
"Infinity"
and NaN as string
"NaN"
validate_default
instance-attribute
validate_default
bool
Whether to validate default values during validation. Defaults to
False
validate_return
instance-attribute
validate_return
bool
Whether to validate the return value from call validators. Defaults to
False
protected_namespaces
instance-attribute
protected_namespaces
tuple
str
Pattern
str
...
tuple
of strings and/or patterns that prevent models from having fields with names that conflict with them.
For strings, we match on a prefix basis. Ex, if 'dog' is in the protected namespace, 'dog_name' will be protected.
For patterns, we match on the entire field name. Ex, if
re.compile(r'^dog$')
is in the protected namespace, 'dog' will be protected, but 'dog_name' will not be.
Defaults to
('model_validate', 'model_dump',)
The reason we've selected these is to prevent collisions with other validation / dumping formats
in the future - ex,
model_validate_{some_newly_supported_format}
Before v2.10, Pydantic used
('model_',)
as the default value for this setting to
prevent collisions between model attributes and
BaseModel
's own methods. This was changed
in v2.10 given feedback that this restriction was limiting in AI and data science contexts,
where it is common to have fields with names like
model_id
model_input
model_output
, etc.
For more details, see https://github.com/pydantic/pydantic/issues/10315.
import
warnings
from
pydantic
import
BaseModel
warnings
filterwarnings
'error'
# Raise warnings as errors
try
class
Model
BaseModel
model_dump_something
str
except
UserWarning
print
'''
Field "model_dump_something" in Model has conflict with protected namespace "model_dump".
You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('model_validate',)`.
'''
You can customize this behavior using the
protected_namespaces
setting:
import
import
warnings
from
pydantic
import
BaseModel
ConfigDict
with
warnings
catch_warnings
record
True
caught_warnings
warnings
simplefilter
'always'
# Catch all warnings
class
Model
BaseModel
safe_field
str
also_protect_field
str
protect_this
str
model_config
ConfigDict
protected_namespaces
'protect_me_'
'also_protect_'
compile
'^protect_this$'
for
warning
caught_warnings
print
warning
message
'''
Field "also_protect_field" in Model has conflict with protected namespace "also_protect_".
You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('protect_me_', re.compile('^protect_this$'))`.
Field "protect_this" in Model has conflict with protected namespace "re.compile('^protect_this$')".
You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ('protect_me_', 'also_protect_')`.
'''
While Pydantic will only emit a warning when an item is in a protected namespace but does not actually have a collision,
an error
raised if there is an actual collision with an existing attribute:
from
pydantic
import
BaseModel
ConfigDict
try
class
Model
BaseModel
model_validate
str
model_config
ConfigDict
protected_namespaces
'model_'
,))
except
NameError
print
'''
Field "model_validate" conflicts with member <bound method BaseModel.model_validate of <class 'pydantic.main.BaseModel'>> of protected namespace "model_".
'''
hide_input_in_errors
instance-attribute
hide_input_in_errors
bool
Whether to hide inputs when printing errors. Defaults to
False
Pydantic shows the input value and type when it raises
ValidationError
during the validation.
from
pydantic
import
BaseModel
ValidationError
class
Model
BaseModel
str
try
Model
123
except
ValidationError
print
'''
1 validation error for Model
Input should be a valid string [type=string_type, input_value=123, input_type=int]
'''
You can hide the input value and type by setting the
hide_input_in_errors
config to
True
from
pydantic
import
BaseModel
ConfigDict
ValidationError
class
Model
BaseModel
str
model_config
ConfigDict
hide_input_in_errors
True
try
Model
123
except
ValidationError
print
'''
1 validation error for Model
Input should be a valid string [type=string_type]
'''
defer_build
instance-attribute
defer_build
bool
Whether to defer model validator and serializer construction until the first model validation. Defaults to False.
This can be useful to avoid the overhead of building models which are only
used nested within other models, or when you want to manually define type namespace via
Model.model_rebuild(_types_namespace=...)
Since v2.10, this setting also applies to pydantic dataclasses and TypeAdapter instances.
plugin_settings
instance-attribute
plugin_settings
dict
str
object
None
dict
of settings for plugins. Defaults to
None
schema_generator
instance-attribute
schema_generator
type
GenerateSchema
None
Warning
schema_generator
is deprecated in v2.10.
Prior to v2.10, this setting was advertised as highly subject to change.
It's possible that this interface may once again become public once the internal core schema generation
API is more stable, but that will likely come after significant performance improvements have been made.
json_schema_serialization_defaults_required
instance-attribute
json_schema_serialization_defaults_required
bool
Whether fields with default values should be marked as required in the serialization schema. Defaults to
False
This ensures that the serialization schema will reflect the fact a field with a default will always be present
when serializing the model, even though it is not required for validation.
However, there are scenarios where this may be undesirable â in particular, if you want to share the schema
between validation and serialization, and don't mind fields with defaults being marked as not required during
serialization. See
#7209
for more details.
from
pydantic
import
BaseModel
ConfigDict
class
Model
BaseModel
str
'a'
model_config
ConfigDict
json_schema_serialization_defaults_required
True
print
Model
model_json_schema
mode
'validation'
'''
'properties': {'a': {'default': 'a', 'title': 'A', 'type': 'string'}},
'title': 'Model',
'type': 'object',
'''
print
Model
model_json_schema
mode
'serialization'
'''
'properties': {'a': {'default': 'a', 'title': 'A', 'type': 'string'}},
'required': ['a'],
'title': 'Model',
'type': 'object',
'''
json_schema_mode_override
instance-attribute
json_schema_mode_override
Literal
"validation"
"serialization"
None
If not
None
, the specified mode will be used to generate the JSON schema regardless of what
mode
was passed to
the function call. Defaults to
None
This provides a way to force the JSON schema generation to reflect a specific mode, e.g., to always use the
validation schema.
It can be useful when using frameworks (such as FastAPI) that may generate different schemas for validation
and serialization that must both be referenced from the same schema; when this happens, we automatically append
-Input
to the definition reference for the validation schema and
-Output
to the definition reference for the
serialization schema. By specifying a
json_schema_mode_override
though, this prevents the conflict between
the validation and serialization schemas (since both will use the specified schema), and so prevents the suffixes
from being added to the definition references.
from
pydantic
import
BaseModel
ConfigDict
Json
class
Model
BaseModel
Json
int
# requires a string to validate, but will dump an int
print
Model
model_json_schema
mode
'serialization'
'''
'properties': {'a': {'title': 'A', 'type': 'integer'}},
'required': ['a'],
'title': 'Model',
'type': 'object',
'''
class
ForceInputModel
Model
# the following ensures that even with mode='serialization', we
# will get the schema that would be generated for validation.
model_config
ConfigDict
json_schema_mode_override
'validation'
print
ForceInputModel
model_json_schema
mode
'serialization'
'''
'properties': {
'a': {
'contentMediaType': 'application/json',
'contentSchema': {'type': 'integer'},
'title': 'A',
'type': 'string',
'required': ['a'],
'title': 'ForceInputModel',
'type': 'object',
'''
coerce_numbers_to_str
instance-attribute
coerce_numbers_to_str
bool
True
, enables automatic coercion of any
Number
type to
str
in "lax" (non-strict) mode. Defaults to
False
Pydantic doesn't allow number types (
int
float
Decimal
) to be coerced as type
str
by default.
from
decimal
import
Decimal
from
pydantic
import
BaseModel
ConfigDict
ValidationError
class
Model
BaseModel
value
str
try
print
Model
value
except
ValidationError
print
'''
1 validation error for Model
value
Input should be a valid string [type=string_type, input_value=42, input_type=int]
'''
class
Model
BaseModel
model_config
ConfigDict
coerce_numbers_to_str
True
value
str
repr
Model
value
value
#> "42"
repr
Model
value
42.13
value
#> "42.13"
repr
Model
value
Decimal
'42.13'
value
#> "42.13"
regex_engine
instance-attribute
regex_engine
Literal
'rust-regex'
'python-re'
The regex engine to be used for pattern validation.
Defaults to
'rust-regex'
rust-regex
uses the
regex
Rust crate,
which is non-backtracking and therefore more DDoS resistant, but does not support all regex features.
python-re
use the
module,
which supports all regex features, but may be slower.
Note
If you use a compiled regex pattern, the python-re engine will be used regardless of this setting.
This is so that flags such as
re.IGNORECASE
are respected.
from
pydantic
import
BaseModel
ConfigDict
Field
ValidationError
class
Model
BaseModel
model_config
ConfigDict
regex_engine
'python-re'
value
str
Field
pattern
'^abc(?=def)'
print
Model
value
'abcdef'
value
#> abcdef
try
print
Model
value
'abxyzcdef'
except
ValidationError
print
'''
1 validation error for Model
value
String should match pattern '^abc(?=def)' [type=string_pattern_mismatch, input_value='abxyzcdef', input_type=str]
'''
validation_error_cause
instance-attribute
validation_error_cause
bool
True
, Python exceptions that were part of a validation failure will be shown as an exception group as a cause. Can be useful for debugging. Defaults to
False
Note
Python 3.10 and older don't support exception groups natively. <=3.10, backport must be installed:
pip install exceptiongroup
Note
The structure of validation errors are likely to change in future Pydantic versions. Pydantic offers no guarantees about their structure. Should be used for visual traceback debugging only.
use_attribute_docstrings
instance-attribute
use_attribute_docstrings
bool
Whether docstrings of attributes (bare string literals immediately following the attribute declaration)
should be used for field descriptions. Defaults to
False
Available in Pydantic v2.7+.
from
pydantic
import
BaseModel
ConfigDict
Field
class
Model
BaseModel
model_config
ConfigDict
use_attribute_docstrings
True
str
"""
Example of an attribute docstring
"""
int
Field
description
"Description in Field"
"""
Description in Field overrides attribute docstring
"""
print
Model
model_fields
"x"
description
# > Example of an attribute docstring
print
Model
model_fields
"y"
description
# > Description in Field
This requires the source code of the class to be available at runtime.
Usage with
TypedDict
and stdlib dataclasses
Due to current limitations, attribute docstrings detection may not work as expected when using
TypedDict
and stdlib dataclasses, in particular when:
inheritance is being used.
multiple classes have the same name in the same source file.
cache_strings
instance-attribute
cache_strings
bool
Literal
'all'
'keys'
'none'
Whether to cache strings to avoid constructing new Python objects. Defaults to True.
Enabling this setting should significantly improve validation performance while increasing memory usage slightly.
True
'all'
(the default): cache all strings
'keys'
: cache only dictionary keys
False
'none'
: no caching
Note
True
'all'
is required to cache strings during general validation because
validators don't know if they're in a key or a value.
Tip
If repeated strings are rare, it's recommended to use
'keys'
'none'
to reduce memory usage,
as the performance difference is minimal if repeated strings are rare.
validate_by_alias
instance-attribute
validate_by_alias
bool
Whether an aliased field may be populated by its alias. Defaults to
True
Note
In v2.11,
validate_by_alias
was introduced in conjunction with
validate_by_name
to empower users with more fine grained validation control. In <v2.11, disabling validation by alias was not possible.
Here's an example of disabling validation by alias:
from
pydantic
import
BaseModel
ConfigDict
Field
class
Model
BaseModel
model_config
ConfigDict
validate_by_name
True
validate_by_alias
False
my_field
str
Field
validation_alias
'my_alias'
Model
my_field
'foo'
print
#> my_field='foo'
Warning
You cannot set both
validate_by_alias
and
validate_by_name
False
This would make it impossible to populate an attribute.
See
usage errors
for an example.
If you set
validate_by_alias
False
, under the hood, Pydantic dynamically sets
validate_by_name
True
to ensure that validation can still occur.
validate_by_name
instance-attribute
validate_by_name
bool
Whether an aliased field may be populated by its name as given by the model
attribute. Defaults to
False
Note
In v2.0-v2.10, the
populate_by_name
configuration setting was used to specify
whether or not a field could be populated by its name
and
alias.
In v2.11,
validate_by_name
was introduced in conjunction with
validate_by_alias
to empower users with more fine grained validation behavior control.
from
pydantic
import
BaseModel
ConfigDict
Field
class
Model
BaseModel
model_config
ConfigDict
validate_by_name
True
validate_by_alias
True
my_field
str
Field
validation_alias
'my_alias'
Model
my_alias
'foo'
print
#> my_field='foo'
Model
my_field
'foo'
print
#> my_field='foo'
Warning
You cannot set both
validate_by_alias
and
validate_by_name
False
This would make it impossible to populate an attribute.
See
usage errors
for an example.
serialize_by_alias
instance-attribute
serialize_by_alias
bool
Whether an aliased field should be serialized by its alias. Defaults to
False
Note: In v2.11,
serialize_by_alias
was introduced to address the
popular request
for consistency with alias behavior for validation and serialization settings.
In v3, the default value is expected to change to
True
for consistency with the validation default.
from
pydantic
import
BaseModel
ConfigDict
Field
class
Model
BaseModel
model_config
ConfigDict
serialize_by_alias
True
my_field
str
Field
serialization_alias
'my_alias'
Model
my_field
'foo'
print
model_dump
())
#> {'my_alias': 'foo'}
with_config
with_config
config
ConfigDict
Callable
_TypeT
_TypeT
with_config
config
ConfigDict
Callable
_TypeT
_TypeT
with_config
config
Unpack
ConfigDict
Callable
_TypeT
_TypeT
with_config
config
ConfigDict
None
None
kwargs
Any
Callable
_TypeT
_TypeT
Usage Documentation
Configuration with other types
A convenience decorator to set a
Pydantic configuration
on a
TypedDict
or a
dataclass
from the standard library.
Although the configuration can be set using the
__pydantic_config__
attribute, it does not play well with type checkers,
especially with
TypedDict
Usage
from
typing_extensions
import
TypedDict
from
pydantic
import
ConfigDict
TypeAdapter
with_config
@with_config
ConfigDict
str_to_lower
True
class
TypedDict
str
TypeAdapter
print
validate_python
'x'
'ABC'
}))
#> {'x': 'abc'}
Source code in
pydantic/config.py
1157
1158
1159
1160
1161
1162
1163
1164
1165
1166
1167
1168
1169
1170
1171
1172
1173
1174
1175
1176
1177
1178
1179
1180
1181
1182
1183
1184
1185
1186
1187
1188
1189
1190
1191
1192
1193
1194
1195
1196
1197
1198
1199
1200
1201
1202
1203
1204
1205
1206
1207
1208
1209
1210
def
with_config
config
ConfigDict
None
None
kwargs
Any
Callable
_TypeT
_TypeT
"""!!! abstract "Usage Documentation"
[Configuration with other types](../concepts/config.md#configuration-on-other-supported-types)
A convenience decorator to set a [Pydantic configuration](config.md) on a `TypedDict` or a `dataclass` from the standard library.
Although the configuration can be set using the `__pydantic_config__` attribute, it does not play well with type checkers,
especially with `TypedDict`.
!!! example "Usage"
```python
from typing_extensions import TypedDict
from pydantic import ConfigDict, TypeAdapter, with_config
@with_config(ConfigDict(str_to_lower=True))
class TD(TypedDict):
x: str
ta = TypeAdapter(TD)
print(ta.validate_python({'x': 'ABC'}))
#> {'x': 'abc'}
```
"""
config
not
None
and
kwargs
raise
ValueError
'Cannot specify both `config` and keyword arguments'
len
kwargs
and
kwargs_conf
kwargs
get
'config'
not
None
warnings
warn
'Passing `config` as a keyword argument is deprecated. Pass `config` as a positional argument instead'
category
PydanticDeprecatedSince211
stacklevel
final_config
cast
ConfigDict
kwargs_conf
else
final_config
config
config
not
None
else
cast
ConfigDict
kwargs
def
inner
class_
_TypeT
_TypeT
# Ideally, we would check for `class_` to either be a `TypedDict` or a stdlib dataclass.
# However, the `@with_config` decorator can be applied *after* `@dataclass`. To avoid
# common mistakes, we at least check for `class_` to not be a Pydantic model.
from
._internal._utils
import
is_model_class
is_model_class
class_
raise
PydanticUserError
'Cannot use `with_config` on
class_
__name__
as it is a Pydantic model'
code
'with-config-on-model'
class_
__pydantic_config__
final_config
return
class_
return
inner
ExtraValues
module-attribute
ExtraValues
Literal
'allow'
'ignore'
'forbid'
pydantic.alias_generators
Alias generators for converting between different capitalization conventions.
to_pascal
to_pascal
snake
str
str
Convert a snake_case string to PascalCase.
Parameters:
Name
Type
Description
Default
snake
str
The string to convert.
required
Returns:
Type
Description
str
The PascalCase string.
Source code in
pydantic/alias_generators.py
def
to_pascal
snake
str
str
"""Convert a snake_case string to PascalCase.
Args:
snake: The string to convert.
Returns:
The PascalCase string.
"""
camel
snake
title
return
sub
'([0-9A-Za-z])_(?=[0-9A-Z])'
lambda
group
camel
to_camel
to_camel
snake
str
str
Convert a snake_case string to camelCase.
Parameters:
Name
Type
Description
Default
snake
str
The string to convert.
required
Returns:
Type
Description
str
The converted camelCase string.
Source code in
pydantic/alias_generators.py
def
to_camel
snake
str
str
"""Convert a snake_case string to camelCase.
Args:
snake: The string to convert.
Returns:
The converted camelCase string.
"""
# If the string is already in camelCase and does not contain a digit followed
# by a lowercase letter, return it as it is
match
'^[a-z]+[A-Za-z0-9]*$'
snake
and
not
search
'\d[a-z]'
snake
return
snake
camel
to_pascal
snake
return
sub
'(^_*[A-Z])'
lambda
group
lower
(),
camel
to_snake
to_snake
camel
str
str
Convert a PascalCase, camelCase, or kebab-case string to snake_case.
Parameters:
Name
Type
Description
Default
camel
str
The string to convert.
required
Returns:
Type
Description
str
The converted string in snake_case.
Source code in
pydantic/alias_generators.py
def
to_snake
camel
str
str
"""Convert a PascalCase, camelCase, or kebab-case string to snake_case.
Args:
camel: The string to convert.
Returns:
The converted string in snake_case.
"""
# Handle the sequence of uppercase letters followed by a lowercase letter
snake
sub
'([A-Z]+)([A-Z][a-z])'
lambda
group
group
camel
# Insert an underscore between a lowercase letter and an uppercase letter
snake
sub
'([a-z])([A-Z])'
lambda
group
group
snake
# Insert an underscore between a digit and an uppercase letter
snake
sub
'([0-9])([A-Z])'
lambda
group
group
snake
# Insert an underscore between a lowercase letter and a digit
snake
sub
'([a-z])([0-9])'
lambda
group
group
snake
# Replace hyphens with underscores to handle kebab-case
snake
snake
replace
'-'
'_'
return
snake
lower
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 022_standard_library_types.txt ---
Standard Library Types
Pydantic supports many common types from the Python standard library. If you need stricter processing see
Strict Types
, including if you need to constrain the values allowed (e.g. to require a positive
int
Note
Pydantic still supports older (3.8-) typing constructs like
typing.List
and
typing.Dict
, but
it's best practice to use the newer types like
list
and
dict
Booleans
A standard
bool
field will raise a
ValidationError
if the value is not one of the following:
A valid boolean (i.e.
True
False
The integers
str
which when converted to lower case is one of
'0', 'off', 'f', 'false', 'n', 'no', '1', 'on', 't', 'true', 'y', 'yes'
bytes
which is valid per the previous rule when decoded to
str
Note
If you want stricter boolean logic (e.g. a field which only permits
True
and
False
) you can
use
StrictBool
Here is a script demonstrating some of these behaviors:
from
pydantic
import
BaseModel
ValidationError
class
BooleanModel
BaseModel
bool_value
bool
print
BooleanModel
bool_value
False
#> bool_value=False
print
BooleanModel
bool_value
'False'
#> bool_value=False
print
BooleanModel
bool_value
#> bool_value=True
try
BooleanModel
bool_value
[])
except
ValidationError
print
str
"""
1 validation error for BooleanModel
bool_value
Input should be a valid boolean [type=bool_type, input_value=[], input_type=list]
"""
Datetime Types
Pydantic supports the following
datetime
types:
datetime.datetime
datetime
fields will accept values of type:
datetime
; an existing
datetime
object
int
float
; assumed as Unix time, i.e. seconds (if >=
-2e10
and <=
2e10
) or milliseconds
(if <
-2e10
or >
2e10
) since 1 January 1970
str
; the following formats are accepted:
YYYY-MM-DD[T]HH:MM[:SS[.ffffff]][Z or [Â±]HH[:]MM]
YYYY-MM-DD
is accepted in lax mode, but not in strict mode
int
float
as a string (assumed as Unix time)
datetime.date
instances are accepted in lax mode, but not in strict mode
from
datetime
import
datetime
from
pydantic
import
BaseModel
class
Event
BaseModel
datetime
None
event
Event
'2032-04-23T10:20:30.400+02:30'
print
event
model_dump
())
"""
{'dt': datetime.datetime(2032, 4, 23, 10, 20, 30, 400000, tzinfo=TzInfo(+02:30))}
"""
datetime.date
date
fields will accept values of type:
date
; an existing
date
object
int
float
; handled the same as described for
datetime
above
str
; the following formats are accepted:
YYYY-MM-DD
int
float
as a string (assumed as Unix time)
from
datetime
import
date
from
pydantic
import
BaseModel
class
Birthday
BaseModel
date
None
my_birthday
Birthday
1679616000.0
print
my_birthday
model_dump
())
#> {'d': datetime.date(2023, 3, 24)}
datetime.time
time
fields will accept values of type:
time
; an existing
time
object
str
; the following formats are accepted:
HH:MM[:SS[.ffffff]][Z or [Â±]HH[:]MM]
from
datetime
import
time
from
pydantic
import
BaseModel
class
Meeting
BaseModel
time
None
Meeting
time
print
model_dump
())
#> {'t': datetime.time(4, 8, 16)}
datetime.timedelta
timedelta
fields will accept values of type:
timedelta
; an existing
timedelta
object
int
float
; assumed to be seconds
str
; the following formats are accepted:
[-][[DD]D,]HH:MM:SS[.ffffff]
Ex:
'1d,01:02:03.000004'
'1D01:02:03.000004'
'01:02:03'
[Â±]P[DD]DT[HH]H[MM]M[SS]S
ISO 8601
format for timedelta)
from
datetime
import
timedelta
from
pydantic
import
BaseModel
class
Model
BaseModel
timedelta
None
Model
'P3DT12H30M5S'
print
model_dump
())
#> {'td': datetime.timedelta(days=3, seconds=45005)}
Number Types
Pydantic supports the following numeric types from the Python standard library:
int
Pydantic uses
int(v)
to coerce types to an
int
see
Data conversion
for details on loss of information during data conversion.
float
Pydantic uses
float(v)
to coerce values to floats.
enum.IntEnum
Validation: Pydantic checks that the value is a valid
IntEnum
instance.
Validation for subclass of
enum.IntEnum
: checks that the value is a valid member of the integer enum;
see
Enums and Choices
for more details.
decimal.Decimal
Validation: Pydantic attempts to convert the value to a string, then passes the string to
Decimal(v)
Serialization: Pydantic serializes
Decimal
types as strings.
You can use a custom serializer to override this behavior if desired. For example:
from
decimal
import
Decimal
from
typing
import
Annotated
from
pydantic
import
BaseModel
PlainSerializer
class
Model
BaseModel
Decimal
Annotated
Decimal
PlainSerializer
lambda
float
return_type
float
when_used
'json'
my_model
Model
Decimal
'1.1'
Decimal
'2.1'
print
my_model
model_dump
())
#> {'x': Decimal('1.1'), 'y': Decimal('2.1')}
print
my_model
model_dump
mode
'json'
#> {'x': '1.1', 'y': 2.1}
print
my_model
model_dump_json
())
#> {"x":"1.1","y":2.1}
complex
Validation: Pydantic supports
complex
types or
str
values that can be converted to a
complex
type.
Serialization: Pydantic serializes
complex
types as strings.
fractions.Fraction
Validation: Pydantic attempts to convert the value to a
Fraction
using
Fraction(v)
Serialization: Pydantic serializes
Fraction
types as strings.
Enum
Pydantic uses Python's standard
enum
classes to define choices.
enum.Enum
checks that the value is a valid
Enum
instance.
Subclass of
enum.Enum
checks that the value is a valid member of the enum.
from
enum
import
Enum
IntEnum
from
pydantic
import
BaseModel
ValidationError
class
FruitEnum
str
Enum
pear
'pear'
banana
'banana'
class
ToolEnum
IntEnum
spanner
wrench
class
CookingModel
BaseModel
fruit
FruitEnum
FruitEnum
pear
tool
ToolEnum
ToolEnum
spanner
print
CookingModel
())
#> fruit=<FruitEnum.pear: 'pear'> tool=<ToolEnum.spanner: 1>
print
CookingModel
tool
fruit
'banana'
#> fruit=<FruitEnum.banana: 'banana'> tool=<ToolEnum.wrench: 2>
try
CookingModel
fruit
'other'
except
ValidationError
print
"""
1 validation error for CookingModel
fruit
Input should be 'pear' or 'banana' [type=enum, input_value='other', input_type=str]
"""
Lists and Tuples
list
Allows
list
tuple
set
frozenset
deque
, or generators and casts to a
list
When a generic parameter is provided, the appropriate validation is applied to all items of the list.
Python 3.9 and above
Python 3.10 and above
from
typing
import
Optional
from
pydantic
import
BaseModel
class
Model
BaseModel
simple_list
Optional
list
None
list_of_ints
Optional
list
int
None
print
Model
simple_list
'1'
'2'
'3'
simple_list
#> ['1', '2', '3']
print
Model
list_of_ints
'1'
'2'
'3'
list_of_ints
#> [1, 2, 3]
from
pydantic
import
BaseModel
class
Model
BaseModel
simple_list
list
None
None
list_of_ints
list
int
None
None
print
Model
simple_list
'1'
'2'
'3'
simple_list
#> ['1', '2', '3']
print
Model
list_of_ints
'1'
'2'
'3'
list_of_ints
#> [1, 2, 3]
tuple
Allows
list
tuple
set
frozenset
deque
, or generators and casts to a
tuple
When generic parameters are provided, the appropriate validation is applied to the respective items of the tuple
typing.Tuple
Handled the same as
tuple
above.
Python 3.9 and above
Python 3.10 and above
from
typing
import
Optional
from
pydantic
import
BaseModel
class
Model
BaseModel
simple_tuple
Optional
tuple
None
tuple_of_different_types
Optional
tuple
int
float
bool
None
print
Model
simple_tuple
simple_tuple
#> (1, 2, 3, 4)
print
Model
tuple_of_different_types
tuple_of_different_types
#> (3, 2.0, True)
from
pydantic
import
BaseModel
class
Model
BaseModel
simple_tuple
tuple
None
None
tuple_of_different_types
tuple
int
float
bool
None
None
print
Model
simple_tuple
simple_tuple
#> (1, 2, 3, 4)
print
Model
tuple_of_different_types
tuple_of_different_types
#> (3, 2.0, True)
typing.NamedTuple
Subclasses of
typing.NamedTuple
are similar to
tuple
, but create instances of the given
namedtuple
class.
Subclasses of
collections.namedtuple
are similar to subclass of
typing.NamedTuple
, but since field types are not specified,
all fields are treated as having type
Any
from
typing
import
NamedTuple
from
pydantic
import
BaseModel
ValidationError
class
Point
NamedTuple
int
int
class
Model
BaseModel
Point
try
Model
'1.3'
'2'
except
ValidationError
print
"""
1 validation error for Model
p.0
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='1.3', input_type=str]
"""
Deque
deque
Allows
list
tuple
set
frozenset
deque
, or generators and casts to a
deque
When generic parameters are provided, the appropriate validation is applied to the respective items of the
deque
typing.Deque
Handled the same as
deque
above.
Python 3.9 and above
Python 3.10 and above
from
typing
import
Deque
Optional
from
pydantic
import
BaseModel
class
Model
BaseModel
deque
Optional
Deque
int
None
print
Model
deque
deque
#> deque([1, 2, 3])
from
typing
import
Deque
from
pydantic
import
BaseModel
class
Model
BaseModel
deque
Deque
int
None
None
print
Model
deque
deque
#> deque([1, 2, 3])
Sets
set
Allows
list
tuple
set
frozenset
deque
, or generators and casts to a
set
When a generic parameter is provided, the appropriate validation is applied to all items of the set.
typing.Set
Handled the same as
set
above.
Python 3.9 and above
Python 3.10 and above
from
typing
import
Optional
Set
from
pydantic
import
BaseModel
class
Model
BaseModel
simple_set
Optional
set
None
set_of_ints
Optional
Set
int
None
print
Model
simple_set
'1'
'2'
'3'
simple_set
#> {'1', '2', '3'}
print
Model
simple_set
'1'
'2'
'3'
simple_set
#> {'1', '2', '3'}
print
Model
set_of_ints
'1'
'2'
'3'
set_of_ints
#> {1, 2, 3}
from
pydantic
import
BaseModel
class
Model
BaseModel
simple_set
set
None
None
set_of_ints
set
int
None
None
print
Model
simple_set
'1'
'2'
'3'
simple_set
#> {'1', '2', '3'}
print
Model
simple_set
'1'
'2'
'3'
simple_set
#> {'1', '2', '3'}
print
Model
set_of_ints
'1'
'2'
'3'
set_of_ints
#> {1, 2, 3}
frozenset
Allows
list
tuple
set
frozenset
deque
, or generators and casts to a
frozenset
When a generic parameter is provided, the appropriate validation is applied to all items of the frozen set.
typing.FrozenSet
Handled the same as
frozenset
above.
Python 3.9 and above
Python 3.10 and above
from
typing
import
FrozenSet
Optional
from
pydantic
import
BaseModel
class
Model
BaseModel
simple_frozenset
Optional
frozenset
None
frozenset_of_ints
Optional
FrozenSet
int
None
Model
simple_frozenset
'1'
'2'
'3'
print
type
simple_frozenset
#> <class 'frozenset'>
print
sorted
simple_frozenset
#> ['1', '2', '3']
Model
frozenset_of_ints
'1'
'2'
'3'
print
type
frozenset_of_ints
#> <class 'frozenset'>
print
sorted
frozenset_of_ints
#> [1, 2, 3]
from
pydantic
import
BaseModel
class
Model
BaseModel
simple_frozenset
frozenset
None
None
frozenset_of_ints
frozenset
int
None
None
Model
simple_frozenset
'1'
'2'
'3'
print
type
simple_frozenset
#> <class 'frozenset'>
print
sorted
simple_frozenset
#> ['1', '2', '3']
Model
frozenset_of_ints
'1'
'2'
'3'
print
type
frozenset_of_ints
#> <class 'frozenset'>
print
sorted
frozenset_of_ints
#> [1, 2, 3]
Other Iterables
typing.Sequence
This is intended for use when the provided value should meet the requirements of the
Sequence
ABC, and it is
desirable to do eager validation of the values in the container. Note that when validation must be performed on the
values of the container, the type of the container may not be preserved since validation may end up replacing values.
We guarantee that the validated value will be a valid
typing.Sequence
, but it may have a different type than was
provided (generally, it will become a
list
typing.Iterable
This is intended for use when the provided value may be an iterable that shouldn't be consumed.
See
Infinite Generators
below for more detail on parsing and validation.
Similar to
typing.Sequence
, we guarantee that the validated result will be a valid
typing.Iterable
but it may have a different type than was provided. In particular, even if a non-generator type such as a
list
is provided, the post-validation value of a field of type
typing.Iterable
will be a generator.
Here is a simple example using
typing.Sequence
Python 3.9 and above
Python 3.10 and above
from
typing
import
Sequence
from
pydantic
import
BaseModel
class
Model
BaseModel
sequence_of_ints
Sequence
int
None
print
Model
sequence_of_ints
sequence_of_ints
#> [1, 2, 3, 4]
print
Model
sequence_of_ints
sequence_of_ints
#> (1, 2, 3, 4)
from
collections.abc
import
Sequence
from
pydantic
import
BaseModel
class
Model
BaseModel
sequence_of_ints
Sequence
int
None
print
Model
sequence_of_ints
sequence_of_ints
#> [1, 2, 3, 4]
print
Model
sequence_of_ints
sequence_of_ints
#> (1, 2, 3, 4)
Infinite Generators
If you have a generator you want to validate, you can still use
Sequence
as described above.
In that case, the generator will be consumed and stored on the model as a list and its values will be
validated against the type parameter of the
Sequence
(e.g.
int
Sequence[int]
However, if you have a generator that you
don't
want to be eagerly consumed (e.g. an infinite
generator or a remote data loader), you can use a field of type
Iterable
Python 3.9 and above
Python 3.10 and above
from
typing
import
Iterable
from
pydantic
import
BaseModel
class
Model
BaseModel
infinite
Iterable
int
def
infinite_ints
():
while
True
yield
Model
infinite
infinite_ints
())
print
"""
infinite=ValidatorIterator(index=0, schema=Some(Int(IntValidator { strict: false })))
"""
for
infinite
print
#> 0
#> 1
#> 2
#> 3
#> 4
#> 5
#> 6
#> 7
#> 8
#> 9
#> 10
break
from
collections.abc
import
Iterable
from
pydantic
import
BaseModel
class
Model
BaseModel
infinite
Iterable
int
def
infinite_ints
():
while
True
yield
Model
infinite
infinite_ints
())
print
"""
infinite=ValidatorIterator(index=0, schema=Some(Int(IntValidator { strict: false })))
"""
for
infinite
print
#> 0
#> 1
#> 2
#> 3
#> 4
#> 5
#> 6
#> 7
#> 8
#> 9
#> 10
break
Warning
During initial validation,
Iterable
fields only perform a simple check that the provided argument is iterable.
To prevent it from being consumed, no validation of the yielded values is performed eagerly.
Though the yielded values are not validated eagerly, they are still validated when yielded, and will raise a
ValidationError
at yield time when appropriate:
Python 3.9 and above
Python 3.10 and above
from
typing
import
Iterable
from
pydantic
import
BaseModel
ValidationError
class
Model
BaseModel
int_iterator
Iterable
int
def
my_iterator
():
yield
yield
'27'
yield
'a'
Model
int_iterator
my_iterator
())
print
next
int_iterator
#> 13
print
next
int_iterator
#> 27
try
next
int_iterator
except
ValidationError
print
"""
1 validation error for ValidatorIterator
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]
"""
from
collections.abc
import
Iterable
from
pydantic
import
BaseModel
ValidationError
class
Model
BaseModel
int_iterator
Iterable
int
def
my_iterator
():
yield
yield
'27'
yield
'a'
Model
int_iterator
my_iterator
())
print
next
int_iterator
#> 13
print
next
int_iterator
#> 27
try
next
int_iterator
except
ValidationError
print
"""
1 validation error for ValidatorIterator
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]
"""
Mapping Types
dict
dict(v)
is used to attempt to convert a dictionary.
from
pydantic
import
BaseModel
ValidationError
class
Model
BaseModel
dict
str
int
Model
'foo'
print
model_dump
())
#> {'x': {'foo': 1}}
try
Model
'foo'
'1'
except
ValidationError
print
"""
1 validation error for Model
Input should be a valid dictionary [type=dict_type, input_value='test', input_type=str]
"""
TypedDict
Note
This is a new feature of the Python standard library as of Python 3.8.
Because of limitations in
typing.TypedDict
before 3.12, the
typing-extensions
package is required for Python <3.12. You'll need to import
TypedDict
from
typing_extensions
instead of
typing
and will
get a build time error if you don't.
TypedDict
declares a dictionary type that expects all of
its instances to have a certain set of keys, where each key is associated with a value of a consistent type.
It is same as
dict
but Pydantic will validate the dictionary since keys are annotated.
Python 3.9 and above
Python 3.13 and above
from
typing_extensions
import
TypedDict
from
pydantic
import
TypeAdapter
ValidationError
class
User
TypedDict
name
str
int
TypeAdapter
User
print
validate_python
'name'
'foo'
'id'
}))
#> {'name': 'foo', 'id': 1}
try
validate_python
'name'
'foo'
except
ValidationError
print
"""
1 validation error for User
Field required [type=missing, input_value={'name': 'foo'}, input_type=dict]
"""
from
typing
import
TypedDict
from
pydantic
import
TypeAdapter
ValidationError
class
User
TypedDict
name
str
int
TypeAdapter
User
print
validate_python
'name'
'foo'
'id'
}))
#> {'name': 'foo', 'id': 1}
try
validate_python
'name'
'foo'
except
ValidationError
print
"""
1 validation error for User
Field required [type=missing, input_value={'name': 'foo'}, input_type=dict]
"""
You can define
__pydantic_config__
to change the model inherited from
TypedDict
See the
ConfigDict
API reference
for more details.
Python 3.9 and above
Python 3.10 and above
Python 3.13 and above
from
typing
import
Optional
from
typing_extensions
import
TypedDict
from
pydantic
import
ConfigDict
TypeAdapter
ValidationError
# `total=False` means keys are non-required
class
UserIdentity
TypedDict
total
False
name
Optional
str
surname
str
class
User
TypedDict
__pydantic_config__
ConfigDict
extra
'forbid'
identity
UserIdentity
age
int
TypeAdapter
User
print
validate_python
'identity'
'name'
'Smith'
'surname'
'John'
'age'
#> {'identity': {'name': 'Smith', 'surname': 'John'}, 'age': 37}
print
validate_python
'identity'
'name'
None
'surname'
'John'
'age'
#> {'identity': {'name': None, 'surname': 'John'}, 'age': 37}
print
validate_python
'identity'
{},
'age'
}))
#> {'identity': {}, 'age': 37}
try
validate_python
'identity'
'name'
'Smith'
'surname'
'John'
'age'
except
ValidationError
print
"""
1 validation error for User
identity.name
Input should be a valid string [type=string_type, input_value=['Smith'], input_type=list]
"""
try
validate_python
'identity'
'name'
'Smith'
'surname'
'John'
'age'
'37'
'email'
'john.smith@me.com'
except
ValidationError
print
"""
1 validation error for User
email
Extra inputs are not permitted [type=extra_forbidden, input_value='john.smith@me.com', input_type=str]
"""
from
typing_extensions
import
TypedDict
from
pydantic
import
ConfigDict
TypeAdapter
ValidationError
# `total=False` means keys are non-required
class
UserIdentity
TypedDict
total
False
name
str
None
surname
str
class
User
TypedDict
__pydantic_config__
ConfigDict
extra
'forbid'
identity
UserIdentity
age
int
TypeAdapter
User
print
validate_python
'identity'
'name'
'Smith'
'surname'
'John'
'age'
#> {'identity': {'name': 'Smith', 'surname': 'John'}, 'age': 37}
print
validate_python
'identity'
'name'
None
'surname'
'John'
'age'
#> {'identity': {'name': None, 'surname': 'John'}, 'age': 37}
print
validate_python
'identity'
{},
'age'
}))
#> {'identity': {}, 'age': 37}
try
validate_python
'identity'
'name'
'Smith'
'surname'
'John'
'age'
except
ValidationError
print
"""
1 validation error for User
identity.name
Input should be a valid string [type=string_type, input_value=['Smith'], input_type=list]
"""
try
validate_python
'identity'
'name'
'Smith'
'surname'
'John'
'age'
'37'
'email'
'john.smith@me.com'
except
ValidationError
print
"""
1 validation error for User
email
Extra inputs are not permitted [type=extra_forbidden, input_value='john.smith@me.com', input_type=str]
"""
from
typing
import
TypedDict
from
pydantic
import
ConfigDict
TypeAdapter
ValidationError
# `total=False` means keys are non-required
class
UserIdentity
TypedDict
total
False
name
str
None
surname
str
class
User
TypedDict
__pydantic_config__
ConfigDict
extra
'forbid'
identity
UserIdentity
age
int
TypeAdapter
User
print
validate_python
'identity'
'name'
'Smith'
'surname'
'John'
'age'
#> {'identity': {'name': 'Smith', 'surname': 'John'}, 'age': 37}
print
validate_python
'identity'
'name'
None
'surname'
'John'
'age'
#> {'identity': {'name': None, 'surname': 'John'}, 'age': 37}
print
validate_python
'identity'
{},
'age'
}))
#> {'identity': {}, 'age': 37}
try
validate_python
'identity'
'name'
'Smith'
'surname'
'John'
'age'
except
ValidationError
print
"""
1 validation error for User
identity.name
Input should be a valid string [type=string_type, input_value=['Smith'], input_type=list]
"""
try
validate_python
'identity'
'name'
'Smith'
'surname'
'John'
'age'
'37'
'email'
'john.smith@me.com'
except
ValidationError
print
"""
1 validation error for User
email
Extra inputs are not permitted [type=extra_forbidden, input_value='john.smith@me.com', input_type=str]
"""
Callable
See below for more detail on parsing and validation
Fields can also be of type
Callable
Python 3.9 and above
Python 3.10 and above
from
typing
import
Callable
from
pydantic
import
BaseModel
class
Foo
BaseModel
callback
Callable
int
int
Foo
callback
lambda
print
#> callback=<function <lambda> at 0x0123456789ab>
from
collections.abc
import
Callable
from
pydantic
import
BaseModel
class
Foo
BaseModel
callback
Callable
int
int
Foo
callback
lambda
print
#> callback=<function <lambda> at 0x0123456789ab>
Warning
Callable fields only perform a simple check that the argument is
callable; no validation of arguments, their types, or the return
type is performed.
IP Address Types
ipaddress.IPv4Address
: Uses the type itself for validation by passing the value to
IPv4Address(v)
ipaddress.IPv4Interface
: Uses the type itself for validation by passing the value to
IPv4Address(v)
ipaddress.IPv4Network
: Uses the type itself for validation by passing the value to
IPv4Network(v)
ipaddress.IPv6Address
: Uses the type itself for validation by passing the value to
IPv6Address(v)
ipaddress.IPv6Interface
: Uses the type itself for validation by passing the value to
IPv6Interface(v)
ipaddress.IPv6Network
: Uses the type itself for validation by passing the value to
IPv6Network(v)
See
Network Types
for other custom IP address types.
UUID
For UUID, Pydantic tries to use the type itself for validation by passing the value to
UUID(v)
There's a fallback to
UUID(bytes=v)
for
bytes
and
bytearray
In case you want to constrain the UUID version, you can check the following types:
UUID1
: requires UUID version 1.
UUID3
: requires UUID version 3.
UUID4
: requires UUID version 4.
UUID5
: requires UUID version 5.
Union
Pydantic has extensive support for union validation, both
typing.Union
and Python 3.10's pipe syntax (
A | B
) are supported.
Read more in the
Unions
section of the concepts docs.
type
Pydantic supports the use of
type[T]
to specify that a field may only accept classes (not instances)
that are subclasses of
from
pydantic
import
BaseModel
ValidationError
class
Foo
pass
class
Bar
Foo
pass
class
Other
pass
class
SimpleModel
BaseModel
just_subclasses
type
Foo
SimpleModel
just_subclasses
Foo
SimpleModel
just_subclasses
Bar
try
SimpleModel
just_subclasses
Other
except
ValidationError
print
"""
1 validation error for SimpleModel
just_subclasses
Input should be a subclass of Foo [type=is_subclass_of, input_value=<class '__main__.Other'>, input_type=type]
"""
You may also use
type
to specify that any class is allowed.
from
pydantic
import
BaseModel
ValidationError
class
Foo
pass
class
LenientSimpleModel
BaseModel
any_class_goes
type
LenientSimpleModel
any_class_goes
int
LenientSimpleModel
any_class_goes
Foo
try
LenientSimpleModel
any_class_goes
Foo
())
except
ValidationError
print
"""
1 validation error for LenientSimpleModel
any_class_goes
Input should be a type [type=is_type, input_value=<__main__.Foo object at 0x0123456789ab>, input_type=Foo]
"""
typing.TypeVar
TypeVar
is supported either unconstrained, constrained or with a bound.
from
typing
import
TypeVar
from
pydantic
import
BaseModel
Foobar
TypeVar
'Foobar'
BoundFloat
TypeVar
'BoundFloat'
bound
float
IntStr
TypeVar
'IntStr'
int
str
class
Model
BaseModel
Foobar
# equivalent of ": Any"
BoundFloat
# equivalent of ": float"
IntStr
# equivalent of ": Union[int, str]"
print
Model
4.2
'x'
#> a=[1] b=4.2 c='x'
# a may be None
print
Model
None
#> a=None b=1.0 c=1
None Types
None
type(None)
, or
Literal[None]
are all equivalent according to
the typing specification
Allows only
None
value.
Strings
str
: Strings are accepted as-is.
bytes
and
bytearray
are converted using the
decode()
method.
Enums inheriting from
str
are converted using the
value
attribute.
All other types cause an error.
Strings aren't Sequences
While instances of
str
are technically valid instances of the
Sequence[str]
protocol from a type-checker's point of
view, this is frequently not intended as is a common source of bugs.
As a result, Pydantic raises a
ValidationError
if you attempt to pass a
str
bytes
instance into a field of type
Sequence[str]
Sequence[bytes]
Python 3.9 and above
Python 3.10 and above
from
typing
import
Optional
Sequence
from
pydantic
import
BaseModel
ValidationError
class
Model
BaseModel
sequence_of_strs
Optional
Sequence
str
None
sequence_of_bytes
Optional
Sequence
bytes
None
print
Model
sequence_of_strs
'a'
'bc'
sequence_of_strs
#> ['a', 'bc']
print
Model
sequence_of_strs
'a'
'bc'
sequence_of_strs
#> ('a', 'bc')
print
Model
sequence_of_bytes
'a'
'bc'
sequence_of_bytes
#> [b'a', b'bc']
print
Model
sequence_of_bytes
'a'
'bc'
sequence_of_bytes
#> (b'a', b'bc')
try
Model
sequence_of_strs
'abc'
except
ValidationError
print
"""
1 validation error for Model
sequence_of_strs
'str' instances are not allowed as a Sequence value [type=sequence_str, input_value='abc', input_type=str]
"""
try
Model
sequence_of_bytes
'abc'
except
ValidationError
print
"""
1 validation error for Model
sequence_of_bytes
'bytes' instances are not allowed as a Sequence value [type=sequence_str, input_value=b'abc', input_type=bytes]
"""
from
collections.abc
import
Sequence
from
pydantic
import
BaseModel
ValidationError
class
Model
BaseModel
sequence_of_strs
Sequence
str
None
None
sequence_of_bytes
Sequence
bytes
None
None
print
Model
sequence_of_strs
'a'
'bc'
sequence_of_strs
#> ['a', 'bc']
print
Model
sequence_of_strs
'a'
'bc'
sequence_of_strs
#> ('a', 'bc')
print
Model
sequence_of_bytes
'a'
'bc'
sequence_of_bytes
#> [b'a', b'bc']
print
Model
sequence_of_bytes
'a'
'bc'
sequence_of_bytes
#> (b'a', b'bc')
try
Model
sequence_of_strs
'abc'
except
ValidationError
print
"""
1 validation error for Model
sequence_of_strs
'str' instances are not allowed as a Sequence value [type=sequence_str, input_value='abc', input_type=str]
"""
try
Model
sequence_of_bytes
'abc'
except
ValidationError
print
"""
1 validation error for Model
sequence_of_bytes
'bytes' instances are not allowed as a Sequence value [type=sequence_str, input_value=b'abc', input_type=bytes]
"""
Bytes
bytes
are accepted as-is.
bytearray
is converted using
bytes(v)
str
are converted using
v.encode()
int
float
, and
Decimal
are coerced using
str(v).encode()
. See
ByteSize
for more details.
typing.Literal
Pydantic supports the use of
typing.Literal
as a lightweight way to specify that a field may accept only specific literal values:
from
typing
import
Literal
from
pydantic
import
BaseModel
ValidationError
class
Pie
BaseModel
flavor
Literal
'apple'
'pumpkin'
Pie
flavor
'apple'
Pie
flavor
'pumpkin'
try
Pie
flavor
'cherry'
except
ValidationError
print
str
"""
1 validation error for Pie
flavor
Input should be 'apple' or 'pumpkin' [type=literal_error, input_value='cherry', input_type=str]
"""
One benefit of this field type is that it can be used to check for equality with one or more specific values
without needing to declare custom validators:
Python 3.9 and above
Python 3.10 and above
from
typing
import
ClassVar
Literal
Union
from
pydantic
import
BaseModel
ValidationError
class
Cake
BaseModel
kind
Literal
'cake'
required_utensils
ClassVar
list
str
'fork'
'knife'
class
IceCream
BaseModel
kind
Literal
'icecream'
required_utensils
ClassVar
list
str
'spoon'
class
Meal
BaseModel
dessert
Union
Cake
IceCream
print
type
Meal
dessert
'kind'
'cake'
dessert
__name__
#> Cake
print
type
Meal
dessert
'kind'
'icecream'
dessert
__name__
#> IceCream
try
Meal
dessert
'kind'
'pie'
except
ValidationError
print
str
"""
2 validation errors for Meal
dessert.Cake.kind
Input should be 'cake' [type=literal_error, input_value='pie', input_type=str]
dessert.IceCream.kind
Input should be 'icecream' [type=literal_error, input_value='pie', input_type=str]
"""
from
typing
import
ClassVar
Literal
from
pydantic
import
BaseModel
ValidationError
class
Cake
BaseModel
kind
Literal
'cake'
required_utensils
ClassVar
list
str
'fork'
'knife'
class
IceCream
BaseModel
kind
Literal
'icecream'
required_utensils
ClassVar
list
str
'spoon'
class
Meal
BaseModel
dessert
Cake
IceCream
print
type
Meal
dessert
'kind'
'cake'
dessert
__name__
#> Cake
print
type
Meal
dessert
'kind'
'icecream'
dessert
__name__
#> IceCream
try
Meal
dessert
'kind'
'pie'
except
ValidationError
print
str
"""
2 validation errors for Meal
dessert.Cake.kind
Input should be 'cake' [type=literal_error, input_value='pie', input_type=str]
dessert.IceCream.kind
Input should be 'icecream' [type=literal_error, input_value='pie', input_type=str]
"""
With proper ordering in an annotated
Union
, you can use this to parse types of decreasing specificity:
Python 3.9 and above
Python 3.10 and above
from
typing
import
Literal
Optional
Union
from
pydantic
import
BaseModel
class
Dessert
BaseModel
kind
str
class
Pie
Dessert
kind
Literal
'pie'
flavor
Optional
str
class
ApplePie
Pie
flavor
Literal
'apple'
class
PumpkinPie
Pie
flavor
Literal
'pumpkin'
class
Meal
BaseModel
dessert
Union
ApplePie
PumpkinPie
Pie
Dessert
print
type
Meal
dessert
'kind'
'pie'
'flavor'
'apple'
dessert
__name__
#> ApplePie
print
type
Meal
dessert
'kind'
'pie'
'flavor'
'pumpkin'
dessert
__name__
#> PumpkinPie
print
type
Meal
dessert
'kind'
'pie'
dessert
__name__
#> Dessert
print
type
Meal
dessert
'kind'
'cake'
dessert
__name__
#> Dessert
from
typing
import
Literal
from
pydantic
import
BaseModel
class
Dessert
BaseModel
kind
str
class
Pie
Dessert
kind
Literal
'pie'
flavor
str
None
class
ApplePie
Pie
flavor
Literal
'apple'
class
PumpkinPie
Pie
flavor
Literal
'pumpkin'
class
Meal
BaseModel
dessert
ApplePie
PumpkinPie
Pie
Dessert
print
type
Meal
dessert
'kind'
'pie'
'flavor'
'apple'
dessert
__name__
#> ApplePie
print
type
Meal
dessert
'kind'
'pie'
'flavor'
'pumpkin'
dessert
__name__
#> PumpkinPie
print
type
Meal
dessert
'kind'
'pie'
dessert
__name__
#> Dessert
print
type
Meal
dessert
'kind'
'cake'
dessert
__name__
#> Dessert
typing.Any
Allows any value, including
None
typing.Hashable
From Python, supports any data that passes an
isinstance(v, Hashable)
check.
From JSON, first loads the data via an
Any
validator, then checks if the data is hashable with
isinstance(v, Hashable)
typing.Annotated
Allows wrapping another type with arbitrary metadata, as per
PEP-593
. The
Annotated
hint may contain a single call to the
Field
function
, but otherwise the additional metadata is ignored and the root type is used.
typing.Pattern
Will cause the input value to be passed to
re.compile(v)
to create a regular expression pattern.
pathlib.Path
Simply uses the type itself for validation by passing the value to
Path(v)
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 020_pydantic_extra_types_semantic_version.txt ---
Semantic Version
SemanticVersion definition that is based on the Semantiv Versioning Specification
semver
SemanticVersion
Semantic version based on the official
semver thread
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 006_dataclasses.txt ---
Pydantic Dataclasses
Provide an enhanced dataclass that performs validation.
dataclass
dataclass
init
Literal
False
False
repr
bool
True
bool
True
order
bool
False
unsafe_hash
bool
False
frozen
bool
False
config
ConfigDict
type
object
None
None
validate_on_init
bool
None
None
kw_only
bool
...
slots
bool
...
Callable
type
]],
type
PydanticDataclass
dataclass
_cls
type
init
Literal
False
False
repr
bool
True
bool
True
order
bool
False
unsafe_hash
bool
False
frozen
bool
None
None
config
ConfigDict
type
object
None
None
validate_on_init
bool
None
None
kw_only
bool
...
slots
bool
...
type
PydanticDataclass
dataclass
init
Literal
False
False
repr
bool
True
bool
True
order
bool
False
unsafe_hash
bool
False
frozen
bool
None
None
config
ConfigDict
type
object
None
None
validate_on_init
bool
None
None
Callable
type
]],
type
PydanticDataclass
dataclass
_cls
type
init
Literal
False
False
repr
bool
True
bool
True
order
bool
False
unsafe_hash
bool
False
frozen
bool
None
None
config
ConfigDict
type
object
None
None
validate_on_init
bool
None
None
type
PydanticDataclass
dataclass
_cls
type
None
None
init
Literal
False
False
repr
bool
True
bool
True
order
bool
False
unsafe_hash
bool
False
frozen
bool
None
None
config
ConfigDict
type
object
None
None
validate_on_init
bool
None
None
kw_only
bool
False
slots
bool
False
Callable
type
]],
type
PydanticDataclass
type
PydanticDataclass
Usage Documentation
dataclasses
A decorator used to create a Pydantic-enhanced dataclass, similar to the standard Python
dataclass
but with added validation.
This function should be used similarly to
dataclasses.dataclass
Parameters:
Name
Type
Description
Default
_cls
type
] | None
The target
dataclass
None
init
Literal
[False]
Included for signature compatibility with
dataclasses.dataclass
, and is passed through to
dataclasses.dataclass
when appropriate. If specified, must be set to
False
, as pydantic inserts its
own
__init__
function.
False
repr
bool
A boolean indicating whether to include the field in the
__repr__
output.
True
bool
Determines if a
__eq__
method should be generated for the class.
True
order
bool
Determines if comparison magic methods should be generated, such as
__lt__
, but not
__eq__
False
unsafe_hash
bool
Determines if a
__hash__
method should be included in the class, as in
dataclasses.dataclass
False
frozen
bool
| None
Determines if the generated class should be a 'frozen'
dataclass
, which does not allow its
attributes to be modified after it has been initialized. If not set, the value from the provided
config
argument will be used (and will default to
False
otherwise).
None
config
ConfigDict
type
object
] | None
The Pydantic config to use for the
dataclass
None
validate_on_init
bool
| None
A deprecated parameter included for backwards compatibility; in V2, all Pydantic dataclasses
are validated on init.
None
kw_only
bool
Determines if
__init__
method parameters must be specified by keyword only. Defaults to
False
False
slots
bool
Determines if the generated class should be a 'slots'
dataclass
, which does not allow the addition of
new attributes after instantiation.
False
Returns:
Type
Description
Callable
type
]],
type
PydanticDataclass
]] |
type
PydanticDataclass
A decorator that accepts a class as its argument and returns a Pydantic
dataclass
Raises:
Type
Description
AssertionError
Raised if
init
is not
False
validate_on_init
False
Source code in
pydantic/dataclasses.py
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
@dataclass_transform
field_specifiers
dataclasses
field
Field
PrivateAttr
def
dataclass
_cls
type
None
None
init
Literal
False
False
repr
bool
True
bool
True
order
bool
False
unsafe_hash
bool
False
frozen
bool
None
None
config
ConfigDict
type
object
None
None
validate_on_init
bool
None
None
kw_only
bool
False
slots
bool
False
Callable
type
]],
type
PydanticDataclass
type
PydanticDataclass
"""!!! abstract "Usage Documentation"
[`dataclasses`](../concepts/dataclasses.md)
A decorator used to create a Pydantic-enhanced dataclass, similar to the standard Python `dataclass`,
but with added validation.
This function should be used similarly to `dataclasses.dataclass`.
Args:
_cls: The target `dataclass`.
init: Included for signature compatibility with `dataclasses.dataclass`, and is passed through to
`dataclasses.dataclass` when appropriate. If specified, must be set to `False`, as pydantic inserts its
own `__init__` function.
repr: A boolean indicating whether to include the field in the `__repr__` output.
eq: Determines if a `__eq__` method should be generated for the class.
order: Determines if comparison magic methods should be generated, such as `__lt__`, but not `__eq__`.
unsafe_hash: Determines if a `__hash__` method should be included in the class, as in `dataclasses.dataclass`.
frozen: Determines if the generated class should be a 'frozen' `dataclass`, which does not allow its
attributes to be modified after it has been initialized. If not set, the value from the provided `config` argument will be used (and will default to `False` otherwise).
config: The Pydantic config to use for the `dataclass`.
validate_on_init: A deprecated parameter included for backwards compatibility; in V2, all Pydantic dataclasses
are validated on init.
kw_only: Determines if `__init__` method parameters must be specified by keyword only. Defaults to `False`.
slots: Determines if the generated class should be a 'slots' `dataclass`, which does not allow the addition of
new attributes after instantiation.
Returns:
A decorator that accepts a class as its argument and returns a Pydantic `dataclass`.
Raises:
AssertionError: Raised if `init` is not `False` or `validate_on_init` is `False`.
"""
assert
init
False
'pydantic.dataclasses.dataclass only supports init=False'
assert
validate_on_init
not
False
'validate_on_init=False is no longer supported'
sys
version_info
kwargs
'kw_only'
kw_only
'slots'
slots
else
kwargs
def
make_pydantic_fields_compatible
cls
type
Any
None
"""Make sure that stdlib `dataclasses` understands `Field` kwargs like `kw_only`
To do that, we simply change
`x: int = pydantic.Field(..., kw_only=True)`
into
`x: int = dataclasses.field(default=pydantic.Field(..., kw_only=True), kw_only=True)`
"""
for
annotation_cls
cls
__mro__
annotations
dict
str
Any
getattr
annotation_cls
'__annotations__'
{})
for
field_name
annotations
field_value
getattr
cls
field_name
None
# Process only if this is an instance of `FieldInfo`.
not
isinstance
field_value
FieldInfo
continue
# Initialize arguments for the standard `dataclasses.field`.
field_args
dict
'default'
field_value
# Handle `kw_only` for Python 3.10+
sys
version_info
and
field_value
kw_only
field_args
'kw_only'
True
# Set `repr` attribute if it's explicitly specified to be not `True`.
field_value
repr
not
True
field_args
'repr'
field_value
repr
setattr
cls
field_name
dataclasses
field
field_args
# In Python 3.9, when subclassing, information is pulled from cls.__dict__['__annotations__']
# for annotations, so we must make sure it's initialized before we add to it.
cls
__dict__
get
'__annotations__'
None
cls
__annotations__
cls
__annotations__
field_name
annotations
field_name
def
create_dataclass
cls
type
Any
type
PydanticDataclass
"""Create a Pydantic dataclass from a regular dataclass.
Args:
cls: The class to create the Pydantic dataclass from.
Returns:
A Pydantic dataclass.
"""
from
._internal._utils
import
is_model_class
is_model_class
cls
raise
PydanticUserError
'Cannot create a Pydantic dataclass from
cls
__name__
as it is already a Pydantic model'
code
'dataclass-on-model'
original_cls
cls
# we warn on conflicting config specifications, but only if the class doesn't have a dataclass base
# because a dataclass base might provide a __pydantic_config__ attribute that we don't want to warn about
has_dataclass_base
any
dataclasses
is_dataclass
base
for
base
cls
__bases__
not
has_dataclass_base
and
config
not
None
and
hasattr
cls
'__pydantic_config__'
warn
'`config` is set via both the `dataclass` decorator and `__pydantic_config__` for dataclass
cls
__name__
. '
'The `config` specification from `dataclass` decorator will take priority.'
category
UserWarning
stacklevel
# if config is not explicitly provided, try to read it from the type
config_dict
config
config
not
None
else
getattr
cls
'__pydantic_config__'
None
config_wrapper
_config
ConfigWrapper
config_dict
decorators
_decorators
DecoratorInfos
build
cls
# Keep track of the original __doc__ so that we can restore it after applying the dataclasses decorator
# Otherwise, classes with no __doc__ will have their signature added into the JSON schema description,
# since dataclasses.dataclass will set this as the __doc__
original_doc
cls
__doc__
_pydantic_dataclasses
is_builtin_dataclass
cls
# Don't preserve the docstring for vanilla dataclasses, as it may include the signature
# This matches v1 behavior, and there was an explicit test for it
original_doc
None
# We don't want to add validation to the existing std lib dataclass, so we will subclass it
# If the class is generic, we need to make sure the subclass also inherits from Generic
# with all the same parameters.
bases
cls
issubclass
cls
Generic
generic_base
Generic
cls
__parameters__
# type: ignore
bases
bases
generic_base
cls
types
new_class
cls
__name__
bases
make_pydantic_fields_compatible
cls
# Respect frozen setting from dataclass constructor and fallback to config setting if not provided
frozen
not
None
frozen_
frozen
config_wrapper
frozen
# It's not recommended to define both, as the setting from the dataclass decorator will take priority.
warn
'`frozen` is set via both the `dataclass` decorator and `config` for dataclass
cls
__name__
!r}
'This is not recommended. The `frozen` specification on `dataclass` will take priority.'
category
UserWarning
stacklevel
else
frozen_
config_wrapper
frozen
False
cls
dataclasses
dataclass
# type: ignore[call-overload]
cls
# the value of init here doesn't affect anything except that it makes it easier to generate a signature
init
True
repr
repr
order
order
unsafe_hash
unsafe_hash
frozen
frozen_
kwargs
# This is an undocumented attribute to distinguish stdlib/Pydantic dataclasses.
# It should be set as early as possible:
cls
__is_pydantic_dataclass__
True
cls
__pydantic_decorators__
decorators
# type: ignore
cls
__doc__
original_doc
cls
__module__
original_cls
__module__
cls
__qualname__
original_cls
__qualname__
cls
__pydantic_fields_complete__
classmethod
_pydantic_fields_complete
cls
__pydantic_complete__
False
# `complete_dataclass` will set it to `True` if successful.
# TODO `parent_namespace` is currently None, but we could do the same thing as Pydantic models:
# fetch the parent ns using `parent_frame_namespace` (if the dataclass was defined in a function),
# and possibly cache it (see the `__pydantic_parent_namespace__` logic for models).
_pydantic_dataclasses
complete_dataclass
cls
config_wrapper
raise_errors
False
return
cls
return
create_dataclass
_cls
None
else
create_dataclass
_cls
rebuild_dataclass
rebuild_dataclass
cls
type
PydanticDataclass
force
bool
False
raise_errors
bool
True
_parent_namespace_depth
int
_types_namespace
MappingNamespace
None
None
bool
None
Try to rebuild the pydantic-core schema for the dataclass.
This may be necessary when one of the annotations is a ForwardRef which could not be resolved during
the initial attempt to build the schema, and automatic rebuilding fails.
This is analogous to
BaseModel.model_rebuild
Parameters:
Name
Type
Description
Default
cls
type
PydanticDataclass
The class to rebuild the pydantic-core schema for.
required
force
bool
Whether to force the rebuilding of the schema, defaults to
False
False
raise_errors
bool
Whether to raise errors, defaults to
True
True
_parent_namespace_depth
int
The depth level of the parent namespace, defaults to 2.
_types_namespace
MappingNamespace
| None
The types namespace, defaults to
None
None
Returns:
Type
Description
bool
| None
Returns
None
if the schema is already "complete" and rebuilding was not required.
bool
| None
If rebuilding
was
required, returns
True
if rebuilding was successful, otherwise
False
Source code in
pydantic/dataclasses.py
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
def
rebuild_dataclass
cls
type
PydanticDataclass
force
bool
False
raise_errors
bool
True
_parent_namespace_depth
int
_types_namespace
MappingNamespace
None
None
bool
None
"""Try to rebuild the pydantic-core schema for the dataclass.
This may be necessary when one of the annotations is a ForwardRef which could not be resolved during
the initial attempt to build the schema, and automatic rebuilding fails.
This is analogous to `BaseModel.model_rebuild`.
Args:
cls: The class to rebuild the pydantic-core schema for.
force: Whether to force the rebuilding of the schema, defaults to `False`.
raise_errors: Whether to raise errors, defaults to `True`.
_parent_namespace_depth: The depth level of the parent namespace, defaults to 2.
_types_namespace: The types namespace, defaults to `None`.
Returns:
Returns `None` if the schema is already "complete" and rebuilding was not required.
If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.
"""
not
force
and
cls
__pydantic_complete__
return
None
for
attr
'__pydantic_core_schema__'
'__pydantic_validator__'
'__pydantic_serializer__'
attr
cls
__dict__
# Deleting the validator/serializer is necessary as otherwise they can get reused in
# pydantic-core. Same applies for the core schema that can be reused in schema generation.
delattr
cls
attr
cls
__pydantic_complete__
False
_types_namespace
not
None
rebuild_ns
_types_namespace
elif
_parent_namespace_depth
rebuild_ns
_typing_extra
parent_frame_namespace
parent_depth
_parent_namespace_depth
force
True
else
rebuild_ns
ns_resolver
_namespace_utils
NsResolver
parent_namespace
rebuild_ns
return
_pydantic_dataclasses
complete_dataclass
cls
_config
ConfigWrapper
cls
__pydantic_config__
check
False
raise_errors
raise_errors
ns_resolver
ns_resolver
# We could provide a different config instead (with `'defer_build'` set to `True`)
# of this explicit `_force_build` argument, but because config can come from the
# decorator parameter or the `__pydantic_config__` attribute, `complete_dataclass`
# will overwrite `__pydantic_config__` with the provided config above:
_force_build
True
is_pydantic_dataclass
is_pydantic_dataclass
class_
type
Any
TypeGuard
type
PydanticDataclass
Whether a class is a pydantic dataclass.
Parameters:
Name
Type
Description
Default
class_
type
Any
The class.
required
Returns:
Type
Description
TypeGuard
type
PydanticDataclass
True
if the class is a pydantic dataclass,
False
otherwise.
Source code in
pydantic/dataclasses.py
371
372
373
374
375
376
377
378
379
380
381
382
383
def
is_pydantic_dataclass
class_
type
Any
TypeGuard
type
PydanticDataclass
]]:
"""Whether a class is a pydantic dataclass.
Args:
class_: The class.
Returns:
`True` if the class is a pydantic dataclass, `False` otherwise.
"""
try
return
'__is_pydantic_dataclass__'
class_
__dict__
and
dataclasses
is_dataclass
class_
except
AttributeError
return
False
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 026_functional_validators.txt ---
Functional Validators
This module contains related classes and functions for validation.
ModelAfterValidatorWithoutInfo
module-attribute
ModelAfterValidatorWithoutInfo
Callable
_ModelType
_ModelType
@model_validator
decorated function signature. This is used when
mode='after'
and the function does not
have info argument.
ModelAfterValidator
module-attribute
ModelAfterValidator
Callable
_ModelType
ValidationInfo
_ModelType
@model_validator
decorated function signature. This is used when
mode='after'
AfterValidator
dataclass
AfterValidator
func
NoInfoValidatorFunction
WithInfoValidatorFunction
Usage Documentation
field
after
validators
A metadata class that indicates that a validation should be applied
after
the inner validation logic.
Attributes:
Name
Type
Description
func
NoInfoValidatorFunction
WithInfoValidatorFunction
The validator function.
Example
from
typing
import
Annotated
from
pydantic
import
AfterValidator
BaseModel
ValidationError
MyInt
Annotated
int
AfterValidator
lambda
class
Model
BaseModel
MyInt
print
Model
#> 2
try
Model
'a'
except
ValidationError
print
json
indent
'''
"type": "int_parsing",
"loc": [
"a"
"msg": "Input should be a valid integer, unable to parse string as an integer",
"input": "a",
"url": "https://errors.pydantic.dev/2/v/int_parsing"
'''
BeforeValidator
dataclass
BeforeValidator
func
NoInfoValidatorFunction
WithInfoValidatorFunction
json_schema_input_type
Any
PydanticUndefined
Usage Documentation
field
before
validators
A metadata class that indicates that a validation should be applied
before
the inner validation logic.
Attributes:
Name
Type
Description
func
NoInfoValidatorFunction
WithInfoValidatorFunction
The validator function.
json_schema_input_type
Any
The input type of the function. This is only used to generate the appropriate
JSON Schema (in validation mode).
Example
from
typing
import
Annotated
from
pydantic
import
BaseModel
BeforeValidator
MyInt
Annotated
int
BeforeValidator
lambda
class
Model
BaseModel
MyInt
print
Model
#> 2
try
Model
'a'
except
TypeError
print
#> can only concatenate str (not "int") to str
PlainValidator
dataclass
PlainValidator
func
NoInfoValidatorFunction
WithInfoValidatorFunction
json_schema_input_type
Any
Any
Usage Documentation
field
plain
validators
A metadata class that indicates that a validation should be applied
instead
of the inner validation logic.
Note
Before v2.9,
PlainValidator
wasn't always compatible with JSON Schema generation for
mode='validation'
You can now use the
json_schema_input_type
argument to specify the input type of the function
to be used in the JSON schema when
mode='validation'
(the default). See the example below for more details.
Attributes:
Name
Type
Description
func
NoInfoValidatorFunction
WithInfoValidatorFunction
The validator function.
json_schema_input_type
Any
The input type of the function. This is only used to generate the appropriate
JSON Schema (in validation mode). If not provided, will default to
Any
Example
from
typing
import
Annotated
Union
from
pydantic
import
BaseModel
PlainValidator
MyInt
Annotated
int
PlainValidator
lambda
int
json_schema_input_type
Union
str
int
class
Model
BaseModel
MyInt
print
Model
'1'
#> 2
print
Model
#> 2
WrapValidator
dataclass
WrapValidator
func
NoInfoWrapValidatorFunction
WithInfoWrapValidatorFunction
json_schema_input_type
Any
PydanticUndefined
Usage Documentation
field
wrap
validators
A metadata class that indicates that a validation should be applied
around
the inner validation logic.
Attributes:
Name
Type
Description
func
NoInfoWrapValidatorFunction
WithInfoWrapValidatorFunction
The validator function.
json_schema_input_type
Any
The input type of the function. This is only used to generate the appropriate
JSON Schema (in validation mode).
from
datetime
import
datetime
from
typing
import
Annotated
from
pydantic
import
BaseModel
ValidationError
WrapValidator
def
validate_timestamp
handler
'now'
# we don't want to bother with further validation, just return the new value
return
datetime
now
try
return
handler
except
ValidationError
# validation failed, in this case we want to return a default value
return
datetime
2000
MyTimestamp
Annotated
datetime
WrapValidator
validate_timestamp
class
Model
BaseModel
MyTimestamp
print
Model
'now'
#> 2032-01-02 03:04:05.000006
print
Model
'invalid'
#> 2000-01-01 00:00:00
ModelWrapValidatorHandler
Bases:
ValidatorFunctionWrapHandler
Protocol
_ModelTypeCo
@model_validator
decorated function handler argument type. This is used when
mode='wrap'
ModelWrapValidatorWithoutInfo
Bases:
Protocol
_ModelType
@model_validator
decorated function signature.
This is used when
mode='wrap'
and the function does not have info argument.
ModelWrapValidator
Bases:
Protocol
_ModelType
@model_validator
decorated function signature. This is used when
mode='wrap'
FreeModelBeforeValidatorWithoutInfo
Bases:
Protocol
@model_validator
decorated function signature.
This is used when
mode='before'
and the function does not have info argument.
ModelBeforeValidatorWithoutInfo
Bases:
Protocol
@model_validator
decorated function signature.
This is used when
mode='before'
and the function does not have info argument.
FreeModelBeforeValidator
Bases:
Protocol
@model_validator
decorated function signature. This is used when
mode='before'
ModelBeforeValidator
Bases:
Protocol
@model_validator
decorated function signature. This is used when
mode='before'
InstanceOf
dataclass
InstanceOf
Generic type for annotating a type that is an instance of a given class.
Example
from
pydantic
import
BaseModel
InstanceOf
class
Foo
...
class
Bar
BaseModel
foo
InstanceOf
Foo
Bar
foo
Foo
())
try
Bar
foo
except
ValidationError
print
"""
â {
â â 'type': 'is_instance_of',
â â 'loc': ('foo',),
â â 'msg': 'Input should be an instance of Foo',
â â 'input': 42,
â â 'ctx': {'class': 'Foo'},
â â 'url': 'https://errors.pydantic.dev/0.38.0/v/is_instance_of'
â }
"""
SkipValidation
dataclass
SkipValidation
If this is applied as an annotation (e.g., via
x: Annotated[int, SkipValidation]
), validation will be
skipped. You can also use
SkipValidation[int]
as a shorthand for
Annotated[int, SkipValidation]
This can be useful if you want to use a type annotation for documentation/IDE/type-checking purposes,
and know that it is safe to skip validation for one or more of the fields.
Because this converts the validation schema to
any_schema
, subsequent annotation-applied transformations
may not have the expected effects. Therefore, when used, this annotation should generally be the final
annotation applied to a type.
field_validator
field_validator
field
str
fields
str
mode
Literal
"wrap"
check_fields
bool
None
...
json_schema_input_type
Any
...
Callable
_V2WrapValidatorType
_V2WrapValidatorType
field_validator
field
str
fields
str
mode
Literal
"before"
"plain"
check_fields
bool
None
...
json_schema_input_type
Any
...
Callable
_V2BeforeAfterOrPlainValidatorType
_V2BeforeAfterOrPlainValidatorType
field_validator
field
str
fields
str
mode
Literal
"after"
...
check_fields
bool
None
...
Callable
_V2BeforeAfterOrPlainValidatorType
_V2BeforeAfterOrPlainValidatorType
field_validator
field
str
fields
str
mode
FieldValidatorModes
"after"
check_fields
bool
None
None
json_schema_input_type
Any
PydanticUndefined
Callable
Any
Any
Usage Documentation
field validators
Decorate methods on the class indicating that they should be used to validate fields.
Example usage:
from
typing
import
Any
from
pydantic
import
BaseModel
ValidationError
field_validator
class
Model
BaseModel
str
@field_validator
'a'
@classmethod
def
ensure_foobar
cls
Any
'foobar'
not
raise
ValueError
'"foobar" not found in a'
return
print
repr
Model
'this is foobar good'
)))
#> Model(a='this is foobar good')
try
Model
'snap'
except
ValidationError
exc_info
print
exc_info
'''
1 validation error for Model
Value error, "foobar" not found in a [type=value_error, input_value='snap', input_type=str]
'''
For more in depth examples, see
Field Validators
Parameters:
Name
Type
Description
Default
field
str
The first field the
field_validator
should be called on; this is separate
from
fields
to ensure an error is raised if you don't pass at least one.
required
*fields
str
Additional field(s) the
field_validator
should be called on.
mode
FieldValidatorModes
Specifies whether to validate the fields before or after validation.
'after'
check_fields
bool
| None
Whether to check that the fields actually exist on the model.
None
json_schema_input_type
Any
The input type of the function. This is only used to generate
the appropriate JSON Schema (in validation mode) and can only specified
when
mode
is either
'before'
'plain'
'wrap'
PydanticUndefined
Returns:
Type
Description
Callable
Any
Any
A decorator that can be used to decorate a function to be used as a field_validator.
Raises:
Type
Description
PydanticUserError
@field_validator
is used bare (with no fields).
If the args passed to
@field_validator
as fields are not strings.
@field_validator
applied to instance methods.
Source code in
pydantic/functional_validators.py
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
def
field_validator
field
str
fields
str
mode
FieldValidatorModes
'after'
check_fields
bool
None
None
json_schema_input_type
Any
PydanticUndefined
Callable
Any
Any
"""!!! abstract "Usage Documentation"
[field validators](../concepts/validators.md#field-validators)
Decorate methods on the class indicating that they should be used to validate fields.
Example usage:
```python
from typing import Any
from pydantic import (
BaseModel,
ValidationError,
field_validator,
class Model(BaseModel):
a: str
@field_validator('a')
@classmethod
def ensure_foobar(cls, v: Any):
if 'foobar' not in v:
raise ValueError('"foobar" not found in a')
return v
print(repr(Model(a='this is foobar good')))
#> Model(a='this is foobar good')
try:
Model(a='snap')
except ValidationError as exc_info:
print(exc_info)
'''
1 validation error for Model
Value error, "foobar" not found in a [type=value_error, input_value='snap', input_type=str]
'''
```
For more in depth examples, see [Field Validators](../concepts/validators.md#field-validators).
Args:
field: The first field the `field_validator` should be called on; this is separate
from `fields` to ensure an error is raised if you don't pass at least one.
*fields: Additional field(s) the `field_validator` should be called on.
mode: Specifies whether to validate the fields before or after validation.
check_fields: Whether to check that the fields actually exist on the model.
json_schema_input_type: The input type of the function. This is only used to generate
the appropriate JSON Schema (in validation mode) and can only specified
when `mode` is either `'before'`, `'plain'` or `'wrap'`.
Returns:
A decorator that can be used to decorate a function to be used as a field_validator.
Raises:
PydanticUserError:
- If `@field_validator` is used bare (with no fields).
- If the args passed to `@field_validator` as fields are not strings.
- If `@field_validator` applied to instance methods.
"""
isinstance
field
FunctionType
raise
PydanticUserError
'`@field_validator` should be used with fields and keyword arguments, not bare. '
"E.g. usage should be `@validator('<field_name>', ...)`"
code
'validator-no-fields'
mode
not
'before'
'plain'
'wrap'
and
json_schema_input_type
not
PydanticUndefined
raise
PydanticUserError
"`json_schema_input_type` can't be used when mode is set to
mode
!r}
code
'validator-input-type'
json_schema_input_type
PydanticUndefined
and
mode
'plain'
json_schema_input_type
Any
fields
field
fields
not
all
isinstance
field
str
for
field
fields
raise
PydanticUserError
'`@field_validator` fields should be passed as separate string args. '
"E.g. usage should be `@validator('<field_name_1>', '<field_name_2>', ...)`"
code
'validator-invalid-fields'
def
dec
Callable
...
Any
staticmethod
Any
Any
classmethod
Any
Any
Any
_decorators
PydanticDescriptorProxy
Any
_decorators
is_instance_method_from_sig
raise
PydanticUserError
'`@field_validator` cannot be applied to instance methods'
code
'validator-instance-method'
# auto apply the @classmethod decorator
_decorators
ensure_classmethod_based_on_signature
dec_info
_decorators
FieldValidatorDecoratorInfo
fields
fields
mode
mode
check_fields
check_fields
json_schema_input_type
json_schema_input_type
return
_decorators
PydanticDescriptorProxy
dec_info
return
dec
model_validator
model_validator
mode
Literal
"wrap"
Callable
_AnyModelWrapValidator
_ModelType
]],
PydanticDescriptorProxy
ModelValidatorDecoratorInfo
model_validator
mode
Literal
"before"
Callable
_AnyModelBeforeValidator
PydanticDescriptorProxy
ModelValidatorDecoratorInfo
model_validator
mode
Literal
"after"
Callable
_AnyModelAfterValidator
_ModelType
]],
PydanticDescriptorProxy
ModelValidatorDecoratorInfo
model_validator
mode
Literal
"wrap"
"before"
"after"
Any
Usage Documentation
Model Validators
Decorate model methods for validation purposes.
Example usage:
from
typing_extensions
import
Self
from
pydantic
import
BaseModel
ValidationError
model_validator
class
Square
BaseModel
width
float
height
float
@model_validator
mode
'after'
def
verify_square
self
Self
self
width
self
height
raise
ValueError
'width and height do not match'
return
self
Square
width
height
print
repr
#> Square(width=1.0, height=1.0)
try
Square
width
height
except
ValidationError
print
'''
1 validation error for Square
Value error, width and height do not match [type=value_error, input_value={'width': 1, 'height': 2}, input_type=dict]
'''
For more in depth examples, see
Model Validators
Parameters:
Name
Type
Description
Default
mode
Literal
['wrap', 'before', 'after']
A required string literal that specifies the validation mode.
It can be one of the following: 'wrap', 'before', or 'after'.
required
Returns:
Type
Description
Any
A decorator that can be used to decorate a function to be used as a model validator.
Source code in
pydantic/functional_validators.py
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
def
model_validator
mode
Literal
'wrap'
'before'
'after'
Any
"""!!! abstract "Usage Documentation"
[Model Validators](../concepts/validators.md#model-validators)
Decorate model methods for validation purposes.
Example usage:
```python
from typing_extensions import Self
from pydantic import BaseModel, ValidationError, model_validator
class Square(BaseModel):
width: float
height: float
@model_validator(mode='after')
def verify_square(self) -> Self:
if self.width != self.height:
raise ValueError('width and height do not match')
return self
s = Square(width=1, height=1)
print(repr(s))
#> Square(width=1.0, height=1.0)
try:
Square(width=1, height=2)
except ValidationError as e:
print(e)
'''
1 validation error for Square
Value error, width and height do not match [type=value_error, input_value={'width': 1, 'height': 2}, input_type=dict]
'''
```
For more in depth examples, see [Model Validators](../concepts/validators.md#model-validators).
Args:
mode: A required string literal that specifies the validation mode.
It can be one of the following: 'wrap', 'before', or 'after'.
Returns:
A decorator that can be used to decorate a function to be used as a model validator.
"""
def
dec
Any
_decorators
PydanticDescriptorProxy
Any
# auto apply the @classmethod decorator
_decorators
ensure_classmethod_based_on_signature
dec_info
_decorators
ModelValidatorDecoratorInfo
mode
mode
return
_decorators
PydanticDescriptorProxy
dec_info
return
dec
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 021_fields_pydantic_fields_FieldInfo.txt ---
Fields
Defining fields on models.
Field
Field
default
ellipsis
alias
str
None
_Unset
alias_priority
int
None
_Unset
validation_alias
str
AliasPath
AliasChoices
None
_Unset
serialization_alias
str
None
_Unset
title
str
None
_Unset
field_title_generator
Callable
str
FieldInfo
str
None
_Unset
description
str
None
_Unset
examples
list
Any
None
_Unset
exclude
bool
None
_Unset
discriminator
str
Discriminator
None
_Unset
deprecated
Deprecated
str
bool
None
_Unset
json_schema_extra
JsonDict
Callable
JsonDict
None
None
_Unset
frozen
bool
None
_Unset
validate_default
bool
None
_Unset
repr
bool
_Unset
init
bool
None
_Unset
init_var
bool
None
_Unset
kw_only
bool
None
_Unset
pattern
str
Pattern
str
None
_Unset
strict
bool
None
_Unset
coerce_numbers_to_str
bool
None
_Unset
SupportsGt
None
_Unset
SupportsGe
None
_Unset
SupportsLt
None
_Unset
SupportsLe
None
_Unset
multiple_of
float
None
_Unset
allow_inf_nan
bool
None
_Unset
max_digits
int
None
_Unset
decimal_places
int
None
_Unset
min_length
int
None
_Unset
max_length
int
None
_Unset
union_mode
Literal
"smart"
"left_to_right"
_Unset
fail_fast
bool
None
_Unset
extra
Unpack
_EmptyKwargs
Any
Field
default
alias
str
None
_Unset
alias_priority
int
None
_Unset
validation_alias
str
AliasPath
AliasChoices
None
_Unset
serialization_alias
str
None
_Unset
title
str
None
_Unset
field_title_generator
Callable
str
FieldInfo
str
None
_Unset
description
str
None
_Unset
examples
list
Any
None
_Unset
exclude
bool
None
_Unset
discriminator
str
Discriminator
None
_Unset
deprecated
Deprecated
str
bool
None
_Unset
json_schema_extra
JsonDict
Callable
JsonDict
None
None
_Unset
frozen
bool
None
_Unset
validate_default
bool
None
_Unset
repr
bool
_Unset
init
bool
None
_Unset
init_var
bool
None
_Unset
kw_only
bool
None
_Unset
pattern
str
Pattern
str
None
_Unset
strict
bool
None
_Unset
coerce_numbers_to_str
bool
None
_Unset
SupportsGt
None
_Unset
SupportsGe
None
_Unset
SupportsLt
None
_Unset
SupportsLe
None
_Unset
multiple_of
float
None
_Unset
allow_inf_nan
bool
None
_Unset
max_digits
int
None
_Unset
decimal_places
int
None
_Unset
min_length
int
None
_Unset
max_length
int
None
_Unset
union_mode
Literal
"smart"
"left_to_right"
_Unset
fail_fast
bool
None
_Unset
extra
Unpack
_EmptyKwargs
Field
default_factory
Callable
[[],
Callable
dict
str
Any
]],
alias
str
None
_Unset
alias_priority
int
None
_Unset
validation_alias
str
AliasPath
AliasChoices
None
_Unset
serialization_alias
str
None
_Unset
title
str
None
_Unset
field_title_generator
Callable
str
FieldInfo
str
None
_Unset
description
str
None
_Unset
examples
list
Any
None
_Unset
exclude
bool
None
_Unset
discriminator
str
Discriminator
None
_Unset
deprecated
Deprecated
str
bool
None
_Unset
json_schema_extra
JsonDict
Callable
JsonDict
None
None
_Unset
frozen
bool
None
_Unset
validate_default
bool
None
_Unset
repr
bool
_Unset
init
bool
None
_Unset
init_var
bool
None
_Unset
kw_only
bool
None
_Unset
pattern
str
Pattern
str
None
_Unset
strict
bool
None
_Unset
coerce_numbers_to_str
bool
None
_Unset
SupportsGt
None
_Unset
SupportsGe
None
_Unset
SupportsLt
None
_Unset
SupportsLe
None
_Unset
multiple_of
float
None
_Unset
allow_inf_nan
bool
None
_Unset
max_digits
int
None
_Unset
decimal_places
int
None
_Unset
min_length
int
None
_Unset
max_length
int
None
_Unset
union_mode
Literal
"smart"
"left_to_right"
_Unset
fail_fast
bool
None
_Unset
extra
Unpack
_EmptyKwargs
Field
alias
str
None
_Unset
alias_priority
int
None
_Unset
validation_alias
str
AliasPath
AliasChoices
None
_Unset
serialization_alias
str
None
_Unset
title
str
None
_Unset
field_title_generator
Callable
str
FieldInfo
str
None
_Unset
description
str
None
_Unset
examples
list
Any
None
_Unset
exclude
bool
None
_Unset
discriminator
str
Discriminator
None
_Unset
deprecated
Deprecated
str
bool
None
_Unset
json_schema_extra
JsonDict
Callable
JsonDict
None
None
_Unset
frozen
bool
None
_Unset
validate_default
bool
None
_Unset
repr
bool
_Unset
init
bool
None
_Unset
init_var
bool
None
_Unset
kw_only
bool
None
_Unset
pattern
str
Pattern
str
None
_Unset
strict
bool
None
_Unset
coerce_numbers_to_str
bool
None
_Unset
SupportsGt
None
_Unset
SupportsGe
None
_Unset
SupportsLt
None
_Unset
SupportsLe
None
_Unset
multiple_of
float
None
_Unset
allow_inf_nan
bool
None
_Unset
max_digits
int
None
_Unset
decimal_places
int
None
_Unset
min_length
int
None
_Unset
max_length
int
None
_Unset
union_mode
Literal
"smart"
"left_to_right"
_Unset
fail_fast
bool
None
_Unset
extra
Unpack
_EmptyKwargs
Any
Field
default
Any
PydanticUndefined
default_factory
Callable
[[],
Any
Callable
dict
str
Any
]],
Any
None
_Unset
alias
str
None
_Unset
alias_priority
int
None
_Unset
validation_alias
str
AliasPath
AliasChoices
None
_Unset
serialization_alias
str
None
_Unset
title
str
None
_Unset
field_title_generator
Callable
str
FieldInfo
str
None
_Unset
description
str
None
_Unset
examples
list
Any
None
_Unset
exclude
bool
None
_Unset
discriminator
str
Discriminator
None
_Unset
deprecated
Deprecated
str
bool
None
_Unset
json_schema_extra
JsonDict
Callable
JsonDict
None
None
_Unset
frozen
bool
None
_Unset
validate_default
bool
None
_Unset
repr
bool
_Unset
init
bool
None
_Unset
init_var
bool
None
_Unset
kw_only
bool
None
_Unset
pattern
str
Pattern
str
None
_Unset
strict
bool
None
_Unset
coerce_numbers_to_str
bool
None
_Unset
SupportsGt
None
_Unset
SupportsGe
None
_Unset
SupportsLt
None
_Unset
SupportsLe
None
_Unset
multiple_of
float
None
_Unset
allow_inf_nan
bool
None
_Unset
max_digits
int
None
_Unset
decimal_places
int
None
_Unset
min_length
int
None
_Unset
max_length
int
None
_Unset
union_mode
Literal
"smart"
"left_to_right"
_Unset
fail_fast
bool
None
_Unset
extra
Unpack
_EmptyKwargs
Any
Usage Documentation
Fields
Create a field for objects that can be configured.
Used to provide extra information about a field, either for the model schema or complex validation. Some arguments
apply only to number fields (
int
float
Decimal
) and some apply only to
str
Note
Any
_Unset
objects will be replaced by the corresponding value defined in the
_DefaultValues
dictionary. If a key for the
_Unset
object is not found in the
_DefaultValues
dictionary, it will default to
None
Parameters:
Name
Type
Description
Default
default
Any
Default value if the field is not set.
PydanticUndefined
default_factory
Callable
[[],
Any
] |
Callable
dict
str
Any
]],
Any
] | None
A callable to generate the default value. The callable can either take 0 arguments
(in which case it is called as is) or a single argument containing the already validated data.
_Unset
alias
str
| None
The name to use for the attribute when validating or serializing by alias.
This is often used for things like converting between snake and camel case.
_Unset
alias_priority
int
| None
Priority of the alias. This affects whether an alias generator is used.
_Unset
validation_alias
str
AliasPath
AliasChoices
| None
Like
alias
, but only affects validation, not serialization.
_Unset
serialization_alias
str
| None
Like
alias
, but only affects serialization, not validation.
_Unset
title
str
| None
Human-readable title.
_Unset
field_title_generator
Callable
str
FieldInfo
str
] | None
A callable that takes a field name and returns title for it.
_Unset
description
str
| None
Human-readable description.
_Unset
examples
list
Any
] | None
Example values for this field.
_Unset
exclude
bool
| None
Whether to exclude the field from the model serialization.
_Unset
discriminator
str
Discriminator
| None
Field name or Discriminator for discriminating the type in a tagged union.
_Unset
deprecated
Deprecated
str
bool
| None
A deprecation message, an instance of
warnings.deprecated
or the
typing_extensions.deprecated
backport,
or a boolean. If
True
, a default deprecation message will be emitted when accessing the field.
_Unset
json_schema_extra
JsonDict
Callable
JsonDict
], None] | None
A dict or callable to provide extra JSON schema properties.
_Unset
frozen
bool
| None
Whether the field is frozen. If true, attempts to change the value on an instance will raise an error.
_Unset
validate_default
bool
| None
True
, apply validation to the default value every time you create an instance.
Otherwise, for performance reasons, the default value of the field is trusted and not validated.
_Unset
repr
bool
A boolean indicating whether to include the field in the
__repr__
output.
_Unset
init
bool
| None
Whether the field should be included in the constructor of the dataclass.
(Only applies to dataclasses.)
_Unset
init_var
bool
| None
Whether the field should
only
be included in the constructor of the dataclass.
(Only applies to dataclasses.)
_Unset
kw_only
bool
| None
Whether the field should be a keyword-only argument in the constructor of the dataclass.
(Only applies to dataclasses.)
_Unset
coerce_numbers_to_str
bool
| None
Whether to enable coercion of any
Number
type to
str
(not applicable in
strict
mode).
_Unset
strict
bool
| None
True
, strict validation is applied to the field.
See
Strict Mode
for details.
_Unset
SupportsGt
| None
Greater than. If set, value must be greater than this. Only applicable to numbers.
_Unset
SupportsGe
| None
Greater than or equal. If set, value must be greater than or equal to this. Only applicable to numbers.
_Unset
SupportsLt
| None
Less than. If set, value must be less than this. Only applicable to numbers.
_Unset
SupportsLe
| None
Less than or equal. If set, value must be less than or equal to this. Only applicable to numbers.
_Unset
multiple_of
float
| None
Value must be a multiple of this. Only applicable to numbers.
_Unset
min_length
int
| None
Minimum length for iterables.
_Unset
max_length
int
| None
Maximum length for iterables.
_Unset
pattern
str
Pattern
str
] | None
Pattern for strings (a regular expression).
_Unset
allow_inf_nan
bool
| None
Allow
inf
-inf
nan
. Only applicable to float and
Decimal
numbers.
_Unset
max_digits
int
| None
Maximum number of allow digits for strings.
_Unset
decimal_places
int
| None
Maximum number of decimal places allowed for numbers.
_Unset
union_mode
Literal
['smart', 'left_to_right']
The strategy to apply when validating a union. Can be
smart
(the default), or
left_to_right
See
Union Mode
for details.
_Unset
fail_fast
bool
| None
True
, validation will stop on the first error. If
False
, all validation errors will be collected.
This option can be applied only to iterable types (list, tuple, set, and frozenset).
_Unset
extra
Unpack
_EmptyKwargs
(Deprecated) Extra fields that will be included in the JSON schema.
Warning
The
extra
kwargs is deprecated. Use
json_schema_extra
instead.
Returns:
Type
Description
Any
A new
FieldInfo
. The return annotation is
Any
Field
can be used on
type-annotated fields without causing a type error.
Source code in
pydantic/fields.py
950
951
952
953
954
955
956
957
958
959
960
961
962
963
964
965
966
967
968
969
970
971
972
973
974
975
976
977
978
979
980
981
982
983
984
985
986
987
988
989
990
991
992
993
994
995
996
997
998
999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
1026
1027
1028
1029
1030
1031
1032
1033
1034
1035
1036
1037
1038
1039
1040
1041
1042
1043
1044
1045
1046
1047
1048
1049
1050
1051
1052
1053
1054
1055
1056
1057
1058
1059
1060
1061
1062
1063
1064
1065
1066
1067
1068
1069
1070
1071
1072
1073
1074
1075
1076
1077
1078
1079
1080
1081
1082
1083
1084
1085
1086
1087
1088
1089
1090
1091
1092
1093
1094
1095
1096
1097
1098
1099
1100
1101
1102
1103
1104
1105
1106
1107
1108
1109
1110
1111
1112
1113
1114
1115
1116
1117
1118
1119
1120
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
1133
1134
1135
1136
1137
1138
1139
1140
1141
1142
1143
1144
1145
1146
1147
1148
1149
1150
1151
1152
1153
1154
1155
def
Field
# noqa: C901
default
Any
PydanticUndefined
default_factory
Callable
[[],
Any
Callable
dict
str
Any
]],
Any
None
_Unset
alias
str
None
_Unset
alias_priority
int
None
_Unset
validation_alias
str
AliasPath
AliasChoices
None
_Unset
serialization_alias
str
None
_Unset
title
str
None
_Unset
field_title_generator
Callable
str
FieldInfo
str
None
_Unset
description
str
None
_Unset
examples
list
Any
None
_Unset
exclude
bool
None
_Unset
discriminator
str
types
Discriminator
None
_Unset
deprecated
Deprecated
str
bool
None
_Unset
json_schema_extra
JsonDict
Callable
JsonDict
None
None
_Unset
frozen
bool
None
_Unset
validate_default
bool
None
_Unset
repr
bool
_Unset
init
bool
None
_Unset
init_var
bool
None
_Unset
kw_only
bool
None
_Unset
pattern
str
typing
Pattern
str
None
_Unset
strict
bool
None
_Unset
coerce_numbers_to_str
bool
None
_Unset
annotated_types
SupportsGt
None
_Unset
annotated_types
SupportsGe
None
_Unset
annotated_types
SupportsLt
None
_Unset
annotated_types
SupportsLe
None
_Unset
multiple_of
float
None
_Unset
allow_inf_nan
bool
None
_Unset
max_digits
int
None
_Unset
decimal_places
int
None
_Unset
min_length
int
None
_Unset
max_length
int
None
_Unset
union_mode
Literal
'smart'
'left_to_right'
_Unset
fail_fast
bool
None
_Unset
extra
Unpack
_EmptyKwargs
Any
"""!!! abstract "Usage Documentation"
[Fields](../concepts/fields.md)
Create a field for objects that can be configured.
Used to provide extra information about a field, either for the model schema or complex validation. Some arguments
apply only to number fields (`int`, `float`, `Decimal`) and some apply only to `str`.
Note:
- Any `_Unset` objects will be replaced by the corresponding value defined in the `_DefaultValues` dictionary. If a key for the `_Unset` object is not found in the `_DefaultValues` dictionary, it will default to `None`
Args:
default: Default value if the field is not set.
default_factory: A callable to generate the default value. The callable can either take 0 arguments
(in which case it is called as is) or a single argument containing the already validated data.
alias: The name to use for the attribute when validating or serializing by alias.
This is often used for things like converting between snake and camel case.
alias_priority: Priority of the alias. This affects whether an alias generator is used.
validation_alias: Like `alias`, but only affects validation, not serialization.
serialization_alias: Like `alias`, but only affects serialization, not validation.
title: Human-readable title.
field_title_generator: A callable that takes a field name and returns title for it.
description: Human-readable description.
examples: Example values for this field.
exclude: Whether to exclude the field from the model serialization.
discriminator: Field name or Discriminator for discriminating the type in a tagged union.
deprecated: A deprecation message, an instance of `warnings.deprecated` or the `typing_extensions.deprecated` backport,
or a boolean. If `True`, a default deprecation message will be emitted when accessing the field.
json_schema_extra: A dict or callable to provide extra JSON schema properties.
frozen: Whether the field is frozen. If true, attempts to change the value on an instance will raise an error.
validate_default: If `True`, apply validation to the default value every time you create an instance.
Otherwise, for performance reasons, the default value of the field is trusted and not validated.
repr: A boolean indicating whether to include the field in the `__repr__` output.
init: Whether the field should be included in the constructor of the dataclass.
(Only applies to dataclasses.)
init_var: Whether the field should _only_ be included in the constructor of the dataclass.
(Only applies to dataclasses.)
kw_only: Whether the field should be a keyword-only argument in the constructor of the dataclass.
(Only applies to dataclasses.)
coerce_numbers_to_str: Whether to enable coercion of any `Number` type to `str` (not applicable in `strict` mode).
strict: If `True`, strict validation is applied to the field.
See [Strict Mode](../concepts/strict_mode.md) for details.
gt: Greater than. If set, value must be greater than this. Only applicable to numbers.
ge: Greater than or equal. If set, value must be greater than or equal to this. Only applicable to numbers.
lt: Less than. If set, value must be less than this. Only applicable to numbers.
le: Less than or equal. If set, value must be less than or equal to this. Only applicable to numbers.
multiple_of: Value must be a multiple of this. Only applicable to numbers.
min_length: Minimum length for iterables.
max_length: Maximum length for iterables.
pattern: Pattern for strings (a regular expression).
allow_inf_nan: Allow `inf`, `-inf`, `nan`. Only applicable to float and [`Decimal`][decimal.Decimal] numbers.
max_digits: Maximum number of allow digits for strings.
decimal_places: Maximum number of decimal places allowed for numbers.
union_mode: The strategy to apply when validating a union. Can be `smart` (the default), or `left_to_right`.
See [Union Mode](../concepts/unions.md#union-modes) for details.
fail_fast: If `True`, validation will stop on the first error. If `False`, all validation errors will be collected.
This option can be applied only to iterable types (list, tuple, set, and frozenset).
extra: (Deprecated) Extra fields that will be included in the JSON schema.
!!! warning Deprecated
The `extra` kwargs is deprecated. Use `json_schema_extra` instead.
Returns:
A new [`FieldInfo`][pydantic.fields.FieldInfo]. The return annotation is `Any` so `Field` can be used on
type-annotated fields without causing a type error.
"""
# Check deprecated and removed params from V1. This logic should eventually be removed.
const
extra
pop
'const'
None
# type: ignore
const
not
None
raise
PydanticUserError
'`const` is removed, use `Literal` instead'
code
'removed-kwargs'
min_items
extra
pop
'min_items'
None
# type: ignore
min_items
not
None
warn
'`min_items` is deprecated and will be removed, use `min_length` instead'
DeprecationWarning
min_length
None
_Unset
min_length
min_items
# type: ignore
max_items
extra
pop
'max_items'
None
# type: ignore
max_items
not
None
warn
'`max_items` is deprecated and will be removed, use `max_length` instead'
DeprecationWarning
max_length
None
_Unset
max_length
max_items
# type: ignore
unique_items
extra
pop
'unique_items'
None
# type: ignore
unique_items
not
None
raise
PydanticUserError
'`unique_items` is removed, use `Set` instead'
'(this feature is discussed in https://github.com/pydantic/pydantic-core/issues/296)'
code
'removed-kwargs'
allow_mutation
extra
pop
'allow_mutation'
None
# type: ignore
allow_mutation
not
None
warn
'`allow_mutation` is deprecated and will be removed. use `frozen` instead'
DeprecationWarning
allow_mutation
False
frozen
True
regex
extra
pop
'regex'
None
# type: ignore
regex
not
None
raise
PydanticUserError
'`regex` is removed. use `pattern` instead'
code
'removed-kwargs'
extra
warn
'Using extra keyword arguments on `Field` is deprecated and will be removed.'
' Use `json_schema_extra` instead.'
' (Extra keys:
", "
join
__repr__
for
extra
keys
())
DeprecationWarning
not
json_schema_extra
json_schema_extra
_Unset
json_schema_extra
extra
# type: ignore
validation_alias
and
validation_alias
not
_Unset
and
not
isinstance
validation_alias
str
AliasChoices
AliasPath
raise
TypeError
'Invalid `validation_alias` type. it should be `str`, `AliasChoices`, or `AliasPath`'
serialization_alias
_Unset
None
and
isinstance
alias
str
serialization_alias
alias
validation_alias
_Unset
None
validation_alias
alias
include
extra
pop
'include'
None
# type: ignore
include
not
None
warn
'`include` is deprecated and does nothing. It will be removed, use `exclude` instead'
DeprecationWarning
return
FieldInfo
from_field
default
default_factory
default_factory
alias
alias
alias_priority
alias_priority
validation_alias
validation_alias
serialization_alias
serialization_alias
title
title
field_title_generator
field_title_generator
description
description
examples
examples
exclude
exclude
discriminator
discriminator
deprecated
deprecated
json_schema_extra
json_schema_extra
frozen
frozen
pattern
pattern
validate_default
validate_default
repr
repr
init
init
init_var
init_var
kw_only
kw_only
coerce_numbers_to_str
coerce_numbers_to_str
strict
strict
multiple_of
multiple_of
min_length
min_length
max_length
max_length
allow_inf_nan
allow_inf_nan
max_digits
max_digits
decimal_places
decimal_places
union_mode
union_mode
fail_fast
fail_fast
FieldInfo
FieldInfo
kwargs
Unpack
_FieldInfoInputs
Bases:
Representation
This class holds information about a field.
FieldInfo
is used for any field definition regardless of whether the
Field()
function is explicitly used.
Warning
You generally shouldn't be creating
FieldInfo
directly, you'll only need to use it when accessing
BaseModel
.model_fields
internals.
Attributes:
Name
Type
Description
annotation
type
Any
] | None
The type annotation of the field.
default
Any
The default value of the field.
default_factory
Callable
[[],
Any
] |
Callable
dict
str
Any
]],
Any
] | None
A callable to generate the default value. The callable can either take 0 arguments
(in which case it is called as is) or a single argument containing the already validated data.
alias
str
| None
The alias name of the field.
alias_priority
int
| None
The priority of the field's alias.
validation_alias
str
AliasPath
AliasChoices
| None
The validation alias of the field.
serialization_alias
str
| None
The serialization alias of the field.
title
str
| None
The title of the field.
field_title_generator
Callable
str
FieldInfo
str
] | None
A callable that takes a field name and returns title for it.
description
str
| None
The description of the field.
examples
list
Any
] | None
List of examples of the field.
exclude
bool
| None
Whether to exclude the field from the model serialization.
discriminator
str
Discriminator
| None
Field name or Discriminator for discriminating the type in a tagged union.
deprecated
Deprecated
str
bool
| None
A deprecation message, an instance of
warnings.deprecated
or the
typing_extensions.deprecated
backport,
or a boolean. If
True
, a default deprecation message will be emitted when accessing the field.
json_schema_extra
JsonDict
Callable
JsonDict
], None] | None
A dict or callable to provide extra JSON schema properties.
frozen
bool
| None
Whether the field is frozen.
validate_default
bool
| None
Whether to validate the default value of the field.
repr
bool
Whether to include the field in representation of the model.
init
bool
| None
Whether the field should be included in the constructor of the dataclass.
init_var
bool
| None
Whether the field should
only
be included in the constructor of the dataclass, and not stored.
kw_only
bool
| None
Whether the field should be a keyword-only argument in the constructor of the dataclass.
metadata
list
Any
List of metadata constraints.
See the signature of
pydantic.fields.Field
for more details about the expected arguments.
Source code in
pydantic/fields.py
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
def
__init__
self
kwargs
Unpack
_FieldInfoInputs
None
"""This class should generally not be initialized directly; instead, use the `pydantic.fields.Field` function
or one of the constructor classmethods.
See the signature of `pydantic.fields.Field` for more details about the expected arguments.
"""
self
_attributes_set
for
kwargs
items
not
_Unset
kwargs
_DefaultValues
get
_Unset
else
for
kwargs
items
()}
# type: ignore
self
annotation
kwargs
get
'annotation'
default
kwargs
pop
'default'
PydanticUndefined
default
Ellipsis
self
default
PydanticUndefined
self
_attributes_set
pop
'default'
None
else
self
default
default
self
default_factory
kwargs
pop
'default_factory'
None
self
default
not
PydanticUndefined
and
self
default_factory
not
None
raise
TypeError
'cannot specify both default and default_factory'
self
alias
kwargs
pop
'alias'
None
self
validation_alias
kwargs
pop
'validation_alias'
None
self
serialization_alias
kwargs
pop
'serialization_alias'
None
alias_is_set
any
alias
not
None
for
alias
self
alias
self
validation_alias
self
serialization_alias
self
alias_priority
kwargs
pop
'alias_priority'
None
alias_is_set
else
None
self
title
kwargs
pop
'title'
None
self
field_title_generator
kwargs
pop
'field_title_generator'
None
self
description
kwargs
pop
'description'
None
self
examples
kwargs
pop
'examples'
None
self
exclude
kwargs
pop
'exclude'
None
self
discriminator
kwargs
pop
'discriminator'
None
# For compatibility with FastAPI<=0.110.0, we preserve the existing value if it is not overridden
self
deprecated
kwargs
pop
'deprecated'
getattr
self
'deprecated'
None
self
repr
kwargs
pop
'repr'
True
self
json_schema_extra
kwargs
pop
'json_schema_extra'
None
self
validate_default
kwargs
pop
'validate_default'
None
self
frozen
kwargs
pop
'frozen'
None
# currently only used on dataclasses
self
init
kwargs
pop
'init'
None
self
init_var
kwargs
pop
'init_var'
None
self
kw_only
kwargs
pop
'kw_only'
None
self
metadata
self
_collect_metadata
kwargs
# type: ignore
# Private attributes:
self
_qualifiers
set
Qualifier
set
# Used to rebuild FieldInfo instances:
self
_complete
True
self
_original_annotation
Any
PydanticUndefined
self
_original_assignment
Any
PydanticUndefined
from_field
staticmethod
from_field
default
Any
PydanticUndefined
kwargs
Unpack
_FromFieldInfoInputs
FieldInfo
Create a new
FieldInfo
object with the
Field
function.
Parameters:
Name
Type
Description
Default
default
Any
The default value for the field. Defaults to Undefined.
PydanticUndefined
**kwargs
Unpack
_FromFieldInfoInputs
Additional arguments dictionary.
Raises:
Type
Description
TypeError
If 'annotation' is passed as a keyword argument.
Returns:
Type
Description
FieldInfo
A new FieldInfo object with the given parameters.
Example
This is how you can create a field with default value like this:
import
pydantic
class
MyModel
pydantic
BaseModel
foo
int
pydantic
Field
Source code in
pydantic/fields.py
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
@staticmethod
def
from_field
default
Any
PydanticUndefined
kwargs
Unpack
_FromFieldInfoInputs
FieldInfo
"""Create a new `FieldInfo` object with the `Field` function.
Args:
default: The default value for the field. Defaults to Undefined.
**kwargs: Additional arguments dictionary.
Raises:
TypeError: If 'annotation' is passed as a keyword argument.
Returns:
A new FieldInfo object with the given parameters.
Example:
This is how you can create a field with default value like this:
```python
import pydantic
class MyModel(pydantic.BaseModel):
foo: int = pydantic.Field(4)
```
"""
'annotation'
kwargs
raise
TypeError
'"annotation" is not permitted as a Field keyword argument'
return
FieldInfo
default
default
kwargs
from_annotation
staticmethod
from_annotation
annotation
type
Any
_source
AnnotationSource
ANY
FieldInfo
Creates a
FieldInfo
instance from a bare annotation.
This function is used internally to create a
FieldInfo
from a bare annotation like this:
import
pydantic
class
MyModel
pydantic
BaseModel
foo
int
# <-- like this
We also account for the case where the annotation can be an instance of
Annotated
and where
one of the (not first) arguments in
Annotated
is an instance of
FieldInfo
, e.g.:
from
typing
import
Annotated
import
annotated_types
import
pydantic
class
MyModel
pydantic
BaseModel
foo
Annotated
int
annotated_types
bar
Annotated
int
pydantic
Field
Parameters:
Name
Type
Description
Default
annotation
type
Any
An annotation object.
required
Returns:
Type
Description
FieldInfo
An instance of the field metadata.
Source code in
pydantic/fields.py
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
@staticmethod
def
from_annotation
annotation
type
Any
_source
AnnotationSource
AnnotationSource
ANY
FieldInfo
"""Creates a `FieldInfo` instance from a bare annotation.
This function is used internally to create a `FieldInfo` from a bare annotation like this:
```python
import pydantic
class MyModel(pydantic.BaseModel):
foo: int  # <-- like this
```
We also account for the case where the annotation can be an instance of `Annotated` and where
one of the (not first) arguments in `Annotated` is an instance of `FieldInfo`, e.g.:
```python
from typing import Annotated
import annotated_types
import pydantic
class MyModel(pydantic.BaseModel):
foo: Annotated[int, annotated_types.Gt(42)]
bar: Annotated[int, pydantic.Field(gt=42)]
```
Args:
annotation: An annotation object.
Returns:
An instance of the field metadata.
"""
try
inspected_ann
inspect_annotation
annotation
annotation_source
_source
unpack_type_aliases
'skip'
except
ForbiddenQualifier
raise
PydanticForbiddenQualifier
qualifier
annotation
# TODO check for classvar and error?
# No assigned value, this happens when using a bare `Final` qualifier (also for other
# qualifiers, but they shouldn't appear here). In this case we infer the type as `Any`
# because we don't have any assigned value.
type_expr
Any
Any
inspected_ann
type
UNKNOWN
else
inspected_ann
type
final
'final'
inspected_ann
qualifiers
metadata
inspected_ann
metadata
not
metadata
# No metadata, e.g. `field: int`, or `field: Final[str]`:
field_info
FieldInfo
annotation
type_expr
frozen
final
None
field_info
_qualifiers
inspected_ann
qualifiers
return
field_info
# With metadata, e.g. `field: Annotated[int, Field(...), Gt(1)]`:
field_info_annotations
for
metadata
isinstance
FieldInfo
field_info
FieldInfo
merge_field_infos
field_info_annotations
annotation
type_expr
new_field_info
field_info
_copy
new_field_info
annotation
type_expr
new_field_info
frozen
final
field_info
frozen
field_metadata
list
Any
for
metadata
typing_objects
is_deprecated
new_field_info
deprecated
message
elif
not
isinstance
FieldInfo
field_metadata
append
else
field_metadata
extend
metadata
new_field_info
metadata
field_metadata
new_field_info
_qualifiers
inspected_ann
qualifiers
return
new_field_info
from_annotated_attribute
staticmethod
from_annotated_attribute
annotation
type
Any
default
Any
_source
AnnotationSource
ANY
FieldInfo
Create
FieldInfo
from an annotation with a default value.
This is used in cases like the following:
from
typing
import
Annotated
import
annotated_types
import
pydantic
class
MyModel
pydantic
BaseModel
foo
int
# <-- like this
bar
Annotated
int
annotated_types
# <-- or this
spam
Annotated
int
pydantic
Field
# <-- or this
Parameters:
Name
Type
Description
Default
annotation
type
Any
The type annotation of the field.
required
default
Any
The default value of the field.
required
Returns:
Type
Description
FieldInfo
A field object with the passed values.
Source code in
pydantic/fields.py
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
@staticmethod
def
from_annotated_attribute
annotation
type
Any
default
Any
_source
AnnotationSource
AnnotationSource
ANY
FieldInfo
"""Create `FieldInfo` from an annotation with a default value.
This is used in cases like the following:
```python
from typing import Annotated
import annotated_types
import pydantic
class MyModel(pydantic.BaseModel):
foo: int = 4  # <-- like this
bar: Annotated[int, annotated_types.Gt(4)] = 4  # <-- or this
spam: Annotated[int, pydantic.Field(gt=4)] = 4  # <-- or this
```
Args:
annotation: The type annotation of the field.
default: The default value of the field.
Returns:
A field object with the passed values.
"""
annotation
default
raise
PydanticUserError
'Error when building FieldInfo from annotated attribute. '
"Make sure you don't have any field name clashing with a type annotation."
code
'unevaluable-type-annotation'
try
inspected_ann
inspect_annotation
annotation
annotation_source
_source
unpack_type_aliases
'skip'
except
ForbiddenQualifier
raise
PydanticForbiddenQualifier
qualifier
annotation
# TODO check for classvar and error?
# TODO infer from the default, this can be done in v3 once we treat final fields with
# a default as proper fields and not class variables:
type_expr
Any
Any
inspected_ann
type
UNKNOWN
else
inspected_ann
type
final
'final'
inspected_ann
qualifiers
metadata
inspected_ann
metadata
isinstance
default
FieldInfo
# e.g. `field: int = Field(...)`
default_metadata
default
metadata
copy
default
copy
default
default
metadata
default_metadata
default
annotation
type_expr
default
metadata
metadata
merged_default
FieldInfo
merge_field_infos
for
metadata
isinstance
FieldInfo
)],
default
annotation
default
annotation
merged_default
frozen
final
merged_default
frozen
merged_default
_qualifiers
inspected_ann
qualifiers
return
merged_default
isinstance
default
dataclasses
Field
# `collect_dataclass_fields()` passes the dataclass Field as a default.
pydantic_field
FieldInfo
_from_dataclass_field
default
pydantic_field
annotation
type_expr
pydantic_field
metadata
metadata
pydantic_field
FieldInfo
merge_field_infos
for
metadata
isinstance
FieldInfo
)],
pydantic_field
annotation
pydantic_field
annotation
pydantic_field
frozen
final
pydantic_field
frozen
pydantic_field
init_var
'init_var'
inspected_ann
qualifiers
pydantic_field
init
getattr
default
'init'
None
pydantic_field
kw_only
getattr
default
'kw_only'
None
pydantic_field
_qualifiers
inspected_ann
qualifiers
return
pydantic_field
not
metadata
# No metadata, e.g. `field: int = ...`, or `field: Final[str] = ...`:
field_info
FieldInfo
annotation
type_expr
default
default
frozen
final
None
field_info
_qualifiers
inspected_ann
qualifiers
return
field_info
# With metadata, e.g. `field: Annotated[int, Field(...), Gt(1)] = ...`:
field_infos
for
metadata
isinstance
FieldInfo
field_info
FieldInfo
merge_field_infos
field_infos
annotation
type_expr
default
default
field_metadata
list
Any
for
metadata
typing_objects
is_deprecated
field_info
deprecated
message
elif
not
isinstance
FieldInfo
field_metadata
append
else
field_metadata
extend
metadata
field_info
metadata
field_metadata
field_info
_qualifiers
inspected_ann
qualifiers
return
field_info
merge_field_infos
staticmethod
merge_field_infos
field_infos
FieldInfo
overrides
Any
FieldInfo
Merge
FieldInfo
instances keeping only explicitly set attributes.
Later
FieldInfo
instances override earlier ones.
Returns:
Name
Type
Description
FieldInfo
FieldInfo
A merged FieldInfo instance.
Source code in
pydantic/fields.py
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
@staticmethod
def
merge_field_infos
field_infos
FieldInfo
overrides
Any
FieldInfo
"""Merge `FieldInfo` instances keeping only explicitly set attributes.
Later `FieldInfo` instances override earlier ones.
Returns:
FieldInfo: A merged FieldInfo instance.
"""
len
field_infos
# No merging necessary, but we still need to make a copy and apply the overrides
field_info
field_infos
_copy
field_info
_attributes_set
update
overrides
default_override
overrides
pop
'default'
PydanticUndefined
default_override
Ellipsis
default_override
PydanticUndefined
default_override
not
PydanticUndefined
field_info
default
default_override
for
overrides
items
():
setattr
field_info
return
field_info
# type: ignore
merged_field_info_kwargs
dict
str
Any
metadata
for
field_info
field_infos
attributes_set
field_info
_attributes_set
copy
try
json_schema_extra
attributes_set
pop
'json_schema_extra'
existing_json_schema_extra
merged_field_info_kwargs
get
'json_schema_extra'
existing_json_schema_extra
None
merged_field_info_kwargs
'json_schema_extra'
json_schema_extra
isinstance
existing_json_schema_extra
dict
isinstance
json_schema_extra
dict
merged_field_info_kwargs
'json_schema_extra'
existing_json_schema_extra
json_schema_extra
callable
json_schema_extra
warn
'Composing `dict` and `callable` type `json_schema_extra` is not supported.'
'The `callable` type is being ignored.'
"If you'd like support for this behavior, please open an issue on pydantic."
PydanticJsonSchemaWarning
elif
callable
json_schema_extra
# if ever there's a case of a callable, we'll just keep the last json schema extra spec
merged_field_info_kwargs
'json_schema_extra'
json_schema_extra
except
KeyError
pass
# later FieldInfo instances override everything except json_schema_extra from earlier FieldInfo instances
merged_field_info_kwargs
update
attributes_set
for
field_info
metadata
not
isinstance
FieldInfo
metadata
type
merged_field_info_kwargs
update
overrides
field_info
FieldInfo
merged_field_info_kwargs
field_info
metadata
list
metadata
values
())
return
field_info
deprecation_message
property
deprecation_message
str
None
The deprecation message to be emitted, or
None
if not set.
default_factory_takes_validated_data
property
default_factory_takes_validated_data
bool
None
Whether the provided default factory callable has a validated data parameter.
Returns
None
if no default factory is set.
get_default
get_default
call_default_factory
Literal
True
validated_data
dict
str
Any
None
None
Any
get_default
call_default_factory
Literal
False
...
Any
get_default
call_default_factory
bool
False
validated_data
dict
str
Any
None
None
Any
Get the default value.
We expose an option for whether to call the default_factory (if present), as calling it may
result in side effects that we want to avoid. However, there are times when it really should
be called (namely, when instantiating a model via
model_construct
Parameters:
Name
Type
Description
Default
call_default_factory
bool
Whether to call the default factory or not.
False
validated_data
dict
str
Any
] | None
The already validated data to be passed to the default factory.
None
Returns:
Type
Description
Any
The default value, calling the default factory if requested or
None
if not set.
Source code in
pydantic/fields.py
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
def
get_default
self
call_default_factory
bool
False
validated_data
dict
str
Any
None
None
Any
"""Get the default value.
We expose an option for whether to call the default_factory (if present), as calling it may
result in side effects that we want to avoid. However, there are times when it really should
be called (namely, when instantiating a model via `model_construct`).
Args:
call_default_factory: Whether to call the default factory or not.
validated_data: The already validated data to be passed to the default factory.
Returns:
The default value, calling the default factory if requested or `None` if not set.
"""
self
default_factory
None
return
_utils
smart_deepcopy
self
default
elif
call_default_factory
self
default_factory_takes_validated_data
fac
cast
'Callable[[dict[str, Any]], Any]'
self
default_factory
validated_data
None
raise
ValueError
"The default factory requires the 'validated_data' argument, which was not provided when calling 'get_default'."
return
fac
validated_data
else
fac
cast
'Callable[[], Any]'
self
default_factory
return
fac
else
return
None
is_required
is_required
bool
Check if the field is required (i.e., does not have a default value or factory).
Returns:
Type
Description
bool
True
if the field is required,
False
otherwise.
Source code in
pydantic/fields.py
660
661
662
663
664
665
666
def
is_required
self
bool
"""Check if the field is required (i.e., does not have a default value or factory).
Returns:
`True` if the field is required, `False` otherwise.
"""
return
self
default
PydanticUndefined
and
self
default_factory
None
rebuild_annotation
rebuild_annotation
Any
Attempts to rebuild the original annotation for use in function signatures.
If metadata is present, it adds it to the original annotation using
Annotated
. Otherwise, it returns the original annotation as-is.
Note that because the metadata has been flattened, the original annotation
may not be reconstructed exactly as originally provided, e.g. if the original
type had unrecognized annotations, or was annotated with a call to
pydantic.Field
Returns:
Type
Description
Any
The rebuilt annotation.
Source code in
pydantic/fields.py
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
def
rebuild_annotation
self
Any
"""Attempts to rebuild the original annotation for use in function signatures.
If metadata is present, it adds it to the original annotation using
`Annotated`. Otherwise, it returns the original annotation as-is.
Note that because the metadata has been flattened, the original annotation
may not be reconstructed exactly as originally provided, e.g. if the original
type had unrecognized annotations, or was annotated with a call to `pydantic.Field`.
Returns:
The rebuilt annotation.
"""
not
self
metadata
return
self
annotation
else
# Annotated arguments must be a tuple
return
Annotated
self
annotation
self
metadata
# type: ignore
apply_typevars_map
apply_typevars_map
typevars_map
Mapping
TypeVar
Any
None
globalns
GlobalsNamespace
None
None
localns
MappingNamespace
None
None
None
Apply a
typevars_map
to the annotation.
This method is used when analyzing parametrized generic types to replace typevars with their concrete types.
This method applies the
typevars_map
to the annotation in place.
Parameters:
Name
Type
Description
Default
typevars_map
Mapping
TypeVar
Any
] | None
A dictionary mapping type variables to their concrete types.
required
globalns
GlobalsNamespace
| None
The globals namespace to use during type annotation evaluation.
None
localns
MappingNamespace
| None
The locals namespace to use during type annotation evaluation.
None
See Also
pydantic._internal._generics.replace_types is used for replacing the typevars with
their concrete types.
Source code in
pydantic/fields.py
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
def
apply_typevars_map
self
typevars_map
Mapping
TypeVar
Any
None
globalns
GlobalsNamespace
None
None
localns
MappingNamespace
None
None
None
"""Apply a `typevars_map` to the annotation.
This method is used when analyzing parametrized generic types to replace typevars with their concrete types.
This method applies the `typevars_map` to the annotation in place.
Args:
typevars_map: A dictionary mapping type variables to their concrete types.
globalns: The globals namespace to use during type annotation evaluation.
localns: The locals namespace to use during type annotation evaluation.
See Also:
pydantic._internal._generics.replace_types is used for replacing the typevars with
their concrete types.
"""
annotation
_generics
replace_types
self
annotation
typevars_map
annotation
evaluated
_typing_extra
try_eval_type
annotation
globalns
localns
self
annotation
annotation
not
evaluated
self
_complete
False
self
_original_annotation
self
annotation
PrivateAttr
PrivateAttr
default
init
Literal
False
False
PrivateAttr
default_factory
Callable
[[],
init
Literal
False
False
PrivateAttr
init
Literal
False
False
Any
PrivateAttr
default
Any
PydanticUndefined
default_factory
Callable
[[],
Any
None
None
init
Literal
False
False
Any
Usage Documentation
Private Model Attributes
Indicates that an attribute is intended for private use and not handled during normal validation/serialization.
Private attributes are not validated by Pydantic, so it's up to you to ensure they are used in a type-safe manner.
Private attributes are stored in
__private_attributes__
on the model.
Parameters:
Name
Type
Description
Default
default
Any
The attribute's default value. Defaults to Undefined.
PydanticUndefined
default_factory
Callable
[[],
Any
] | None
Callable that will be
called when a default value is needed for this attribute.
If both
default
and
default_factory
are set, an error will be raised.
None
init
Literal
[False]
Whether the attribute should be included in the constructor of the dataclass. Always
False
False
Returns:
Type
Description
Any
An instance of
ModelPrivateAttr
class.
Raises:
Type
Description
ValueError
If both
default
and
default_factory
are set.
Source code in
pydantic/fields.py
1245
1246
1247
1248
1249
1250
1251
1252
1253
1254
1255
1256
1257
1258
1259
1260
1261
1262
1263
1264
1265
1266
1267
1268
1269
1270
1271
1272
1273
1274
1275
1276
1277
1278
1279
def
PrivateAttr
default
Any
PydanticUndefined
default_factory
Callable
[[],
Any
None
None
init
Literal
False
False
Any
"""!!! abstract "Usage Documentation"
[Private Model Attributes](../concepts/models.md#private-model-attributes)
Indicates that an attribute is intended for private use and not handled during normal validation/serialization.
Private attributes are not validated by Pydantic, so it's up to you to ensure they are used in a type-safe manner.
Private attributes are stored in `__private_attributes__` on the model.
Args:
default: The attribute's default value. Defaults to Undefined.
default_factory: Callable that will be
called when a default value is needed for this attribute.
If both `default` and `default_factory` are set, an error will be raised.
init: Whether the attribute should be included in the constructor of the dataclass. Always `False`.
Returns:
An instance of [`ModelPrivateAttr`][pydantic.fields.ModelPrivateAttr] class.
Raises:
ValueError: If both `default` and `default_factory` are set.
"""
default
not
PydanticUndefined
and
default_factory
not
None
raise
TypeError
'cannot specify both default and default_factory'
return
ModelPrivateAttr
default
default_factory
default_factory
ModelPrivateAttr
ModelPrivateAttr
default
Any
PydanticUndefined
default_factory
Callable
[[],
Any
None
None
Bases:
Representation
A descriptor for private attributes in class models.
Warning
You generally shouldn't be creating
ModelPrivateAttr
instances directly, instead use
pydantic.fields.PrivateAttr
. (This is similar to
FieldInfo
vs.
Field
Attributes:
Name
Type
Description
default
The default value of the attribute if not provided.
default_factory
A callable function that generates the default value of the
attribute if not provided.
Source code in
pydantic/fields.py
1177
1178
1179
1180
1181
1182
1183
1184
def
__init__
self
default
Any
PydanticUndefined
default_factory
typing
Callable
[[],
Any
None
None
None
default
Ellipsis
self
default
PydanticUndefined
else
self
default
default
self
default_factory
default_factory
get_default
get_default
Any
Retrieve the default value of the object.
self.default_factory
None
, the method will return a deep copy of the
self.default
object.
self.default_factory
is not
None
, it will call
self.default_factory
and return the value returned.
Returns:
Type
Description
Any
The default value of the object.
Source code in
pydantic/fields.py
1207
1208
1209
1210
1211
1212
1213
1214
1215
1216
1217
def
get_default
self
Any
"""Retrieve the default value of the object.
If `self.default_factory` is `None`, the method will return a deep copy of the `self.default` object.
If `self.default_factory` is not `None`, it will call `self.default_factory` and return the value returned.
Returns:
The default value of the object.
"""
return
_utils
smart_deepcopy
self
default
self
default_factory
None
else
self
default_factory
computed_field
computed_field
func
PropertyT
PropertyT
computed_field
alias
str
None
None
alias_priority
int
None
None
title
str
None
None
field_title_generator
Callable
str
ComputedFieldInfo
str
None
None
description
str
None
None
deprecated
Deprecated
str
bool
None
None
examples
list
Any
None
None
json_schema_extra
JsonDict
Callable
JsonDict
None
None
None
repr
bool
True
return_type
Any
PydanticUndefined
Callable
PropertyT
PropertyT
computed_field
func
PropertyT
None
None
alias
str
None
None
alias_priority
int
None
None
title
str
None
None
field_title_generator
Callable
str
ComputedFieldInfo
str
None
None
description
str
None
None
deprecated
Deprecated
str
bool
None
None
examples
list
Any
None
None
json_schema_extra
JsonDict
Callable
JsonDict
None
None
None
repr
bool
None
None
return_type
Any
PydanticUndefined
PropertyT
Callable
PropertyT
PropertyT
Usage Documentation
The
computed_field
decorator
Decorator to include
property
and
cached_property
when serializing models or dataclasses.
This is useful for fields that are computed from other fields, or for fields that are expensive to compute and should be cached.
from
pydantic
import
BaseModel
computed_field
class
Rectangle
BaseModel
width
int
length
int
@computed_field
@property
def
area
self
int
return
self
width
self
length
print
Rectangle
width
length
model_dump
())
#> {'width': 3, 'length': 2, 'area': 6}
If applied to functions not yet decorated with
@property
@cached_property
, the function is
automatically wrapped with
property
. Although this is more concise, you will lose IntelliSense in your IDE,
and confuse static type checkers, thus explicit use of
@property
is recommended.
Mypy Warning
Even with the
@property
@cached_property
applied to your function before
@computed_field
mypy may throw a
Decorated property not supported
error.
See
mypy issue #1362
, for more information.
To avoid this error message, add
# type: ignore[prop-decorator]
to the
@computed_field
line.
pyright
supports
@computed_field
without error.
import
random
from
pydantic
import
BaseModel
computed_field
class
Square
BaseModel
width
float
@computed_field
def
area
self
float
# converted to a `property` by `computed_field`
return
round
self
width
@area
setter
def
area
self
new_area
float
None
self
width
new_area
0.5
@computed_field
alias
'the magic number'
repr
False
def
random_number
self
int
return
random
randint
1_000
square
Square
width
1.3
# `random_number` does not appear in representation
print
repr
square
#> Square(width=1.3, area=1.69)
print
square
random_number
#> 3
square
area
print
square
model_dump_json
by_alias
True
#> {"width":2.0,"area":4.0,"the magic number":3}
Overriding with
computed_field
You can't override a field from a parent class with a
computed_field
in the child class.
mypy
complains about this behavior if allowed, and
dataclasses
doesn't allow this pattern either.
See the example below:
from
pydantic
import
BaseModel
computed_field
class
Parent
BaseModel
str
try
class
Child
Parent
@computed_field
@property
def
self
str
return
'new a'
except
TypeError
print
'''
Field 'a' of class 'Child' overrides symbol of same name in a parent class. This override with a computed_field is incompatible.
'''
Private properties decorated with
@computed_field
have
repr=False
by default.
from
functools
import
cached_property
from
pydantic
import
BaseModel
computed_field
class
Model
BaseModel
foo
int
@computed_field
@cached_property
def
_private_cached_property
self
int
return
self
foo
@computed_field
@property
def
_private_property
self
int
return
self
foo
Model
foo
print
repr
#> Model(foo=1)
Parameters:
Name
Type
Description
Default
func
PropertyT
| None
the function to wrap.
None
alias
str
| None
alias to use when serializing this computed field, only used when
by_alias=True
None
alias_priority
int
| None
priority of the alias. This affects whether an alias generator is used
None
title
str
| None
Title to use when including this computed field in JSON Schema
None
field_title_generator
Callable
str
ComputedFieldInfo
str
] | None
A callable that takes a field name and returns title for it.
None
description
str
| None
Description to use when including this computed field in JSON Schema, defaults to the function's
docstring
None
deprecated
Deprecated
str
bool
| None
A deprecation message (or an instance of
warnings.deprecated
or the
typing_extensions.deprecated
backport).
to be emitted when accessing the field. Or a boolean. This will automatically be set if the property is decorated with the
deprecated
decorator.
None
examples
list
Any
] | None
Example values to use when including this computed field in JSON Schema
None
json_schema_extra
JsonDict
Callable
JsonDict
], None] | None
A dict or callable to provide extra JSON schema properties.
None
repr
bool
| None
whether to include this computed field in model repr.
Default is
False
for private properties and
True
for public properties.
None
return_type
Any
optional return for serialization logic to expect when serializing to JSON, if included
this must be correct, otherwise a
TypeError
is raised.
If you don't include a return type Any is used, which does runtime introspection to handle arbitrary
objects.
PydanticUndefined
Returns:
Type
Description
PropertyT
Callable
PropertyT
PropertyT
A proxy wrapper for the property.
Source code in
pydantic/fields.py
1362
1363
1364
1365
1366
1367
1368
1369
1370
1371
1372
1373
1374
1375
1376
1377
1378
1379
1380
1381
1382
1383
1384
1385
1386
1387
1388
1389
1390
1391
1392
1393
1394
1395
1396
1397
1398
1399
1400
1401
1402
1403
1404
1405
1406
1407
1408
1409
1410
1411
1412
1413
1414
1415
1416
1417
1418
1419
1420
1421
1422
1423
1424
1425
1426
1427
1428
1429
1430
1431
1432
1433
1434
1435
1436
1437
1438
1439
1440
1441
1442
1443
1444
1445
1446
1447
1448
1449
1450
1451
1452
1453
1454
1455
1456
1457
1458
1459
1460
1461
1462
1463
1464
1465
1466
1467
1468
1469
1470
1471
1472
1473
1474
1475
1476
1477
1478
1479
1480
1481
1482
1483
1484
1485
1486
1487
1488
1489
1490
1491
1492
1493
1494
1495
1496
1497
1498
1499
1500
1501
1502
1503
1504
1505
1506
1507
1508
1509
1510
1511
1512
1513
1514
1515
1516
1517
1518
1519
1520
1521
1522
1523
1524
1525
1526
1527
1528
1529
1530
1531
1532
1533
1534
1535
1536
1537
1538
1539
1540
1541
1542
1543
1544
1545
1546
1547
1548
1549
1550
1551
1552
1553
1554
1555
1556
1557
1558
1559
def
computed_field
func
PropertyT
None
None
alias
str
None
None
alias_priority
int
None
None
title
str
None
None
field_title_generator
typing
Callable
str
ComputedFieldInfo
str
None
None
description
str
None
None
deprecated
Deprecated
str
bool
None
None
examples
list
Any
None
None
json_schema_extra
JsonDict
typing
Callable
JsonDict
None
None
None
repr
bool
None
None
return_type
Any
PydanticUndefined
PropertyT
typing
Callable
PropertyT
PropertyT
"""!!! abstract "Usage Documentation"
[The `computed_field` decorator](../concepts/fields.md#the-computed_field-decorator)
Decorator to include `property` and `cached_property` when serializing models or dataclasses.
This is useful for fields that are computed from other fields, or for fields that are expensive to compute and should be cached.
```python
from pydantic import BaseModel, computed_field
class Rectangle(BaseModel):
width: int
length: int
@computed_field
@property
def area(self) -> int:
return self.width * self.length
print(Rectangle(width=3, length=2).model_dump())
#> {'width': 3, 'length': 2, 'area': 6}
```
If applied to functions not yet decorated with `@property` or `@cached_property`, the function is
automatically wrapped with `property`. Although this is more concise, you will lose IntelliSense in your IDE,
and confuse static type checkers, thus explicit use of `@property` is recommended.
!!! warning "Mypy Warning"
Even with the `@property` or `@cached_property` applied to your function before `@computed_field`,
mypy may throw a `Decorated property not supported` error.
See [mypy issue #1362](https://github.com/python/mypy/issues/1362), for more information.
To avoid this error message, add `# type: ignore[prop-decorator]` to the `@computed_field` line.
[pyright](https://github.com/microsoft/pyright) supports `@computed_field` without error.
```python
import random
from pydantic import BaseModel, computed_field
class Square(BaseModel):
width: float
@computed_field
def area(self) -> float:  # converted to a `property` by `computed_field`
return round(self.width**2, 2)
@area.setter
def area(self, new_area: float) -> None:
self.width = new_area**0.5
@computed_field(alias='the magic number', repr=False)
def random_number(self) -> int:
return random.randint(0, 1_000)
square = Square(width=1.3)
# `random_number` does not appear in representation
print(repr(square))
#> Square(width=1.3, area=1.69)
print(square.random_number)
#> 3
square.area = 4
print(square.model_dump_json(by_alias=True))
#> {"width":2.0,"area":4.0,"the magic number":3}
```
!!! warning "Overriding with `computed_field`"
You can't override a field from a parent class with a `computed_field` in the child class.
`mypy` complains about this behavior if allowed, and `dataclasses` doesn't allow this pattern either.
See the example below:
```python
from pydantic import BaseModel, computed_field
class Parent(BaseModel):
a: str
try:
class Child(Parent):
@computed_field
@property
def a(self) -> str:
return 'new a'
except TypeError as e:
print(e)
'''
Field 'a' of class 'Child' overrides symbol of same name in a parent class. This override with a computed_field is incompatible.
'''
```
Private properties decorated with `@computed_field` have `repr=False` by default.
```python
from functools import cached_property
from pydantic import BaseModel, computed_field
class Model(BaseModel):
foo: int
@computed_field
@cached_property
def _private_cached_property(self) -> int:
return -self.foo
@computed_field
@property
def _private_property(self) -> int:
return -self.foo
m = Model(foo=1)
print(repr(m))
#> Model(foo=1)
```
Args:
func: the function to wrap.
alias: alias to use when serializing this computed field, only used when `by_alias=True`
alias_priority: priority of the alias. This affects whether an alias generator is used
title: Title to use when including this computed field in JSON Schema
field_title_generator: A callable that takes a field name and returns title for it.
description: Description to use when including this computed field in JSON Schema, defaults to the function's
docstring
deprecated: A deprecation message (or an instance of `warnings.deprecated` or the `typing_extensions.deprecated` backport).
to be emitted when accessing the field. Or a boolean. This will automatically be set if the property is decorated with the
`deprecated` decorator.
examples: Example values to use when including this computed field in JSON Schema
json_schema_extra: A dict or callable to provide extra JSON schema properties.
repr: whether to include this computed field in model repr.
Default is `False` for private properties and `True` for public properties.
return_type: optional return for serialization logic to expect when serializing to JSON, if included
this must be correct, otherwise a `TypeError` is raised.
If you don't include a return type Any is used, which does runtime introspection to handle arbitrary
objects.
Returns:
A proxy wrapper for the property.
"""
def
dec
Any
Any
nonlocal
description
deprecated
return_type
alias_priority
unwrapped
_decorators
unwrap_wrapped_function
description
None
and
unwrapped
__doc__
description
inspect
cleandoc
unwrapped
__doc__
deprecated
None
and
hasattr
unwrapped
'__deprecated__'
deprecated
unwrapped
__deprecated__
# if the function isn't already decorated with `@property` (or another descriptor), then we wrap it now
_decorators
ensure_property
alias_priority
alias_priority
alias
not
None
else
None
repr
None
repr_
bool
not
_wrapped_property_is_private
property_
else
repr_
repr
dec_info
ComputedFieldInfo
return_type
alias
alias_priority
title
field_title_generator
description
deprecated
examples
json_schema_extra
repr_
return
_decorators
PydanticDescriptorProxy
dec_info
func
None
return
dec
else
return
dec
func
ComputedFieldInfo
dataclass
ComputedFieldInfo
wrapped_property
property
return_type
Any
alias
str
None
alias_priority
int
None
title
str
None
field_title_generator
Callable
str
ComputedFieldInfo
str
None
description
str
None
deprecated
Deprecated
str
bool
None
examples
list
Any
None
json_schema_extra
JsonDict
Callable
JsonDict
None
None
repr
bool
A container for data from
@computed_field
so that we can access it while building the pydantic-core schema.
Attributes:
Name
Type
Description
decorator_repr
str
A class variable representing the decorator string, '@computed_field'.
wrapped_property
property
The wrapped computed field property.
return_type
Any
The type of the computed field property's return value.
alias
str
| None
The alias of the property to be used during serialization.
alias_priority
int
| None
The priority of the alias. This affects whether an alias generator is used.
title
str
| None
Title of the computed field to include in the serialization JSON schema.
field_title_generator
Callable
str
ComputedFieldInfo
str
] | None
A callable that takes a field name and returns title for it.
description
str
| None
Description of the computed field to include in the serialization JSON schema.
deprecated
Deprecated
str
bool
| None
A deprecation message, an instance of
warnings.deprecated
or the
typing_extensions.deprecated
backport,
or a boolean. If
True
, a default deprecation message will be emitted when accessing the field.
examples
list
Any
] | None
Example values of the computed field to include in the serialization JSON schema.
json_schema_extra
JsonDict
Callable
JsonDict
], None] | None
A dict or callable to provide extra JSON schema properties.
repr
bool
A boolean indicating whether to include the field in the
repr
output.
deprecation_message
property
deprecation_message
str
None
The deprecation message to be emitted, or
None
if not set.
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 027_version_pydantic___version__.txt ---
Version Information
pydantic.__version__
module-attribute
__version__
VERSION
pydantic.version.version_info
version_info
str
Return complete version information for Pydantic and its dependencies.
Source code in
pydantic/version.py
def
version_info
str
"""Return complete version information for Pydantic and its dependencies."""
import
importlib.metadata
import
platform
import
sys
from
pathlib
import
Path
import
pydantic_core._pydantic_core
pdc
from
._internal
import
_git
git
# get data about packages that are closely related to pydantic, use pydantic or often conflict with pydantic
package_names
'email-validator'
'fastapi'
'mypy'
'pydantic-extra-types'
'pydantic-settings'
'pyright'
'typing_extensions'
related_packages
for
dist
importlib
metadata
distributions
():
name
dist
metadata
'Name'
name
package_names
related_packages
append
name
dist
version
pydantic_dir
Path
__file__
parents
resolve
most_recent_commit
git
git_revision
pydantic_dir
git
is_git_repo
pydantic_dir
and
git
have_git
else
'unknown'
info
'pydantic version'
VERSION
'pydantic-core version'
pdc
__version__
'pydantic-core build'
getattr
pdc
'build_info'
None
pdc
build_profile
'python version'
sys
version
'platform'
platform
platform
(),
'related packages'
' '
join
related_packages
'commit'
most_recent_commit
return
join
{:>30}
format
':'
str
replace
' '
for
info
items
())
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 004_base_model_pydantic_BaseModel_model_dump.txt ---
BaseModel
Pydantic models are simply classes which inherit from
BaseModel
and define fields as annotated attributes.
pydantic.BaseModel
Usage Documentation
Models
A base class for creating Pydantic models.
Attributes:
Name
Type
Description
__class_vars__
set
str
The names of the class variables defined on the model.
__private_attributes__
Dict
str
ModelPrivateAttr
Metadata about the private attributes of the model.
__signature__
Signature
The synthesized
__init__
Signature
of the model.
__pydantic_complete__
bool
Whether model building is completed, or if there are still undefined fields.
__pydantic_core_schema__
CoreSchema
The core schema of the model.
__pydantic_custom_init__
bool
Whether the model has a custom
__init__
function.
__pydantic_decorators__
DecoratorInfos
Metadata containing the decorators defined on the model.
This replaces
Model.__validators__
and
Model.__root_validators__
from Pydantic V1.
__pydantic_generic_metadata__
PydanticGenericMetadata
Metadata for generic models; contains data used for a similar purpose to
args
origin
parameters
in typing-module generics. May eventually be replaced by these.
__pydantic_parent_namespace__
Dict
str
Any
] | None
Parent namespace of the model, used for automatic rebuilding of models.
__pydantic_post_init__
None |
Literal
['model_post_init']
The name of the post-init method for the model, if defined.
__pydantic_root_model__
bool
Whether the model is a
RootModel
__pydantic_serializer__
SchemaSerializer
The
pydantic-core
SchemaSerializer
used to dump instances of the model.
__pydantic_validator__
SchemaValidator
PluggableSchemaValidator
The
pydantic-core
SchemaValidator
used to validate instances of the model.
__pydantic_fields__
Dict
str
FieldInfo
A dictionary of field names and their corresponding
FieldInfo
objects.
__pydantic_computed_fields__
Dict
str
ComputedFieldInfo
A dictionary of computed field names and their corresponding
ComputedFieldInfo
objects.
__pydantic_extra__
dict
str
Any
] | None
A dictionary containing extra values, if
extra
is set to
'allow'
__pydantic_fields_set__
set
str
The names of fields explicitly set during instantiation.
__pydantic_private__
dict
str
Any
] | None
Values of private attributes set on the model instance.
Source code in
pydantic/main.py
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899
900
901
902
903
904
905
906
907
908
909
910
911
912
913
914
915
916
917
918
919
920
921
922
923
924
925
926
927
928
929
930
931
932
933
934
935
936
937
938
939
940
941
942
943
944
945
946
947
948
949
950
951
952
953
954
955
956
957
958
959
960
961
962
963
964
965
966
967
968
969
970
971
972
973
974
975
976
977
978
979
980
981
982
983
984
985
986
987
988
989
990
991
992
993
994
995
996
997
998
999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
1026
1027
1028
1029
1030
1031
1032
1033
1034
1035
1036
1037
1038
1039
1040
1041
1042
1043
1044
1045
1046
1047
1048
1049
1050
1051
1052
1053
1054
1055
1056
1057
1058
1059
1060
1061
1062
1063
1064
1065
1066
1067
1068
1069
1070
1071
1072
1073
1074
1075
1076
1077
1078
1079
1080
1081
1082
1083
1084
1085
1086
1087
1088
1089
1090
1091
1092
1093
1094
1095
1096
1097
1098
1099
1100
1101
1102
1103
1104
1105
1106
1107
1108
1109
1110
1111
1112
1113
1114
1115
1116
1117
1118
1119
1120
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
1133
1134
1135
1136
1137
1138
1139
1140
1141
1142
1143
1144
1145
1146
1147
1148
1149
1150
1151
1152
1153
1154
1155
1156
1157
1158
1159
1160
1161
1162
1163
1164
1165
1166
1167
1168
1169
1170
1171
1172
1173
1174
1175
1176
1177
1178
1179
1180
1181
1182
1183
1184
1185
1186
1187
1188
1189
1190
1191
1192
1193
1194
1195
1196
1197
1198
1199
1200
1201
1202
1203
1204
1205
1206
1207
1208
1209
1210
1211
1212
1213
1214
1215
1216
1217
1218
1219
1220
1221
1222
1223
1224
1225
1226
1227
1228
1229
1230
1231
1232
1233
1234
1235
1236
1237
1238
1239
1240
1241
1242
1243
1244
1245
1246
1247
1248
1249
1250
1251
1252
1253
1254
1255
1256
1257
1258
1259
1260
1261
1262
1263
1264
1265
1266
1267
1268
1269
1270
1271
1272
1273
1274
1275
1276
1277
1278
1279
1280
1281
1282
1283
1284
1285
1286
1287
1288
1289
1290
1291
1292
1293
1294
1295
1296
1297
1298
1299
1300
1301
1302
1303
1304
1305
1306
1307
1308
1309
1310
1311
1312
1313
1314
1315
1316
1317
1318
1319
1320
1321
1322
1323
1324
1325
1326
1327
1328
1329
1330
1331
1332
1333
1334
1335
1336
1337
1338
1339
1340
1341
1342
1343
1344
1345
1346
1347
1348
1349
1350
1351
1352
1353
1354
1355
1356
1357
1358
1359
1360
1361
1362
1363
1364
1365
1366
1367
1368
1369
1370
1371
1372
1373
1374
1375
1376
1377
1378
1379
1380
1381
1382
1383
1384
1385
1386
1387
1388
1389
1390
1391
1392
1393
1394
1395
1396
1397
1398
1399
1400
1401
1402
1403
1404
1405
1406
1407
1408
1409
1410
1411
1412
1413
1414
1415
1416
1417
1418
1419
1420
1421
1422
1423
1424
1425
1426
1427
1428
1429
1430
1431
1432
1433
1434
1435
1436
1437
1438
1439
1440
1441
1442
1443
1444
1445
1446
1447
1448
1449
1450
1451
1452
1453
1454
1455
1456
1457
1458
1459
1460
1461
1462
1463
1464
1465
1466
1467
1468
1469
1470
1471
1472
1473
1474
1475
1476
1477
1478
1479
1480
1481
1482
1483
1484
1485
1486
1487
1488
1489
1490
1491
1492
1493
1494
1495
1496
1497
1498
1499
1500
1501
1502
1503
1504
1505
1506
1507
1508
1509
1510
1511
1512
1513
1514
1515
1516
1517
1518
1519
1520
1521
1522
1523
1524
1525
1526
1527
1528
1529
1530
1531
1532
1533
1534
1535
1536
1537
1538
1539
1540
1541
1542
1543
1544
1545
1546
1547
1548
1549
1550
1551
1552
1553
1554
1555
1556
1557
1558
1559
1560
1561
1562
1563
1564
1565
1566
1567
1568
1569
1570
1571
1572
1573
1574
1575
1576
1577
1578
1579
1580
1581
1582
1583
1584
1585
1586
1587
1588
1589
1590
1591
1592
1593
1594
1595
1596
1597
1598
1599
1600
1601
1602
1603
1604
1605
1606
1607
1608
1609
1610
1611
1612
1613
1614
1615
1616
1617
1618
1619
1620
1621
1622
1623
1624
1625
1626
1627
1628
1629
1630
1631
1632
1633
1634
1635
1636
1637
1638
1639
1640
1641
1642
1643
class
BaseModel
metaclass
_model_construction
ModelMetaclass
"""!!! abstract "Usage Documentation"
[Models](../concepts/models.md)
A base class for creating Pydantic models.
Attributes:
__class_vars__: The names of the class variables defined on the model.
__private_attributes__: Metadata about the private attributes of the model.
__signature__: The synthesized `__init__` [`Signature`][inspect.Signature] of the model.
__pydantic_complete__: Whether model building is completed, or if there are still undefined fields.
__pydantic_core_schema__: The core schema of the model.
__pydantic_custom_init__: Whether the model has a custom `__init__` function.
__pydantic_decorators__: Metadata containing the decorators defined on the model.
This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1.
__pydantic_generic_metadata__: Metadata for generic models; contains data used for a similar purpose to
__args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these.
__pydantic_parent_namespace__: Parent namespace of the model, used for automatic rebuilding of models.
__pydantic_post_init__: The name of the post-init method for the model, if defined.
__pydantic_root_model__: Whether the model is a [`RootModel`][pydantic.root_model.RootModel].
__pydantic_serializer__: The `pydantic-core` `SchemaSerializer` used to dump instances of the model.
__pydantic_validator__: The `pydantic-core` `SchemaValidator` used to validate instances of the model.
__pydantic_fields__: A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.
__pydantic_computed_fields__: A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects.
__pydantic_extra__: A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra]
is set to `'allow'`.
__pydantic_fields_set__: The names of fields explicitly set during instantiation.
__pydantic_private__: Values of private attributes set on the model instance.
"""
# Note: Many of the below class vars are defined in the metaclass, but we define them here for type checking purposes.
model_config
ClassVar
ConfigDict
ConfigDict
"""
Configuration for the model, should be a dictionary conforming to [`ConfigDict`][pydantic.config.ConfigDict].
"""
__class_vars__
ClassVar
set
str
"""The names of the class variables defined on the model."""
__private_attributes__
ClassVar
Dict
str
ModelPrivateAttr
# noqa: UP006
"""Metadata about the private attributes of the model."""
__signature__
ClassVar
Signature
"""The synthesized `__init__` [`Signature`][inspect.Signature] of the model."""
__pydantic_complete__
ClassVar
bool
False
"""Whether model building is completed, or if there are still undefined fields."""
__pydantic_core_schema__
ClassVar
CoreSchema
"""The core schema of the model."""
__pydantic_custom_init__
ClassVar
bool
"""Whether the model has a custom `__init__` method."""
# Must be set for `GenerateSchema.model_schema` to work for a plain `BaseModel` annotation.
__pydantic_decorators__
ClassVar
_decorators
DecoratorInfos
_decorators
DecoratorInfos
"""Metadata containing the decorators defined on the model.
This replaces `Model.__validators__` and `Model.__root_validators__` from Pydantic V1."""
__pydantic_generic_metadata__
ClassVar
_generics
PydanticGenericMetadata
"""Metadata for generic models; contains data used for a similar purpose to
__args__, __origin__, __parameters__ in typing-module generics. May eventually be replaced by these."""
__pydantic_parent_namespace__
ClassVar
Dict
str
Any
None
None
# noqa: UP006
"""Parent namespace of the model, used for automatic rebuilding of models."""
__pydantic_post_init__
ClassVar
None
Literal
'model_post_init'
"""The name of the post-init method for the model, if defined."""
__pydantic_root_model__
ClassVar
bool
False
"""Whether the model is a [`RootModel`][pydantic.root_model.RootModel]."""
__pydantic_serializer__
ClassVar
SchemaSerializer
"""The `pydantic-core` `SchemaSerializer` used to dump instances of the model."""
__pydantic_validator__
ClassVar
SchemaValidator
PluggableSchemaValidator
"""The `pydantic-core` `SchemaValidator` used to validate instances of the model."""
__pydantic_fields__
ClassVar
Dict
str
FieldInfo
# noqa: UP006
"""A dictionary of field names and their corresponding [`FieldInfo`][pydantic.fields.FieldInfo] objects.
This replaces `Model.__fields__` from Pydantic V1.
"""
__pydantic_setattr_handlers__
ClassVar
Dict
str
Callable
BaseModel
str
Any
None
]]]
# noqa: UP006
"""`__setattr__` handlers. Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`"""
__pydantic_computed_fields__
ClassVar
Dict
str
ComputedFieldInfo
# noqa: UP006
"""A dictionary of computed field names and their corresponding [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] objects."""
__pydantic_extra__
dict
str
Any
None
_model_construction
NoInitField
init
False
"""A dictionary containing extra values, if [`extra`][pydantic.config.ConfigDict.extra] is set to `'allow'`."""
__pydantic_fields_set__
set
str
_model_construction
NoInitField
init
False
"""The names of fields explicitly set during instantiation."""
__pydantic_private__
dict
str
Any
None
_model_construction
NoInitField
init
False
"""Values of private attributes set on the model instance."""
not
TYPE_CHECKING
# Prevent `BaseModel` from being instantiated directly
# (defined in an `if not TYPE_CHECKING` block for clarity and to avoid type checking errors):
__pydantic_core_schema__
_mock_val_ser
MockCoreSchema
'Pydantic models should inherit from BaseModel, BaseModel cannot be instantiated directly'
code
'base-model-instantiated'
__pydantic_validator__
_mock_val_ser
MockValSer
'Pydantic models should inherit from BaseModel, BaseModel cannot be instantiated directly'
val_or_ser
'validator'
code
'base-model-instantiated'
__pydantic_serializer__
_mock_val_ser
MockValSer
'Pydantic models should inherit from BaseModel, BaseModel cannot be instantiated directly'
val_or_ser
'serializer'
code
'base-model-instantiated'
__slots__
'__dict__'
'__pydantic_fields_set__'
'__pydantic_extra__'
'__pydantic_private__'
def
__init__
self
data
Any
None
"""Create a new model by parsing and validating input data from keyword arguments.
Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be
validated to form a valid model.
`self` is explicitly positional-only to allow `self` as a field name.
"""
# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks
__tracebackhide__
True
validated_self
self
__pydantic_validator__
validate_python
data
self_instance
self
self
not
validated_self
warnings
warn
'A custom validator is returning a value other than `self`.
"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.
'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.'
stacklevel
# The following line sets a flag that we use to determine when `__init__` gets overridden by the user
__init__
__pydantic_base_init__
True
# pyright: ignore[reportFunctionMemberAccess]
@_utils
deprecated_instance_property
@classmethod
def
model_fields
cls
dict
str
FieldInfo
"""A mapping of field names to their respective [`FieldInfo`][pydantic.fields.FieldInfo] instances.
!!! warning
Accessing this attribute from a model instance is deprecated, and will not work in Pydantic V3.
Instead, you should access this attribute from the model class.
"""
return
getattr
cls
'__pydantic_fields__'
{})
@_utils
deprecated_instance_property
@classmethod
def
model_computed_fields
cls
dict
str
ComputedFieldInfo
"""A mapping of computed field names to their respective [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] instances.
!!! warning
Accessing this attribute from a model instance is deprecated, and will not work in Pydantic V3.
Instead, you should access this attribute from the model class.
"""
return
getattr
cls
'__pydantic_computed_fields__'
{})
@property
def
model_extra
self
dict
str
Any
None
"""Get extra fields set during validation.
Returns:
A dictionary of extra fields, or `None` if `config.extra` is not set to `"allow"`.
"""
return
self
__pydantic_extra__
@property
def
model_fields_set
self
set
str
"""Returns the set of fields that have been explicitly set on this model instance.
Returns:
A set of strings representing the fields that have been set,
i.e. that were not filled from defaults.
"""
return
self
__pydantic_fields_set__
@classmethod
def
model_construct
cls
_fields_set
set
str
None
None
values
Any
Self
# noqa: C901
"""Creates a new instance of the `Model` class with validated data.
Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.
Default values are respected, but no other validation is performed.
!!! note
`model_construct()` generally respects the `model_config.extra` setting on the provided model.
That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`
and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.
Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in
an error if extra values are passed, but they will be ignored.
Args:
_fields_set: A set of field names that were originally explicitly set during instantiation. If provided,
this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.
Otherwise, the field names from the `values` argument will be used.
values: Trusted or pre-validated data dictionary.
Returns:
A new instance of the `Model` class with validated data.
"""
cls
__new__
cls
fields_values
dict
str
Any
fields_set
set
for
name
field
cls
__pydantic_fields__
items
():
field
alias
not
None
and
field
alias
values
fields_values
name
values
pop
field
alias
fields_set
add
name
name
not
fields_set
and
field
validation_alias
not
None
validation_aliases
list
str
AliasPath
field
validation_alias
choices
isinstance
field
validation_alias
AliasChoices
else
field
validation_alias
for
alias
validation_aliases
isinstance
alias
str
and
alias
values
fields_values
name
values
pop
alias
fields_set
add
name
break
elif
isinstance
alias
AliasPath
value
alias
search_dict_for_path
values
value
not
PydanticUndefined
fields_values
name
value
fields_set
add
name
break
name
not
fields_set
name
values
fields_values
name
values
pop
name
fields_set
add
name
elif
not
field
is_required
():
fields_values
name
field
get_default
call_default_factory
True
validated_data
fields_values
_fields_set
None
_fields_set
fields_set
_extra
dict
str
Any
None
values
cls
model_config
get
'extra'
'allow'
else
None
_object_setattr
'__dict__'
fields_values
_object_setattr
'__pydantic_fields_set__'
_fields_set
not
cls
__pydantic_root_model__
_object_setattr
'__pydantic_extra__'
_extra
cls
__pydantic_post_init__
model_post_init
None
# update private attributes with values set
hasattr
'__pydantic_private__'
and
__pydantic_private__
not
None
for
values
items
():
__private_attributes__
__pydantic_private__
elif
not
cls
__pydantic_root_model__
# Note: if there are any private attributes, cls.__pydantic_post_init__ would exist
# Since it doesn't, that means that `__pydantic_private__` should be set to None
_object_setattr
'__pydantic_private__'
None
return
def
model_copy
self
update
Mapping
str
Any
None
None
deep
bool
False
Self
"""!!! abstract "Usage Documentation"
[`model_copy`](../concepts/serialization.md#model_copy)
Returns a copy of the model.
!!! note
The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This
might have unexpected side effects if you store anything in it, on top of the model
fields (e.g. the value of [cached properties][functools.cached_property]).
Args:
update: Values to change/add in the new model. Note: the data is not validated
before creating the new model. You should trust this data.
deep: Set to `True` to make a deep copy of the model.
Returns:
New model instance.
"""
copied
self
__deepcopy__
deep
else
self
__copy__
update
self
model_config
get
'extra'
'allow'
for
update
items
():
self
__pydantic_fields__
copied
__dict__
else
copied
__pydantic_extra__
None
copied
__pydantic_extra__
copied
__pydantic_extra__
else
copied
__dict__
update
update
copied
__pydantic_fields_set__
update
update
keys
())
return
copied
def
model_dump
self
mode
Literal
'json'
'python'
str
'python'
include
IncEx
None
None
exclude
IncEx
None
None
context
Any
None
None
by_alias
bool
None
None
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
round_trip
bool
False
warnings
bool
Literal
'none'
'warn'
'error'
True
fallback
Callable
Any
Any
None
None
serialize_as_any
bool
False
dict
str
Any
"""!!! abstract "Usage Documentation"
[`model_dump`](../concepts/serialization.md#modelmodel_dump)
Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.
Args:
mode: The mode in which `to_python` should run.
If mode is 'json', the output will only contain JSON serializable types.
If mode is 'python', the output may contain non-JSON-serializable Python objects.
include: A set of fields to include in the output.
exclude: A set of fields to exclude from the output.
context: Additional context to pass to the serializer.
by_alias: Whether to use the field's alias in the dictionary key if defined.
exclude_unset: Whether to exclude fields that have not been explicitly set.
exclude_defaults: Whether to exclude fields that are set to their default value.
exclude_none: Whether to exclude fields that have a value of `None`.
round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].
warnings: How to handle serialization errors. False/"none" ignores them, True/"warn" logs errors,
"error" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].
fallback: A function to call when an unknown value is encountered. If not provided,
a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.
serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.
Returns:
A dictionary representation of the model.
"""
return
self
__pydantic_serializer__
to_python
self
mode
mode
by_alias
by_alias
include
include
exclude
exclude
context
context
exclude_unset
exclude_unset
exclude_defaults
exclude_defaults
exclude_none
exclude_none
round_trip
round_trip
warnings
warnings
fallback
fallback
serialize_as_any
serialize_as_any
def
model_dump_json
self
indent
int
None
None
include
IncEx
None
None
exclude
IncEx
None
None
context
Any
None
None
by_alias
bool
None
None
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
round_trip
bool
False
warnings
bool
Literal
'none'
'warn'
'error'
True
fallback
Callable
Any
Any
None
None
serialize_as_any
bool
False
str
"""!!! abstract "Usage Documentation"
[`model_dump_json`](../concepts/serialization.md#modelmodel_dump_json)
Generates a JSON representation of the model using Pydantic's `to_json` method.
Args:
indent: Indentation to use in the JSON output. If None is passed, the output will be compact.
include: Field(s) to include in the JSON output.
exclude: Field(s) to exclude from the JSON output.
context: Additional context to pass to the serializer.
by_alias: Whether to serialize using field aliases.
exclude_unset: Whether to exclude fields that have not been explicitly set.
exclude_defaults: Whether to exclude fields that are set to their default value.
exclude_none: Whether to exclude fields that have a value of `None`.
round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].
warnings: How to handle serialization errors. False/"none" ignores them, True/"warn" logs errors,
"error" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].
fallback: A function to call when an unknown value is encountered. If not provided,
a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.
serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.
Returns:
A JSON string representation of the model.
"""
return
self
__pydantic_serializer__
to_json
self
indent
indent
include
include
exclude
exclude
context
context
by_alias
by_alias
exclude_unset
exclude_unset
exclude_defaults
exclude_defaults
exclude_none
exclude_none
round_trip
round_trip
warnings
warnings
fallback
fallback
serialize_as_any
serialize_as_any
decode
@classmethod
def
model_json_schema
cls
by_alias
bool
True
ref_template
str
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
GenerateJsonSchema
mode
JsonSchemaMode
'validation'
dict
str
Any
"""Generates a JSON schema for a model class.
Args:
by_alias: Whether to use attribute aliases or not.
ref_template: The reference template.
schema_generator: To override the logic used to generate the JSON schema, as a subclass of
`GenerateJsonSchema` with your desired modifications
mode: The mode in which to generate the schema.
Returns:
The JSON schema for the given model class.
"""
return
model_json_schema
cls
by_alias
by_alias
ref_template
ref_template
schema_generator
schema_generator
mode
mode
@classmethod
def
model_parametrized_name
cls
params
tuple
type
Any
...
str
"""Compute the class name for parametrizations of generic classes.
This method can be overridden to achieve a custom naming scheme for generic BaseModels.
Args:
params: Tuple of types of the class. Given a generic class
`Model` with 2 type variables and a concrete model `Model[str, int]`,
the value `(str, int)` would be passed to `params`.
Returns:
String representing the new class where `params` are passed to `cls` as type variables.
Raises:
TypeError: Raised when trying to generate concrete names for non-generic models.
"""
not
issubclass
cls
typing
Generic
raise
TypeError
'Concrete names should only be generated for generic models.'
# Any strings received should represent forward references, so we handle them specially below.
# If we eventually move toward wrapping them in a ForwardRef in __class_getitem__ in the future,
# we may be able to remove this special case.
param_names
param
isinstance
param
str
else
_repr
display_as_type
param
for
param
params
params_component
', '
join
param_names
return
cls
__name__
params_component
def
model_post_init
self
context
Any
None
"""Override this method to perform additional initialization after `__init__` and `model_construct`.
This is useful if you want to do some validation that requires the entire model to be initialized.
"""
pass
@classmethod
def
model_rebuild
cls
force
bool
False
raise_errors
bool
True
_parent_namespace_depth
int
_types_namespace
MappingNamespace
None
None
bool
None
"""Try to rebuild the pydantic-core schema for the model.
This may be necessary when one of the annotations is a ForwardRef which could not be resolved during
the initial attempt to build the schema, and automatic rebuilding fails.
Args:
force: Whether to force the rebuilding of the model schema, defaults to `False`.
raise_errors: Whether to raise errors, defaults to `True`.
_parent_namespace_depth: The depth level of the parent namespace, defaults to 2.
_types_namespace: The types namespace, defaults to `None`.
Returns:
Returns `None` if the schema is already "complete" and rebuilding was not required.
If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.
"""
not
force
and
cls
__pydantic_complete__
return
None
for
attr
'__pydantic_core_schema__'
'__pydantic_validator__'
'__pydantic_serializer__'
attr
cls
__dict__
and
not
isinstance
getattr
cls
attr
_mock_val_ser
MockValSer
# Deleting the validator/serializer is necessary as otherwise they can get reused in
# pydantic-core. We do so only if they aren't mock instances, otherwise â as `model_rebuild()`
# isn't thread-safe â concurrent model instantiations can lead to the parent validator being used.
# Same applies for the core schema that can be reused in schema generation.
delattr
cls
attr
cls
__pydantic_complete__
False
_types_namespace
not
None
rebuild_ns
_types_namespace
elif
_parent_namespace_depth
rebuild_ns
_typing_extra
parent_frame_namespace
parent_depth
_parent_namespace_depth
force
True
else
rebuild_ns
parent_ns
_model_construction
unpack_lenient_weakvaluedict
cls
__pydantic_parent_namespace__
ns_resolver
_namespace_utils
NsResolver
parent_namespace
rebuild_ns
parent_ns
not
cls
__pydantic_fields_complete__
typevars_map
_generics
get_model_typevars_map
cls
try
cls
__pydantic_fields__
_fields
rebuild_model_fields
cls
ns_resolver
ns_resolver
typevars_map
typevars_map
except
NameError
exc
PydanticUndefinedAnnotation
from_name_error
_mock_val_ser
set_model_mocks
cls
exc
name
raise_errors
raise
exc
from
not
raise_errors
and
not
cls
__pydantic_fields_complete__
# No need to continue with schema gen, it is guaranteed to fail
return
False
assert
cls
__pydantic_fields_complete__
return
_model_construction
complete_model_class
cls
_config
ConfigWrapper
cls
model_config
check
False
raise_errors
raise_errors
ns_resolver
ns_resolver
@classmethod
def
model_validate
cls
obj
Any
strict
bool
None
None
from_attributes
bool
None
None
context
Any
None
None
by_alias
bool
None
None
by_name
bool
None
None
Self
"""Validate a pydantic model instance.
Args:
obj: The object to validate.
strict: Whether to enforce types strictly.
from_attributes: Whether to extract data from object attributes.
context: Additional context to pass to the validator.
by_alias: Whether to use the field's alias when validating against the provided input data.
by_name: Whether to use the field's name when validating against the provided input data.
Raises:
ValidationError: If the object could not be validated.
Returns:
The validated model instance.
"""
# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks
__tracebackhide__
True
by_alias
False
and
by_name
not
True
raise
PydanticUserError
'At least one of `by_alias` or `by_name` must be set to True.'
code
'validate-by-alias-and-name-false'
return
cls
__pydantic_validator__
validate_python
obj
strict
strict
from_attributes
from_attributes
context
context
by_alias
by_alias
by_name
by_name
@classmethod
def
model_validate_json
cls
json_data
str
bytes
bytearray
strict
bool
None
None
context
Any
None
None
by_alias
bool
None
None
by_name
bool
None
None
Self
"""!!! abstract "Usage Documentation"
[JSON Parsing](../concepts/json.md#json-parsing)
Validate the given JSON data against the Pydantic model.
Args:
json_data: The JSON data to validate.
strict: Whether to enforce types strictly.
context: Extra variables to pass to the validator.
by_alias: Whether to use the field's alias when validating against the provided input data.
by_name: Whether to use the field's name when validating against the provided input data.
Returns:
The validated Pydantic model.
Raises:
ValidationError: If `json_data` is not a JSON string or the object could not be validated.
"""
# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks
__tracebackhide__
True
by_alias
False
and
by_name
not
True
raise
PydanticUserError
'At least one of `by_alias` or `by_name` must be set to True.'
code
'validate-by-alias-and-name-false'
return
cls
__pydantic_validator__
validate_json
json_data
strict
strict
context
context
by_alias
by_alias
by_name
by_name
@classmethod
def
model_validate_strings
cls
obj
Any
strict
bool
None
None
context
Any
None
None
by_alias
bool
None
None
by_name
bool
None
None
Self
"""Validate the given object with string data against the Pydantic model.
Args:
obj: The object containing string data to validate.
strict: Whether to enforce types strictly.
context: Extra variables to pass to the validator.
by_alias: Whether to use the field's alias when validating against the provided input data.
by_name: Whether to use the field's name when validating against the provided input data.
Returns:
The validated Pydantic model.
"""
# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks
__tracebackhide__
True
by_alias
False
and
by_name
not
True
raise
PydanticUserError
'At least one of `by_alias` or `by_name` must be set to True.'
code
'validate-by-alias-and-name-false'
return
cls
__pydantic_validator__
validate_strings
obj
strict
strict
context
context
by_alias
by_alias
by_name
by_name
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
BaseModel
handler
GetCoreSchemaHandler
CoreSchema
# This warning is only emitted when calling `super().__get_pydantic_core_schema__` from a model subclass.
# In the generate schema logic, this method (`BaseModel.__get_pydantic_core_schema__`) is special cased to
# *not* be called if not overridden.
warnings
warn
'The `__get_pydantic_core_schema__` method of the `BaseModel` class is deprecated. If you are calling '
'`super().__get_pydantic_core_schema__` when overriding the method on a Pydantic model, consider using '
'`handler(source)` instead. However, note that overriding this method on models can lead to unexpected '
'side effects.'
PydanticDeprecatedSince211
stacklevel
# Logic copied over from `GenerateSchema._model_schema`:
schema
cls
__dict__
get
'__pydantic_core_schema__'
schema
not
None
and
not
isinstance
schema
_mock_val_ser
MockCoreSchema
return
cls
__pydantic_core_schema__
return
handler
source
@classmethod
def
__get_pydantic_json_schema__
cls
core_schema
CoreSchema
handler
GetJsonSchemaHandler
JsonSchemaValue
"""Hook into generating the model's JSON schema.
Args:
core_schema: A `pydantic-core` CoreSchema.
You can ignore this argument and call the handler with a new CoreSchema,
wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),
or just call the handler with the original schema.
handler: Call into Pydantic's internal JSON schema generation.
This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema
generation fails.
Since this gets called by `BaseModel.model_json_schema` you can override the
`schema_generator` argument to that function to change JSON schema generation globally
for a type.
Returns:
A JSON schema, as a Python object.
"""
return
handler
core_schema
@classmethod
def
__pydantic_init_subclass__
cls
kwargs
Any
None
"""This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`
only after the class is actually fully initialized. In particular, attributes like `model_fields` will
be present when this is called.
This is necessary because `__init_subclass__` will always be called by `type.__new__`,
and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that
`type.__new__` was called in such a manner that the class would already be sufficiently initialized.
This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,
any kwargs passed to the class definition that aren't used internally by pydantic.
Args:
**kwargs: Any keyword arguments passed to the class definition that aren't used internally
by pydantic.
"""
pass
def
__class_getitem__
cls
typevar_values
type
Any
tuple
type
Any
...
type
BaseModel
_forward_ref
PydanticRecursiveRef
cached
_generics
get_cached_generic_type_early
cls
typevar_values
cached
not
None
return
cached
cls
BaseModel
raise
TypeError
'Type parameters should be placed on typing.Generic, not BaseModel'
not
hasattr
cls
'__parameters__'
raise
TypeError
cls
cannot be parametrized because it does not inherit from typing.Generic'
not
cls
__pydantic_generic_metadata__
'parameters'
and
typing
Generic
not
cls
__bases__
raise
TypeError
cls
is not a generic class'
not
isinstance
typevar_values
tuple
typevar_values
typevar_values
# For a model `class Model[T, U, V = int](BaseModel): ...` parametrized with `(str, bool)`,
# this gives us `{T: str, U: bool, V: int}`:
typevars_map
_generics
map_generic_model_arguments
cls
typevar_values
# We also update the provided args to use defaults values (`(str, bool)` becomes `(str, bool, int)`):
typevar_values
tuple
for
typevars_map
values
())
_utils
all_identical
typevars_map
keys
(),
typevars_map
values
())
and
typevars_map
submodel
cls
# if arguments are equal to parameters it's the same object
_generics
set_cached_generic_type
cls
typevar_values
submodel
else
parent_args
cls
__pydantic_generic_metadata__
'args'
not
parent_args
args
typevar_values
else
args
tuple
_generics
replace_types
arg
typevars_map
for
arg
parent_args
origin
cls
__pydantic_generic_metadata__
'origin'
cls
model_name
origin
model_parametrized_name
args
params
tuple
param
None
for
param
_generics
iter_contained_typevars
typevars_map
values
())}
# use dict as ordered set
with
_generics
generic_recursion_self_type
origin
args
maybe_self_type
cached
_generics
get_cached_generic_type_late
cls
typevar_values
origin
args
cached
not
None
return
cached
maybe_self_type
not
None
return
maybe_self_type
# Attempt to rebuild the origin in case new types have been defined
try
# depth 2 gets you above this __class_getitem__ call.
# Note that we explicitly provide the parent ns, otherwise
# `model_rebuild` will use the parent ns no matter if it is the ns of a module.
# We don't want this here, as this has unexpected effects when a model
# is being parametrized during a forward annotation evaluation.
parent_ns
_typing_extra
parent_frame_namespace
parent_depth
origin
model_rebuild
_types_namespace
parent_ns
except
PydanticUndefinedAnnotation
# It's okay if it fails, it just means there are still undefined types
# that could be evaluated later.
pass
submodel
_generics
create_generic_submodel
model_name
origin
args
params
_generics
set_cached_generic_type
cls
typevar_values
submodel
origin
args
return
submodel
def
__copy__
self
Self
"""Returns a shallow copy of the model."""
cls
type
self
cls
__new__
cls
_object_setattr
'__dict__'
copy
self
__dict__
_object_setattr
'__pydantic_extra__'
copy
self
__pydantic_extra__
_object_setattr
'__pydantic_fields_set__'
copy
self
__pydantic_fields_set__
not
hasattr
self
'__pydantic_private__'
self
__pydantic_private__
None
_object_setattr
'__pydantic_private__'
None
else
_object_setattr
'__pydantic_private__'
for
self
__pydantic_private__
items
not
PydanticUndefined
return
def
__deepcopy__
self
memo
dict
int
Any
None
None
Self
"""Returns a deep copy of the model."""
cls
type
self
cls
__new__
cls
_object_setattr
'__dict__'
deepcopy
self
__dict__
memo
memo
_object_setattr
'__pydantic_extra__'
deepcopy
self
__pydantic_extra__
memo
memo
# This next line doesn't need a deepcopy because __pydantic_fields_set__ is a set[str],
# and attempting a deepcopy would be marginally slower.
_object_setattr
'__pydantic_fields_set__'
copy
self
__pydantic_fields_set__
not
hasattr
self
'__pydantic_private__'
self
__pydantic_private__
None
_object_setattr
'__pydantic_private__'
None
else
_object_setattr
'__pydantic_private__'
deepcopy
for
self
__pydantic_private__
items
not
PydanticUndefined
memo
memo
return
not
TYPE_CHECKING
# We put `__getattr__` in a non-TYPE_CHECKING block because otherwise, mypy allows arbitrary attribute access
# The same goes for __setattr__ and __delattr__, see: https://github.com/pydantic/pydantic/issues/8643
def
__getattr__
self
item
str
Any
private_attributes
object
__getattribute__
self
'__private_attributes__'
item
private_attributes
attribute
private_attributes
item
hasattr
attribute
'__get__'
return
attribute
__get__
self
type
self
# type: ignore
try
# Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
return
self
__pydantic_private__
item
# type: ignore
except
KeyError
exc
raise
AttributeError
type
self
__name__
!r}
object has no attribute
item
!r}
from
exc
else
# `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
# See `BaseModel.__repr_args__` for more details
try
pydantic_extra
object
__getattribute__
self
'__pydantic_extra__'
except
AttributeError
pydantic_extra
None
pydantic_extra
try
return
pydantic_extra
item
except
KeyError
exc
raise
AttributeError
type
self
__name__
!r}
object has no attribute
item
!r}
from
exc
else
hasattr
self
__class__
item
return
super
__getattribute__
item
# Raises AttributeError if appropriate
else
# this is the current error
raise
AttributeError
type
self
__name__
!r}
object has no attribute
item
!r}
def
__setattr__
self
name
str
value
Any
None
setattr_handler
self
__pydantic_setattr_handlers__
get
name
not
None
setattr_handler
self
name
value
# if None is returned from _setattr_handler, the attribute was set directly
elif
setattr_handler
self
_setattr_handler
name
value
not
None
setattr_handler
self
name
value
# call here to not memo on possibly unknown fields
self
__pydantic_setattr_handlers__
name
setattr_handler
# memoize the handler for faster access
def
_setattr_handler
self
name
str
value
Any
Callable
BaseModel
str
Any
None
None
"""Get a handler for setting an attribute on the model instance.
Returns:
A handler for setting an attribute on the model instance. Used for memoization of the handler.
Memoizing the handlers leads to a dramatic performance improvement in `__setattr__`
Returns `None` when memoization is not safe, then the attribute is set directly.
"""
cls
self
__class__
name
cls
__class_vars__
raise
AttributeError
name
!r}
is a ClassVar of `
cls
__name__
` and cannot be set on an instance. '
'If you want to set a value on the class, use `
cls
__name__
name
= value`.'
elif
not
_fields
is_valid_field_name
name
attribute
cls
__private_attributes__
get
name
not
None
hasattr
attribute
'__set__'
return
lambda
model
_name
val
attribute
__set__
model
val
else
return
_SIMPLE_SETATTR_HANDLERS
'private'
else
_object_setattr
self
name
value
return
None
# Can not return memoized handler with possibly freeform attr names
attr
getattr
cls
name
None
# NOTE: We currently special case properties and `cached_property`, but we might need
# to generalize this to all data/non-data descriptors at some point. For non-data descriptors
# (such as `cached_property`), it isn't obvious though. `cached_property` caches the value
# to the instance's `__dict__`, but other non-data descriptors might do things differently.
isinstance
attr
cached_property
return
_SIMPLE_SETATTR_HANDLERS
'cached_property'
_check_frozen
cls
name
value
# We allow properties to be set only on non frozen models for now (to match dataclasses).
# This can be changed if it ever gets requested.
isinstance
attr
property
return
lambda
model
_name
val
attr
__set__
model
val
elif
cls
model_config
get
'validate_assignment'
return
_SIMPLE_SETATTR_HANDLERS
'validate_assignment'
elif
name
not
cls
__pydantic_fields__
cls
model_config
get
'extra'
'allow'
# TODO - matching error
raise
ValueError
cls
__name__
" object has no field "
name
elif
attr
None
# attribute does not exist, so put it in extra
self
__pydantic_extra__
name
value
return
None
# Can not return memoized handler with possibly freeform attr names
else
# attribute _does_ exist, and was not in extra, so update it
return
_SIMPLE_SETATTR_HANDLERS
'extra_known'
else
return
_SIMPLE_SETATTR_HANDLERS
'model_field'
def
__delattr__
self
item
str
Any
cls
self
__class__
item
self
__private_attributes__
attribute
self
__private_attributes__
item
hasattr
attribute
'__delete__'
attribute
__delete__
self
# type: ignore
return
try
# Note: self.__pydantic_private__ cannot be None if self.__private_attributes__ has items
del
self
__pydantic_private__
item
# type: ignore
return
except
KeyError
exc
raise
AttributeError
cls
__name__
!r}
object has no attribute
item
!r}
from
exc
# Allow cached properties to be deleted (even if the class is frozen):
attr
getattr
cls
item
None
isinstance
attr
cached_property
return
object
__delattr__
self
item
_check_frozen
cls
name
item
value
None
item
self
__pydantic_fields__
object
__delattr__
self
item
elif
self
__pydantic_extra__
not
None
and
item
self
__pydantic_extra__
del
self
__pydantic_extra__
item
else
try
object
__delattr__
self
item
except
AttributeError
raise
AttributeError
type
self
__name__
!r}
object has no attribute
item
!r}
# Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by
# type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:
def
__replace__
self
changes
Any
Self
return
self
model_copy
update
changes
def
__getstate__
self
dict
Any
Any
private
self
__pydantic_private__
private
private
for
private
items
not
PydanticUndefined
return
'__dict__'
self
__dict__
'__pydantic_extra__'
self
__pydantic_extra__
'__pydantic_fields_set__'
self
__pydantic_fields_set__
'__pydantic_private__'
private
def
__setstate__
self
state
dict
Any
Any
None
_object_setattr
self
'__pydantic_fields_set__'
state
get
'__pydantic_fields_set__'
{}))
_object_setattr
self
'__pydantic_extra__'
state
get
'__pydantic_extra__'
{}))
_object_setattr
self
'__pydantic_private__'
state
get
'__pydantic_private__'
{}))
_object_setattr
self
'__dict__'
state
get
'__dict__'
{}))
not
TYPE_CHECKING
def
__eq__
self
other
Any
bool
isinstance
other
BaseModel
# When comparing instances of generic types for equality, as long as all field values are equal,
# only require their generic origin types to be equal, rather than exact type equality.
# This prevents headaches like MyGeneric(x=1) != MyGeneric[Any](x=1).
self_type
self
__pydantic_generic_metadata__
'origin'
self
__class__
other_type
other
__pydantic_generic_metadata__
'origin'
other
__class__
# Perform common checks first
not
self_type
other_type
and
getattr
self
'__pydantic_private__'
None
getattr
other
'__pydantic_private__'
None
and
self
__pydantic_extra__
other
__pydantic_extra__
return
False
# We only want to compare pydantic fields but ignoring fields is costly.
# We'll perform a fast check first, and fallback only when needed
# See GH-7444 and GH-7825 for rationale and a performance benchmark
# First, do the fast (and sometimes faulty) __dict__ comparison
self
__dict__
other
__dict__
# If the check above passes, then pydantic fields are equal, we can return early
return
True
# We don't want to trigger unnecessary costly filtering of __dict__ on all unequal objects, so we return
# early if there are no keys to ignore (we would just return False later on anyway)
model_fields
type
self
__pydantic_fields__
keys
self
__dict__
keys
model_fields
and
other
__dict__
keys
model_fields
return
False
# If we reach here, there are non-pydantic-fields keys, mapped to unequal values, that we need to ignore
# Resort to costly filtering of the __dict__ objects
# We use operator.itemgetter because it is much faster than dict comprehensions
# NOTE: Contrary to standard python class and instances, when the Model class has a default value for an
# attribute and the model instance doesn't have a corresponding attribute, accessing the missing attribute
# raises an error in BaseModel.__getattr__ instead of returning the class attribute
# So we can use operator.itemgetter() instead of operator.attrgetter()
getter
operator
itemgetter
model_fields
model_fields
else
lambda
_utils
_SENTINEL
try
return
getter
self
__dict__
getter
other
__dict__
except
KeyError
# In rare cases (such as when using the deprecated BaseModel.copy() method),
# the __dict__ may not contain all model fields, which is how we can get here.
# getter(self.__dict__) is much faster than any 'safe' method that accounts
# for missing keys, and wrapping it in a `try` doesn't slow things down much
# in the common case.
self_fields_proxy
_utils
SafeGetItemProxy
self
__dict__
other_fields_proxy
_utils
SafeGetItemProxy
other
__dict__
return
getter
self_fields_proxy
getter
other_fields_proxy
# other instance is not a BaseModel
else
return
NotImplemented
# delegate to the other item in the comparison
TYPE_CHECKING
# We put `__init_subclass__` in a TYPE_CHECKING block because, even though we want the type-checking benefits
# described in the signature of `__init_subclass__` below, we don't want to modify the default behavior of
# subclass initialization.
def
__init_subclass__
cls
kwargs
Unpack
ConfigDict
]):
"""This signature is included purely to help type-checkers check arguments to class declaration, which
provides a way to conveniently set model_config key/value pairs.
```python
from pydantic import BaseModel
class MyModel(BaseModel, extra='allow'): ...
```
However, this may be deceiving, since the _actual_ calls to `__init_subclass__` will not receive any
of the config arguments, and will only receive any keyword arguments passed during class initialization
that are _not_ expected keys in ConfigDict. (This is due to the way `ModelMetaclass.__new__` works.)
Args:
**kwargs: Keyword arguments passed to the class definition, which set model_config
Note:
You may want to override `__pydantic_init_subclass__` instead, which behaves similarly but is called
*after* the class is fully initialized.
"""
def
__iter__
self
TupleGenerator
"""So `dict(model)` works."""
yield from
for
self
__dict__
items
not
startswith
'_'
extra
self
__pydantic_extra__
extra
yield from
extra
items
def
__repr__
self
str
return
self
__repr_name__
self
__repr_str__
", "
def
__repr_args__
self
_repr
ReprArgs
# Eagerly create the repr of computed fields, as this may trigger access of cached properties and as such
# modify the instance's `__dict__`. If we don't do it now, it could happen when iterating over the `__dict__`
# below if the instance happens to be referenced in a field, and would modify the `__dict__` size *during* iteration.
computed_fields_repr_args
getattr
self
for
self
__pydantic_computed_fields__
items
repr
for
self
__dict__
items
():
field
self
__pydantic_fields__
get
field
and
field
repr
not
self
yield
else
yield
self
__repr_recursion__
# `__pydantic_extra__` can fail to be set if the model is not yet fully initialized.
# This can happen if a `ValidationError` is raised during initialization and the instance's
# repr is generated as part of the exception handling. Therefore, we use `getattr` here
# with a fallback, even though the type hints indicate the attribute will always be present.
try
pydantic_extra
object
__getattribute__
self
'__pydantic_extra__'
except
AttributeError
pydantic_extra
None
pydantic_extra
not
None
yield from
for
pydantic_extra
items
())
yield from
computed_fields_repr_args
# take logic from `_repr.Representation` without the side effects of inheritance, see #5740
__repr_name__
_repr
Representation
__repr_name__
__repr_recursion__
_repr
Representation
__repr_recursion__
__repr_str__
_repr
Representation
__repr_str__
__pretty__
_repr
Representation
__pretty__
__rich_repr__
_repr
Representation
__rich_repr__
def
__str__
self
str
return
self
__repr_str__
' '
# ##### Deprecated methods from v1 #####
@property
@typing_extensions
deprecated
'The `__fields__` attribute is deprecated, use `model_fields` instead.'
category
None
def
__fields__
self
dict
str
FieldInfo
warnings
warn
'The `__fields__` attribute is deprecated, use `model_fields` instead.'
category
PydanticDeprecatedSince20
stacklevel
return
getattr
type
self
'__pydantic_fields__'
{})
@property
@typing_extensions
deprecated
'The `__fields_set__` attribute is deprecated, use `model_fields_set` instead.'
category
None
def
__fields_set__
self
set
str
warnings
warn
'The `__fields_set__` attribute is deprecated, use `model_fields_set` instead.'
category
PydanticDeprecatedSince20
stacklevel
return
self
__pydantic_fields_set__
@typing_extensions
deprecated
'The `dict` method is deprecated; use `model_dump` instead.'
category
None
def
dict
# noqa: D102
self
include
IncEx
None
None
exclude
IncEx
None
None
by_alias
bool
False
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
Dict
str
Any
# noqa UP006
warnings
warn
'The `dict` method is deprecated; use `model_dump` instead.'
category
PydanticDeprecatedSince20
stacklevel
return
self
model_dump
include
include
exclude
exclude
by_alias
by_alias
exclude_unset
exclude_unset
exclude_defaults
exclude_defaults
exclude_none
exclude_none
@typing_extensions
deprecated
'The `json` method is deprecated; use `model_dump_json` instead.'
category
None
def
json
# noqa: D102
self
include
IncEx
None
None
exclude
IncEx
None
None
by_alias
bool
False
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
encoder
Callable
Any
Any
None
PydanticUndefined
# type: ignore[assignment]
models_as_dict
bool
PydanticUndefined
# type: ignore[assignment]
dumps_kwargs
Any
str
warnings
warn
'The `json` method is deprecated; use `model_dump_json` instead.'
category
PydanticDeprecatedSince20
stacklevel
encoder
not
PydanticUndefined
raise
TypeError
'The `encoder` argument is no longer supported; use field serializers instead.'
models_as_dict
not
PydanticUndefined
raise
TypeError
'The `models_as_dict` argument is no longer supported; use a model serializer instead.'
dumps_kwargs
raise
TypeError
'`dumps_kwargs` keyword arguments are no longer supported.'
return
self
model_dump_json
include
include
exclude
exclude
by_alias
by_alias
exclude_unset
exclude_unset
exclude_defaults
exclude_defaults
exclude_none
exclude_none
@classmethod
@typing_extensions
deprecated
'The `parse_obj` method is deprecated; use `model_validate` instead.'
category
None
def
parse_obj
cls
obj
Any
Self
# noqa: D102
warnings
warn
'The `parse_obj` method is deprecated; use `model_validate` instead.'
category
PydanticDeprecatedSince20
stacklevel
return
cls
model_validate
obj
@classmethod
@typing_extensions
deprecated
'The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, '
'otherwise load the data then use `model_validate` instead.'
category
None
def
parse_raw
# noqa: D102
cls
str
bytes
content_type
str
None
None
encoding
str
'utf8'
proto
DeprecatedParseProtocol
None
None
allow_pickle
bool
False
Self
# pragma: no cover
warnings
warn
'The `parse_raw` method is deprecated; if your data is JSON use `model_validate_json`, '
'otherwise load the data then use `model_validate` instead.'
category
PydanticDeprecatedSince20
stacklevel
from
.deprecated
import
parse
try
obj
parse
load_str_bytes
proto
proto
content_type
content_type
encoding
encoding
allow_pickle
allow_pickle
except
ValueError
TypeError
exc
import
json
# try to match V1
isinstance
exc
UnicodeDecodeError
type_str
'value_error.unicodedecode'
elif
isinstance
exc
json
JSONDecodeError
type_str
'value_error.jsondecode'
elif
isinstance
exc
ValueError
type_str
'value_error'
else
type_str
'type_error'
# ctx is missing here, but since we've added `input` to the error, we're not pretending it's the same
error
pydantic_core
InitErrorDetails
# The type: ignore on the next line is to ignore the requirement of LiteralString
'type'
pydantic_core
PydanticCustomError
type_str
str
exc
)),
# type: ignore
'loc'
'__root__'
,),
'input'
raise
pydantic_core
ValidationError
from_exception_data
cls
__name__
error
return
cls
model_validate
obj
@classmethod
@typing_extensions
deprecated
'The `parse_file` method is deprecated; load the data from file, then if your data is JSON '
'use `model_validate_json`, otherwise `model_validate` instead.'
category
None
def
parse_file
# noqa: D102
cls
path
str
Path
content_type
str
None
None
encoding
str
'utf8'
proto
DeprecatedParseProtocol
None
None
allow_pickle
bool
False
Self
warnings
warn
'The `parse_file` method is deprecated; load the data from file, then if your data is JSON '
'use `model_validate_json`, otherwise `model_validate` instead.'
category
PydanticDeprecatedSince20
stacklevel
from
.deprecated
import
parse
obj
parse
load_file
path
proto
proto
content_type
content_type
encoding
encoding
allow_pickle
allow_pickle
return
cls
parse_obj
obj
@classmethod
@typing_extensions
deprecated
'The `from_orm` method is deprecated; set '
"`model_config['from_attributes']=True` and use `model_validate` instead."
category
None
def
from_orm
cls
obj
Any
Self
# noqa: D102
warnings
warn
'The `from_orm` method is deprecated; set '
"`model_config['from_attributes']=True` and use `model_validate` instead."
category
PydanticDeprecatedSince20
stacklevel
not
cls
model_config
get
'from_attributes'
None
raise
PydanticUserError
'You must set the config attribute `from_attributes=True` to use from_orm'
code
None
return
cls
model_validate
obj
@classmethod
@typing_extensions
deprecated
'The `construct` method is deprecated; use `model_construct` instead.'
category
None
def
construct
cls
_fields_set
set
str
None
None
values
Any
Self
# noqa: D102
warnings
warn
'The `construct` method is deprecated; use `model_construct` instead.'
category
PydanticDeprecatedSince20
stacklevel
return
cls
model_construct
_fields_set
_fields_set
values
@typing_extensions
deprecated
'The `copy` method is deprecated; use `model_copy` instead. '
'See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`.'
category
None
def
copy
self
include
AbstractSetIntStr
MappingIntStrAny
None
None
exclude
AbstractSetIntStr
MappingIntStrAny
None
None
update
Dict
str
Any
None
None
# noqa UP006
deep
bool
False
Self
# pragma: no cover
"""Returns a copy of the model.
!!! warning "Deprecated"
This method is now deprecated; use `model_copy` instead.
If you need `include` or `exclude`, use:
```python {test="skip" lint="skip"}
data = self.model_dump(include=include, exclude=exclude, round_trip=True)
data = {**data, **(update or {})}
copied = self.model_validate(data)
```
Args:
include: Optional set or mapping specifying which fields to include in the copied model.
exclude: Optional set or mapping specifying which fields to exclude in the copied model.
update: Optional dictionary of field-value pairs to override field values in the copied model.
deep: If True, the values of fields that are Pydantic models will be deep-copied.
Returns:
A copy of the model with included, excluded and updated fields as specified.
"""
warnings
warn
'The `copy` method is deprecated; use `model_copy` instead. '
'See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`.'
category
PydanticDeprecatedSince20
stacklevel
from
.deprecated
import
copy_internals
values
dict
copy_internals
_iter
self
to_dict
False
by_alias
False
include
include
exclude
exclude
exclude_unset
False
update
{}),
self
__pydantic_private__
None
private
None
else
private
for
self
__pydantic_private__
items
not
PydanticUndefined
self
__pydantic_extra__
None
extra
dict
str
Any
None
None
else
extra
self
__pydantic_extra__
copy
for
list
self
__pydantic_extra__
not
values
# k was in the exclude
extra
pop
for
list
values
self
__pydantic_extra__
# k must have come from extra
extra
values
pop
# new `__pydantic_fields_set__` can have unset optional fields with a set value in `update` kwarg
update
fields_set
self
__pydantic_fields_set__
update
keys
else
fields_set
set
self
__pydantic_fields_set__
# removing excluded fields from `__pydantic_fields_set__`
exclude
fields_set
set
exclude
return
copy_internals
_copy_and_set_values
self
values
fields_set
extra
private
deep
deep
@classmethod
@typing_extensions
deprecated
'The `schema` method is deprecated; use `model_json_schema` instead.'
category
None
def
schema
# noqa: D102
cls
by_alias
bool
True
ref_template
str
DEFAULT_REF_TEMPLATE
Dict
str
Any
# noqa UP006
warnings
warn
'The `schema` method is deprecated; use `model_json_schema` instead.'
category
PydanticDeprecatedSince20
stacklevel
return
cls
model_json_schema
by_alias
by_alias
ref_template
ref_template
@classmethod
@typing_extensions
deprecated
'The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead.'
category
None
def
schema_json
# noqa: D102
cls
by_alias
bool
True
ref_template
str
DEFAULT_REF_TEMPLATE
dumps_kwargs
Any
str
# pragma: no cover
warnings
warn
'The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead.'
category
PydanticDeprecatedSince20
stacklevel
import
json
from
.deprecated.json
import
pydantic_encoder
return
json
dumps
cls
model_json_schema
by_alias
by_alias
ref_template
ref_template
default
pydantic_encoder
dumps_kwargs
@classmethod
@typing_extensions
deprecated
'The `validate` method is deprecated; use `model_validate` instead.'
category
None
def
validate
cls
value
Any
Self
# noqa: D102
warnings
warn
'The `validate` method is deprecated; use `model_validate` instead.'
category
PydanticDeprecatedSince20
stacklevel
return
cls
model_validate
value
@classmethod
@typing_extensions
deprecated
'The `update_forward_refs` method is deprecated; use `model_rebuild` instead.'
category
None
def
update_forward_refs
cls
localns
Any
None
# noqa: D102
warnings
warn
'The `update_forward_refs` method is deprecated; use `model_rebuild` instead.'
category
PydanticDeprecatedSince20
stacklevel
localns
# pragma: no cover
raise
TypeError
'`localns` arguments are not longer accepted.'
cls
model_rebuild
force
True
@typing_extensions
deprecated
'The private method `_iter` will be removed and should no longer be used.'
category
None
def
_iter
self
args
Any
kwargs
Any
Any
warnings
warn
'The private method `_iter` will be removed and should no longer be used.'
category
PydanticDeprecatedSince20
stacklevel
from
.deprecated
import
copy_internals
return
copy_internals
_iter
self
args
kwargs
@typing_extensions
deprecated
'The private method `_copy_and_set_values` will be removed and should no longer be used.'
category
None
def
_copy_and_set_values
self
args
Any
kwargs
Any
Any
warnings
warn
'The private method `_copy_and_set_values` will be removed and should no longer be used.'
category
PydanticDeprecatedSince20
stacklevel
from
.deprecated
import
copy_internals
return
copy_internals
_copy_and_set_values
self
args
kwargs
@classmethod
@typing_extensions
deprecated
'The private method `_get_value` will be removed and should no longer be used.'
category
None
def
_get_value
cls
args
Any
kwargs
Any
Any
warnings
warn
'The private method `_get_value` will be removed and should no longer be used.'
category
PydanticDeprecatedSince20
stacklevel
from
.deprecated
import
copy_internals
return
copy_internals
_get_value
cls
args
kwargs
@typing_extensions
deprecated
'The private method `_calculate_keys` will be removed and should no longer be used.'
category
None
def
_calculate_keys
self
args
Any
kwargs
Any
Any
warnings
warn
'The private method `_calculate_keys` will be removed and should no longer be used.'
category
PydanticDeprecatedSince20
stacklevel
from
.deprecated
import
copy_internals
return
copy_internals
_calculate_keys
self
args
kwargs
__init__
__init__
data
Any
None
Raises
ValidationError
if the input data cannot be
validated to form a valid model.
self
is explicitly positional-only to allow
self
as a field name.
Source code in
pydantic/main.py
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
def
__init__
self
data
Any
None
"""Create a new model by parsing and validating input data from keyword arguments.
Raises [`ValidationError`][pydantic_core.ValidationError] if the input data cannot be
validated to form a valid model.
`self` is explicitly positional-only to allow `self` as a field name.
"""
# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks
__tracebackhide__
True
validated_self
self
__pydantic_validator__
validate_python
data
self_instance
self
self
not
validated_self
warnings
warn
'A custom validator is returning a value other than `self`.
"Returning anything other than `self` from a top level model validator isn't supported when validating via `__init__`.
'See the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.'
stacklevel
model_config
class-attribute
model_config
ConfigDict
ConfigDict
Configuration for the model, should be a dictionary conforming to
ConfigDict
model_fields
classmethod
model_fields
dict
str
FieldInfo
A mapping of field names to their respective
FieldInfo
instances.
Warning
Accessing this attribute from a model instance is deprecated, and will not work in Pydantic V3.
Instead, you should access this attribute from the model class.
Source code in
pydantic/main.py
265
266
267
268
269
270
271
272
273
274
@_utils
deprecated_instance_property
@classmethod
def
model_fields
cls
dict
str
FieldInfo
"""A mapping of field names to their respective [`FieldInfo`][pydantic.fields.FieldInfo] instances.
!!! warning
Accessing this attribute from a model instance is deprecated, and will not work in Pydantic V3.
Instead, you should access this attribute from the model class.
"""
return
getattr
cls
'__pydantic_fields__'
{})
model_computed_fields
classmethod
model_computed_fields
dict
str
ComputedFieldInfo
A mapping of computed field names to their respective
ComputedFieldInfo
instances.
Warning
Accessing this attribute from a model instance is deprecated, and will not work in Pydantic V3.
Instead, you should access this attribute from the model class.
Source code in
pydantic/main.py
276
277
278
279
280
281
282
283
284
285
@_utils
deprecated_instance_property
@classmethod
def
model_computed_fields
cls
dict
str
ComputedFieldInfo
"""A mapping of computed field names to their respective [`ComputedFieldInfo`][pydantic.fields.ComputedFieldInfo] instances.
!!! warning
Accessing this attribute from a model instance is deprecated, and will not work in Pydantic V3.
Instead, you should access this attribute from the model class.
"""
return
getattr
cls
'__pydantic_computed_fields__'
{})
__pydantic_core_schema__
class-attribute
__pydantic_core_schema__
CoreSchema
The core schema of the model.
model_extra
property
model_extra
dict
str
Any
None
Get extra fields set during validation.
Returns:
Type
Description
dict
str
Any
] | None
A dictionary of extra fields, or
None
config.extra
is not set to
"allow"
model_fields_set
property
model_fields_set
set
str
Returns the set of fields that have been explicitly set on this model instance.
Returns:
Type
Description
set
str
A set of strings representing the fields that have been set,
i.e. that were not filled from defaults.
model_construct
classmethod
model_construct
_fields_set
set
str
None
None
values
Any
Self
Creates a new instance of the
Model
class with validated data.
Creates a new model setting
__dict__
and
__pydantic_fields_set__
from trusted or pre-validated data.
Default values are respected, but no other validation is performed.
Note
model_construct()
generally respects the
model_config.extra
setting on the provided model.
That is, if
model_config.extra == 'allow'
, then all extra passed values are added to the model instance's
__dict__
and
__pydantic_extra__
fields. If
model_config.extra == 'ignore'
(the default), then all extra passed values are ignored.
Because no validation is performed with a call to
model_construct()
, having
model_config.extra == 'forbid'
does not result in
an error if extra values are passed, but they will be ignored.
Parameters:
Name
Type
Description
Default
_fields_set
set
str
] | None
A set of field names that were originally explicitly set during instantiation. If provided,
this is directly used for the
model_fields_set
attribute.
Otherwise, the field names from the
values
argument will be used.
None
values
Any
Trusted or pre-validated data dictionary.
Returns:
Type
Description
Self
A new instance of the
Model
class with validated data.
Source code in
pydantic/main.py
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
@classmethod
def
model_construct
cls
_fields_set
set
str
None
None
values
Any
Self
# noqa: C901
"""Creates a new instance of the `Model` class with validated data.
Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.
Default values are respected, but no other validation is performed.
!!! note
`model_construct()` generally respects the `model_config.extra` setting on the provided model.
That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`
and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.
Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in
an error if extra values are passed, but they will be ignored.
Args:
_fields_set: A set of field names that were originally explicitly set during instantiation. If provided,
this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.
Otherwise, the field names from the `values` argument will be used.
values: Trusted or pre-validated data dictionary.
Returns:
A new instance of the `Model` class with validated data.
"""
cls
__new__
cls
fields_values
dict
str
Any
fields_set
set
for
name
field
cls
__pydantic_fields__
items
():
field
alias
not
None
and
field
alias
values
fields_values
name
values
pop
field
alias
fields_set
add
name
name
not
fields_set
and
field
validation_alias
not
None
validation_aliases
list
str
AliasPath
field
validation_alias
choices
isinstance
field
validation_alias
AliasChoices
else
field
validation_alias
for
alias
validation_aliases
isinstance
alias
str
and
alias
values
fields_values
name
values
pop
alias
fields_set
add
name
break
elif
isinstance
alias
AliasPath
value
alias
search_dict_for_path
values
value
not
PydanticUndefined
fields_values
name
value
fields_set
add
name
break
name
not
fields_set
name
values
fields_values
name
values
pop
name
fields_set
add
name
elif
not
field
is_required
():
fields_values
name
field
get_default
call_default_factory
True
validated_data
fields_values
_fields_set
None
_fields_set
fields_set
_extra
dict
str
Any
None
values
cls
model_config
get
'extra'
'allow'
else
None
_object_setattr
'__dict__'
fields_values
_object_setattr
'__pydantic_fields_set__'
_fields_set
not
cls
__pydantic_root_model__
_object_setattr
'__pydantic_extra__'
_extra
cls
__pydantic_post_init__
model_post_init
None
# update private attributes with values set
hasattr
'__pydantic_private__'
and
__pydantic_private__
not
None
for
values
items
():
__private_attributes__
__pydantic_private__
elif
not
cls
__pydantic_root_model__
# Note: if there are any private attributes, cls.__pydantic_post_init__ would exist
# Since it doesn't, that means that `__pydantic_private__` should be set to None
_object_setattr
'__pydantic_private__'
None
return
model_copy
model_copy
update
Mapping
str
Any
None
None
deep
bool
False
Self
Usage Documentation
model_copy
Returns a copy of the model.
Note
The underlying instance's
__dict__
attribute is copied. This
might have unexpected side effects if you store anything in it, on top of the model
fields (e.g. the value of
cached properties
Parameters:
Name
Type
Description
Default
update
Mapping
str
Any
] | None
Values to change/add in the new model. Note: the data is not validated
before creating the new model. You should trust this data.
None
deep
bool
Set to
True
to make a deep copy of the model.
False
Returns:
Type
Description
Self
New model instance.
Source code in
pydantic/main.py
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
def
model_copy
self
update
Mapping
str
Any
None
None
deep
bool
False
Self
"""!!! abstract "Usage Documentation"
[`model_copy`](../concepts/serialization.md#model_copy)
Returns a copy of the model.
!!! note
The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This
might have unexpected side effects if you store anything in it, on top of the model
fields (e.g. the value of [cached properties][functools.cached_property]).
Args:
update: Values to change/add in the new model. Note: the data is not validated
before creating the new model. You should trust this data.
deep: Set to `True` to make a deep copy of the model.
Returns:
New model instance.
"""
copied
self
__deepcopy__
deep
else
self
__copy__
update
self
model_config
get
'extra'
'allow'
for
update
items
():
self
__pydantic_fields__
copied
__dict__
else
copied
__pydantic_extra__
None
copied
__pydantic_extra__
copied
__pydantic_extra__
else
copied
__dict__
update
update
copied
__pydantic_fields_set__
update
update
keys
())
return
copied
model_dump
model_dump
mode
Literal
"json"
"python"
str
"python"
include
IncEx
None
None
exclude
IncEx
None
None
context
Any
None
None
by_alias
bool
None
None
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
round_trip
bool
False
warnings
bool
Literal
"none"
"warn"
"error"
True
fallback
Callable
Any
Any
None
None
serialize_as_any
bool
False
dict
str
Any
Usage Documentation
model_dump
Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.
Parameters:
Name
Type
Description
Default
mode
Literal
['json', 'python'] |
str
The mode in which
to_python
should run.
If mode is 'json', the output will only contain JSON serializable types.
If mode is 'python', the output may contain non-JSON-serializable Python objects.
'python'
include
IncEx
| None
A set of fields to include in the output.
None
exclude
IncEx
| None
A set of fields to exclude from the output.
None
context
Any
| None
Additional context to pass to the serializer.
None
by_alias
bool
| None
Whether to use the field's alias in the dictionary key if defined.
None
exclude_unset
bool
Whether to exclude fields that have not been explicitly set.
False
exclude_defaults
bool
Whether to exclude fields that are set to their default value.
False
exclude_none
bool
Whether to exclude fields that have a value of
None
False
round_trip
bool
If True, dumped values should be valid as input for non-idempotent types such as Json[T].
False
warnings
bool
Literal
['none', 'warn', 'error']
How to handle serialization errors. False/"none" ignores them, True/"warn" logs errors,
"error" raises a
PydanticSerializationError
True
fallback
Callable
Any
Any
] | None
A function to call when an unknown value is encountered. If not provided,
PydanticSerializationError
error is raised.
None
serialize_as_any
bool
Whether to serialize fields with duck-typing serialization behavior.
False
Returns:
Type
Description
dict
str
Any
A dictionary representation of the model.
Source code in
pydantic/main.py
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
def
model_dump
self
mode
Literal
'json'
'python'
str
'python'
include
IncEx
None
None
exclude
IncEx
None
None
context
Any
None
None
by_alias
bool
None
None
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
round_trip
bool
False
warnings
bool
Literal
'none'
'warn'
'error'
True
fallback
Callable
Any
Any
None
None
serialize_as_any
bool
False
dict
str
Any
"""!!! abstract "Usage Documentation"
[`model_dump`](../concepts/serialization.md#modelmodel_dump)
Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.
Args:
mode: The mode in which `to_python` should run.
If mode is 'json', the output will only contain JSON serializable types.
If mode is 'python', the output may contain non-JSON-serializable Python objects.
include: A set of fields to include in the output.
exclude: A set of fields to exclude from the output.
context: Additional context to pass to the serializer.
by_alias: Whether to use the field's alias in the dictionary key if defined.
exclude_unset: Whether to exclude fields that have not been explicitly set.
exclude_defaults: Whether to exclude fields that are set to their default value.
exclude_none: Whether to exclude fields that have a value of `None`.
round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].
warnings: How to handle serialization errors. False/"none" ignores them, True/"warn" logs errors,
"error" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].
fallback: A function to call when an unknown value is encountered. If not provided,
a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.
serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.
Returns:
A dictionary representation of the model.
"""
return
self
__pydantic_serializer__
to_python
self
mode
mode
by_alias
by_alias
include
include
exclude
exclude
context
context
exclude_unset
exclude_unset
exclude_defaults
exclude_defaults
exclude_none
exclude_none
round_trip
round_trip
warnings
warnings
fallback
fallback
serialize_as_any
serialize_as_any
model_dump_json
model_dump_json
indent
int
None
None
include
IncEx
None
None
exclude
IncEx
None
None
context
Any
None
None
by_alias
bool
None
None
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
round_trip
bool
False
warnings
bool
Literal
"none"
"warn"
"error"
True
fallback
Callable
Any
Any
None
None
serialize_as_any
bool
False
str
Usage Documentation
model_dump_json
Generates a JSON representation of the model using Pydantic's
to_json
method.
Parameters:
Name
Type
Description
Default
indent
int
| None
Indentation to use in the JSON output. If None is passed, the output will be compact.
None
include
IncEx
| None
Field(s) to include in the JSON output.
None
exclude
IncEx
| None
Field(s) to exclude from the JSON output.
None
context
Any
| None
Additional context to pass to the serializer.
None
by_alias
bool
| None
Whether to serialize using field aliases.
None
exclude_unset
bool
Whether to exclude fields that have not been explicitly set.
False
exclude_defaults
bool
Whether to exclude fields that are set to their default value.
False
exclude_none
bool
Whether to exclude fields that have a value of
None
False
round_trip
bool
If True, dumped values should be valid as input for non-idempotent types such as Json[T].
False
warnings
bool
Literal
['none', 'warn', 'error']
How to handle serialization errors. False/"none" ignores them, True/"warn" logs errors,
"error" raises a
PydanticSerializationError
True
fallback
Callable
Any
Any
] | None
A function to call when an unknown value is encountered. If not provided,
PydanticSerializationError
error is raised.
None
serialize_as_any
bool
Whether to serialize fields with duck-typing serialization behavior.
False
Returns:
Type
Description
str
A JSON string representation of the model.
Source code in
pydantic/main.py
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
def
model_dump_json
self
indent
int
None
None
include
IncEx
None
None
exclude
IncEx
None
None
context
Any
None
None
by_alias
bool
None
None
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
round_trip
bool
False
warnings
bool
Literal
'none'
'warn'
'error'
True
fallback
Callable
Any
Any
None
None
serialize_as_any
bool
False
str
"""!!! abstract "Usage Documentation"
[`model_dump_json`](../concepts/serialization.md#modelmodel_dump_json)
Generates a JSON representation of the model using Pydantic's `to_json` method.
Args:
indent: Indentation to use in the JSON output. If None is passed, the output will be compact.
include: Field(s) to include in the JSON output.
exclude: Field(s) to exclude from the JSON output.
context: Additional context to pass to the serializer.
by_alias: Whether to serialize using field aliases.
exclude_unset: Whether to exclude fields that have not been explicitly set.
exclude_defaults: Whether to exclude fields that are set to their default value.
exclude_none: Whether to exclude fields that have a value of `None`.
round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].
warnings: How to handle serialization errors. False/"none" ignores them, True/"warn" logs errors,
"error" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].
fallback: A function to call when an unknown value is encountered. If not provided,
a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.
serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.
Returns:
A JSON string representation of the model.
"""
return
self
__pydantic_serializer__
to_json
self
indent
indent
include
include
exclude
exclude
context
context
by_alias
by_alias
exclude_unset
exclude_unset
exclude_defaults
exclude_defaults
exclude_none
exclude_none
round_trip
round_trip
warnings
warnings
fallback
fallback
serialize_as_any
serialize_as_any
decode
model_json_schema
classmethod
model_json_schema
by_alias
bool
True
ref_template
str
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
GenerateJsonSchema
mode
JsonSchemaMode
"validation"
dict
str
Any
Generates a JSON schema for a model class.
Parameters:
Name
Type
Description
Default
by_alias
bool
Whether to use attribute aliases or not.
True
ref_template
str
The reference template.
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
To override the logic used to generate the JSON schema, as a subclass of
GenerateJsonSchema
with your desired modifications
GenerateJsonSchema
mode
JsonSchemaMode
The mode in which to generate the schema.
'validation'
Returns:
Type
Description
dict
str
Any
The JSON schema for the given model class.
Source code in
pydantic/main.py
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
@classmethod
def
model_json_schema
cls
by_alias
bool
True
ref_template
str
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
GenerateJsonSchema
mode
JsonSchemaMode
'validation'
dict
str
Any
"""Generates a JSON schema for a model class.
Args:
by_alias: Whether to use attribute aliases or not.
ref_template: The reference template.
schema_generator: To override the logic used to generate the JSON schema, as a subclass of
`GenerateJsonSchema` with your desired modifications
mode: The mode in which to generate the schema.
Returns:
The JSON schema for the given model class.
"""
return
model_json_schema
cls
by_alias
by_alias
ref_template
ref_template
schema_generator
schema_generator
mode
mode
model_parametrized_name
classmethod
model_parametrized_name
params
tuple
type
Any
...
str
Compute the class name for parametrizations of generic classes.
This method can be overridden to achieve a custom naming scheme for generic BaseModels.
Parameters:
Name
Type
Description
Default
params
tuple
type
Any
], ...]
Tuple of types of the class. Given a generic class
Model
with 2 type variables and a concrete model
Model[str, int]
the value
(str, int)
would be passed to
params
required
Returns:
Type
Description
str
String representing the new class where
params
are passed to
cls
as type variables.
Raises:
Type
Description
TypeError
Raised when trying to generate concrete names for non-generic models.
Source code in
pydantic/main.py
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
@classmethod
def
model_parametrized_name
cls
params
tuple
type
Any
...
str
"""Compute the class name for parametrizations of generic classes.
This method can be overridden to achieve a custom naming scheme for generic BaseModels.
Args:
params: Tuple of types of the class. Given a generic class
`Model` with 2 type variables and a concrete model `Model[str, int]`,
the value `(str, int)` would be passed to `params`.
Returns:
String representing the new class where `params` are passed to `cls` as type variables.
Raises:
TypeError: Raised when trying to generate concrete names for non-generic models.
"""
not
issubclass
cls
typing
Generic
raise
TypeError
'Concrete names should only be generated for generic models.'
# Any strings received should represent forward references, so we handle them specially below.
# If we eventually move toward wrapping them in a ForwardRef in __class_getitem__ in the future,
# we may be able to remove this special case.
param_names
param
isinstance
param
str
else
_repr
display_as_type
param
for
param
params
params_component
', '
join
param_names
return
cls
__name__
params_component
model_post_init
model_post_init
context
Any
None
Override this method to perform additional initialization after
__init__
and
model_construct
This is useful if you want to do some validation that requires the entire model to be initialized.
Source code in
pydantic/main.py
586
587
588
589
590
def
model_post_init
self
context
Any
None
"""Override this method to perform additional initialization after `__init__` and `model_construct`.
This is useful if you want to do some validation that requires the entire model to be initialized.
"""
pass
model_rebuild
classmethod
model_rebuild
force
bool
False
raise_errors
bool
True
_parent_namespace_depth
int
_types_namespace
MappingNamespace
None
None
bool
None
Try to rebuild the pydantic-core schema for the model.
This may be necessary when one of the annotations is a ForwardRef which could not be resolved during
the initial attempt to build the schema, and automatic rebuilding fails.
Parameters:
Name
Type
Description
Default
force
bool
Whether to force the rebuilding of the model schema, defaults to
False
False
raise_errors
bool
Whether to raise errors, defaults to
True
True
_parent_namespace_depth
int
The depth level of the parent namespace, defaults to 2.
_types_namespace
MappingNamespace
| None
The types namespace, defaults to
None
None
Returns:
Type
Description
bool
| None
Returns
None
if the schema is already "complete" and rebuilding was not required.
bool
| None
If rebuilding
was
required, returns
True
if rebuilding was successful, otherwise
False
Source code in
pydantic/main.py
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
@classmethod
def
model_rebuild
cls
force
bool
False
raise_errors
bool
True
_parent_namespace_depth
int
_types_namespace
MappingNamespace
None
None
bool
None
"""Try to rebuild the pydantic-core schema for the model.
This may be necessary when one of the annotations is a ForwardRef which could not be resolved during
the initial attempt to build the schema, and automatic rebuilding fails.
Args:
force: Whether to force the rebuilding of the model schema, defaults to `False`.
raise_errors: Whether to raise errors, defaults to `True`.
_parent_namespace_depth: The depth level of the parent namespace, defaults to 2.
_types_namespace: The types namespace, defaults to `None`.
Returns:
Returns `None` if the schema is already "complete" and rebuilding was not required.
If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.
"""
not
force
and
cls
__pydantic_complete__
return
None
for
attr
'__pydantic_core_schema__'
'__pydantic_validator__'
'__pydantic_serializer__'
attr
cls
__dict__
and
not
isinstance
getattr
cls
attr
_mock_val_ser
MockValSer
# Deleting the validator/serializer is necessary as otherwise they can get reused in
# pydantic-core. We do so only if they aren't mock instances, otherwise â as `model_rebuild()`
# isn't thread-safe â concurrent model instantiations can lead to the parent validator being used.
# Same applies for the core schema that can be reused in schema generation.
delattr
cls
attr
cls
__pydantic_complete__
False
_types_namespace
not
None
rebuild_ns
_types_namespace
elif
_parent_namespace_depth
rebuild_ns
_typing_extra
parent_frame_namespace
parent_depth
_parent_namespace_depth
force
True
else
rebuild_ns
parent_ns
_model_construction
unpack_lenient_weakvaluedict
cls
__pydantic_parent_namespace__
ns_resolver
_namespace_utils
NsResolver
parent_namespace
rebuild_ns
parent_ns
not
cls
__pydantic_fields_complete__
typevars_map
_generics
get_model_typevars_map
cls
try
cls
__pydantic_fields__
_fields
rebuild_model_fields
cls
ns_resolver
ns_resolver
typevars_map
typevars_map
except
NameError
exc
PydanticUndefinedAnnotation
from_name_error
_mock_val_ser
set_model_mocks
cls
exc
name
raise_errors
raise
exc
from
not
raise_errors
and
not
cls
__pydantic_fields_complete__
# No need to continue with schema gen, it is guaranteed to fail
return
False
assert
cls
__pydantic_fields_complete__
return
_model_construction
complete_model_class
cls
_config
ConfigWrapper
cls
model_config
check
False
raise_errors
raise_errors
ns_resolver
ns_resolver
model_validate
classmethod
model_validate
obj
Any
strict
bool
None
None
from_attributes
bool
None
None
context
Any
None
None
by_alias
bool
None
None
by_name
bool
None
None
Self
Validate a pydantic model instance.
Parameters:
Name
Type
Description
Default
obj
Any
The object to validate.
required
strict
bool
| None
Whether to enforce types strictly.
None
from_attributes
bool
| None
Whether to extract data from object attributes.
None
context
Any
| None
Additional context to pass to the validator.
None
by_alias
bool
| None
Whether to use the field's alias when validating against the provided input data.
None
by_name
bool
| None
Whether to use the field's name when validating against the provided input data.
None
Raises:
Type
Description
ValidationError
If the object could not be validated.
Returns:
Type
Description
Self
The validated model instance.
Source code in
pydantic/main.py
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
@classmethod
def
model_validate
cls
obj
Any
strict
bool
None
None
from_attributes
bool
None
None
context
Any
None
None
by_alias
bool
None
None
by_name
bool
None
None
Self
"""Validate a pydantic model instance.
Args:
obj: The object to validate.
strict: Whether to enforce types strictly.
from_attributes: Whether to extract data from object attributes.
context: Additional context to pass to the validator.
by_alias: Whether to use the field's alias when validating against the provided input data.
by_name: Whether to use the field's name when validating against the provided input data.
Raises:
ValidationError: If the object could not be validated.
Returns:
The validated model instance.
"""
# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks
__tracebackhide__
True
by_alias
False
and
by_name
not
True
raise
PydanticUserError
'At least one of `by_alias` or `by_name` must be set to True.'
code
'validate-by-alias-and-name-false'
return
cls
__pydantic_validator__
validate_python
obj
strict
strict
from_attributes
from_attributes
context
context
by_alias
by_alias
by_name
by_name
model_validate_json
classmethod
model_validate_json
json_data
str
bytes
bytearray
strict
bool
None
None
context
Any
None
None
by_alias
bool
None
None
by_name
bool
None
None
Self
Usage Documentation
JSON Parsing
Validate the given JSON data against the Pydantic model.
Parameters:
Name
Type
Description
Default
json_data
str
bytes
bytearray
The JSON data to validate.
required
strict
bool
| None
Whether to enforce types strictly.
None
context
Any
| None
Extra variables to pass to the validator.
None
by_alias
bool
| None
Whether to use the field's alias when validating against the provided input data.
None
by_name
bool
| None
Whether to use the field's name when validating against the provided input data.
None
Returns:
Type
Description
Self
The validated Pydantic model.
Raises:
Type
Description
ValidationError
json_data
is not a JSON string or the object could not be validated.
Source code in
pydantic/main.py
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
@classmethod
def
model_validate_json
cls
json_data
str
bytes
bytearray
strict
bool
None
None
context
Any
None
None
by_alias
bool
None
None
by_name
bool
None
None
Self
"""!!! abstract "Usage Documentation"
[JSON Parsing](../concepts/json.md#json-parsing)
Validate the given JSON data against the Pydantic model.
Args:
json_data: The JSON data to validate.
strict: Whether to enforce types strictly.
context: Extra variables to pass to the validator.
by_alias: Whether to use the field's alias when validating against the provided input data.
by_name: Whether to use the field's name when validating against the provided input data.
Returns:
The validated Pydantic model.
Raises:
ValidationError: If `json_data` is not a JSON string or the object could not be validated.
"""
# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks
__tracebackhide__
True
by_alias
False
and
by_name
not
True
raise
PydanticUserError
'At least one of `by_alias` or `by_name` must be set to True.'
code
'validate-by-alias-and-name-false'
return
cls
__pydantic_validator__
validate_json
json_data
strict
strict
context
context
by_alias
by_alias
by_name
by_name
model_validate_strings
classmethod
model_validate_strings
obj
Any
strict
bool
None
None
context
Any
None
None
by_alias
bool
None
None
by_name
bool
None
None
Self
Validate the given object with string data against the Pydantic model.
Parameters:
Name
Type
Description
Default
obj
Any
The object containing string data to validate.
required
strict
bool
| None
Whether to enforce types strictly.
None
context
Any
| None
Extra variables to pass to the validator.
None
by_alias
bool
| None
Whether to use the field's alias when validating against the provided input data.
None
by_name
bool
| None
Whether to use the field's name when validating against the provided input data.
None
Returns:
Type
Description
Self
The validated Pydantic model.
Source code in
pydantic/main.py
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
@classmethod
def
model_validate_strings
cls
obj
Any
strict
bool
None
None
context
Any
None
None
by_alias
bool
None
None
by_name
bool
None
None
Self
"""Validate the given object with string data against the Pydantic model.
Args:
obj: The object containing string data to validate.
strict: Whether to enforce types strictly.
context: Extra variables to pass to the validator.
by_alias: Whether to use the field's alias when validating against the provided input data.
by_name: Whether to use the field's name when validating against the provided input data.
Returns:
The validated Pydantic model.
"""
# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks
__tracebackhide__
True
by_alias
False
and
by_name
not
True
raise
PydanticUserError
'At least one of `by_alias` or `by_name` must be set to True.'
code
'validate-by-alias-and-name-false'
return
cls
__pydantic_validator__
validate_strings
obj
strict
strict
context
context
by_alias
by_alias
by_name
by_name
pydantic.create_model
create_model
model_name
str
__config__
ConfigDict
None
None
__doc__
str
None
None
__base__
None
None
__module__
str
__name__
__validators__
dict
str
Callable
...
Any
None
None
__cls_kwargs__
dict
str
Any
None
None
field_definitions
Any
tuple
str
Any
type
BaseModel
create_model
model_name
str
__config__
ConfigDict
None
None
__doc__
str
None
None
__base__
type
ModelT
tuple
type
ModelT
...
__module__
str
__name__
__validators__
dict
str
Callable
...
Any
None
None
__cls_kwargs__
dict
str
Any
None
None
field_definitions
Any
tuple
str
Any
type
ModelT
create_model
model_name
str
__config__
ConfigDict
None
None
__doc__
str
None
None
__base__
type
ModelT
tuple
type
ModelT
...
None
None
__module__
str
None
None
__validators__
dict
str
Callable
...
Any
None
None
__cls_kwargs__
dict
str
Any
None
None
field_definitions
Any
tuple
str
Any
type
ModelT
Usage Documentation
Dynamic Model Creation
Dynamically creates and returns a new Pydantic model, in other words,
create_model
dynamically creates a
subclass of
BaseModel
Parameters:
Name
Type
Description
Default
model_name
str
required
__config__
ConfigDict
| None
The configuration of the new model.
None
__doc__
str
| None
The docstring of the new model.
None
__base__
type
ModelT
] |
tuple
type
ModelT
], ...] | None
The base class or classes for the new model.
None
__module__
str
| None
The name of the module that the model belongs to;
None
, the value is taken from
sys._getframe(1)
None
__validators__
dict
str
Callable
[...,
Any
]] | None
A dictionary of methods that validate fields. The keys are the names of the validation methods to
be added to the model, and the values are the validation methods themselves. You can read more about functional
validators
here
None
__cls_kwargs__
dict
str
Any
] | None
A dictionary of keyword arguments for class creation, such as
metaclass
None
**field_definitions
Any
tuple
str
Any
Field definitions of the new model. Either:
a single element, representing the type annotation of the field.
a two-tuple, the first element being the type and the second element the assigned value
(either a default or the
Field()
function).
Returns:
Type
Description
type
ModelT
The new
model
Raises:
Type
Description
PydanticUserError
__base__
and
__config__
are both passed.
Source code in
pydantic/main.py
1679
1680
1681
1682
1683
1684
1685
1686
1687
1688
1689
1690
1691
1692
1693
1694
1695
1696
1697
1698
1699
1700
1701
1702
1703
1704
1705
1706
1707
1708
1709
1710
1711
1712
1713
1714
1715
1716
1717
1718
1719
1720
1721
1722
1723
1724
1725
1726
1727
1728
1729
1730
1731
1732
1733
1734
1735
1736
1737
1738
1739
1740
1741
1742
1743
1744
1745
1746
1747
1748
1749
1750
1751
1752
1753
1754
1755
1756
1757
1758
1759
1760
1761
1762
1763
1764
1765
1766
1767
1768
1769
1770
def
create_model
# noqa: C901
model_name
str
__config__
ConfigDict
None
None
__doc__
str
None
None
__base__
type
ModelT
tuple
type
ModelT
...
None
None
__module__
str
None
None
__validators__
dict
str
Callable
...
Any
None
None
__cls_kwargs__
dict
str
Any
None
None
# TODO PEP 747: replace `Any` by the TypeForm:
field_definitions
Any
tuple
str
Any
type
ModelT
"""!!! abstract "Usage Documentation"
[Dynamic Model Creation](../concepts/models.md#dynamic-model-creation)
Dynamically creates and returns a new Pydantic model, in other words, `create_model` dynamically creates a
subclass of [`BaseModel`][pydantic.BaseModel].
Args:
__config__: The configuration of the new model.
__doc__: The docstring of the new model.
__base__: The base class or classes for the new model.
__module__: The name of the module that the model belongs to;
if `None`, the value is taken from `sys._getframe(1)`
__validators__: A dictionary of methods that validate fields. The keys are the names of the validation methods to
be added to the model, and the values are the validation methods themselves. You can read more about functional
validators [here](https://docs.pydantic.dev/2.9/concepts/validators/#field-validators).
__cls_kwargs__: A dictionary of keyword arguments for class creation, such as `metaclass`.
**field_definitions: Field definitions of the new model. Either:
- a single element, representing the type annotation of the field.
- a two-tuple, the first element being the type and the second element the assigned value
(either a default or the [`Field()`][pydantic.Field] function).
Returns:
The new [model][pydantic.BaseModel].
Raises:
PydanticUserError: If `__base__` and `__config__` are both passed.
"""
__base__
None
__base__
cast
'type[ModelT]'
BaseModel
),)
elif
not
isinstance
__base__
tuple
__base__
__base__
__cls_kwargs__
__cls_kwargs__
fields
dict
str
Any
annotations
dict
str
Any
for
f_name
f_def
field_definitions
items
():
isinstance
f_def
tuple
len
f_def
raise
PydanticUserError
'Field definition for
f_name
!r}
should a single element representing the type or a two-tuple, the first element '
'being the type and the second element the assigned value (either a default or the `Field()` function).'
code
'create-model-field-definitions'
annotations
f_name
f_def
fields
f_name
f_def
else
annotations
f_name
f_def
__module__
None
sys
_getframe
__module__
f_globals
'__name__'
namespace
dict
str
Any
'__annotations__'
annotations
'__module__'
__module__
__doc__
namespace
update
'__doc__'
__doc__
__validators__
namespace
update
__validators__
namespace
update
fields
__config__
namespace
'model_config'
__config__
resolved_bases
types
resolve_bases
__base__
meta
kwds
types
prepare_class
model_name
resolved_bases
kwds
__cls_kwargs__
resolved_bases
not
__base__
'__orig_bases__'
__base__
namespace
update
return
meta
model_name
resolved_bases
namespace
__pydantic_reset_parent_namespace__
False
_create_model_module
__module__
kwds
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 030_pydantic_settings.txt ---
Pydantic Settings
BaseSettings
BaseSettings
__pydantic_self__
_case_sensitive
bool
None
None
_nested_model_default_partial_update
bool
None
None
_env_prefix
str
None
None
_env_file
DotenvType
None
ENV_FILE_SENTINEL
_env_file_encoding
str
None
None
_env_ignore_empty
bool
None
None
_env_nested_delimiter
str
None
None
_env_parse_none_str
str
None
None
_env_parse_enums
bool
None
None
_cli_prog_name
str
None
None
_cli_parse_args
bool
list
str
tuple
str
...
None
None
_cli_settings_source
CliSettingsSource
Any
None
None
_cli_parse_none_str
str
None
None
_cli_hide_none_type
bool
None
None
_cli_avoid_json
bool
None
None
_cli_enforce_required
bool
None
None
_cli_use_class_docs_for_groups
bool
None
None
_cli_exit_on_error
bool
None
None
_cli_prefix
str
None
None
_cli_flag_prefix_char
str
None
None
_cli_implicit_flags
bool
None
None
_cli_ignore_unknown_args
bool
None
None
_cli_kebab_case
bool
None
None
_secrets_dir
PathType
None
None
values
Any
Bases:
BaseModel
Base class for settings, allowing values to be overridden by environment variables.
This is useful in production for secrets you do not wish to save in code, it plays nicely with docker(-compose),
Heroku and any 12 factor app design.
All the below attributes can be set via
model_config
Parameters:
Name
Type
Description
Default
_case_sensitive
bool
| None
Whether environment and CLI variable names should be read with case-sensitivity.
Defaults to
None
None
_nested_model_default_partial_update
bool
| None
Whether to allow partial updates on nested model default object fields.
Defaults to
False
None
_env_prefix
str
| None
Prefix for all environment variables. Defaults to
None
None
_env_file
DotenvType
| None
The env file(s) to load settings values from. Defaults to
Path('')
, which
means that the value from
model_config['env_file']
should be used. You can also pass
None
to indicate that environment variables should not be loaded from an env file.
ENV_FILE_SENTINEL
_env_file_encoding
str
| None
The env file encoding, e.g.
'latin-1'
. Defaults to
None
None
_env_ignore_empty
bool
| None
Ignore environment variables where the value is an empty string. Default to
False
None
_env_nested_delimiter
str
| None
The nested env values delimiter. Defaults to
None
None
_env_parse_none_str
str
| None
The env string value that should be parsed (e.g. "null", "void", "None", etc.)
into
None
type(None). Defaults to
None
type(None), which means no parsing should occur.
None
_env_parse_enums
bool
| None
Parse enum field names to values. Defaults to
None.
, which means no parsing should occur.
None
_cli_prog_name
str
| None
The CLI program name to display in help text. Defaults to
None
if _cli_parse_args is
None
Otherwse, defaults to sys.argv[0].
None
_cli_parse_args
bool
list
str
] |
tuple
str
, ...] | None
The list of CLI arguments to parse. Defaults to None.
If set to
True
, defaults to sys.argv[1:].
None
_cli_settings_source
CliSettingsSource
Any
] | None
Override the default CLI settings source with a user defined instance. Defaults to None.
None
_cli_parse_none_str
str
| None
The CLI string value that should be parsed (e.g. "null", "void", "None", etc.) into
None
type(None). Defaults to _env_parse_none_str value if set. Otherwise, defaults to "null" if
_cli_avoid_json is
False
, and "None" if _cli_avoid_json is
True
None
_cli_hide_none_type
bool
| None
Hide
None
values in CLI help text. Defaults to
False
None
_cli_avoid_json
bool
| None
Avoid complex JSON objects in CLI help text. Defaults to
False
None
_cli_enforce_required
bool
| None
Enforce required fields at the CLI. Defaults to
False
None
_cli_use_class_docs_for_groups
bool
| None
Use class docstrings in CLI group help text instead of field descriptions.
Defaults to
False
None
_cli_exit_on_error
bool
| None
Determines whether or not the internal parser exits with error info when an error occurs.
Defaults to
True
None
_cli_prefix
str
| None
The root parser command line arguments prefix. Defaults to "".
None
_cli_flag_prefix_char
str
| None
The flag prefix character to use for CLI optional arguments. Defaults to '-'.
None
_cli_implicit_flags
bool
| None
Whether
bool
fields should be implicitly converted into CLI boolean flags.
(e.g. --flag, --no-flag). Defaults to
False
None
_cli_ignore_unknown_args
bool
| None
Whether to ignore unknown CLI args and parse only known ones. Defaults to
False
None
_cli_kebab_case
bool
| None
CLI args use kebab case. Defaults to
False
None
_secrets_dir
PathType
| None
The secret files directory or a sequence of directories. Defaults to
None
None
Source code in
pydantic_settings/main.py
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
def
__init__
__pydantic_self__
_case_sensitive
bool
None
None
_nested_model_default_partial_update
bool
None
None
_env_prefix
str
None
None
_env_file
DotenvType
None
ENV_FILE_SENTINEL
_env_file_encoding
str
None
None
_env_ignore_empty
bool
None
None
_env_nested_delimiter
str
None
None
_env_parse_none_str
str
None
None
_env_parse_enums
bool
None
None
_cli_prog_name
str
None
None
_cli_parse_args
bool
list
str
tuple
str
...
None
None
_cli_settings_source
CliSettingsSource
Any
None
None
_cli_parse_none_str
str
None
None
_cli_hide_none_type
bool
None
None
_cli_avoid_json
bool
None
None
_cli_enforce_required
bool
None
None
_cli_use_class_docs_for_groups
bool
None
None
_cli_exit_on_error
bool
None
None
_cli_prefix
str
None
None
_cli_flag_prefix_char
str
None
None
_cli_implicit_flags
bool
None
None
_cli_ignore_unknown_args
bool
None
None
_cli_kebab_case
bool
None
None
_secrets_dir
PathType
None
None
values
Any
None
# Uses something other than `self` the first arg to allow "self" as a settable attribute
super
__init__
__pydantic_self__
_settings_build_values
values
_case_sensitive
_case_sensitive
_nested_model_default_partial_update
_nested_model_default_partial_update
_env_prefix
_env_prefix
_env_file
_env_file
_env_file_encoding
_env_file_encoding
_env_ignore_empty
_env_ignore_empty
_env_nested_delimiter
_env_nested_delimiter
_env_parse_none_str
_env_parse_none_str
_env_parse_enums
_env_parse_enums
_cli_prog_name
_cli_prog_name
_cli_parse_args
_cli_parse_args
_cli_settings_source
_cli_settings_source
_cli_parse_none_str
_cli_parse_none_str
_cli_hide_none_type
_cli_hide_none_type
_cli_avoid_json
_cli_avoid_json
_cli_enforce_required
_cli_enforce_required
_cli_use_class_docs_for_groups
_cli_use_class_docs_for_groups
_cli_exit_on_error
_cli_exit_on_error
_cli_prefix
_cli_prefix
_cli_flag_prefix_char
_cli_flag_prefix_char
_cli_implicit_flags
_cli_implicit_flags
_cli_ignore_unknown_args
_cli_ignore_unknown_args
_cli_kebab_case
_cli_kebab_case
_secrets_dir
_secrets_dir
settings_customise_sources
classmethod
settings_customise_sources
settings_cls
type
BaseSettings
init_settings
PydanticBaseSettingsSource
env_settings
PydanticBaseSettingsSource
dotenv_settings
PydanticBaseSettingsSource
file_secret_settings
PydanticBaseSettingsSource
tuple
PydanticBaseSettingsSource
...
Define the sources and their order for loading the settings values.
Parameters:
Name
Type
Description
Default
settings_cls
type
BaseSettings
The Settings class.
required
init_settings
PydanticBaseSettingsSource
The
InitSettingsSource
instance.
required
env_settings
PydanticBaseSettingsSource
The
EnvSettingsSource
instance.
required
dotenv_settings
PydanticBaseSettingsSource
The
DotEnvSettingsSource
instance.
required
file_secret_settings
PydanticBaseSettingsSource
The
SecretsSettingsSource
instance.
required
Returns:
Type
Description
tuple
PydanticBaseSettingsSource
, ...]
A tuple containing the sources and their order for loading the settings values.
Source code in
pydantic_settings/main.py
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
@classmethod
def
settings_customise_sources
cls
settings_cls
type
BaseSettings
init_settings
PydanticBaseSettingsSource
env_settings
PydanticBaseSettingsSource
dotenv_settings
PydanticBaseSettingsSource
file_secret_settings
PydanticBaseSettingsSource
tuple
PydanticBaseSettingsSource
...
"""
Define the sources and their order for loading the settings values.
Args:
settings_cls: The Settings class.
init_settings: The `InitSettingsSource` instance.
env_settings: The `EnvSettingsSource` instance.
dotenv_settings: The `DotEnvSettingsSource` instance.
file_secret_settings: The `SecretsSettingsSource` instance.
Returns:
A tuple containing the sources and their order for loading the settings values.
"""
return
init_settings
env_settings
dotenv_settings
file_secret_settings
CliApp
A utility class for running Pydantic
BaseSettings
BaseModel
, or
pydantic.dataclasses.dataclass
CLI applications.
run
staticmethod
run
model_cls
type
cli_args
list
str
Namespace
SimpleNamespace
dict
str
Any
None
None
cli_settings_source
CliSettingsSource
Any
None
None
cli_exit_on_error
bool
None
None
cli_cmd_method_name
str
"cli_cmd"
model_init_data
Any
Runs a Pydantic
BaseSettings
BaseModel
, or
pydantic.dataclasses.dataclass
as a CLI application.
Running a model as a CLI application requires the
cli_cmd
method to be defined in the model class.
Parameters:
Name
Type
Description
Default
model_cls
type
The model class to run as a CLI application.
required
cli_args
list
str
] |
Namespace
SimpleNamespace
dict
str
Any
] | None
The list of CLI arguments to parse. If
cli_settings_source
is specified, this may
also be a namespace or dictionary of pre-parsed CLI arguments. Defaults to
sys.argv[1:]
None
cli_settings_source
CliSettingsSource
Any
] | None
Override the default CLI settings source with a user defined instance.
Defaults to
None
None
cli_exit_on_error
bool
| None
Determines whether this function exits on error. If model is subclass of
BaseSettings
, defaults to BaseSettings
cli_exit_on_error
value. Otherwise, defaults to
True
None
cli_cmd_method_name
str
The CLI command method name to run. Defaults to "cli_cmd".
'cli_cmd'
model_init_data
Any
The model init data.
Returns:
Type
Description
The ran instance of model.
Raises:
Type
Description
SettingsError
If model_cls is not subclass of
BaseModel
pydantic.dataclasses.dataclass
SettingsError
If model_cls does not have a
cli_cmd
entrypoint defined.
Source code in
pydantic_settings/main.py
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
@staticmethod
def
run
model_cls
type
cli_args
list
str
Namespace
SimpleNamespace
dict
str
Any
None
None
cli_settings_source
CliSettingsSource
Any
None
None
cli_exit_on_error
bool
None
None
cli_cmd_method_name
str
'cli_cmd'
model_init_data
Any
"""
Runs a Pydantic `BaseSettings`, `BaseModel`, or `pydantic.dataclasses.dataclass` as a CLI application.
Running a model as a CLI application requires the `cli_cmd` method to be defined in the model class.
Args:
model_cls: The model class to run as a CLI application.
cli_args: The list of CLI arguments to parse. If `cli_settings_source` is specified, this may
also be a namespace or dictionary of pre-parsed CLI arguments. Defaults to `sys.argv[1:]`.
cli_settings_source: Override the default CLI settings source with a user defined instance.
Defaults to `None`.
cli_exit_on_error: Determines whether this function exits on error. If model is subclass of
`BaseSettings`, defaults to BaseSettings `cli_exit_on_error` value. Otherwise, defaults to
`True`.
cli_cmd_method_name: The CLI command method name to run. Defaults to "cli_cmd".
model_init_data: The model init data.
Returns:
The ran instance of model.
Raises:
SettingsError: If model_cls is not subclass of `BaseModel` or `pydantic.dataclasses.dataclass`.
SettingsError: If model_cls does not have a `cli_cmd` entrypoint defined.
"""
not
is_pydantic_dataclass
model_cls
is_model_class
model_cls
)):
raise
SettingsError
'Error:
model_cls
__name__
is not subclass of BaseModel or pydantic.dataclasses.dataclass'
cli_settings
None
cli_parse_args
True
cli_args
None
else
cli_args
cli_settings_source
not
None
isinstance
cli_parse_args
Namespace
SimpleNamespace
dict
)):
cli_settings
cli_settings_source
parsed_args
cli_parse_args
else
cli_settings
cli_settings_source
args
cli_parse_args
elif
isinstance
cli_parse_args
Namespace
SimpleNamespace
dict
)):
raise
SettingsError
'Error: `cli_args` must be list[str] or None when `cli_settings_source` is not used'
model_init_data
'_cli_parse_args'
cli_parse_args
model_init_data
'_cli_exit_on_error'
cli_exit_on_error
model_init_data
'_cli_settings_source'
cli_settings
not
issubclass
model_cls
BaseSettings
class
CliAppBaseSettings
BaseSettings
model_cls
# type: ignore
model_config
SettingsConfigDict
nested_model_default_partial_update
True
case_sensitive
True
cli_hide_none_type
True
cli_avoid_json
True
cli_enforce_required
True
cli_implicit_flags
True
cli_kebab_case
True
model
CliAppBaseSettings
model_init_data
model_init_data
for
field_name
field_info
model
model_fields
items
():
model_init_data
_field_name_for_signature
field_name
field_info
getattr
model
field_name
return
CliApp
_run_cli_cmd
model_cls
model_init_data
cli_cmd_method_name
is_required
False
run_subcommand
staticmethod
run_subcommand
model
PydanticModel
cli_exit_on_error
bool
None
None
cli_cmd_method_name
str
"cli_cmd"
PydanticModel
Runs the model subcommand. Running a model subcommand requires the
cli_cmd
method to be defined in
the nested model subcommand class.
Parameters:
Name
Type
Description
Default
model
PydanticModel
The model to run the subcommand from.
required
cli_exit_on_error
bool
| None
Determines whether this function exits with error if no subcommand is found.
Defaults to model_config
cli_exit_on_error
value if set. Otherwise, defaults to
True
None
cli_cmd_method_name
str
The CLI command method name to run. Defaults to "cli_cmd".
'cli_cmd'
Returns:
Type
Description
PydanticModel
The ran subcommand model.
Raises:
Type
Description
SystemExit
When no subcommand is found and cli_exit_on_error=
True
(the default).
SettingsError
When no subcommand is found and cli_exit_on_error=
False
Source code in
pydantic_settings/main.py
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
@staticmethod
def
run_subcommand
model
PydanticModel
cli_exit_on_error
bool
None
None
cli_cmd_method_name
str
'cli_cmd'
PydanticModel
"""
Runs the model subcommand. Running a model subcommand requires the `cli_cmd` method to be defined in
the nested model subcommand class.
Args:
model: The model to run the subcommand from.
cli_exit_on_error: Determines whether this function exits with error if no subcommand is found.
Defaults to model_config `cli_exit_on_error` value if set. Otherwise, defaults to `True`.
cli_cmd_method_name: The CLI command method name to run. Defaults to "cli_cmd".
Returns:
The ran subcommand model.
Raises:
SystemExit: When no subcommand is found and cli_exit_on_error=`True` (the default).
SettingsError: When no subcommand is found and cli_exit_on_error=`False`.
"""
subcommand
get_subcommand
model
is_required
True
cli_exit_on_error
cli_exit_on_error
return
CliApp
_run_cli_cmd
subcommand
cli_cmd_method_name
is_required
True
SettingsConfigDict
Bases:
ConfigDict
pyproject_toml_depth
instance-attribute
pyproject_toml_depth
int
Number of levels
from the current working directory to attempt to find a pyproject.toml
file.
This is only used when a pyproject.toml file is not found in the current working directory.
pyproject_toml_table_header
instance-attribute
pyproject_toml_table_header
tuple
str
...
Header of the TOML table within a pyproject.toml file to use when filling variables.
This is supplied as a
tuple[str, ...]
instead of a
str
to accommodate for headers
containing a
For example,
toml_table_header = ("tool", "my.tool", "foo")
can be used to fill variable
values from a table with header
[tool."my.tool".foo]
To use the root table, exclude this config setting or provide an empty tuple.
CliSettingsSource
CliSettingsSource
settings_cls
type
BaseSettings
cli_prog_name
str
None
None
cli_parse_args
bool
list
str
tuple
str
...
None
None
cli_parse_none_str
str
None
None
cli_hide_none_type
bool
None
None
cli_avoid_json
bool
None
None
cli_enforce_required
bool
None
None
cli_use_class_docs_for_groups
bool
None
None
cli_exit_on_error
bool
None
None
cli_prefix
str
None
None
cli_flag_prefix_char
str
None
None
cli_implicit_flags
bool
None
None
cli_ignore_unknown_args
bool
None
None
cli_kebab_case
bool
None
None
case_sensitive
bool
None
True
root_parser
Any
None
parse_args_method
Callable
...
Any
None
None
add_argument_method
Callable
...
Any
None
add_argument
add_argument_group_method
Callable
...
Any
None
add_argument_group
add_parser_method
Callable
...
Any
None
add_parser
add_subparsers_method
Callable
...
Any
None
add_subparsers
formatter_class
Any
RawDescriptionHelpFormatter
Bases:
EnvSettingsSource
Generic
Source class for loading settings values from CLI.
Note
CliSettingsSource
connects with a
root_parser
object by using the parser methods to add
settings_cls
fields as command line arguments. The
CliSettingsSource
internal parser representation
is based upon the
argparse
parsing library, and therefore, requires the parser methods to support
the same attributes as their
argparse
library counterparts.
Parameters:
Name
Type
Description
Default
cli_prog_name
str
| None
The CLI program name to display in help text. Defaults to
None
if cli_parse_args is
None
Otherwse, defaults to sys.argv[0].
None
cli_parse_args
bool
list
str
] |
tuple
str
, ...] | None
The list of CLI arguments to parse. Defaults to None.
If set to
True
, defaults to sys.argv[1:].
None
cli_parse_none_str
str
| None
The CLI string value that should be parsed (e.g. "null", "void", "None", etc.) into
None
type(None). Defaults to "null" if cli_avoid_json is
False
, and "None" if cli_avoid_json is
True
None
cli_hide_none_type
bool
| None
Hide
None
values in CLI help text. Defaults to
False
None
cli_avoid_json
bool
| None
Avoid complex JSON objects in CLI help text. Defaults to
False
None
cli_enforce_required
bool
| None
Enforce required fields at the CLI. Defaults to
False
None
cli_use_class_docs_for_groups
bool
| None
Use class docstrings in CLI group help text instead of field descriptions.
Defaults to
False
None
cli_exit_on_error
bool
| None
Determines whether or not the internal parser exits with error info when an error occurs.
Defaults to
True
None
cli_prefix
str
| None
Prefix for command line arguments added under the root parser. Defaults to "".
None
cli_flag_prefix_char
str
| None
The flag prefix character to use for CLI optional arguments. Defaults to '-'.
None
cli_implicit_flags
bool
| None
Whether
bool
fields should be implicitly converted into CLI boolean flags.
(e.g. --flag, --no-flag). Defaults to
False
None
cli_ignore_unknown_args
bool
| None
Whether to ignore unknown CLI args and parse only known ones. Defaults to
False
None
cli_kebab_case
bool
| None
CLI args use kebab case. Defaults to
False
None
case_sensitive
bool
| None
Whether CLI "--arg" names should be read with case-sensitivity. Defaults to
True
Note: Case-insensitive matching is only supported on the internal root parser and does not apply to CLI
subcommands.
True
root_parser
Any
The root parser object.
None
parse_args_method
Callable
[...,
Any
] | None
The root parser parse args method. Defaults to
argparse.ArgumentParser.parse_args
None
add_argument_method
Callable
[...,
Any
] | None
The root parser add argument method. Defaults to
argparse.ArgumentParser.add_argument
add_argument
add_argument_group_method
Callable
[...,
Any
] | None
The root parser add argument group method.
Defaults to
argparse.ArgumentParser.add_argument_group
add_argument_group
add_parser_method
Callable
[...,
Any
] | None
The root parser add new parser (sub-command) method.
Defaults to
argparse._SubParsersAction.add_parser
add_parser
add_subparsers_method
Callable
[...,
Any
] | None
The root parser add subparsers (sub-commands) method.
Defaults to
argparse.ArgumentParser.add_subparsers
add_subparsers
formatter_class
Any
A class for customizing the root parser help text. Defaults to
argparse.RawDescriptionHelpFormatter
RawDescriptionHelpFormatter
Source code in
pydantic_settings/sources.py
1115
1116
1117
1118
1119
1120
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
1133
1134
1135
1136
1137
1138
1139
1140
1141
1142
1143
1144
1145
1146
1147
1148
1149
1150
1151
1152
1153
1154
1155
1156
1157
1158
1159
1160
1161
1162
1163
1164
1165
1166
1167
1168
1169
1170
1171
1172
1173
1174
1175
1176
1177
1178
1179
1180
1181
1182
1183
1184
1185
1186
1187
1188
1189
1190
1191
1192
1193
1194
1195
1196
1197
1198
1199
1200
1201
1202
1203
1204
1205
1206
1207
1208
1209
1210
1211
1212
1213
1214
1215
1216
1217
1218
1219
1220
1221
1222
1223
1224
1225
1226
1227
1228
1229
1230
1231
1232
1233
1234
1235
1236
def
__init__
self
settings_cls
type
BaseSettings
cli_prog_name
str
None
None
cli_parse_args
bool
list
str
tuple
str
...
None
None
cli_parse_none_str
str
None
None
cli_hide_none_type
bool
None
None
cli_avoid_json
bool
None
None
cli_enforce_required
bool
None
None
cli_use_class_docs_for_groups
bool
None
None
cli_exit_on_error
bool
None
None
cli_prefix
str
None
None
cli_flag_prefix_char
str
None
None
cli_implicit_flags
bool
None
None
cli_ignore_unknown_args
bool
None
None
cli_kebab_case
bool
None
None
case_sensitive
bool
None
True
root_parser
Any
None
parse_args_method
Callable
...
Any
None
None
add_argument_method
Callable
...
Any
None
ArgumentParser
add_argument
add_argument_group_method
Callable
...
Any
None
ArgumentParser
add_argument_group
add_parser_method
Callable
...
Any
None
_SubParsersAction
add_parser
add_subparsers_method
Callable
...
Any
None
ArgumentParser
add_subparsers
formatter_class
Any
RawDescriptionHelpFormatter
None
self
cli_prog_name
cli_prog_name
cli_prog_name
not
None
else
settings_cls
model_config
get
'cli_prog_name'
sys
argv
self
cli_hide_none_type
cli_hide_none_type
cli_hide_none_type
not
None
else
settings_cls
model_config
get
'cli_hide_none_type'
False
self
cli_avoid_json
cli_avoid_json
cli_avoid_json
not
None
else
settings_cls
model_config
get
'cli_avoid_json'
False
not
cli_parse_none_str
cli_parse_none_str
'None'
self
cli_avoid_json
True
else
'null'
self
cli_parse_none_str
cli_parse_none_str
self
cli_enforce_required
cli_enforce_required
cli_enforce_required
not
None
else
settings_cls
model_config
get
'cli_enforce_required'
False
self
cli_use_class_docs_for_groups
cli_use_class_docs_for_groups
cli_use_class_docs_for_groups
not
None
else
settings_cls
model_config
get
'cli_use_class_docs_for_groups'
False
self
cli_exit_on_error
cli_exit_on_error
cli_exit_on_error
not
None
else
settings_cls
model_config
get
'cli_exit_on_error'
True
self
cli_prefix
cli_prefix
cli_prefix
not
None
else
settings_cls
model_config
get
'cli_prefix'
self
cli_flag_prefix_char
cli_flag_prefix_char
cli_flag_prefix_char
not
None
else
settings_cls
model_config
get
'cli_flag_prefix_char'
'-'
self
_cli_flag_prefix
self
cli_flag_prefix_char
self
cli_prefix
cli_prefix
startswith
'.'
cli_prefix
endswith
'.'
not
cli_prefix
replace
'.'
isidentifier
():
# type: ignore
raise
SettingsError
'CLI settings source prefix is invalid:
cli_prefix
self
cli_prefix
'.'
self
cli_implicit_flags
cli_implicit_flags
cli_implicit_flags
not
None
else
settings_cls
model_config
get
'cli_implicit_flags'
False
self
cli_ignore_unknown_args
cli_ignore_unknown_args
cli_ignore_unknown_args
not
None
else
settings_cls
model_config
get
'cli_ignore_unknown_args'
False
self
cli_kebab_case
cli_kebab_case
cli_kebab_case
not
None
else
settings_cls
model_config
get
'cli_kebab_case'
False
case_sensitive
case_sensitive
case_sensitive
not
None
else
True
not
case_sensitive
and
root_parser
not
None
raise
SettingsError
'Case-insensitive matching is only supported on the internal root parser'
super
__init__
settings_cls
env_nested_delimiter
'.'
env_parse_none_str
self
cli_parse_none_str
env_parse_enums
True
env_prefix
self
cli_prefix
case_sensitive
case_sensitive
root_parser
_CliInternalArgParser
cli_exit_on_error
self
cli_exit_on_error
prog
self
cli_prog_name
description
None
settings_cls
__doc__
None
else
dedent
settings_cls
__doc__
formatter_class
formatter_class
prefix_chars
self
cli_flag_prefix_char
allow_abbrev
False
root_parser
None
else
root_parser
self
_connect_root_parser
root_parser
root_parser
parse_args_method
parse_args_method
add_argument_method
add_argument_method
add_argument_group_method
add_argument_group_method
add_parser_method
add_parser_method
add_subparsers_method
add_subparsers_method
formatter_class
formatter_class
cli_parse_args
not
None
False
cli_parse_args
True
cli_parse_args
sys
argv
elif
not
isinstance
cli_parse_args
list
tuple
)):
raise
SettingsError
'cli_parse_args must be List[str] or Tuple[str, ...], recieved
type
cli_parse_args
self
_load_env_vars
parsed_args
self
_parse_args
self
root_parser
cli_parse_args
root_parser
property
root_parser
The connected root parser instance.
DotEnvSettingsSource
DotEnvSettingsSource
settings_cls
type
BaseSettings
env_file
DotenvType
None
ENV_FILE_SENTINEL
env_file_encoding
str
None
None
case_sensitive
bool
None
None
env_prefix
str
None
None
env_nested_delimiter
str
None
None
env_ignore_empty
bool
None
None
env_parse_none_str
str
None
None
env_parse_enums
bool
None
None
Bases:
EnvSettingsSource
Source class for loading settings values from env files.
Source code in
pydantic_settings/sources.py
959
960
961
962
963
964
965
966
967
968
969
970
971
972
973
974
975
976
977
978
979
980
981
982
983
def
__init__
self
settings_cls
type
BaseSettings
env_file
DotenvType
None
ENV_FILE_SENTINEL
env_file_encoding
str
None
None
case_sensitive
bool
None
None
env_prefix
str
None
None
env_nested_delimiter
str
None
None
env_ignore_empty
bool
None
None
env_parse_none_str
str
None
None
env_parse_enums
bool
None
None
None
self
env_file
env_file
env_file
ENV_FILE_SENTINEL
else
settings_cls
model_config
get
'env_file'
self
env_file_encoding
env_file_encoding
env_file_encoding
not
None
else
settings_cls
model_config
get
'env_file_encoding'
super
__init__
settings_cls
case_sensitive
env_prefix
env_nested_delimiter
env_ignore_empty
env_parse_none_str
env_parse_enums
EnvSettingsSource
EnvSettingsSource
settings_cls
type
BaseSettings
case_sensitive
bool
None
None
env_prefix
str
None
None
env_nested_delimiter
str
None
None
env_ignore_empty
bool
None
None
env_parse_none_str
str
None
None
env_parse_enums
bool
None
None
Bases:
PydanticBaseEnvSettingsSource
Source class for loading settings values from environment variables.
Source code in
pydantic_settings/sources.py
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
def
__init__
self
settings_cls
type
BaseSettings
case_sensitive
bool
None
None
env_prefix
str
None
None
env_nested_delimiter
str
None
None
env_ignore_empty
bool
None
None
env_parse_none_str
str
None
None
env_parse_enums
bool
None
None
None
super
__init__
settings_cls
case_sensitive
env_prefix
env_ignore_empty
env_parse_none_str
env_parse_enums
self
env_nested_delimiter
env_nested_delimiter
env_nested_delimiter
not
None
else
self
config
get
'env_nested_delimiter'
self
env_prefix_len
len
self
env_prefix
self
env_vars
self
_load_env_vars
get_field_value
get_field_value
field
FieldInfo
field_name
str
tuple
Any
str
bool
Gets the value for field from environment variables and a flag to determine whether value is complex.
Parameters:
Name
Type
Description
Default
field
FieldInfo
The field.
required
field_name
str
The field name.
required
Returns:
Type
Description
tuple
Any
str
bool
A tuple that contains the value (
None
if not found), key, and
a flag to determine whether value is complex.
Source code in
pydantic_settings/sources.py
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
def
get_field_value
self
field
FieldInfo
field_name
str
tuple
Any
str
bool
"""
Gets the value for field from environment variables and a flag to determine whether value is complex.
Args:
field: The field.
field_name: The field name.
Returns:
A tuple that contains the value (`None` if not found), key, and
a flag to determine whether value is complex.
"""
env_val
str
None
None
for
field_key
env_name
value_is_complex
self
_extract_field_info
field
field_name
env_val
self
env_vars
get
env_name
env_val
not
None
break
return
env_val
field_key
value_is_complex
prepare_field_value
prepare_field_value
field_name
str
field
FieldInfo
value
Any
value_is_complex
bool
Any
Prepare value for the field.
Extract value for nested field.
Deserialize value to python object for complex field.
Parameters:
Name
Type
Description
Default
field
FieldInfo
The field.
required
field_name
str
The field name.
required
Returns:
Type
Description
Any
A tuple contains prepared value for the field.
Raises:
Type
Description
ValuesError
When There is an error in deserializing value for complex field.
Source code in
pydantic_settings/sources.py
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
def
prepare_field_value
self
field_name
str
field
FieldInfo
value
Any
value_is_complex
bool
Any
"""
Prepare value for the field.
* Extract value for nested field.
* Deserialize value to python object for complex field.
Args:
field: The field.
field_name: The field name.
Returns:
A tuple contains prepared value for the field.
Raises:
ValuesError: When There is an error in deserializing value for complex field.
"""
is_complex
allow_parse_failure
self
_field_is_complex
field
self
env_parse_enums
enum_val
_annotation_enum_name_to_val
field
annotation
value
value
value
enum_val
None
else
enum_val
is_complex
value_is_complex
isinstance
value
EnvNoneType
return
value
elif
value
None
# field is complex but no value found so far, try explode_env_vars
env_val_built
self
explode_env_vars
field_name
field
self
env_vars
env_val_built
return
env_val_built
else
# field is complex and there's a value, decode that as JSON, then add explode_env_vars
try
value
self
decode_complex_value
field_name
field
value
except
ValueError
not
allow_parse_failure
raise
isinstance
value
dict
return
deep_update
value
self
explode_env_vars
field_name
field
self
env_vars
else
return
value
elif
value
not
None
# simplest case, field is not complex, we only need to add the value if it was found
return
value
next_field
next_field
field
FieldInfo
Any
None
key
str
case_sensitive
bool
None
None
FieldInfo
None
Find the field in a sub model by key(env name)
By having the following models:
```py
class SubSubModel(BaseSettings):
dvals: Dict
class SubModel(BaseSettings):
vals: list[str]
sub_sub_model: SubSubModel
class Cfg(BaseSettings):
sub_model: SubModel
```
Then
next_field(sub_model, 'vals') Returns the
vals
field of
SubModel
class
next_field(sub_model, 'sub_sub_model') Returns
sub_sub_model
field of
SubModel
class
Parameters:
Name
Type
Description
Default
field
FieldInfo
Any
| None
The field.
required
key
str
The key (env name).
required
case_sensitive
bool
| None
Whether to search for key case sensitively.
None
Returns:
Type
Description
FieldInfo
| None
Field if it finds the next field otherwise
None
Source code in
pydantic_settings/sources.py
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
def
next_field
self
field
FieldInfo
Any
None
key
str
case_sensitive
bool
None
None
FieldInfo
None
"""
Find the field in a sub model by key(env name)
By having the following models:
```py
class SubSubModel(BaseSettings):
dvals: Dict
class SubModel(BaseSettings):
vals: list[str]
sub_sub_model: SubSubModel
class Cfg(BaseSettings):
sub_model: SubModel
```
Then:
next_field(sub_model, 'vals') Returns the `vals` field of `SubModel` class
next_field(sub_model, 'sub_sub_model') Returns `sub_sub_model` field of `SubModel` class
Args:
field: The field.
key: The key (env name).
case_sensitive: Whether to search for key case sensitively.
Returns:
Field if it finds the next field otherwise `None`.
"""
not
field
return
None
annotation
field
annotation
isinstance
field
FieldInfo
else
field
origin_is_union
get_origin
annotation
isinstance
annotation
WithArgsTypes
for
type_
get_args
annotation
type_has_key
self
next_field
type_
key
case_sensitive
type_has_key
return
type_has_key
elif
is_model_class
annotation
is_pydantic_dataclass
annotation
fields
_get_model_fields
annotation
# `case_sensitive is None` is here to be compatible with the old behavior.
# Has to be removed in V3.
for
field_name
fields
items
():
for
env_name
self
_extract_field_info
field_name
case_sensitive
None
case_sensitive
field_name
key
env_name
key
return
elif
field_name
lower
key
lower
env_name
lower
key
lower
():
return
return
None
explode_env_vars
explode_env_vars
field_name
str
field
FieldInfo
env_vars
Mapping
str
str
None
dict
str
Any
Process env_vars and extract the values of keys containing env_nested_delimiter into nested dictionaries.
This is applied to a single field, hence filtering by env_var prefix.
Parameters:
Name
Type
Description
Default
field_name
str
The field name.
required
field
FieldInfo
The field.
required
env_vars
Mapping
str
str
| None]
Environment variables.
required
Returns:
Type
Description
dict
str
Any
A dictionary contains extracted values from nested env values.
Source code in
pydantic_settings/sources.py
892
893
894
895
896
897
898
899
900
901
902
903
904
905
906
907
908
909
910
911
912
913
914
915
916
917
918
919
920
921
922
923
924
925
926
927
928
929
930
931
932
933
934
935
936
937
938
939
940
941
942
943
944
945
def
explode_env_vars
self
field_name
str
field
FieldInfo
env_vars
Mapping
str
str
None
dict
str
Any
"""
Process env_vars and extract the values of keys containing env_nested_delimiter into nested dictionaries.
This is applied to a single field, hence filtering by env_var prefix.
Args:
field_name: The field name.
field: The field.
env_vars: Environment variables.
Returns:
A dictionary contains extracted values from nested env values.
"""
is_dict
lenient_issubclass
get_origin
field
annotation
dict
prefixes
env_name
self
env_nested_delimiter
for
env_name
self
_extract_field_info
field
field_name
result
dict
str
Any
for
env_name
env_val
env_vars
items
():
not
any
env_name
startswith
prefix
for
prefix
prefixes
continue
# we remove the prefix before splitting in case the prefix has characters in common with the delimiter
env_name_without_prefix
env_name
self
env_prefix_len
keys
last_key
env_name_without_prefix
split
self
env_nested_delimiter
env_var
result
target_field
FieldInfo
None
field
for
key
keys
target_field
self
next_field
target_field
key
self
case_sensitive
isinstance
env_var
dict
env_var
env_var
setdefault
key
{})
# get proper field with last_key
target_field
self
next_field
target_field
last_key
self
case_sensitive
# check if env_val maps to a complex field and if so, parse the env_val
target_field
is_dict
and
env_val
target_field
is_complex
allow_json_failure
self
_field_is_complex
target_field
else
# nested field type is dict
is_complex
allow_json_failure
True
True
is_complex
try
env_val
self
decode_complex_value
last_key
target_field
env_val
# type: ignore
except
ValueError
not
allow_json_failure
raise
isinstance
env_var
dict
last_key
not
env_var
not
isinstance
env_val
EnvNoneType
env_var
last_key
{}:
env_var
last_key
env_val
return
result
ForceDecode
Annotation to force decoding of a field value.
InitSettingsSource
InitSettingsSource
settings_cls
type
BaseSettings
init_kwargs
dict
str
Any
nested_model_default_partial_update
bool
None
None
Bases:
PydanticBaseSettingsSource
Source class for loading values provided during settings class initialization.
Source code in
pydantic_settings/sources.py
385
386
387
388
389
390
391
392
393
394
395
396
397
def
__init__
self
settings_cls
type
BaseSettings
init_kwargs
dict
str
Any
nested_model_default_partial_update
bool
None
None
self
init_kwargs
init_kwargs
super
__init__
settings_cls
self
nested_model_default_partial_update
nested_model_default_partial_update
nested_model_default_partial_update
not
None
else
self
config
get
'nested_model_default_partial_update'
False
JsonConfigSettingsSource
JsonConfigSettingsSource
settings_cls
type
BaseSettings
json_file
PathType
None
DEFAULT_PATH
json_file_encoding
str
None
None
Bases:
InitSettingsSource
ConfigFileSourceMixin
A source class that loads variables from a JSON file
Source code in
pydantic_settings/sources.py
2015
2016
2017
2018
2019
2020
2021
2022
2023
2024
2025
2026
2027
2028
def
__init__
self
settings_cls
type
BaseSettings
json_file
PathType
None
DEFAULT_PATH
json_file_encoding
str
None
None
self
json_file_path
json_file
json_file
DEFAULT_PATH
else
settings_cls
model_config
get
'json_file'
self
json_file_encoding
json_file_encoding
json_file_encoding
not
None
else
settings_cls
model_config
get
'json_file_encoding'
self
json_data
self
_read_files
self
json_file_path
super
__init__
settings_cls
self
json_data
NoDecode
Annotation to prevent decoding of a field value.
PydanticBaseSettingsSource
PydanticBaseSettingsSource
settings_cls
type
BaseSettings
Bases:
ABC
Abstract base class for settings sources, every settings source classes should inherit from it.
Source code in
pydantic_settings/sources.py
236
237
238
239
240
def
__init__
self
settings_cls
type
BaseSettings
]):
self
settings_cls
settings_cls
self
config
settings_cls
model_config
self
_current_state
dict
str
Any
self
_settings_sources_data
dict
str
dict
str
Any
current_state
property
current_state
dict
str
Any
The current state of the settings, populated by the previous settings sources.
settings_sources_data
property
settings_sources_data
dict
str
dict
str
Any
The state of all previous settings sources.
get_field_value
abstractmethod
get_field_value
field
FieldInfo
field_name
str
tuple
Any
str
bool
Gets the value, the key for model creation, and a flag to determine whether value is complex.
This is an abstract method that should be overridden in every settings source classes.
Parameters:
Name
Type
Description
Default
field
FieldInfo
The field.
required
field_name
str
The field name.
required
Returns:
Type
Description
tuple
Any
str
bool
A tuple that contains the value, key and a flag to determine whether value is complex.
Source code in
pydantic_settings/sources.py
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
@abstractmethod
def
get_field_value
self
field
FieldInfo
field_name
str
tuple
Any
str
bool
"""
Gets the value, the key for model creation, and a flag to determine whether value is complex.
This is an abstract method that should be overridden in every settings source classes.
Args:
field: The field.
field_name: The field name.
Returns:
A tuple that contains the value, key and a flag to determine whether value is complex.
"""
pass
field_is_complex
field_is_complex
field
FieldInfo
bool
Checks whether a field is complex, in which case it will attempt to be parsed as JSON.
Parameters:
Name
Type
Description
Default
field
FieldInfo
The field.
required
Returns:
Type
Description
bool
Whether the field is complex.
Source code in
pydantic_settings/sources.py
286
287
288
289
290
291
292
293
294
295
296
def
field_is_complex
self
field
FieldInfo
bool
"""
Checks whether a field is complex, in which case it will attempt to be parsed as JSON.
Args:
field: The field.
Returns:
Whether the field is complex.
"""
return
_annotation_is_complex
field
annotation
field
metadata
prepare_field_value
prepare_field_value
field_name
str
field
FieldInfo
value
Any
value_is_complex
bool
Any
Prepares the value of a field.
Parameters:
Name
Type
Description
Default
field_name
str
The field name.
required
field
FieldInfo
The field.
required
value
Any
The value of the field that has to be prepared.
required
value_is_complex
bool
A flag to determine whether value is complex.
required
Returns:
Type
Description
Any
The prepared value.
Source code in
pydantic_settings/sources.py
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
def
prepare_field_value
self
field_name
str
field
FieldInfo
value
Any
value_is_complex
bool
Any
"""
Prepares the value of a field.
Args:
field_name: The field name.
field: The field.
value: The value of the field that has to be prepared.
value_is_complex: A flag to determine whether value is complex.
Returns:
The prepared value.
"""
value
not
None
and
self
field_is_complex
field
value_is_complex
return
self
decode_complex_value
field_name
field
value
return
value
decode_complex_value
decode_complex_value
field_name
str
field
FieldInfo
value
Any
Any
Decode the value for a complex field
Parameters:
Name
Type
Description
Default
field_name
str
The field name.
required
field
FieldInfo
The field.
required
value
Any
The value of the field that has to be prepared.
required
Returns:
Type
Description
Any
The decoded value for further preparation
Source code in
pydantic_settings/sources.py
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
def
decode_complex_value
self
field_name
str
field
FieldInfo
value
Any
Any
"""
Decode the value for a complex field
Args:
field_name: The field name.
field: The field.
value: The value of the field that has to be prepared.
Returns:
The decoded value for further preparation
"""
field
and
NoDecode
field
metadata
self
config
get
'enable_decoding'
False
and
ForceDecode
not
field
metadata
return
value
return
json
loads
value
PyprojectTomlConfigSettingsSource
PyprojectTomlConfigSettingsSource
settings_cls
type
BaseSettings
toml_file
Path
None
None
Bases:
TomlConfigSettingsSource
A source class that loads variables from a
pyproject.toml
file.
Source code in
pydantic_settings/sources.py
2068
2069
2070
2071
2072
2073
2074
2075
2076
2077
2078
2079
2080
2081
2082
def
__init__
self
settings_cls
type
BaseSettings
toml_file
Path
None
None
None
self
toml_file_path
self
_pick_pyproject_toml_file
toml_file
settings_cls
model_config
get
'pyproject_toml_depth'
self
toml_table_header
tuple
str
...
settings_cls
model_config
get
'pyproject_toml_table_header'
'tool'
'pydantic-settings'
self
toml_data
self
_read_files
self
toml_file_path
for
key
self
toml_table_header
self
toml_data
self
toml_data
get
key
{})
super
TomlConfigSettingsSource
self
__init__
settings_cls
self
toml_data
SecretsSettingsSource
SecretsSettingsSource
settings_cls
type
BaseSettings
secrets_dir
PathType
None
None
case_sensitive
bool
None
None
env_prefix
str
None
None
env_ignore_empty
bool
None
None
env_parse_none_str
str
None
None
env_parse_enums
bool
None
None
Bases:
PydanticBaseEnvSettingsSource
Source class for loading settings values from secret files.
Source code in
pydantic_settings/sources.py
629
630
631
632
633
634
635
636
637
638
639
640
641
642
def
__init__
self
settings_cls
type
BaseSettings
secrets_dir
PathType
None
None
case_sensitive
bool
None
None
env_prefix
str
None
None
env_ignore_empty
bool
None
None
env_parse_none_str
str
None
None
env_parse_enums
bool
None
None
None
super
__init__
settings_cls
case_sensitive
env_prefix
env_ignore_empty
env_parse_none_str
env_parse_enums
self
secrets_dir
secrets_dir
secrets_dir
not
None
else
self
config
get
'secrets_dir'
find_case_path
classmethod
find_case_path
dir_path
Path
file_name
str
case_sensitive
bool
Path
None
Find a file within path's directory matching filename, optionally ignoring case.
Parameters:
Name
Type
Description
Default
dir_path
Path
Directory path.
required
file_name
str
File name.
required
case_sensitive
bool
Whether to search for file name case sensitively.
required
Returns:
Type
Description
Path
| None
Whether file path or
None
if file does not exist in directory.
Source code in
pydantic_settings/sources.py
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
@classmethod
def
find_case_path
cls
dir_path
Path
file_name
str
case_sensitive
bool
Path
None
"""
Find a file within path's directory matching filename, optionally ignoring case.
Args:
dir_path: Directory path.
file_name: File name.
case_sensitive: Whether to search for file name case sensitively.
Returns:
Whether file path or `None` if file does not exist in directory.
"""
for
dir_path
iterdir
():
name
file_name
return
elif
not
case_sensitive
and
name
lower
file_name
lower
():
return
return
None
get_field_value
get_field_value
field
FieldInfo
field_name
str
tuple
Any
str
bool
Gets the value for field from secret file and a flag to determine whether value is complex.
Parameters:
Name
Type
Description
Default
field
FieldInfo
The field.
required
field_name
str
The field name.
required
Returns:
Type
Description
tuple
Any
str
bool
A tuple that contains the value (
None
if the file does not exist), key, and
a flag to determine whether value is complex.
Source code in
pydantic_settings/sources.py
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
def
get_field_value
self
field
FieldInfo
field_name
str
tuple
Any
str
bool
"""
Gets the value for field from secret file and a flag to determine whether value is complex.
Args:
field: The field.
field_name: The field name.
Returns:
A tuple that contains the value (`None` if the file does not exist), key, and
a flag to determine whether value is complex.
"""
for
field_key
env_name
value_is_complex
self
_extract_field_info
field
field_name
# paths reversed to match the last-wins behaviour of `env_file`
for
secrets_path
reversed
self
secrets_paths
path
self
find_case_path
secrets_path
env_name
self
case_sensitive
not
path
# path does not exist, we currently don't return a warning for this
continue
path
is_file
():
return
path
read_text
strip
(),
field_key
value_is_complex
else
warnings
warn
'attempted to load secret file "
path
" but found a
path_type_label
path
instead.'
stacklevel
return
None
field_key
value_is_complex
TomlConfigSettingsSource
TomlConfigSettingsSource
settings_cls
type
BaseSettings
toml_file
PathType
None
DEFAULT_PATH
Bases:
InitSettingsSource
ConfigFileSourceMixin
A source class that loads variables from a TOML file
Source code in
pydantic_settings/sources.py
2043
2044
2045
2046
2047
2048
2049
2050
def
__init__
self
settings_cls
type
BaseSettings
toml_file
PathType
None
DEFAULT_PATH
self
toml_file_path
toml_file
toml_file
DEFAULT_PATH
else
settings_cls
model_config
get
'toml_file'
self
toml_data
self
_read_files
self
toml_file_path
super
__init__
settings_cls
self
toml_data
YamlConfigSettingsSource
YamlConfigSettingsSource
settings_cls
type
BaseSettings
yaml_file
PathType
None
DEFAULT_PATH
yaml_file_encoding
str
None
None
Bases:
InitSettingsSource
ConfigFileSourceMixin
A source class that loads variables from a yaml file
Source code in
pydantic_settings/sources.py
2114
2115
2116
2117
2118
2119
2120
2121
2122
2123
2124
2125
2126
2127
def
__init__
self
settings_cls
type
BaseSettings
yaml_file
PathType
None
DEFAULT_PATH
yaml_file_encoding
str
None
None
self
yaml_file_path
yaml_file
yaml_file
DEFAULT_PATH
else
settings_cls
model_config
get
'yaml_file'
self
yaml_file_encoding
yaml_file_encoding
yaml_file_encoding
not
None
else
settings_cls
model_config
get
'yaml_file_encoding'
self
yaml_data
self
_read_files
self
yaml_file_path
super
__init__
settings_cls
self
yaml_data
get_subcommand
get_subcommand
model
PydanticModel
is_required
bool
True
cli_exit_on_error
bool
None
None
Optional
PydanticModel
Get the subcommand from a model.
Parameters:
Name
Type
Description
Default
model
PydanticModel
The model to get the subcommand from.
required
is_required
bool
Determines whether a model must have subcommand set and raises error if not
found. Defaults to
True
True
cli_exit_on_error
bool
| None
Determines whether this function exits with error if no subcommand is found.
Defaults to model_config
cli_exit_on_error
value if set. Otherwise, defaults to
True
None
Returns:
Type
Description
Optional
PydanticModel
The subcommand model if found, otherwise
None
Raises:
Type
Description
SystemExit
When no subcommand is found and is_required=
True
and cli_exit_on_error=
True
(the default).
SettingsError
When no subcommand is found and is_required=
True
and
cli_exit_on_error=
False
Source code in
pydantic_settings/sources.py
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
def
get_subcommand
model
PydanticModel
is_required
bool
True
cli_exit_on_error
bool
None
None
Optional
PydanticModel
"""
Get the subcommand from a model.
Args:
model: The model to get the subcommand from.
is_required: Determines whether a model must have subcommand set and raises error if not
found. Defaults to `True`.
cli_exit_on_error: Determines whether this function exits with error if no subcommand is found.
Defaults to model_config `cli_exit_on_error` value if set. Otherwise, defaults to `True`.
Returns:
The subcommand model if found, otherwise `None`.
Raises:
SystemExit: When no subcommand is found and is_required=`True` and cli_exit_on_error=`True`
(the default).
SettingsError: When no subcommand is found and is_required=`True` and
cli_exit_on_error=`False`.
"""
model_cls
type
model
cli_exit_on_error
None
and
is_model_class
model_cls
model_default
model_cls
model_config
get
'cli_exit_on_error'
isinstance
model_default
bool
cli_exit_on_error
model_default
cli_exit_on_error
None
cli_exit_on_error
True
subcommands
list
str
for
field_name
field_info
_get_model_fields
model_cls
items
():
_CliSubCommand
field_info
metadata
getattr
model
field_name
not
None
return
getattr
model
field_name
subcommands
append
field_name
is_required
error_message
'Error: CLI subcommand is required
", "
join
subcommands
subcommands
else
'Error: CLI subcommand is required but no subcommands were found.'
raise
SystemExit
error_message
cli_exit_on_error
else
SettingsError
error_message
return
None
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 032_root_model.txt ---
RootModel
RootModel class and type definitions.
RootModel
RootModel
root
RootModelRootType
PydanticUndefined
data
Bases:
BaseModel
Generic
RootModelRootType
Usage Documentation
RootModel
and Custom Root Types
A Pydantic
BaseModel
for the root object of the model.
Attributes:
Name
Type
Description
root
RootModelRootType
The root object of the model.
__pydantic_root_model__
Whether the model is a RootModel.
__pydantic_private__
Private fields in the model.
__pydantic_extra__
Extra fields in the model.
Source code in
pydantic/root_model.py
def
__init__
self
root
RootModelRootType
PydanticUndefined
data
None
# type: ignore
__tracebackhide__
True
data
root
not
PydanticUndefined
raise
ValueError
'"RootModel.__init__" accepts either a single positional argument or arbitrary keyword arguments'
root
data
# type: ignore
self
__pydantic_validator__
validate_python
root
self_instance
self
model_construct
classmethod
model_construct
root
RootModelRootType
_fields_set
set
str
None
None
Self
Create a new model using the provided root object and update fields set.
Parameters:
Name
Type
Description
Default
root
RootModelRootType
The root object of the model.
required
_fields_set
set
str
] | None
The set of fields to be updated.
None
Returns:
Type
Description
Self
The new model.
Raises:
Type
Description
NotImplemented
If the model is not a subclass of
RootModel
Source code in
pydantic/root_model.py
@classmethod
def
model_construct
cls
root
RootModelRootType
_fields_set
set
str
None
None
Self
# type: ignore
"""Create a new model using the provided root object and update fields set.
Args:
root: The root object of the model.
_fields_set: The set of fields to be updated.
Returns:
The new model.
Raises:
NotImplemented: If the model is not a subclass of `RootModel`.
"""
return
super
model_construct
root
root
_fields_set
_fields_set
model_dump
model_dump
mode
Literal
"json"
"python"
str
"python"
include
Any
None
exclude
Any
None
context
dict
str
Any
None
None
by_alias
bool
None
None
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
round_trip
bool
False
warnings
bool
Literal
"none"
"warn"
"error"
True
serialize_as_any
bool
False
Any
This method is included just to get a more accurate return type for type checkers.
It is included in this
if TYPE_CHECKING:
block since no override is actually necessary.
See the documentation of
BaseModel.model_dump
for more details about the arguments.
Generally, this method will have a return type of
RootModelRootType
, assuming that
RootModelRootType
not a
BaseModel
subclass. If
RootModelRootType
is a
BaseModel
subclass, then the return
type will likely be
dict[str, Any]
, as
model_dump
calls are recursive. The return type could
even be something different, in the case of a custom serializer.
Thus,
Any
is used here to catch all of these cases.
Source code in
pydantic/root_model.py
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
def
model_dump
# type: ignore
self
mode
Literal
'json'
'python'
str
'python'
include
Any
None
exclude
Any
None
context
dict
str
Any
None
None
by_alias
bool
None
None
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
round_trip
bool
False
warnings
bool
Literal
'none'
'warn'
'error'
True
serialize_as_any
bool
False
Any
"""This method is included just to get a more accurate return type for type checkers.
It is included in this `if TYPE_CHECKING:` block since no override is actually necessary.
See the documentation of `BaseModel.model_dump` for more details about the arguments.
Generally, this method will have a return type of `RootModelRootType`, assuming that `RootModelRootType` is
not a `BaseModel` subclass. If `RootModelRootType` is a `BaseModel` subclass, then the return
type will likely be `dict[str, Any]`, as `model_dump` calls are recursive. The return type could
even be something different, in the case of a custom serializer.
Thus, `Any` is used here to catch all of these cases.
"""
...
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 037_pydantic_extra_types_currency_code.txt ---
Currency
Currency definitions that are based on the
ISO4217
ISO4217
Bases:
str
ISO4217 parses Currency in the
ISO 4217
format.
from
pydantic
import
BaseModel
from
pydantic_extra_types.currency_code
import
ISO4217
class
Currency
BaseModel
alpha_3
ISO4217
currency
Currency
alpha_3
'AED'
print
currency
# > alpha_3='AED'
Currency
Bases:
str
Currency parses currency subset of the
ISO 4217
format.
It excludes bonds testing codes and precious metals.
from
pydantic
import
BaseModel
from
pydantic_extra_types.currency_code
import
Currency
class
currency
BaseModel
alpha_3
Currency
cur
currency
alpha_3
'AED'
print
cur
# > alpha_3='AED'
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 003_pydantic_extra_types_phone_numbers_pydantic_extra_types_phone_numbers_PhoneNumber_default_region_code.txt ---
Phone Numbers
The
pydantic_extra_types.phone_numbers
module provides the
PhoneNumber
data type.
This class depends on the [phonenumbers] package, which is a Python port of Google's [libphonenumber].
PhoneNumber
Bases:
str
A wrapper around
phonenumbers
package, which
is a Python port of Google's
libphonenumber
supported_regions
class-attribute
instance-attribute
supported_regions
list
str
The supported regions. If empty, all regions are supported.
default_region_code
class-attribute
default_region_code
str
None
None
The default region code to use when parsing phone numbers without an international prefix.
phone_format
class-attribute
instance-attribute
phone_format
str
'RFC3966'
The format of the phone number.
PhoneNumberValidator
dataclass
PhoneNumberValidator
default_region
Optional
str
None
number_format
str
"RFC3966"
supported_regions
Optional
Sequence
str
None
A pydantic before validator for phone numbers using the
phonenumbers
package,
a Python port of Google's
libphonenumber
Intended to be used to create custom pydantic data types using the
typing.Annotated
type construct.
Parameters:
Name
Type
Description
Default
default_region
str
| None
The default region code to use when parsing phone numbers without an international prefix.
None
(default), the region must be supplied in the phone number as an international prefix.
None
number_format
str
The format of the phone number to return. See
phonenumbers.PhoneNumberFormat
for valid values.
'RFC3966'
supported_regions
list
str
The supported regions. If empty, all regions are supported (default).
None
Returns:
str: The formatted phone number.
Example
MyNumberType = Annotated[
Union[str, phonenumbers.PhoneNumber],
PhoneNumberValidator()
USNumberType = Annotated[
Union[str, phonenumbers.PhoneNumber],
PhoneNumberValidator(supported_regions=['US'], default_region='US')
class SomeModel(BaseModel):
phone_number: MyNumberType
us_number: USNumberType
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 012_json_schema_pydantic_json_schema_JsonSchemaMode.txt ---
JSON Schema
Usage Documentation
JSON Schema
The
json_schema
module contains classes and functions to allow the way
JSON Schema
is generated to be customized.
In general you shouldn't need to use this module directly; instead, you can use
BaseModel.model_json_schema
and
TypeAdapter.json_schema
CoreSchemaOrFieldType
module-attribute
CoreSchemaOrFieldType
Literal
CoreSchemaType
CoreSchemaFieldType
A type alias for defined schema types that represents a union of
core_schema.CoreSchemaType
and
core_schema.CoreSchemaFieldType
JsonSchemaValue
module-attribute
JsonSchemaValue
dict
str
Any
A type alias for a JSON schema value. This is a dictionary of string keys to arbitrary JSON values.
JsonSchemaMode
module-attribute
JsonSchemaMode
Literal
'validation'
'serialization'
A type alias that represents the mode of a JSON schema; either 'validation' or 'serialization'.
For some types, the inputs to validation differ from the outputs of serialization. For example,
computed fields will only be present when serializing, and should not be provided when
validating. This flag provides a way to indicate whether you want the JSON schema required
for validation inputs, or that will be matched by serialization outputs.
JsonSchemaWarningKind
module-attribute
JsonSchemaWarningKind
Literal
"skipped-choice"
"non-serializable-default"
"skipped-discriminator"
A type alias representing the kinds of warnings that can be emitted during JSON schema generation.
See
GenerateJsonSchema.render_warning_message
for more details.
NoDefault
module-attribute
NoDefault
object
A sentinel value used to indicate that no default value should be used when generating a JSON Schema
for a core schema with a default value.
DEFAULT_REF_TEMPLATE
module-attribute
DEFAULT_REF_TEMPLATE
'#/$defs/
{model}
The default format string used to generate reference names.
PydanticJsonSchemaWarning
Bases:
UserWarning
This class is used to emit warnings produced during JSON schema generation.
See the
GenerateJsonSchema.emit_warning
and
GenerateJsonSchema.render_warning_message
methods for more details; these can be overridden to control warning behavior.
GenerateJsonSchema
GenerateJsonSchema
by_alias
bool
True
ref_template
str
DEFAULT_REF_TEMPLATE
Usage Documentation
Customizing the JSON Schema Generation Process
A class for generating JSON schemas.
This class generates JSON schemas based on configured parameters. The default schema dialect
https://json-schema.org/draft/2020-12/schema
The class uses
by_alias
to configure how fields with
multiple names are handled and
ref_template
to format reference names.
Attributes:
Name
Type
Description
schema_dialect
The JSON schema dialect used to generate the schema. See
Declaring a Dialect
in the JSON Schema documentation for more information about dialects.
ignored_warning_kinds
set
JsonSchemaWarningKind
Warnings to ignore when generating the schema.
self.render_warning_message
will
do nothing if its argument
kind
is in
ignored_warning_kinds
this value can be modified on subclasses to easily control which warnings are emitted.
by_alias
Whether to use field aliases when generating the schema.
ref_template
The format string used when generating reference names.
core_to_json_refs
dict
CoreModeRef
JsonRef
A mapping of core refs to JSON refs.
core_to_defs_refs
dict
CoreModeRef
DefsRef
A mapping of core refs to definition refs.
defs_to_core_refs
dict
DefsRef
CoreModeRef
A mapping of definition refs to core refs.
json_to_defs_refs
dict
JsonRef
DefsRef
A mapping of JSON refs to definition refs.
definitions
dict
DefsRef
JsonSchemaValue
Definitions in the schema.
Parameters:
Name
Type
Description
Default
by_alias
bool
Whether to use field aliases in the generated schemas.
True
ref_template
str
The format string to use when generating reference names.
DEFAULT_REF_TEMPLATE
Raises:
Type
Description
JsonSchemaError
If the instance of the class is inadvertently reused after generating a schema.
Source code in
pydantic/json_schema.py
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
def
__init__
self
by_alias
bool
True
ref_template
str
DEFAULT_REF_TEMPLATE
self
by_alias
by_alias
self
ref_template
ref_template
self
core_to_json_refs
dict
CoreModeRef
JsonRef
self
core_to_defs_refs
dict
CoreModeRef
DefsRef
self
defs_to_core_refs
dict
DefsRef
CoreModeRef
self
json_to_defs_refs
dict
JsonRef
DefsRef
self
definitions
dict
DefsRef
JsonSchemaValue
self
_config_wrapper_stack
_config
ConfigWrapperStack
_config
ConfigWrapper
({}))
self
_mode
JsonSchemaMode
'validation'
# The following includes a mapping of a fully-unique defs ref choice to a list of preferred
# alternatives, which are generally simpler, such as only including the class name.
# At the end of schema generation, we use these to produce a JSON schema with more human-readable
# definitions, which would also work better in a generated OpenAPI client, etc.
self
_prioritized_defsref_choices
dict
DefsRef
list
DefsRef
self
_collision_counter
dict
str
int
defaultdict
int
self
_collision_index
dict
str
int
self
_schema_type_to_method
self
build_schema_type_to_method
# When we encounter definitions we need to try to build them immediately
# so that they are available schemas that reference them
# But it's possible that CoreSchema was never going to be used
# (e.g. because the CoreSchema that references short circuits is JSON schema generation without needing
# the reference) so instead of failing altogether if we can't build a definition we
# store the error raised and re-throw it if we end up needing that def
self
_core_defs_invalid_for_json_schema
dict
DefsRef
PydanticInvalidForJsonSchema
# This changes to True after generating a schema, to prevent issues caused by accidental reuse
# of a single instance of a schema generator
self
_used
False
ValidationsMapping
This class just contains mappings from core_schema attribute names to the corresponding
JSON schema attribute names. While I suspect it is unlikely to be necessary, you can in
principle override this class in a subclass of GenerateJsonSchema (by inheriting from
GenerateJsonSchema.ValidationsMapping) to change these mappings.
build_schema_type_to_method
build_schema_type_to_method
dict
CoreSchemaOrFieldType
Callable
CoreSchemaOrField
JsonSchemaValue
Builds a dictionary mapping fields to methods for generating JSON schemas.
Returns:
Type
Description
dict
CoreSchemaOrFieldType
Callable
CoreSchemaOrField
JsonSchemaValue
A dictionary containing the mapping of
CoreSchemaOrFieldType
to a handler method.
Raises:
Type
Description
TypeError
If no method has been defined for generating a JSON schema for a given pydantic core schema type.
Source code in
pydantic/json_schema.py
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
def
build_schema_type_to_method
self
dict
CoreSchemaOrFieldType
Callable
CoreSchemaOrField
JsonSchemaValue
]]:
"""Builds a dictionary mapping fields to methods for generating JSON schemas.
Returns:
A dictionary containing the mapping of `CoreSchemaOrFieldType` to a handler method.
Raises:
TypeError: If no method has been defined for generating a JSON schema for a given pydantic core schema type.
"""
mapping
dict
CoreSchemaOrFieldType
Callable
CoreSchemaOrField
JsonSchemaValue
core_schema_types
list
CoreSchemaOrFieldType
list
get_literal_values
CoreSchemaOrFieldType
for
key
core_schema_types
method_name
key
replace
"-"
"_"
_schema'
try
mapping
key
getattr
self
method_name
except
AttributeError
# pragma: no cover
getenv
'PYDANTIC_PRIVATE_ALLOW_UNHANDLED_SCHEMA_TYPES'
continue
raise
TypeError
'No method for generating JsonSchema for core_schema.type=
key
!r}
'(expected:
type
self
__name__
method_name
from
return
mapping
generate_definitions
generate_definitions
inputs
Sequence
tuple
JsonSchemaKeyT
JsonSchemaMode
CoreSchema
tuple
dict
tuple
JsonSchemaKeyT
JsonSchemaMode
JsonSchemaValue
dict
DefsRef
JsonSchemaValue
Generates JSON schema definitions from a list of core schemas, pairing the generated definitions with a
mapping that links the input keys to the definition references.
Parameters:
Name
Type
Description
Default
inputs
Sequence
tuple
JsonSchemaKeyT
JsonSchemaMode
CoreSchema
A sequence of tuples, where:
The first element is a JSON schema key type.
The second element is the JSON mode: either 'validation' or 'serialization'.
The third element is a core schema.
required
Returns:
Type
Description
tuple
dict
tuple
JsonSchemaKeyT
JsonSchemaMode
JsonSchemaValue
dict
DefsRef
JsonSchemaValue
A tuple where:
The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and
whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have
JsonRef references to definitions that are defined in the second returned element.)
The second element is a dictionary whose keys are definition references for the JSON schemas
from the first returned element, and whose values are the actual JSON schema definitions.
Raises:
Type
Description
PydanticUserError
Raised if the JSON schema generator has already been used to generate a JSON schema.
Source code in
pydantic/json_schema.py
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
def
generate_definitions
self
inputs
Sequence
tuple
JsonSchemaKeyT
JsonSchemaMode
core_schema
CoreSchema
tuple
dict
tuple
JsonSchemaKeyT
JsonSchemaMode
JsonSchemaValue
dict
DefsRef
JsonSchemaValue
]]:
"""Generates JSON schema definitions from a list of core schemas, pairing the generated definitions with a
mapping that links the input keys to the definition references.
Args:
inputs: A sequence of tuples, where:
- The first element is a JSON schema key type.
- The second element is the JSON mode: either 'validation' or 'serialization'.
- The third element is a core schema.
Returns:
A tuple where:
- The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and
whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have
JsonRef references to definitions that are defined in the second returned element.)
- The second element is a dictionary whose keys are definition references for the JSON schemas
from the first returned element, and whose values are the actual JSON schema definitions.
Raises:
PydanticUserError: Raised if the JSON schema generator has already been used to generate a JSON schema.
"""
self
_used
raise
PydanticUserError
'This JSON schema generator has already been used to generate a JSON schema. '
'You must create a new instance of
type
self
__name__
to generate a new JSON schema.'
code
'json-schema-already-used'
for
mode
schema
inputs
self
_mode
mode
self
generate_inner
schema
definitions_remapping
self
_build_definitions_remapping
json_schemas_map
dict
tuple
JsonSchemaKeyT
JsonSchemaMode
DefsRef
for
key
mode
schema
inputs
self
_mode
mode
json_schema
self
generate_inner
schema
json_schemas_map
key
mode
definitions_remapping
remap_json_schema
json_schema
json_schema
'$defs'
self
definitions
json_schema
definitions_remapping
remap_json_schema
json_schema
self
_used
True
return
json_schemas_map
self
sort
json_schema
'$defs'
# type: ignore
generate
generate
schema
CoreSchema
mode
JsonSchemaMode
"validation"
JsonSchemaValue
Generates a JSON schema for a specified schema in a specified mode.
Parameters:
Name
Type
Description
Default
schema
CoreSchema
A Pydantic model.
required
mode
JsonSchemaMode
The mode in which to generate the schema. Defaults to 'validation'.
'validation'
Returns:
Type
Description
JsonSchemaValue
A JSON schema representing the specified schema.
Raises:
Type
Description
PydanticUserError
If the JSON schema generator has already been used to generate a JSON schema.
Source code in
pydantic/json_schema.py
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
def
generate
self
schema
CoreSchema
mode
JsonSchemaMode
'validation'
JsonSchemaValue
"""Generates a JSON schema for a specified schema in a specified mode.
Args:
schema: A Pydantic model.
mode: The mode in which to generate the schema. Defaults to 'validation'.
Returns:
A JSON schema representing the specified schema.
Raises:
PydanticUserError: If the JSON schema generator has already been used to generate a JSON schema.
"""
self
_mode
mode
self
_used
raise
PydanticUserError
'This JSON schema generator has already been used to generate a JSON schema. '
'You must create a new instance of
type
self
__name__
to generate a new JSON schema.'
code
'json-schema-already-used'
json_schema
JsonSchemaValue
self
generate_inner
schema
json_ref_counts
self
get_json_ref_counts
json_schema
ref
cast
JsonRef
json_schema
get
'$ref'
while
ref
not
None
# may need to unpack multiple levels
ref_json_schema
self
get_schema_from_definitions
ref
json_ref_counts
ref
and
ref_json_schema
not
None
and
len
json_schema
# "Unpack" the ref since this is the only reference and there are no sibling keys
json_schema
ref_json_schema
copy
# copy to prevent recursive dict reference
json_ref_counts
ref
ref
cast
JsonRef
json_schema
get
'$ref'
ref
None
self
_garbage_collect_definitions
json_schema
definitions_remapping
self
_build_definitions_remapping
self
definitions
json_schema
'$defs'
self
definitions
json_schema
definitions_remapping
remap_json_schema
json_schema
# For now, we will not set the $schema key. However, if desired, this can be easily added by overriding
# this method and adding the following line after a call to super().generate(schema):
# json_schema['$schema'] = self.schema_dialect
self
_used
True
return
self
sort
json_schema
generate_inner
generate_inner
schema
CoreSchemaOrField
JsonSchemaValue
Generates a JSON schema for a given core schema.
Parameters:
Name
Type
Description
Default
schema
CoreSchemaOrField
The given core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
TODO: the nested function definitions here seem like bad practice, I'd like to unpack these
in a future PR. It'd be great if we could shorten the call stack a bit for JSON schema generation,
and I think there's potential for that here.
Source code in
pydantic/json_schema.py
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
def
generate_inner
self
schema
CoreSchemaOrField
JsonSchemaValue
# noqa: C901
"""Generates a JSON schema for a given core schema.
Args:
schema: The given core schema.
Returns:
The generated JSON schema.
TODO: the nested function definitions here seem like bad practice, I'd like to unpack these
in a future PR. It'd be great if we could shorten the call stack a bit for JSON schema generation,
and I think there's potential for that here.
"""
# If a schema with the same CoreRef has been handled, just return a reference to it
# Note that this assumes that it will _never_ be the case that the same CoreRef is used
# on types that should have different JSON schemas
'ref'
schema
core_ref
CoreRef
schema
'ref'
# type: ignore[typeddict-item]
core_mode_ref
core_ref
self
mode
core_mode_ref
self
core_to_defs_refs
and
self
core_to_defs_refs
core_mode_ref
self
definitions
return
'$ref'
self
core_to_json_refs
core_mode_ref
def
populate_defs
core_schema
CoreSchema
json_schema
JsonSchemaValue
JsonSchemaValue
'ref'
core_schema
core_ref
CoreRef
core_schema
'ref'
# type: ignore[typeddict-item]
defs_ref
ref_json_schema
self
get_cache_defs_ref_schema
core_ref
json_ref
JsonRef
ref_json_schema
'$ref'
# Replace the schema if it's not a reference to itself
# What we want to avoid is having the def be just a ref to itself
# which is what would happen if we blindly assigned any
json_schema
get
'$ref'
None
json_ref
self
definitions
defs_ref
json_schema
self
_core_defs_invalid_for_json_schema
pop
defs_ref
None
json_schema
ref_json_schema
return
json_schema
def
handler_func
schema_or_field
CoreSchemaOrField
JsonSchemaValue
"""Generate a JSON schema based on the input schema.
Args:
schema_or_field: The core schema to generate a JSON schema from.
Returns:
The generated JSON schema.
Raises:
TypeError: If an unexpected schema type is encountered.
"""
# Generate the core-schema-type-specific bits of the schema generation:
json_schema
JsonSchemaValue
None
None
self
mode
'serialization'
and
'serialization'
schema_or_field
# In this case, we skip the JSON Schema generation of the schema
# and use the `'serialization'` schema instead (canonical example:
# `Annotated[int, PlainSerializer(str)]`).
ser_schema
schema_or_field
'serialization'
# type: ignore
json_schema
self
ser_schema
ser_schema
# It might be that the 'serialization'` is skipped depending on `when_used`.
# This is only relevant for `nullable` schemas though, so we special case here.
json_schema
not
None
and
ser_schema
get
'when_used'
'unless-none'
'json-unless-none'
and
schema_or_field
'type'
'nullable'
json_schema
self
get_flattened_anyof
([{
'type'
'null'
json_schema
json_schema
None
_core_utils
is_core_schema
schema_or_field
_core_utils
is_core_schema_field
schema_or_field
generate_for_schema_type
self
_schema_type_to_method
schema_or_field
'type'
json_schema
generate_for_schema_type
schema_or_field
else
raise
TypeError
'Unexpected schema type: schema=
schema_or_field
return
json_schema
current_handler
_schema_generation_shared
GenerateJsonSchemaHandler
self
handler_func
metadata
cast
_core_metadata
CoreMetadata
schema
get
'metadata'
{}))
# TODO: I dislike that we have to wrap these basic dict updates in callables, is there any way around this?
js_updates
metadata
get
'pydantic_js_updates'
def
js_updates_handler_func
schema_or_field
CoreSchemaOrField
current_handler
GetJsonSchemaHandler
current_handler
JsonSchemaValue
json_schema
current_handler
schema_or_field
js_updates
return
json_schema
current_handler
_schema_generation_shared
GenerateJsonSchemaHandler
self
js_updates_handler_func
js_extra
metadata
get
'pydantic_js_extra'
def
js_extra_handler_func
schema_or_field
CoreSchemaOrField
current_handler
GetJsonSchemaHandler
current_handler
JsonSchemaValue
json_schema
current_handler
schema_or_field
isinstance
js_extra
dict
json_schema
update
to_jsonable_python
js_extra
elif
callable
js_extra
# similar to typing issue in _update_class_schema when we're working with callable js extra
js_extra
json_schema
# type: ignore
return
json_schema
current_handler
_schema_generation_shared
GenerateJsonSchemaHandler
self
js_extra_handler_func
for
js_modify_function
metadata
get
'pydantic_js_functions'
()):
def
new_handler_func
schema_or_field
CoreSchemaOrField
current_handler
GetJsonSchemaHandler
current_handler
js_modify_function
GetJsonSchemaFunction
js_modify_function
JsonSchemaValue
json_schema
js_modify_function
schema_or_field
current_handler
_core_utils
is_core_schema
schema_or_field
json_schema
populate_defs
schema_or_field
json_schema
original_schema
current_handler
resolve_ref_schema
json_schema
ref
json_schema
pop
'$ref'
None
ref
and
json_schema
original_schema
update
json_schema
return
original_schema
current_handler
_schema_generation_shared
GenerateJsonSchemaHandler
self
new_handler_func
for
js_modify_function
metadata
get
'pydantic_js_annotation_functions'
()):
def
new_handler_func
schema_or_field
CoreSchemaOrField
current_handler
GetJsonSchemaHandler
current_handler
js_modify_function
GetJsonSchemaFunction
js_modify_function
JsonSchemaValue
return
js_modify_function
schema_or_field
current_handler
current_handler
_schema_generation_shared
GenerateJsonSchemaHandler
self
new_handler_func
json_schema
current_handler
schema
_core_utils
is_core_schema
schema
json_schema
populate_defs
schema
json_schema
return
json_schema
sort
sort
value
JsonSchemaValue
parent_key
str
None
None
JsonSchemaValue
Override this method to customize the sorting of the JSON schema (e.g., don't sort at all, sort all keys unconditionally, etc.)
By default, alphabetically sort the keys in the JSON schema, skipping the 'properties' and 'default' keys to preserve field definition order.
This sort is recursive, so it will sort all nested dictionaries as well.
Source code in
pydantic/json_schema.py
568
569
570
571
572
573
574
575
576
577
578
579
580
def
sort
self
value
JsonSchemaValue
parent_key
str
None
None
JsonSchemaValue
"""Override this method to customize the sorting of the JSON schema (e.g., don't sort at all, sort all keys unconditionally, etc.)
By default, alphabetically sort the keys in the JSON schema, skipping the 'properties' and 'default' keys to preserve field definition order.
This sort is recursive, so it will sort all nested dictionaries as well.
"""
sorted_dict
dict
str
JsonSchemaValue
keys
value
keys
parent_key
not
'properties'
'default'
keys
sorted
keys
for
key
keys
sorted_dict
key
self
_sort_recursive
value
key
parent_key
key
return
sorted_dict
invalid_schema
invalid_schema
schema
InvalidSchema
JsonSchemaValue
Placeholder - should never be called.
Source code in
pydantic/json_schema.py
602
603
604
605
def
invalid_schema
self
schema
core_schema
InvalidSchema
JsonSchemaValue
"""Placeholder - should never be called."""
raise
RuntimeError
'Cannot generate schema for invalid_schema. This is a bug! Please report it.'
any_schema
any_schema
schema
AnySchema
JsonSchemaValue
Generates a JSON schema that matches any value.
Parameters:
Name
Type
Description
Default
schema
AnySchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
607
608
609
610
611
612
613
614
615
616
def
any_schema
self
schema
core_schema
AnySchema
JsonSchemaValue
"""Generates a JSON schema that matches any value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
none_schema
none_schema
schema
NoneSchema
JsonSchemaValue
Generates a JSON schema that matches
None
Parameters:
Name
Type
Description
Default
schema
NoneSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
618
619
620
621
622
623
624
625
626
627
def
none_schema
self
schema
core_schema
NoneSchema
JsonSchemaValue
"""Generates a JSON schema that matches `None`.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
'type'
'null'
bool_schema
bool_schema
schema
BoolSchema
JsonSchemaValue
Generates a JSON schema that matches a bool value.
Parameters:
Name
Type
Description
Default
schema
BoolSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
629
630
631
632
633
634
635
636
637
638
def
bool_schema
self
schema
core_schema
BoolSchema
JsonSchemaValue
"""Generates a JSON schema that matches a bool value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
'type'
'boolean'
int_schema
int_schema
schema
IntSchema
JsonSchemaValue
Generates a JSON schema that matches an int value.
Parameters:
Name
Type
Description
Default
schema
IntSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
640
641
642
643
644
645
646
647
648
649
650
651
652
def
int_schema
self
schema
core_schema
IntSchema
JsonSchemaValue
"""Generates a JSON schema that matches an int value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
json_schema
dict
str
Any
'type'
'integer'
self
update_with_validations
json_schema
schema
self
ValidationsMapping
numeric
json_schema
for
json_schema
items
not
math
inf
math
inf
return
json_schema
float_schema
float_schema
schema
FloatSchema
JsonSchemaValue
Generates a JSON schema that matches a float value.
Parameters:
Name
Type
Description
Default
schema
FloatSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
654
655
656
657
658
659
660
661
662
663
664
665
666
def
float_schema
self
schema
core_schema
FloatSchema
JsonSchemaValue
"""Generates a JSON schema that matches a float value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
json_schema
dict
str
Any
'type'
'number'
self
update_with_validations
json_schema
schema
self
ValidationsMapping
numeric
json_schema
for
json_schema
items
not
math
inf
math
inf
return
json_schema
decimal_schema
decimal_schema
schema
DecimalSchema
JsonSchemaValue
Generates a JSON schema that matches a decimal value.
Parameters:
Name
Type
Description
Default
schema
DecimalSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
def
decimal_schema
self
schema
core_schema
DecimalSchema
JsonSchemaValue
"""Generates a JSON schema that matches a decimal value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
json_schema
self
str_schema
core_schema
str_schema
())
self
mode
'validation'
multiple_of
schema
get
'multiple_of'
schema
get
'le'
schema
get
'ge'
schema
get
'lt'
schema
get
'gt'
json_schema
'anyOf'
self
float_schema
core_schema
float_schema
allow_inf_nan
schema
get
'allow_inf_nan'
multiple_of
None
multiple_of
None
else
float
multiple_of
None
None
else
float
None
None
else
float
None
None
else
float
None
None
else
float
json_schema
return
json_schema
str_schema
str_schema
schema
StringSchema
JsonSchemaValue
Generates a JSON schema that matches a string value.
Parameters:
Name
Type
Description
Default
schema
StringSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
def
str_schema
self
schema
core_schema
StringSchema
JsonSchemaValue
"""Generates a JSON schema that matches a string value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
json_schema
'type'
'string'
self
update_with_validations
json_schema
schema
self
ValidationsMapping
string
isinstance
json_schema
get
'pattern'
Pattern
# TODO: should we add regex flags to the pattern?
json_schema
'pattern'
json_schema
get
'pattern'
pattern
# type: ignore
return
json_schema
bytes_schema
bytes_schema
schema
BytesSchema
JsonSchemaValue
Generates a JSON schema that matches a bytes value.
Parameters:
Name
Type
Description
Default
schema
BytesSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
717
718
719
720
721
722
723
724
725
726
727
728
def
bytes_schema
self
schema
core_schema
BytesSchema
JsonSchemaValue
"""Generates a JSON schema that matches a bytes value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
json_schema
'type'
'string'
'format'
'base64url'
self
_config
ser_json_bytes
'base64'
else
'binary'
self
update_with_validations
json_schema
schema
self
ValidationsMapping
bytes
return
json_schema
date_schema
date_schema
schema
DateSchema
JsonSchemaValue
Generates a JSON schema that matches a date value.
Parameters:
Name
Type
Description
Default
schema
DateSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
730
731
732
733
734
735
736
737
738
739
def
date_schema
self
schema
core_schema
DateSchema
JsonSchemaValue
"""Generates a JSON schema that matches a date value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
'type'
'string'
'format'
'date'
time_schema
time_schema
schema
TimeSchema
JsonSchemaValue
Generates a JSON schema that matches a time value.
Parameters:
Name
Type
Description
Default
schema
TimeSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
741
742
743
744
745
746
747
748
749
750
def
time_schema
self
schema
core_schema
TimeSchema
JsonSchemaValue
"""Generates a JSON schema that matches a time value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
'type'
'string'
'format'
'time'
datetime_schema
datetime_schema
schema
DatetimeSchema
JsonSchemaValue
Generates a JSON schema that matches a datetime value.
Parameters:
Name
Type
Description
Default
schema
DatetimeSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
752
753
754
755
756
757
758
759
760
761
def
datetime_schema
self
schema
core_schema
DatetimeSchema
JsonSchemaValue
"""Generates a JSON schema that matches a datetime value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
'type'
'string'
'format'
'date-time'
timedelta_schema
timedelta_schema
schema
TimedeltaSchema
JsonSchemaValue
Generates a JSON schema that matches a timedelta value.
Parameters:
Name
Type
Description
Default
schema
TimedeltaSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
763
764
765
766
767
768
769
770
771
772
773
774
def
timedelta_schema
self
schema
core_schema
TimedeltaSchema
JsonSchemaValue
"""Generates a JSON schema that matches a timedelta value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
self
_config
ser_json_timedelta
'float'
return
'type'
'number'
return
'type'
'string'
'format'
'duration'
literal_schema
literal_schema
schema
LiteralSchema
JsonSchemaValue
Generates a JSON schema that matches a literal value.
Parameters:
Name
Type
Description
Default
schema
LiteralSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
def
literal_schema
self
schema
core_schema
LiteralSchema
JsonSchemaValue
"""Generates a JSON schema that matches a literal value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
expected
to_jsonable_python
value
isinstance
Enum
else
for
schema
'expected'
result
dict
str
Any
len
expected
result
'const'
expected
else
result
'enum'
expected
types
type
for
expected
types
str
result
'type'
'string'
elif
types
int
result
'type'
'integer'
elif
types
float
result
'type'
'number'
elif
types
bool
result
'type'
'boolean'
elif
types
list
result
'type'
'array'
elif
types
type
None
)}:
result
'type'
'null'
return
result
enum_schema
enum_schema
schema
EnumSchema
JsonSchemaValue
Generates a JSON schema that matches an Enum value.
Parameters:
Name
Type
Description
Default
schema
EnumSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
def
enum_schema
self
schema
core_schema
EnumSchema
JsonSchemaValue
"""Generates a JSON schema that matches an Enum value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
enum_type
schema
'cls'
description
None
not
enum_type
__doc__
else
inspect
cleandoc
enum_type
__doc__
description
'An enumeration.'
# This is the default value provided by enum.EnumMeta.__new__; don't use it
description
None
result
dict
str
Any
'title'
enum_type
__name__
'description'
description
result
for
result
items
not
None
expected
to_jsonable_python
value
for
schema
'members'
result
'enum'
expected
types
type
for
expected
isinstance
enum_type
str
types
str
result
'type'
'string'
elif
isinstance
enum_type
int
types
int
result
'type'
'integer'
elif
isinstance
enum_type
float
types
float
result
'type'
'number'
elif
types
bool
result
'type'
'boolean'
elif
types
list
result
'type'
'array'
return
result
is_instance_schema
is_instance_schema
schema
IsInstanceSchema
JsonSchemaValue
Handles JSON schema generation for a core schema that checks if a value is an instance of a class.
Unless overridden in a subclass, this raises an error.
Parameters:
Name
Type
Description
Default
schema
IsInstanceSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
844
845
846
847
848
849
850
851
852
853
854
855
def
is_instance_schema
self
schema
core_schema
IsInstanceSchema
JsonSchemaValue
"""Handles JSON schema generation for a core schema that checks if a value is an instance of a class.
Unless overridden in a subclass, this raises an error.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
self
handle_invalid_for_json_schema
schema
'core_schema.IsInstanceSchema (
schema
"cls"
is_subclass_schema
is_subclass_schema
schema
IsSubclassSchema
JsonSchemaValue
Handles JSON schema generation for a core schema that checks if a value is a subclass of a class.
For backwards compatibility with v1, this does not raise an error, but can be overridden to change this.
Parameters:
Name
Type
Description
Default
schema
IsSubclassSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
857
858
859
860
861
862
863
864
865
866
867
868
869
def
is_subclass_schema
self
schema
core_schema
IsSubclassSchema
JsonSchemaValue
"""Handles JSON schema generation for a core schema that checks if a value is a subclass of a class.
For backwards compatibility with v1, this does not raise an error, but can be overridden to change this.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
# Note: This is for compatibility with V1; you can override if you want different behavior.
return
callable_schema
callable_schema
schema
CallableSchema
JsonSchemaValue
Generates a JSON schema that matches a callable value.
Unless overridden in a subclass, this raises an error.
Parameters:
Name
Type
Description
Default
schema
CallableSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
871
872
873
874
875
876
877
878
879
880
881
882
def
callable_schema
self
schema
core_schema
CallableSchema
JsonSchemaValue
"""Generates a JSON schema that matches a callable value.
Unless overridden in a subclass, this raises an error.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
self
handle_invalid_for_json_schema
schema
'core_schema.CallableSchema'
list_schema
list_schema
schema
ListSchema
JsonSchemaValue
Returns a schema that matches a list schema.
Parameters:
Name
Type
Description
Default
schema
ListSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
884
885
886
887
888
889
890
891
892
893
894
895
896
def
list_schema
self
schema
core_schema
ListSchema
JsonSchemaValue
"""Returns a schema that matches a list schema.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
items_schema
'items_schema'
not
schema
else
self
generate_inner
schema
'items_schema'
json_schema
'type'
'array'
'items'
items_schema
self
update_with_validations
json_schema
schema
self
ValidationsMapping
array
return
json_schema
tuple_positional_schema
tuple_positional_schema
schema
TupleSchema
JsonSchemaValue
Replaced by
tuple_schema
Source code in
pydantic/json_schema.py
898
899
900
901
902
903
904
905
906
907
@deprecated
'`tuple_positional_schema` is deprecated. Use `tuple_schema` instead.'
category
None
@final
def
tuple_positional_schema
self
schema
core_schema
TupleSchema
JsonSchemaValue
"""Replaced by `tuple_schema`."""
warnings
warn
'`tuple_positional_schema` is deprecated. Use `tuple_schema` instead.'
PydanticDeprecatedSince26
stacklevel
return
self
tuple_schema
schema
tuple_variable_schema
tuple_variable_schema
schema
TupleSchema
JsonSchemaValue
Replaced by
tuple_schema
Source code in
pydantic/json_schema.py
909
910
911
912
913
914
915
916
917
918
@deprecated
'`tuple_variable_schema` is deprecated. Use `tuple_schema` instead.'
category
None
@final
def
tuple_variable_schema
self
schema
core_schema
TupleSchema
JsonSchemaValue
"""Replaced by `tuple_schema`."""
warnings
warn
'`tuple_variable_schema` is deprecated. Use `tuple_schema` instead.'
PydanticDeprecatedSince26
stacklevel
return
self
tuple_schema
schema
tuple_schema
tuple_schema
schema
TupleSchema
JsonSchemaValue
Generates a JSON schema that matches a tuple schema e.g.
tuple[int,
str, bool]
tuple[int, ...]
Parameters:
Name
Type
Description
Default
schema
TupleSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
920
921
922
923
924
925
926
927
928
929
930
931
932
933
934
935
936
937
938
939
940
941
942
943
944
945
946
947
948
949
950
951
952
953
def
tuple_schema
self
schema
core_schema
TupleSchema
JsonSchemaValue
"""Generates a JSON schema that matches a tuple schema e.g. `tuple[int,
str, bool]` or `tuple[int, ...]`.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
json_schema
JsonSchemaValue
'type'
'array'
'variadic_item_index'
schema
variadic_item_index
schema
'variadic_item_index'
variadic_item_index
json_schema
'minItems'
variadic_item_index
json_schema
'prefixItems'
self
generate_inner
item
for
item
schema
'items_schema'
][:
variadic_item_index
variadic_item_index
len
schema
'items_schema'
]):
# if the variadic item is the last item, then represent it faithfully
json_schema
'items'
self
generate_inner
schema
'items_schema'
variadic_item_index
else
# otherwise, 'items' represents the schema for the variadic
# item plus the suffix, so just allow anything for simplicity
# for now
json_schema
'items'
True
else
prefixItems
self
generate_inner
item
for
item
schema
'items_schema'
prefixItems
json_schema
'prefixItems'
prefixItems
json_schema
'minItems'
len
prefixItems
json_schema
'maxItems'
len
prefixItems
self
update_with_validations
json_schema
schema
self
ValidationsMapping
array
return
json_schema
set_schema
set_schema
schema
SetSchema
JsonSchemaValue
Generates a JSON schema that matches a set schema.
Parameters:
Name
Type
Description
Default
schema
SetSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
955
956
957
958
959
960
961
962
963
964
def
set_schema
self
schema
core_schema
SetSchema
JsonSchemaValue
"""Generates a JSON schema that matches a set schema.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
self
_common_set_schema
schema
frozenset_schema
frozenset_schema
schema
FrozenSetSchema
JsonSchemaValue
Generates a JSON schema that matches a frozenset schema.
Parameters:
Name
Type
Description
Default
schema
FrozenSetSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
966
967
968
969
970
971
972
973
974
975
def
frozenset_schema
self
schema
core_schema
FrozenSetSchema
JsonSchemaValue
"""Generates a JSON schema that matches a frozenset schema.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
self
_common_set_schema
schema
generator_schema
generator_schema
schema
GeneratorSchema
JsonSchemaValue
Returns a JSON schema that represents the provided GeneratorSchema.
Parameters:
Name
Type
Description
Default
schema
GeneratorSchema
The schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
983
984
985
986
987
988
989
990
991
992
993
994
995
def
generator_schema
self
schema
core_schema
GeneratorSchema
JsonSchemaValue
"""Returns a JSON schema that represents the provided GeneratorSchema.
Args:
schema: The schema.
Returns:
The generated JSON schema.
"""
items_schema
'items_schema'
not
schema
else
self
generate_inner
schema
'items_schema'
json_schema
'type'
'array'
'items'
items_schema
self
update_with_validations
json_schema
schema
self
ValidationsMapping
array
return
json_schema
dict_schema
dict_schema
schema
DictSchema
JsonSchemaValue
Generates a JSON schema that matches a dict schema.
Parameters:
Name
Type
Description
Default
schema
DictSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
997
998
999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
1026
1027
1028
1029
1030
1031
1032
1033
1034
1035
1036
1037
1038
1039
1040
1041
1042
1043
def
dict_schema
self
schema
core_schema
DictSchema
JsonSchemaValue
"""Generates a JSON schema that matches a dict schema.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
json_schema
JsonSchemaValue
'type'
'object'
keys_schema
self
generate_inner
schema
'keys_schema'
copy
'keys_schema'
schema
else
'$ref'
not
keys_schema
keys_pattern
keys_schema
pop
'pattern'
None
# Don't give a title to patternProperties/propertyNames:
keys_schema
pop
'title'
None
else
# Here, we assume that if the keys schema is a definition reference,
# it can't be a simple string core schema (and thus no pattern can exist).
# However, this is only in practice (in theory, a definition reference core
# schema could be generated for a simple string schema).
# Note that we avoid calling `self.resolve_ref_schema`, as it might not exist yet.
keys_pattern
None
values_schema
self
generate_inner
schema
'values_schema'
copy
'values_schema'
schema
else
# don't give a title to additionalProperties:
values_schema
pop
'title'
None
values_schema
keys_pattern
not
None
keys_pattern
None
json_schema
'additionalProperties'
values_schema
else
json_schema
'patternProperties'
keys_pattern
values_schema
else
# for `dict[str, Any]`, we allow any key and any value, since `str` is the default key type
json_schema
'additionalProperties'
True
# The len check indicates that constraints are probably present:
keys_schema
get
'type'
'string'
and
len
keys_schema
# If this is a definition reference schema, it most likely has constraints:
'$ref'
keys_schema
keys_schema
pop
'type'
None
json_schema
'propertyNames'
keys_schema
self
update_with_validations
json_schema
schema
self
ValidationsMapping
object
return
json_schema
function_before_schema
function_before_schema
schema
BeforeValidatorFunctionSchema
JsonSchemaValue
Generates a JSON schema that matches a function-before schema.
Parameters:
Name
Type
Description
Default
schema
BeforeValidatorFunctionSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1045
1046
1047
1048
1049
1050
1051
1052
1053
1054
1055
1056
1057
def
function_before_schema
self
schema
core_schema
BeforeValidatorFunctionSchema
JsonSchemaValue
"""Generates a JSON schema that matches a function-before schema.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
self
mode
'validation'
and
input_schema
schema
get
'json_schema_input_schema'
)):
return
self
generate_inner
input_schema
return
self
generate_inner
schema
'schema'
function_after_schema
function_after_schema
schema
AfterValidatorFunctionSchema
JsonSchemaValue
Generates a JSON schema that matches a function-after schema.
Parameters:
Name
Type
Description
Default
schema
AfterValidatorFunctionSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1059
1060
1061
1062
1063
1064
1065
1066
1067
1068
def
function_after_schema
self
schema
core_schema
AfterValidatorFunctionSchema
JsonSchemaValue
"""Generates a JSON schema that matches a function-after schema.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
self
generate_inner
schema
'schema'
function_plain_schema
function_plain_schema
schema
PlainValidatorFunctionSchema
JsonSchemaValue
Generates a JSON schema that matches a function-plain schema.
Parameters:
Name
Type
Description
Default
schema
PlainValidatorFunctionSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1070
1071
1072
1073
1074
1075
1076
1077
1078
1079
1080
1081
1082
1083
1084
def
function_plain_schema
self
schema
core_schema
PlainValidatorFunctionSchema
JsonSchemaValue
"""Generates a JSON schema that matches a function-plain schema.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
self
mode
'validation'
and
input_schema
schema
get
'json_schema_input_schema'
)):
return
self
generate_inner
input_schema
return
self
handle_invalid_for_json_schema
schema
'core_schema.PlainValidatorFunctionSchema (
schema
"function"
function_wrap_schema
function_wrap_schema
schema
WrapValidatorFunctionSchema
JsonSchemaValue
Generates a JSON schema that matches a function-wrap schema.
Parameters:
Name
Type
Description
Default
schema
WrapValidatorFunctionSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1086
1087
1088
1089
1090
1091
1092
1093
1094
1095
1096
1097
1098
def
function_wrap_schema
self
schema
core_schema
WrapValidatorFunctionSchema
JsonSchemaValue
"""Generates a JSON schema that matches a function-wrap schema.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
self
mode
'validation'
and
input_schema
schema
get
'json_schema_input_schema'
)):
return
self
generate_inner
input_schema
return
self
generate_inner
schema
'schema'
default_schema
default_schema
schema
WithDefaultSchema
JsonSchemaValue
Generates a JSON schema that matches a schema with a default value.
Parameters:
Name
Type
Description
Default
schema
WithDefaultSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1100
1101
1102
1103
1104
1105
1106
1107
1108
1109
1110
1111
1112
1113
1114
1115
1116
1117
1118
1119
1120
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
1133
1134
1135
1136
1137
1138
1139
1140
1141
1142
1143
1144
1145
1146
1147
1148
1149
1150
1151
1152
def
default_schema
self
schema
core_schema
WithDefaultSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema with a default value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
json_schema
self
generate_inner
schema
'schema'
default
self
get_default_value
schema
default
NoDefault
return
json_schema
# we reflect the application of custom plain, no-info serializers to defaults for
# JSON Schemas viewed in serialization mode:
# TODO: improvements along with https://github.com/pydantic/pydantic/issues/8208
self
mode
'serialization'
and
ser_schema
schema
'schema'
get
'serialization'
and
ser_func
ser_schema
get
'function'
and
ser_schema
get
'type'
'function-plain'
and
not
ser_schema
get
'info_arg'
and
not
default
None
and
ser_schema
get
'when_used'
'unless-none'
'json-unless-none'
try
default
ser_func
default
# type: ignore
except
Exception
# It might be that the provided default needs to be validated (read: parsed) first
# (assuming `validate_default` is enabled). However, we can't perform
# such validation during JSON Schema generation so we don't support
# this pattern for now.
# (One example is when using `foo: ByteSize = '1MB'`, which validates and
# serializes as an int. In this case, `ser_func` is `int` and `int('1MB')` fails).
self
emit_warning
'non-serializable-default'
'Unable to serialize value
default
!r}
with the plain serializer; excluding default from JSON schema'
return
json_schema
try
encoded_default
self
encode_default
default
except
pydantic_core
PydanticSerializationError
self
emit_warning
'non-serializable-default'
'Default value
default
is not JSON serializable; excluding default from JSON schema'
# Return the inner schema, as though there was no default
return
json_schema
json_schema
'default'
encoded_default
return
json_schema
get_default_value
get_default_value
schema
WithDefaultSchema
Any
Get the default value to be used when generating a JSON Schema for a core schema with a default.
The default implementation is to use the statically defined default value. This method can be overridden
if you want to make use of the default factory.
Parameters:
Name
Type
Description
Default
schema
WithDefaultSchema
The
'with-default'
core schema.
required
Returns:
Type
Description
Any
The default value to use, or
NoDefault
if no default
value is available.
Source code in
pydantic/json_schema.py
1154
1155
1156
1157
1158
1159
1160
1161
1162
1163
1164
1165
1166
1167
def
get_default_value
self
schema
core_schema
WithDefaultSchema
Any
"""Get the default value to be used when generating a JSON Schema for a core schema with a default.
The default implementation is to use the statically defined default value. This method can be overridden
if you want to make use of the default factory.
Args:
schema: The `'with-default'` core schema.
Returns:
The default value to use, or [`NoDefault`][pydantic.json_schema.NoDefault] if no default
value is available.
"""
return
schema
get
'default'
NoDefault
nullable_schema
nullable_schema
schema
NullableSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that allows null values.
Parameters:
Name
Type
Description
Default
schema
NullableSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1169
1170
1171
1172
1173
1174
1175
1176
1177
1178
1179
1180
1181
1182
1183
1184
1185
1186
def
nullable_schema
self
schema
core_schema
NullableSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that allows null values.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
null_schema
'type'
'null'
inner_json_schema
self
generate_inner
schema
'schema'
inner_json_schema
null_schema
return
null_schema
else
# Thanks to the equality check against `null_schema` above, I think 'oneOf' would also be valid here;
# I'll use 'anyOf' for now, but it could be changed it if it would work better with some external tooling
return
self
get_flattened_anyof
inner_json_schema
null_schema
union_schema
union_schema
schema
UnionSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that allows values matching any of the given schemas.
Parameters:
Name
Type
Description
Default
schema
UnionSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1188
1189
1190
1191
1192
1193
1194
1195
1196
1197
1198
1199
1200
1201
1202
1203
1204
1205
1206
1207
1208
1209
1210
1211
def
union_schema
self
schema
core_schema
UnionSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that allows values matching any of the given schemas.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
generated
list
JsonSchemaValue
choices
schema
'choices'
for
choice
choices
# choice will be a tuple if an explicit label was provided
choice_schema
choice
isinstance
choice
tuple
else
choice
try
generated
append
self
generate_inner
choice_schema
except
PydanticOmit
continue
except
PydanticInvalidForJsonSchema
exc
self
emit_warning
'skipped-choice'
exc
message
len
generated
return
generated
return
self
get_flattened_anyof
generated
tagged_union_schema
tagged_union_schema
schema
TaggedUnionSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that allows values matching any of the given schemas, where
the schemas are tagged with a discriminator field that indicates which schema should be used to validate
the value.
Parameters:
Name
Type
Description
Default
schema
TaggedUnionSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1213
1214
1215
1216
1217
1218
1219
1220
1221
1222
1223
1224
1225
1226
1227
1228
1229
1230
1231
1232
1233
1234
1235
1236
1237
1238
1239
1240
1241
1242
1243
1244
1245
1246
1247
1248
def
tagged_union_schema
self
schema
core_schema
TaggedUnionSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that allows values matching any of the given schemas, where
the schemas are tagged with a discriminator field that indicates which schema should be used to validate
the value.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
generated
dict
str
JsonSchemaValue
for
schema
'choices'
items
():
isinstance
Enum
value
try
# Use str(k) since keys must be strings for json; while not technically correct,
# it's the closest that can be represented in valid JSON
generated
str
self
generate_inner
copy
except
PydanticOmit
continue
except
PydanticInvalidForJsonSchema
exc
self
emit_warning
'skipped-choice'
exc
message
one_of_choices
_deduplicate_schemas
generated
values
())
json_schema
JsonSchemaValue
'oneOf'
one_of_choices
# This reflects the v1 behavior; TODO: we should make it possible to exclude OpenAPI stuff from the JSON schema
openapi_discriminator
self
_extract_discriminator
schema
one_of_choices
openapi_discriminator
not
None
json_schema
'discriminator'
'propertyName'
openapi_discriminator
'mapping'
get
'$ref'
for
generated
items
()},
return
json_schema
chain_schema
chain_schema
schema
ChainSchema
JsonSchemaValue
Generates a JSON schema that matches a core_schema.ChainSchema.
When generating a schema for validation, we return the validation JSON schema for the first step in the chain.
For serialization, we return the serialization JSON schema for the last step in the chain.
Parameters:
Name
Type
Description
Default
schema
ChainSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1295
1296
1297
1298
1299
1300
1301
1302
1303
1304
1305
1306
1307
1308
def
chain_schema
self
schema
core_schema
ChainSchema
JsonSchemaValue
"""Generates a JSON schema that matches a core_schema.ChainSchema.
When generating a schema for validation, we return the validation JSON schema for the first step in the chain.
For serialization, we return the serialization JSON schema for the last step in the chain.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
step_index
self
mode
'validation'
else
# use first step for validation, last for serialization
return
self
generate_inner
schema
'steps'
step_index
lax_or_strict_schema
lax_or_strict_schema
schema
LaxOrStrictSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that allows values matching either the lax schema or the
strict schema.
Parameters:
Name
Type
Description
Default
schema
LaxOrStrictSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1310
1311
1312
1313
1314
1315
1316
1317
1318
1319
1320
1321
1322
1323
1324
1325
1326
1327
def
lax_or_strict_schema
self
schema
core_schema
LaxOrStrictSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that allows values matching either the lax schema or the
strict schema.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
# TODO: Need to read the default value off of model config or whatever
use_strict
schema
get
'strict'
False
# TODO: replace this default False
# If your JSON schema fails to generate it is probably
# because one of the following two branches failed.
use_strict
return
self
generate_inner
schema
'strict_schema'
else
return
self
generate_inner
schema
'lax_schema'
json_or_python_schema
json_or_python_schema
schema
JsonOrPythonSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that allows values matching either the JSON schema or the
Python schema.
The JSON schema is used instead of the Python schema. If you want to use the Python schema, you should override
this method.
Parameters:
Name
Type
Description
Default
schema
JsonOrPythonSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1329
1330
1331
1332
1333
1334
1335
1336
1337
1338
1339
1340
1341
1342
def
json_or_python_schema
self
schema
core_schema
JsonOrPythonSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that allows values matching either the JSON schema or the
Python schema.
The JSON schema is used instead of the Python schema. If you want to use the Python schema, you should override
this method.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
self
generate_inner
schema
'json_schema'
typed_dict_schema
typed_dict_schema
schema
TypedDictSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a typed dict.
Parameters:
Name
Type
Description
Default
schema
TypedDictSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1344
1345
1346
1347
1348
1349
1350
1351
1352
1353
1354
1355
1356
1357
1358
1359
1360
1361
1362
1363
1364
1365
1366
1367
1368
1369
1370
1371
1372
1373
1374
1375
def
typed_dict_schema
self
schema
core_schema
TypedDictSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a typed dict.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
total
schema
get
'total'
True
named_required_fields
list
tuple
str
bool
CoreSchemaField
name
self
field_is_required
field
total
field
for
name
field
schema
'fields'
items
self
field_is_present
field
self
mode
'serialization'
named_required_fields
extend
self
_name_required_computed_fields
schema
get
'computed_fields'
[])))
cls
schema
get
'cls'
config
_get_typed_dict_config
cls
with
self
_config_wrapper_stack
push
config
json_schema
self
_named_required_fields_schema
named_required_fields
cls
not
None
self
_update_class_schema
json_schema
cls
config
else
extra
config
get
'extra'
extra
'forbid'
json_schema
'additionalProperties'
False
elif
extra
'allow'
json_schema
'additionalProperties'
True
return
json_schema
typed_dict_field_schema
typed_dict_field_schema
schema
TypedDictField
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a typed dict field.
Parameters:
Name
Type
Description
Default
schema
TypedDictField
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1429
1430
1431
1432
1433
1434
1435
1436
1437
1438
def
typed_dict_field_schema
self
schema
core_schema
TypedDictField
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a typed dict field.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
self
generate_inner
schema
'schema'
dataclass_field_schema
dataclass_field_schema
schema
DataclassField
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a dataclass field.
Parameters:
Name
Type
Description
Default
schema
DataclassField
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1440
1441
1442
1443
1444
1445
1446
1447
1448
1449
def
dataclass_field_schema
self
schema
core_schema
DataclassField
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a dataclass field.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
self
generate_inner
schema
'schema'
model_field_schema
model_field_schema
schema
ModelField
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a model field.
Parameters:
Name
Type
Description
Default
schema
ModelField
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1451
1452
1453
1454
1455
1456
1457
1458
1459
1460
def
model_field_schema
self
schema
core_schema
ModelField
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a model field.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
self
generate_inner
schema
'schema'
computed_field_schema
computed_field_schema
schema
ComputedField
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a computed field.
Parameters:
Name
Type
Description
Default
schema
ComputedField
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1462
1463
1464
1465
1466
1467
1468
1469
1470
1471
def
computed_field_schema
self
schema
core_schema
ComputedField
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a computed field.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
self
generate_inner
schema
'return_schema'
model_schema
model_schema
schema
ModelSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a model.
Parameters:
Name
Type
Description
Default
schema
ModelSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1473
1474
1475
1476
1477
1478
1479
1480
1481
1482
1483
1484
1485
1486
1487
1488
1489
1490
1491
1492
def
model_schema
self
schema
core_schema
ModelSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a model.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
# We do not use schema['model'].model_json_schema() here
# because it could lead to inconsistent refs handling, etc.
cls
cast
'type[BaseModel]'
schema
'cls'
config
cls
model_config
with
self
_config_wrapper_stack
push
config
json_schema
self
generate_inner
schema
'schema'
self
_update_class_schema
json_schema
cls
config
return
json_schema
resolve_ref_schema
resolve_ref_schema
json_schema
JsonSchemaValue
JsonSchemaValue
Resolve a JsonSchemaValue to the non-ref schema if it is a $ref schema.
Parameters:
Name
Type
Description
Default
json_schema
JsonSchemaValue
The schema to resolve.
required
Returns:
Type
Description
JsonSchemaValue
The resolved schema.
Raises:
Type
Description
RuntimeError
If the schema reference can't be found in definitions.
Source code in
pydantic/json_schema.py
1565
1566
1567
1568
1569
1570
1571
1572
1573
1574
1575
1576
1577
1578
1579
1580
1581
1582
1583
def
resolve_ref_schema
self
json_schema
JsonSchemaValue
JsonSchemaValue
"""Resolve a JsonSchemaValue to the non-ref schema if it is a $ref schema.
Args:
json_schema: The schema to resolve.
Returns:
The resolved schema.
Raises:
RuntimeError: If the schema reference can't be found in definitions.
"""
while
'$ref'
json_schema
ref
json_schema
'$ref'
schema_to_update
self
get_schema_from_definitions
JsonRef
ref
schema_to_update
None
raise
RuntimeError
'Cannot update undefined schema for $ref=
ref
json_schema
schema_to_update
return
json_schema
model_fields_schema
model_fields_schema
schema
ModelFieldsSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a model's fields.
Parameters:
Name
Type
Description
Default
schema
ModelFieldsSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1585
1586
1587
1588
1589
1590
1591
1592
1593
1594
1595
1596
1597
1598
1599
1600
1601
1602
1603
1604
1605
1606
def
model_fields_schema
self
schema
core_schema
ModelFieldsSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a model's fields.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
named_required_fields
list
tuple
str
bool
CoreSchemaField
name
self
field_is_required
field
total
True
field
for
name
field
schema
'fields'
items
self
field_is_present
field
self
mode
'serialization'
named_required_fields
extend
self
_name_required_computed_fields
schema
get
'computed_fields'
[])))
json_schema
self
_named_required_fields_schema
named_required_fields
extras_schema
schema
get
'extras_schema'
None
extras_schema
not
None
schema_to_update
self
resolve_ref_schema
json_schema
schema_to_update
'additionalProperties'
self
generate_inner
extras_schema
return
json_schema
field_is_present
field_is_present
field
CoreSchemaField
bool
Whether the field should be included in the generated JSON schema.
Parameters:
Name
Type
Description
Default
field
CoreSchemaField
The schema for the field itself.
required
Returns:
Type
Description
bool
True
if the field should be included in the generated JSON schema,
False
otherwise.
Source code in
pydantic/json_schema.py
1608
1609
1610
1611
1612
1613
1614
1615
1616
1617
1618
1619
1620
1621
1622
1623
1624
def
field_is_present
self
field
CoreSchemaField
bool
"""Whether the field should be included in the generated JSON schema.
Args:
field: The schema for the field itself.
Returns:
`True` if the field should be included in the generated JSON schema, `False` otherwise.
"""
self
mode
'serialization'
# If you still want to include the field in the generated JSON schema,
# override this method and return True
return
not
field
get
'serialization_exclude'
elif
self
mode
'validation'
return
True
else
assert_never
self
mode
field_is_required
field_is_required
field
ModelField
DataclassField
TypedDictField
total
bool
bool
Whether the field should be marked as required in the generated JSON schema.
(Note that this is irrelevant if the field is not present in the JSON schema.).
Parameters:
Name
Type
Description
Default
field
ModelField
DataclassField
TypedDictField
The schema for the field itself.
required
total
bool
Only applies to
TypedDictField
Indicates if the
TypedDict
this field belongs to is total, in which case any fields that don't
explicitly specify
required=False
are required.
required
Returns:
Type
Description
bool
True
if the field should be marked as required in the generated JSON schema,
False
otherwise.
Source code in
pydantic/json_schema.py
1626
1627
1628
1629
1630
1631
1632
1633
1634
1635
1636
1637
1638
1639
1640
1641
1642
1643
1644
1645
1646
1647
1648
1649
def
field_is_required
self
field
core_schema
ModelField
core_schema
DataclassField
core_schema
TypedDictField
total
bool
bool
"""Whether the field should be marked as required in the generated JSON schema.
(Note that this is irrelevant if the field is not present in the JSON schema.).
Args:
field: The schema for the field itself.
total: Only applies to `TypedDictField`s.
Indicates if the `TypedDict` this field belongs to is total, in which case any fields that don't
explicitly specify `required=False` are required.
Returns:
`True` if the field should be marked as required in the generated JSON schema, `False` otherwise.
"""
self
mode
'serialization'
and
self
_config
json_schema_serialization_defaults_required
return
not
field
get
'serialization_exclude'
else
field
'type'
'typed-dict-field'
return
field
get
'required'
total
else
return
field
'schema'
'type'
'default'
dataclass_args_schema
dataclass_args_schema
schema
DataclassArgsSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a dataclass's constructor arguments.
Parameters:
Name
Type
Description
Default
schema
DataclassArgsSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1651
1652
1653
1654
1655
1656
1657
1658
1659
1660
1661
1662
1663
1664
1665
1666
1667
def
dataclass_args_schema
self
schema
core_schema
DataclassArgsSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a dataclass's constructor arguments.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
named_required_fields
list
tuple
str
bool
CoreSchemaField
field
'name'
self
field_is_required
field
total
True
field
for
field
schema
'fields'
self
field_is_present
field
self
mode
'serialization'
named_required_fields
extend
self
_name_required_computed_fields
schema
get
'computed_fields'
[])))
return
self
_named_required_fields_schema
named_required_fields
dataclass_schema
dataclass_schema
schema
DataclassSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a dataclass.
Parameters:
Name
Type
Description
Default
schema
DataclassSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1669
1670
1671
1672
1673
1674
1675
1676
1677
1678
1679
1680
1681
1682
1683
1684
1685
1686
1687
1688
1689
1690
1691
1692
1693
1694
1695
1696
1697
def
dataclass_schema
self
schema
core_schema
DataclassSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a dataclass.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
from
._internal._dataclasses
import
is_builtin_dataclass
cls
schema
'cls'
config
ConfigDict
getattr
cls
'__pydantic_config__'
cast
'ConfigDict'
{}))
with
self
_config_wrapper_stack
push
config
json_schema
self
generate_inner
schema
'schema'
copy
self
_update_class_schema
json_schema
cls
config
# Dataclass-specific handling of description
is_builtin_dataclass
cls
# vanilla dataclass; don't use cls.__doc__ as it will contain the class signature by default
description
None
else
description
None
cls
__doc__
None
else
inspect
cleandoc
cls
__doc__
description
json_schema
'description'
description
return
json_schema
arguments_schema
arguments_schema
schema
ArgumentsSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a function's arguments.
Parameters:
Name
Type
Description
Default
schema
ArgumentsSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1699
1700
1701
1702
1703
1704
1705
1706
1707
1708
1709
1710
1711
1712
1713
1714
1715
1716
1717
1718
1719
1720
1721
1722
1723
1724
1725
1726
1727
1728
1729
1730
1731
1732
1733
def
arguments_schema
self
schema
core_schema
ArgumentsSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a function's arguments.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
prefer_positional
schema
get
'metadata'
{})
get
'pydantic_js_prefer_positional_arguments'
arguments
schema
'arguments_schema'
kw_only_arguments
for
arguments
get
'mode'
'keyword_only'
kw_or_p_arguments
for
arguments
get
'mode'
'positional_or_keyword'
None
p_only_arguments
for
arguments
get
'mode'
'positional_only'
var_args_schema
schema
get
'var_args_schema'
var_kwargs_schema
schema
get
'var_kwargs_schema'
prefer_positional
positional_possible
not
kw_only_arguments
and
not
var_kwargs_schema
positional_possible
return
self
p_arguments_schema
p_only_arguments
kw_or_p_arguments
var_args_schema
keyword_possible
not
p_only_arguments
and
not
var_args_schema
keyword_possible
return
self
kw_arguments_schema
kw_or_p_arguments
kw_only_arguments
var_kwargs_schema
not
prefer_positional
positional_possible
not
kw_only_arguments
and
not
var_kwargs_schema
positional_possible
return
self
p_arguments_schema
p_only_arguments
kw_or_p_arguments
var_args_schema
raise
PydanticInvalidForJsonSchema
'Unable to generate JSON schema for arguments validator with positional-only and keyword-only arguments'
kw_arguments_schema
kw_arguments_schema
arguments
list
ArgumentsParameter
var_kwargs_schema
CoreSchema
None
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a function's keyword arguments.
Parameters:
Name
Type
Description
Default
arguments
list
ArgumentsParameter
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1735
1736
1737
1738
1739
1740
1741
1742
1743
1744
1745
1746
1747
1748
1749
1750
1751
1752
1753
1754
1755
1756
1757
1758
1759
1760
1761
1762
1763
1764
1765
1766
1767
1768
1769
1770
def
kw_arguments_schema
self
arguments
list
core_schema
ArgumentsParameter
var_kwargs_schema
CoreSchema
None
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a function's keyword arguments.
Args:
arguments: The core schema.
Returns:
The generated JSON schema.
"""
properties
dict
str
JsonSchemaValue
required
list
str
for
argument
arguments
name
self
get_argument_name
argument
argument_schema
self
generate_inner
argument
'schema'
copy
argument_schema
'title'
self
get_title_from_name
name
properties
name
argument_schema
argument
'schema'
'type'
'default'
# This assumes that if the argument has a default value,
# the inner schema must be of type WithDefaultSchema.
# I believe this is true, but I am not 100% sure
required
append
name
json_schema
JsonSchemaValue
'type'
'object'
'properties'
properties
required
json_schema
'required'
required
var_kwargs_schema
additional_properties_schema
self
generate_inner
var_kwargs_schema
additional_properties_schema
json_schema
'additionalProperties'
additional_properties_schema
else
json_schema
'additionalProperties'
False
return
json_schema
p_arguments_schema
p_arguments_schema
arguments
list
ArgumentsParameter
var_args_schema
CoreSchema
None
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a function's positional arguments.
Parameters:
Name
Type
Description
Default
arguments
list
ArgumentsParameter
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1772
1773
1774
1775
1776
1777
1778
1779
1780
1781
1782
1783
1784
1785
1786
1787
1788
1789
1790
1791
1792
1793
1794
1795
1796
1797
1798
1799
1800
1801
1802
1803
1804
1805
1806
1807
1808
1809
1810
1811
1812
def
p_arguments_schema
self
arguments
list
core_schema
ArgumentsParameter
var_args_schema
CoreSchema
None
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a function's positional arguments.
Args:
arguments: The core schema.
Returns:
The generated JSON schema.
"""
prefix_items
list
JsonSchemaValue
min_items
for
argument
arguments
name
self
get_argument_name
argument
argument_schema
self
generate_inner
argument
'schema'
copy
argument_schema
'title'
self
get_title_from_name
name
prefix_items
append
argument_schema
argument
'schema'
'type'
'default'
# This assumes that if the argument has a default value,
# the inner schema must be of type WithDefaultSchema.
# I believe this is true, but I am not 100% sure
min_items
json_schema
JsonSchemaValue
'type'
'array'
prefix_items
json_schema
'prefixItems'
prefix_items
min_items
json_schema
'minItems'
min_items
var_args_schema
items_schema
self
generate_inner
var_args_schema
items_schema
json_schema
'items'
items_schema
else
json_schema
'maxItems'
len
prefix_items
return
json_schema
get_argument_name
get_argument_name
argument
ArgumentsParameter
ArgumentsV3Parameter
str
Retrieves the name of an argument.
Parameters:
Name
Type
Description
Default
argument
ArgumentsParameter
ArgumentsV3Parameter
The core schema.
required
Returns:
Type
Description
str
The name of the argument.
Source code in
pydantic/json_schema.py
1814
1815
1816
1817
1818
1819
1820
1821
1822
1823
1824
1825
1826
1827
1828
1829
1830
def
get_argument_name
self
argument
core_schema
ArgumentsParameter
core_schema
ArgumentsV3Parameter
str
"""Retrieves the name of an argument.
Args:
argument: The core schema.
Returns:
The name of the argument.
"""
name
argument
'name'
self
by_alias
alias
argument
get
'alias'
isinstance
alias
str
name
alias
else
pass
# might want to do something else?
return
name
arguments_v3_schema
arguments_v3_schema
schema
ArgumentsV3Schema
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a function's arguments.
Parameters:
Name
Type
Description
Default
schema
ArgumentsV3Schema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1832
1833
1834
1835
1836
1837
1838
1839
1840
1841
1842
1843
1844
1845
1846
1847
1848
1849
1850
1851
1852
1853
1854
1855
1856
1857
1858
1859
1860
1861
1862
1863
1864
1865
1866
1867
1868
1869
def
arguments_v3_schema
self
schema
core_schema
ArgumentsV3Schema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a function's arguments.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
arguments
schema
'arguments_schema'
properties
dict
str
JsonSchemaValue
required
list
str
for
argument
arguments
mode
argument
get
'mode'
'positional_or_keyword'
name
self
get_argument_name
argument
argument_schema
self
generate_inner
argument
'schema'
copy
mode
'var_args'
argument_schema
'type'
'array'
'items'
argument_schema
elif
mode
'var_kwargs_uniform'
argument_schema
'type'
'object'
'additionalProperties'
argument_schema
argument_schema
setdefault
'title'
self
get_title_from_name
name
properties
name
argument_schema
mode
'var_kwargs_unpacked_typed_dict'
and
'required'
argument_schema
mode
not
'var_args'
'var_kwargs_uniform'
'var_kwargs_unpacked_typed_dict'
and
argument
'schema'
'type'
'default'
# This assumes that if the argument has a default value,
# the inner schema must be of type WithDefaultSchema.
# I believe this is true, but I am not 100% sure
required
append
name
json_schema
JsonSchemaValue
'type'
'object'
'properties'
properties
required
json_schema
'required'
required
return
json_schema
call_schema
call_schema
schema
CallSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a function call.
Parameters:
Name
Type
Description
Default
schema
CallSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1871
1872
1873
1874
1875
1876
1877
1878
1879
1880
def
call_schema
self
schema
core_schema
CallSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a function call.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
self
generate_inner
schema
'arguments_schema'
custom_error_schema
custom_error_schema
schema
CustomErrorSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a custom error.
Parameters:
Name
Type
Description
Default
schema
CustomErrorSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1882
1883
1884
1885
1886
1887
1888
1889
1890
1891
def
custom_error_schema
self
schema
core_schema
CustomErrorSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a custom error.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
self
generate_inner
schema
'schema'
json_schema
json_schema
schema
JsonSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a JSON object.
Parameters:
Name
Type
Description
Default
schema
JsonSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1893
1894
1895
1896
1897
1898
1899
1900
1901
1902
1903
1904
1905
1906
1907
1908
def
json_schema
self
schema
core_schema
JsonSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a JSON object.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
content_core_schema
schema
get
'schema'
core_schema
any_schema
content_json_schema
self
generate_inner
content_core_schema
self
mode
'validation'
return
'type'
'string'
'contentMediaType'
'application/json'
'contentSchema'
content_json_schema
else
# self.mode == 'serialization'
return
content_json_schema
url_schema
url_schema
schema
UrlSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a URL.
Parameters:
Name
Type
Description
Default
schema
UrlSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1910
1911
1912
1913
1914
1915
1916
1917
1918
1919
1920
1921
def
url_schema
self
schema
core_schema
UrlSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a URL.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
json_schema
'type'
'string'
'format'
'uri'
'minLength'
self
update_with_validations
json_schema
schema
self
ValidationsMapping
string
return
json_schema
multi_host_url_schema
multi_host_url_schema
schema
MultiHostUrlSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a URL that can be used with multiple hosts.
Parameters:
Name
Type
Description
Default
schema
MultiHostUrlSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1923
1924
1925
1926
1927
1928
1929
1930
1931
1932
1933
1934
1935
def
multi_host_url_schema
self
schema
core_schema
MultiHostUrlSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a URL that can be used with multiple hosts.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
# Note: 'multi-host-uri' is a custom/pydantic-specific format, not part of the JSON Schema spec
json_schema
'type'
'string'
'format'
'multi-host-uri'
'minLength'
self
update_with_validations
json_schema
schema
self
ValidationsMapping
string
return
json_schema
uuid_schema
uuid_schema
schema
UuidSchema
JsonSchemaValue
Generates a JSON schema that matches a UUID.
Parameters:
Name
Type
Description
Default
schema
UuidSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1937
1938
1939
1940
1941
1942
1943
1944
1945
1946
def
uuid_schema
self
schema
core_schema
UuidSchema
JsonSchemaValue
"""Generates a JSON schema that matches a UUID.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
'type'
'string'
'format'
'uuid'
definitions_schema
definitions_schema
schema
DefinitionsSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that defines a JSON object with definitions.
Parameters:
Name
Type
Description
Default
schema
DefinitionsSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1948
1949
1950
1951
1952
1953
1954
1955
1956
1957
1958
1959
1960
1961
1962
1963
1964
def
definitions_schema
self
schema
core_schema
DefinitionsSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that defines a JSON object with definitions.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
for
definition
schema
'definitions'
try
self
generate_inner
definition
except
PydanticInvalidForJsonSchema
core_ref
CoreRef
CoreRef
definition
'ref'
# type: ignore
self
_core_defs_invalid_for_json_schema
self
get_defs_ref
core_ref
self
mode
))]
continue
return
self
generate_inner
schema
'schema'
definition_ref_schema
definition_ref_schema
schema
DefinitionReferenceSchema
JsonSchemaValue
Generates a JSON schema that matches a schema that references a definition.
Parameters:
Name
Type
Description
Default
schema
DefinitionReferenceSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
1966
1967
1968
1969
1970
1971
1972
1973
1974
1975
1976
1977
def
definition_ref_schema
self
schema
core_schema
DefinitionReferenceSchema
JsonSchemaValue
"""Generates a JSON schema that matches a schema that references a definition.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
core_ref
CoreRef
schema
'schema_ref'
ref_json_schema
self
get_cache_defs_ref_schema
core_ref
return
ref_json_schema
ser_schema
ser_schema
schema
SerSchema
IncExSeqSerSchema
IncExDictSerSchema
JsonSchemaValue
None
Generates a JSON schema that matches a schema that defines a serialized object.
Parameters:
Name
Type
Description
Default
schema
SerSchema
IncExSeqSerSchema
IncExDictSerSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
| None
The generated JSON schema.
Source code in
pydantic/json_schema.py
1979
1980
1981
1982
1983
1984
1985
1986
1987
1988
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
def
ser_schema
self
schema
core_schema
SerSchema
core_schema
IncExSeqSerSchema
core_schema
IncExDictSerSchema
JsonSchemaValue
None
"""Generates a JSON schema that matches a schema that defines a serialized object.
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
schema_type
schema
'type'
schema_type
'function-plain'
schema_type
'function-wrap'
# PlainSerializerFunctionSerSchema or WrapSerializerFunctionSerSchema
return_schema
schema
get
'return_schema'
return_schema
not
None
return
self
generate_inner
return_schema
elif
schema_type
'format'
schema_type
'to-string'
# FormatSerSchema or ToStringSerSchema
return
self
str_schema
core_schema
str_schema
())
elif
schema
'type'
'model'
# ModelSerSchema
return
self
generate_inner
schema
'schema'
return
None
complex_schema
complex_schema
schema
ComplexSchema
JsonSchemaValue
Generates a JSON schema that matches a complex number.
JSON has no standard way to represent complex numbers. Complex number is not a numeric
type. Here we represent complex number as strings following the rule defined by Python.
For instance, '1+2j' is an accepted complex string. Details can be found in
Python's
complex
documentation
Parameters:
Name
Type
Description
Default
schema
ComplexSchema
The core schema.
required
Returns:
Type
Description
JsonSchemaValue
The generated JSON schema.
Source code in
pydantic/json_schema.py
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
2014
2015
2016
2017
2018
def
complex_schema
self
schema
core_schema
ComplexSchema
JsonSchemaValue
"""Generates a JSON schema that matches a complex number.
JSON has no standard way to represent complex numbers. Complex number is not a numeric
type. Here we represent complex number as strings following the rule defined by Python.
For instance, '1+2j' is an accepted complex string. Details can be found in
[Python's `complex` documentation][complex].
Args:
schema: The core schema.
Returns:
The generated JSON schema.
"""
return
'type'
'string'
get_title_from_name
get_title_from_name
name
str
str
Retrieves a title from a name.
Parameters:
Name
Type
Description
Default
name
str
The name to retrieve a title from.
required
Returns:
Type
Description
str
The title.
Source code in
pydantic/json_schema.py
2022
2023
2024
2025
2026
2027
2028
2029
2030
2031
def
get_title_from_name
self
name
str
str
"""Retrieves a title from a name.
Args:
name: The name to retrieve a title from.
Returns:
The title.
"""
return
name
title
replace
'_'
' '
strip
field_title_should_be_set
field_title_should_be_set
schema
CoreSchemaOrField
bool
Returns true if a field with the given schema should have a title set based on the field name.
Intuitively, we want this to return true for schemas that wouldn't otherwise provide their own title
(e.g., int, float, str), and false for those that would (e.g., BaseModel subclasses).
Parameters:
Name
Type
Description
Default
schema
CoreSchemaOrField
The schema to check.
required
Returns:
Type
Description
bool
True
if the field should have a title set,
False
otherwise.
Source code in
pydantic/json_schema.py
2033
2034
2035
2036
2037
2038
2039
2040
2041
2042
2043
2044
2045
2046
2047
2048
2049
2050
2051
2052
2053
2054
2055
2056
2057
2058
2059
2060
2061
2062
2063
2064
2065
2066
def
field_title_should_be_set
self
schema
CoreSchemaOrField
bool
"""Returns true if a field with the given schema should have a title set based on the field name.
Intuitively, we want this to return true for schemas that wouldn't otherwise provide their own title
(e.g., int, float, str), and false for those that would (e.g., BaseModel subclasses).
Args:
schema: The schema to check.
Returns:
`True` if the field should have a title set, `False` otherwise.
"""
_core_utils
is_core_schema_field
schema
schema
'type'
'computed-field'
field_schema
schema
'return_schema'
else
field_schema
schema
'schema'
return
self
field_title_should_be_set
field_schema
elif
_core_utils
is_core_schema
schema
schema
get
'ref'
# things with refs, such as models and enums, should not have titles set
return
False
schema
'type'
'default'
'nullable'
'definitions'
return
self
field_title_should_be_set
schema
'schema'
# type: ignore[typeddict-item]
_core_utils
is_function_with_inner_schema
schema
return
self
field_title_should_be_set
schema
'schema'
schema
'type'
'definition-ref'
# Referenced schemas should not have titles set for the same reason
# schemas with refs should not
return
False
return
True
# anything else should have title set
else
raise
PydanticInvalidForJsonSchema
'Unexpected schema type: schema=
schema
# pragma: no cover
normalize_name
normalize_name
name
str
str
Normalizes a name to be used as a key in a dictionary.
Parameters:
Name
Type
Description
Default
name
str
The name to normalize.
required
Returns:
Type
Description
str
The normalized name.
Source code in
pydantic/json_schema.py
2068
2069
2070
2071
2072
2073
2074
2075
2076
2077
def
normalize_name
self
name
str
str
"""Normalizes a name to be used as a key in a dictionary.
Args:
name: The name to normalize.
Returns:
The normalized name.
"""
return
sub
'[^a-zA-Z0-9.\-_]'
'_'
name
replace
'.'
'__'
get_defs_ref
get_defs_ref
core_mode_ref
CoreModeRef
DefsRef
Override this method to change the way that definitions keys are generated from a core reference.
Parameters:
Name
Type
Description
Default
core_mode_ref
CoreModeRef
The core reference.
required
Returns:
Type
Description
DefsRef
The definitions key.
Source code in
pydantic/json_schema.py
2079
2080
2081
2082
2083
2084
2085
2086
2087
2088
2089
2090
2091
2092
2093
2094
2095
2096
2097
2098
2099
2100
2101
2102
2103
2104
2105
2106
2107
2108
2109
2110
2111
2112
2113
2114
2115
2116
2117
2118
2119
2120
2121
2122
2123
2124
2125
def
get_defs_ref
self
core_mode_ref
CoreModeRef
DefsRef
"""Override this method to change the way that definitions keys are generated from a core reference.
Args:
core_mode_ref: The core reference.
Returns:
The definitions key.
"""
# Split the core ref into "components"; generic origins and arguments are each separate components
core_ref
mode
core_mode_ref
components
split
'([\][,])'
core_ref
# Remove IDs from each component
components
rsplit
':'
for
components
core_ref_no_id
join
components
# Remove everything before the last period from each "component"
components
sub
'(?:[^.[\]]+\.)+((?:[^.[\]]+))'
'\1'
for
components
short_ref
join
components
mode_title
_MODE_TITLE_MAPPING
mode
# It is important that the generated defs_ref values be such that at least one choice will not
# be generated for any other core_ref. Currently, this should be the case because we include
# the id of the source type in the core_ref
name
DefsRef
self
normalize_name
short_ref
name_mode
DefsRef
self
normalize_name
short_ref
mode_title
module_qualname
DefsRef
self
normalize_name
core_ref_no_id
module_qualname_mode
DefsRef
module_qualname
mode_title
module_qualname_id
DefsRef
self
normalize_name
core_ref
occurrence_index
self
_collision_index
get
module_qualname_id
occurrence_index
None
self
_collision_counter
module_qualname
occurrence_index
self
_collision_index
module_qualname_id
self
_collision_counter
module_qualname
module_qualname_occurrence
DefsRef
module_qualname
occurrence_index
module_qualname_occurrence_mode
DefsRef
module_qualname_mode
occurrence_index
self
_prioritized_defsref_choices
module_qualname_occurrence_mode
name
name_mode
module_qualname
module_qualname_mode
module_qualname_occurrence
module_qualname_occurrence_mode
return
module_qualname_occurrence_mode
get_cache_defs_ref_schema
get_cache_defs_ref_schema
core_ref
CoreRef
tuple
DefsRef
JsonSchemaValue
This method wraps the get_defs_ref method with some cache-lookup/population logic,
and returns both the produced defs_ref and the JSON schema that will refer to the right definition.
Parameters:
Name
Type
Description
Default
core_ref
CoreRef
The core reference to get the definitions reference for.
required
Returns:
Type
Description
tuple
DefsRef
JsonSchemaValue
A tuple of the definitions reference and the JSON schema that will refer to it.
Source code in
pydantic/json_schema.py
2127
2128
2129
2130
2131
2132
2133
2134
2135
2136
2137
2138
2139
2140
2141
2142
2143
2144
2145
2146
2147
2148
2149
2150
2151
2152
2153
def
get_cache_defs_ref_schema
self
core_ref
CoreRef
tuple
DefsRef
JsonSchemaValue
"""This method wraps the get_defs_ref method with some cache-lookup/population logic,
and returns both the produced defs_ref and the JSON schema that will refer to the right definition.
Args:
core_ref: The core reference to get the definitions reference for.
Returns:
A tuple of the definitions reference and the JSON schema that will refer to it.
"""
core_mode_ref
core_ref
self
mode
maybe_defs_ref
self
core_to_defs_refs
get
core_mode_ref
maybe_defs_ref
not
None
json_ref
self
core_to_json_refs
core_mode_ref
return
maybe_defs_ref
'$ref'
json_ref
defs_ref
self
get_defs_ref
core_mode_ref
# populate the ref translation mappings
self
core_to_defs_refs
core_mode_ref
defs_ref
self
defs_to_core_refs
defs_ref
core_mode_ref
json_ref
JsonRef
self
ref_template
format
model
defs_ref
self
core_to_json_refs
core_mode_ref
json_ref
self
json_to_defs_refs
json_ref
defs_ref
ref_json_schema
'$ref'
json_ref
return
defs_ref
ref_json_schema
handle_ref_overrides
handle_ref_overrides
json_schema
JsonSchemaValue
JsonSchemaValue
Remove any sibling keys that are redundant with the referenced schema.
Parameters:
Name
Type
Description
Default
json_schema
JsonSchemaValue
The schema to remove redundant sibling keys from.
required
Returns:
Type
Description
JsonSchemaValue
The schema with redundant sibling keys removed.
Source code in
pydantic/json_schema.py
2155
2156
2157
2158
2159
2160
2161
2162
2163
2164
2165
2166
2167
2168
2169
2170
2171
2172
2173
2174
2175
2176
2177
2178
2179
2180
def
handle_ref_overrides
self
json_schema
JsonSchemaValue
JsonSchemaValue
"""Remove any sibling keys that are redundant with the referenced schema.
Args:
json_schema: The schema to remove redundant sibling keys from.
Returns:
The schema with redundant sibling keys removed.
"""
'$ref'
json_schema
# prevent modifications to the input; this copy may be safe to drop if there is significant overhead
json_schema
json_schema
copy
referenced_json_schema
self
get_schema_from_definitions
JsonRef
json_schema
'$ref'
]))
referenced_json_schema
None
# This can happen when building schemas for models with not-yet-defined references.
# It may be a good idea to do a recursive pass at the end of the generation to remove
# any redundant override keys.
return
json_schema
for
list
json_schema
items
()):
'$ref'
continue
referenced_json_schema
and
referenced_json_schema
del
json_schema
# redundant key
return
json_schema
encode_default
encode_default
dft
Any
Any
Encode a default value to a JSON-serializable value.
This is used to encode default values for fields in the generated JSON schema.
Parameters:
Name
Type
Description
Default
dft
Any
The default value to encode.
required
Returns:
Type
Description
Any
The encoded default value.
Source code in
pydantic/json_schema.py
2193
2194
2195
2196
2197
2198
2199
2200
2201
2202
2203
2204
2205
2206
2207
2208
2209
2210
2211
2212
2213
2214
2215
2216
2217
2218
2219
2220
def
encode_default
self
dft
Any
Any
"""Encode a default value to a JSON-serializable value.
This is used to encode default values for fields in the generated JSON schema.
Args:
dft: The default value to encode.
Returns:
The encoded default value.
"""
from
.type_adapter
import
TypeAdapter
_type_has_config
config
self
_config
try
default
dft
_type_has_config
type
dft
else
TypeAdapter
type
dft
config
config
config_dict
dump_python
dft
by_alias
self
by_alias
mode
'json'
except
PydanticSchemaGenerationError
raise
pydantic_core
PydanticSerializationError
'Unable to encode default value
dft
return
pydantic_core
to_jsonable_python
default
timedelta_mode
config
ser_json_timedelta
bytes_mode
config
ser_json_bytes
by_alias
self
by_alias
update_with_validations
update_with_validations
json_schema
JsonSchemaValue
core_schema
CoreSchema
mapping
dict
str
str
None
Update the json_schema with the corresponding validations specified in the core_schema,
using the provided mapping to translate keys in core_schema to the appropriate keys for a JSON schema.
Parameters:
Name
Type
Description
Default
json_schema
JsonSchemaValue
The JSON schema to update.
required
core_schema
CoreSchema
The core schema to get the validations from.
required
mapping
dict
str
str
A mapping from core_schema attribute names to the corresponding JSON schema attribute names.
required
Source code in
pydantic/json_schema.py
2222
2223
2224
2225
2226
2227
2228
2229
2230
2231
2232
2233
2234
2235
def
update_with_validations
self
json_schema
JsonSchemaValue
core_schema
CoreSchema
mapping
dict
str
str
None
"""Update the json_schema with the corresponding validations specified in the core_schema,
using the provided mapping to translate keys in core_schema to the appropriate keys for a JSON schema.
Args:
json_schema: The JSON schema to update.
core_schema: The core schema to get the validations from.
mapping: A mapping from core_schema attribute names to the corresponding JSON schema attribute names.
"""
for
core_key
json_schema_key
mapping
items
():
core_key
core_schema
json_schema
json_schema_key
core_schema
core_key
get_json_ref_counts
get_json_ref_counts
json_schema
JsonSchemaValue
dict
JsonRef
int
Get all values corresponding to the key '$ref' anywhere in the json_schema.
Source code in
pydantic/json_schema.py
2281
2282
2283
2284
2285
2286
2287
2288
2289
2290
2291
2292
2293
2294
2295
2296
2297
2298
2299
2300
2301
2302
2303
2304
2305
2306
2307
2308
2309
2310
2311
2312
2313
2314
2315
def
get_json_ref_counts
self
json_schema
JsonSchemaValue
dict
JsonRef
int
"""Get all values corresponding to the key '$ref' anywhere in the json_schema."""
json_refs
dict
JsonRef
int
Counter
def
_add_json_refs
schema
Any
None
isinstance
schema
dict
'$ref'
schema
json_ref
JsonRef
schema
'$ref'
not
isinstance
json_ref
str
return
# in this case, '$ref' might have been the name of a property
already_visited
json_ref
json_refs
json_refs
json_ref
already_visited
return
# prevent recursion on a definition that was already visited
try
defs_ref
self
json_to_defs_refs
json_ref
defs_ref
self
_core_defs_invalid_for_json_schema
raise
self
_core_defs_invalid_for_json_schema
defs_ref
_add_json_refs
self
definitions
defs_ref
except
KeyError
not
json_ref
startswith
'http://'
'https://'
)):
raise
for
schema
items
():
'examples'
and
isinstance
list
# Skip examples that may contain arbitrary values and references
# (see the comment in `_get_all_json_refs` for more details).
continue
_add_json_refs
elif
isinstance
schema
list
for
schema
_add_json_refs
_add_json_refs
json_schema
return
json_refs
emit_warning
emit_warning
kind
JsonSchemaWarningKind
detail
str
None
This method simply emits PydanticJsonSchemaWarnings based on handling in the
warning_message
method.
Source code in
pydantic/json_schema.py
2320
2321
2322
2323
2324
def
emit_warning
self
kind
JsonSchemaWarningKind
detail
str
None
"""This method simply emits PydanticJsonSchemaWarnings based on handling in the `warning_message` method."""
message
self
render_warning_message
kind
detail
message
not
None
warnings
warn
message
PydanticJsonSchemaWarning
render_warning_message
render_warning_message
kind
JsonSchemaWarningKind
detail
str
str
None
This method is responsible for ignoring warnings as desired, and for formatting the warning messages.
You can override the value of
ignored_warning_kinds
in a subclass of GenerateJsonSchema
to modify what warnings are generated. If you want more control, you can override this method;
just return None in situations where you don't want warnings to be emitted.
Parameters:
Name
Type
Description
Default
kind
JsonSchemaWarningKind
The kind of warning to render. It can be one of the following:
'skipped-choice': A choice field was skipped because it had no valid choices.
'non-serializable-default': A default value was skipped because it was not JSON-serializable.
required
detail
str
A string with additional details about the warning.
required
Returns:
Type
Description
str
| None
The formatted warning message, or
None
if no warning should be emitted.
Source code in
pydantic/json_schema.py
2326
2327
2328
2329
2330
2331
2332
2333
2334
2335
2336
2337
2338
2339
2340
2341
2342
2343
2344
2345
def
render_warning_message
self
kind
JsonSchemaWarningKind
detail
str
str
None
"""This method is responsible for ignoring warnings as desired, and for formatting the warning messages.
You can override the value of `ignored_warning_kinds` in a subclass of GenerateJsonSchema
to modify what warnings are generated. If you want more control, you can override this method;
just return None in situations where you don't want warnings to be emitted.
Args:
kind: The kind of warning to render. It can be one of the following:
- 'skipped-choice': A choice field was skipped because it had no valid choices.
- 'non-serializable-default': A default value was skipped because it was not JSON-serializable.
detail: A string with additional details about the warning.
Returns:
The formatted warning message, or `None` if no warning should be emitted.
"""
kind
self
ignored_warning_kinds
return
None
return
detail
kind
WithJsonSchema
dataclass
WithJsonSchema
json_schema
JsonSchemaValue
None
mode
Literal
"validation"
"serialization"
None
None
Usage Documentation
WithJsonSchema
Annotation
Add this as an annotation on a field to override the (base) JSON schema that would be generated for that field.
This provides a way to set a JSON schema for types that would otherwise raise errors when producing a JSON schema,
such as Callable, or types that have an is-instance core schema, without needing to go so far as creating a
custom subclass of pydantic.json_schema.GenerateJsonSchema.
Note that any
modifications
to the schema that would normally be made (such as setting the title for model fields)
will still be performed.
mode
is set this will only apply to that schema generation mode, allowing you
to set different json schemas for validation and serialization.
Examples
Examples
examples
dict
str
Any
mode
Literal
"validation"
"serialization"
None
None
Examples
examples
list
Any
mode
Literal
"validation"
"serialization"
None
None
Examples
examples
dict
str
Any
list
Any
mode
Literal
"validation"
"serialization"
None
None
Add examples to a JSON schema.
If the JSON Schema already contains examples, the provided examples
will be appended.
mode
is set this will only apply to that schema generation mode,
allowing you to add different examples for validation and serialization.
Source code in
pydantic/json_schema.py
2539
2540
2541
2542
2543
2544
2545
2546
2547
2548
2549
def
__init__
self
examples
dict
str
Any
list
Any
mode
Literal
'validation'
'serialization'
None
None
None
isinstance
examples
dict
warnings
warn
'Using a dict for `examples` is deprecated, use a list instead.'
PydanticDeprecatedSince29
stacklevel
self
examples
examples
self
mode
mode
SkipJsonSchema
dataclass
SkipJsonSchema
Usage Documentation
SkipJsonSchema
Annotation
Add this as an annotation on a field to skip generating a JSON schema for that field.
Example
from
pprint
import
pprint
from
typing
import
Union
from
pydantic
import
BaseModel
from
pydantic.json_schema
import
SkipJsonSchema
class
Model
BaseModel
Union
int
None
None
Union
int
SkipJsonSchema
None
None
SkipJsonSchema
Union
int
None
None
pprint
Model
model_json_schema
())
'''
'properties': {
'a': {
'anyOf': [
{'type': 'integer'},
{'type': 'null'}
'default': None,
'title': 'A'
'b': {
'default': None,
'title': 'B',
'type': 'integer'
'title': 'Model',
'type': 'object'
'''
model_json_schema
model_json_schema
cls
type
BaseModel
type
PydanticDataclass
by_alias
bool
True
ref_template
str
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
GenerateJsonSchema
mode
JsonSchemaMode
"validation"
dict
str
Any
Utility function to generate a JSON Schema for a model.
Parameters:
Name
Type
Description
Default
cls
type
BaseModel
] |
type
PydanticDataclass
The model class to generate a JSON Schema for.
required
by_alias
bool
True
(the default), fields will be serialized according to their alias.
False
, fields will be serialized according to their attribute name.
True
ref_template
str
The template to use for generating JSON Schema references.
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
The class to use for generating the JSON Schema.
GenerateJsonSchema
mode
JsonSchemaMode
The mode to use for generating the JSON Schema. It can be one of the following:
'validation': Generate a JSON Schema for validating data.
'serialization': Generate a JSON Schema for serializing data.
'validation'
Returns:
Type
Description
dict
str
Any
The generated JSON Schema.
Source code in
pydantic/json_schema.py
2379
2380
2381
2382
2383
2384
2385
2386
2387
2388
2389
2390
2391
2392
2393
2394
2395
2396
2397
2398
2399
2400
2401
2402
2403
2404
2405
2406
2407
2408
2409
2410
2411
2412
2413
def
model_json_schema
cls
type
BaseModel
type
PydanticDataclass
by_alias
bool
True
ref_template
str
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
GenerateJsonSchema
mode
JsonSchemaMode
'validation'
dict
str
Any
"""Utility function to generate a JSON Schema for a model.
Args:
cls: The model class to generate a JSON Schema for.
by_alias: If `True` (the default), fields will be serialized according to their alias.
If `False`, fields will be serialized according to their attribute name.
ref_template: The template to use for generating JSON Schema references.
schema_generator: The class to use for generating the JSON Schema.
mode: The mode to use for generating the JSON Schema. It can be one of the following:
- 'validation': Generate a JSON Schema for validating data.
- 'serialization': Generate a JSON Schema for serializing data.
Returns:
The generated JSON Schema.
"""
from
.main
import
BaseModel
schema_generator_instance
schema_generator
by_alias
by_alias
ref_template
ref_template
isinstance
cls
__pydantic_core_schema__
_mock_val_ser
MockCoreSchema
cls
__pydantic_core_schema__
rebuild
cls
BaseModel
raise
AttributeError
'model_json_schema() must be called on a subclass of BaseModel, not BaseModel itself.'
assert
not
isinstance
cls
__pydantic_core_schema__
_mock_val_ser
MockCoreSchema
'this is a bug! please report it'
return
schema_generator_instance
generate
cls
__pydantic_core_schema__
mode
mode
models_json_schema
models_json_schema
models
Sequence
tuple
type
BaseModel
type
PydanticDataclass
JsonSchemaMode
by_alias
bool
True
title
str
None
None
description
str
None
None
ref_template
str
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
GenerateJsonSchema
tuple
dict
tuple
type
BaseModel
type
PydanticDataclass
JsonSchemaMode
JsonSchemaValue
JsonSchemaValue
Utility function to generate a JSON Schema for multiple models.
Parameters:
Name
Type
Description
Default
models
Sequence
tuple
type
BaseModel
] |
type
PydanticDataclass
JsonSchemaMode
A sequence of tuples of the form (model, mode).
required
by_alias
bool
Whether field aliases should be used as keys in the generated JSON Schema.
True
title
str
| None
The title of the generated JSON Schema.
None
description
str
| None
The description of the generated JSON Schema.
None
ref_template
str
The reference template to use for generating JSON Schema references.
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
The schema generator to use for generating the JSON Schema.
GenerateJsonSchema
Returns:
Type
Description
tuple
dict
tuple
type
BaseModel
] |
type
PydanticDataclass
JsonSchemaMode
JsonSchemaValue
JsonSchemaValue
A tuple where:
- The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and
whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have
JsonRef references to definitions that are defined in the second returned element.)
- The second element is a JSON schema containing all definitions referenced in the first returned
element, along with the optional title and description keys.
Source code in
pydantic/json_schema.py
2416
2417
2418
2419
2420
2421
2422
2423
2424
2425
2426
2427
2428
2429
2430
2431
2432
2433
2434
2435
2436
2437
2438
2439
2440
2441
2442
2443
2444
2445
2446
2447
2448
2449
2450
2451
2452
2453
2454
2455
2456
2457
2458
2459
2460
2461
def
models_json_schema
models
Sequence
tuple
type
BaseModel
type
PydanticDataclass
JsonSchemaMode
]],
by_alias
bool
True
title
str
None
None
description
str
None
None
ref_template
str
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
GenerateJsonSchema
tuple
dict
tuple
type
BaseModel
type
PydanticDataclass
JsonSchemaMode
JsonSchemaValue
JsonSchemaValue
"""Utility function to generate a JSON Schema for multiple models.
Args:
models: A sequence of tuples of the form (model, mode).
by_alias: Whether field aliases should be used as keys in the generated JSON Schema.
title: The title of the generated JSON Schema.
description: The description of the generated JSON Schema.
ref_template: The reference template to use for generating JSON Schema references.
schema_generator: The schema generator to use for generating the JSON Schema.
Returns:
A tuple where:
- The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and
whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have
JsonRef references to definitions that are defined in the second returned element.)
- The second element is a JSON schema containing all definitions referenced in the first returned
element, along with the optional title and description keys.
"""
for
cls
models
isinstance
cls
__pydantic_core_schema__
_mock_val_ser
MockCoreSchema
cls
__pydantic_core_schema__
rebuild
instance
schema_generator
by_alias
by_alias
ref_template
ref_template
inputs
list
tuple
type
BaseModel
type
PydanticDataclass
JsonSchemaMode
CoreSchema
mode
__pydantic_core_schema__
for
mode
models
json_schemas_map
definitions
instance
generate_definitions
inputs
json_schema
dict
str
Any
definitions
json_schema
'$defs'
definitions
title
json_schema
'title'
title
description
json_schema
'description'
description
return
json_schemas_map
json_schema
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 028_pydantic_extra_types_country.txt ---
Country
Country definitions that are based on the
ISO 3166
CountryAlpha2
Bases:
str
CountryAlpha2 parses country codes in the
ISO 3166-1 alpha-2
format.
from
pydantic
import
BaseModel
from
pydantic_extra_types.country
import
CountryAlpha2
class
Product
BaseModel
made_in
CountryAlpha2
product
Product
made_in
'ES'
print
product
#> made_in='ES'
alpha3
property
alpha3
str
The country code in the
ISO 3166-1 alpha-3
format.
numeric_code
property
numeric_code
str
The country code in the
ISO 3166-1 numeric
format.
short_name
property
short_name
str
The country short name.
CountryAlpha3
Bases:
str
CountryAlpha3 parses country codes in the
ISO 3166-1 alpha-3
format.
from
pydantic
import
BaseModel
from
pydantic_extra_types.country
import
CountryAlpha3
class
Product
BaseModel
made_in
CountryAlpha3
product
Product
made_in
"USA"
print
product
#> made_in='USA'
alpha2
property
alpha2
str
The country code in the
ISO 3166-1 alpha-2
format.
numeric_code
property
numeric_code
str
The country code in the
ISO 3166-1 numeric
format.
short_name
property
short_name
str
The country short name.
CountryNumericCode
Bases:
str
CountryNumericCode parses country codes in the
ISO 3166-1 numeric
format.
from
pydantic
import
BaseModel
from
pydantic_extra_types.country
import
CountryNumericCode
class
Product
BaseModel
made_in
CountryNumericCode
product
Product
made_in
"840"
print
product
#> made_in='840'
alpha2
property
alpha2
str
The country code in the
ISO 3166-1 alpha-2
format.
alpha3
property
alpha3
str
The country code in the
ISO 3166-1 alpha-3
format.
short_name
property
short_name
str
The country short name.
CountryShortName
Bases:
str
CountryShortName parses country codes in the short name format.
from
pydantic
import
BaseModel
from
pydantic_extra_types.country
import
CountryShortName
class
Product
BaseModel
made_in
CountryShortName
product
Product
made_in
"United States"
print
product
#> made_in='United States'
alpha2
property
alpha2
str
The country code in the
ISO 3166-1 alpha-2
format.
alpha3
property
alpha3
str
The country code in the
ISO 3166-1 alpha-3
format.
numeric_code
property
numeric_code
str
The country code in the
ISO 3166-1 numeric
format.
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 001_pydantic_extra_types_coordinate_pydantic_extra_types_coordinate.txt ---
Coordinate
The
pydantic_extra_types.coordinate
module provides the
Latitude
Longitude
, and
Coordinate
data types.
Latitude
Bases:
float
Latitude value should be between -90 and 90, inclusive.
from
pydantic
import
BaseModel
from
pydantic_extra_types.coordinate
import
Latitude
class
Location
BaseModel
latitude
Latitude
location
Location
latitude
41.40338
print
location
#> latitude=41.40338
Longitude
Bases:
float
Longitude value should be between -180 and 180, inclusive.
from
pydantic
import
BaseModel
from
pydantic_extra_types.coordinate
import
Longitude
class
Location
BaseModel
longitude
Longitude
location
Location
longitude
2.17403
print
location
#> longitude=2.17403
Coordinate
dataclass
Coordinate
latitude
Latitude
longitude
Longitude
Bases:
Representation
Coordinate parses Latitude and Longitude.
You can use the
Coordinate
data type for storing coordinates. Coordinates can be
defined using one of the following formats:
Tuple:
(Latitude, Longitude)
. For example:
(41.40338, 2.17403)
Coordinate
instance:
Coordinate(latitude=Latitude, longitude=Longitude)
from
pydantic
import
BaseModel
from
pydantic_extra_types.coordinate
import
Coordinate
class
Location
BaseModel
coordinate
Coordinate
location
Location
coordinate
41.40338
2.17403
#> coordinate=Coordinate(latitude=41.40338, longitude=2.17403)
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 023_aliases_pydantic_aliases_AliasChoices_convert_to_aliases.txt ---
Aliases
Support for alias configurations.
AliasPath
dataclass
AliasPath
first_arg
str
args
str
int
Usage Documentation
AliasPath
and
AliasChoices
A data class used by
validation_alias
as a convenience to create aliases.
Attributes:
Name
Type
Description
path
list
int
str
A list of string or integer aliases.
Source code in
pydantic/aliases.py
def
__init__
self
first_arg
str
args
str
int
None
self
path
first_arg
list
args
convert_to_aliases
convert_to_aliases
list
str
int
Converts arguments to a list of string or integer aliases.
Returns:
Type
Description
list
str
int
The list of aliases.
Source code in
pydantic/aliases.py
def
convert_to_aliases
self
list
str
int
"""Converts arguments to a list of string or integer aliases.
Returns:
The list of aliases.
"""
return
self
path
search_dict_for_path
search_dict_for_path
dict
Any
Searches a dictionary for the path specified by the alias.
Returns:
Type
Description
Any
The value at the specified path, or
PydanticUndefined
if the path is not found.
Source code in
pydantic/aliases.py
def
search_dict_for_path
self
dict
Any
"""Searches a dictionary for the path specified by the alias.
Returns:
The value at the specified path, or `PydanticUndefined` if the path is not found.
"""
for
self
path
isinstance
str
# disallow indexing into a str, like for AliasPath('x', 0) and x='abc'
return
PydanticUndefined
try
except
KeyError
IndexError
TypeError
return
PydanticUndefined
return
AliasChoices
dataclass
AliasChoices
first_choice
str
AliasPath
choices
str
AliasPath
Usage Documentation
AliasPath
and
AliasChoices
A data class used by
validation_alias
as a convenience to create aliases.
Attributes:
Name
Type
Description
choices
list
str
AliasPath
A list containing a string or
AliasPath
Source code in
pydantic/aliases.py
def
__init__
self
first_choice
str
AliasPath
choices
str
AliasPath
None
self
choices
first_choice
list
choices
convert_to_aliases
convert_to_aliases
list
list
str
int
Converts arguments to a list of lists containing string or integer aliases.
Returns:
Type
Description
list
list
str
int
The list of aliases.
Source code in
pydantic/aliases.py
def
convert_to_aliases
self
list
list
str
int
]]:
"""Converts arguments to a list of lists containing string or integer aliases.
Returns:
The list of aliases.
"""
aliases
list
list
str
int
for
self
choices
isinstance
AliasPath
aliases
append
convert_to_aliases
())
else
aliases
append
return
aliases
AliasGenerator
dataclass
AliasGenerator
alias
Callable
str
str
None
None
validation_alias
Callable
str
str
AliasPath
AliasChoices
None
None
serialization_alias
Callable
str
str
None
None
Usage Documentation
Using an
AliasGenerator
A data class used by
alias_generator
as a convenience to create various aliases.
Attributes:
Name
Type
Description
alias
Callable
str
str
] | None
A callable that takes a field name and returns an alias for it.
validation_alias
Callable
str
str
AliasPath
AliasChoices
] | None
A callable that takes a field name and returns a validation alias for it.
serialization_alias
Callable
str
str
] | None
A callable that takes a field name and returns a serialization alias for it.
generate_aliases
generate_aliases
field_name
str
tuple
str
None
str
AliasPath
AliasChoices
None
str
None
Generate
alias
validation_alias
, and
serialization_alias
for a field.
Returns:
Type
Description
tuple
str
| None,
str
AliasPath
AliasChoices
| None,
str
| None]
A tuple of three aliases - validation, alias, and serialization.
Source code in
pydantic/aliases.py
125
126
127
128
129
130
131
132
133
134
135
def
generate_aliases
self
field_name
str
tuple
str
None
str
AliasPath
AliasChoices
None
str
None
"""Generate `alias`, `validation_alias`, and `serialization_alias` for a field.
Returns:
A tuple of three aliases - validation, alias, and serialization.
"""
alias
self
_generate_alias
'alias'
str
,),
field_name
validation_alias
self
_generate_alias
'validation_alias'
str
AliasChoices
AliasPath
field_name
serialization_alias
self
_generate_alias
'serialization_alias'
str
,),
field_name
return
alias
validation_alias
serialization_alias
# type: ignore
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 029_type_adapter.txt ---
TypeAdapter
Bases:
Generic
Usage Documentation
TypeAdapter
Type adapters provide a flexible way to perform validation and serialization based on a Python type.
TypeAdapter
instance exposes some of the functionality from
BaseModel
instance methods
for types that do not have such methods (such as dataclasses, primitive types, and more).
Note:
TypeAdapter
instances are not types, and cannot be used as type annotations for fields.
Parameters:
Name
Type
Description
Default
type
Any
The type associated with the
TypeAdapter
required
config
ConfigDict
| None
Configuration for the
TypeAdapter
, should be a dictionary conforming to
ConfigDict
Note
You cannot provide a configuration when instantiating a
TypeAdapter
if the type you're using
has its own config that cannot be overridden (ex:
BaseModel
TypedDict
, and
dataclass
). A
type-adapter-config-unused
error will
be raised in this case.
None
_parent_depth
int
Depth at which to search for the
parent frame
. This frame is used when
resolving forward annotations during schema building, by looking for the globals and locals of this
frame. Defaults to 2, which will result in the frame where the
TypeAdapter
was instantiated.
Note
This parameter is named with an underscore to suggest its private nature and discourage use.
It may be deprecated in a minor version, so we only recommend using it if you're comfortable
with potential change in behavior/support. It's default value is 2 because internally,
the
TypeAdapter
class makes another call to fetch the frame.
module
str
| None
The module that passes to plugin if provided.
None
Attributes:
Name
Type
Description
core_schema
CoreSchema
The core schema for the type.
validator
SchemaValidator
PluggableSchemaValidator
The schema validator for the type.
serializer
SchemaSerializer
The schema serializer for the type.
pydantic_complete
bool
Whether the core schema for the type is successfully built.
Compatibility with
mypy
Depending on the type used,
mypy
might raise an error when instantiating a
TypeAdapter
. As a workaround, you can explicitly
annotate your variable:
from
typing
import
Union
from
pydantic
import
TypeAdapter
TypeAdapter
Union
str
int
TypeAdapter
Union
str
int
# type: ignore[arg-type]
Namespace management nuances and implementation details
Here, we collect some notes on namespace management, and subtle differences from
BaseModel
BaseModel
uses its own
__module__
to find out where it was defined
and then looks for symbols to resolve forward references in those globals.
On the other hand,
TypeAdapter
can be initialized with arbitrary objects,
which may not be types and thus do not have a
__module__
available.
So instead we look at the globals in our parent stack frame.
It is expected that the
ns_resolver
passed to this function will have the correct
namespace for the type we're adapting. See the source code for
TypeAdapter.__init__
and
TypeAdapter.rebuild
for various ways to construct this namespace.
This works for the case where this function is called in a module that
has the target of forward references in its scope, but
does not always work for more complex cases.
For example, take the following:
a.py
IntList
list
int
OuterDict
dict
str
'IntList'
b.py
from
import
OuterDict
from
pydantic
import
TypeAdapter
IntList
int
# replaces the symbol the forward reference is looking for
TypeAdapter
OuterDict
'x'
# should fail but doesn't
OuterDict
were a
BaseModel
, this would work because it would resolve
the forward reference within the
a.py
namespace.
But
TypeAdapter(OuterDict)
can't determine what module
OuterDict
came from.
In other words, the assumption that
all
forward references exist in the
module we are being called from is not technically always true.
Although most of the time it is and it works fine for recursive models and such,
BaseModel
's behavior isn't perfect either and
can
break in similar ways,
so there is no right or wrong between the two.
But at the very least this behavior is
subtly
different from
BaseModel
's.
Source code in
pydantic/type_adapter.py
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
def
__init__
self
type
Any
config
ConfigDict
None
None
_parent_depth
int
module
str
None
None
None
_type_has_config
type
and
config
not
None
raise
PydanticUserError
'Cannot use `config` when the type is a BaseModel, dataclass or TypedDict.'
' These types can have their own config and setting the config via the `config`'
' parameter to TypeAdapter will not override it, thus the `config` you passed to'
' TypeAdapter becomes meaningless, which is probably not what you want.'
code
'type-adapter-config-unused'
self
_type
type
self
_config
config
self
_parent_depth
_parent_depth
self
pydantic_complete
False
parent_frame
self
_fetch_parent_frame
parent_frame
not
None
globalns
parent_frame
f_globals
# Do not provide a local ns if the type adapter happens to be instantiated at the module level:
localns
parent_frame
f_locals
parent_frame
f_locals
not
globalns
else
else
globalns
localns
self
_module_name
module
cast
str
globalns
get
'__name__'
self
_init_core_attrs
ns_resolver
_namespace_utils
NsResolver
namespaces_tuple
_namespace_utils
NamespacesTuple
locals
localns
globals
globalns
parent_namespace
localns
force
False
rebuild
rebuild
force
bool
False
raise_errors
bool
True
_parent_namespace_depth
int
_types_namespace
MappingNamespace
None
None
bool
None
Try to rebuild the pydantic-core schema for the adapter's type.
This may be necessary when one of the annotations is a ForwardRef which could not be resolved during
the initial attempt to build the schema, and automatic rebuilding fails.
Parameters:
Name
Type
Description
Default
force
bool
Whether to force the rebuilding of the type adapter's schema, defaults to
False
False
raise_errors
bool
Whether to raise errors, defaults to
True
True
_parent_namespace_depth
int
Depth at which to search for the
parent frame
. This
frame is used when resolving forward annotations during schema rebuilding, by looking for
the locals of this frame. Defaults to 2, which will result in the frame where the method
was called.
_types_namespace
MappingNamespace
| None
An explicit types namespace to use, instead of using the local namespace
from the parent frame. Defaults to
None
None
Returns:
Type
Description
bool
| None
Returns
None
if the schema is already "complete" and rebuilding was not required.
bool
| None
If rebuilding
was
required, returns
True
if rebuilding was successful, otherwise
False
Source code in
pydantic/type_adapter.py
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
def
rebuild
self
force
bool
False
raise_errors
bool
True
_parent_namespace_depth
int
_types_namespace
_namespace_utils
MappingNamespace
None
None
bool
None
"""Try to rebuild the pydantic-core schema for the adapter's type.
This may be necessary when one of the annotations is a ForwardRef which could not be resolved during
the initial attempt to build the schema, and automatic rebuilding fails.
Args:
force: Whether to force the rebuilding of the type adapter's schema, defaults to `False`.
raise_errors: Whether to raise errors, defaults to `True`.
_parent_namespace_depth: Depth at which to search for the [parent frame][frame-objects]. This
frame is used when resolving forward annotations during schema rebuilding, by looking for
the locals of this frame. Defaults to 2, which will result in the frame where the method
was called.
_types_namespace: An explicit types namespace to use, instead of using the local namespace
from the parent frame. Defaults to `None`.
Returns:
Returns `None` if the schema is already "complete" and rebuilding was not required.
If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.
"""
not
force
and
self
pydantic_complete
return
None
_types_namespace
not
None
rebuild_ns
_types_namespace
elif
_parent_namespace_depth
rebuild_ns
_typing_extra
parent_frame_namespace
parent_depth
_parent_namespace_depth
force
True
else
rebuild_ns
# we have to manually fetch globals here because there's no type on the stack of the NsResolver
# and so we skip the globalns = get_module_ns_of(typ) call that would normally happen
globalns
sys
_getframe
max
_parent_namespace_depth
f_globals
ns_resolver
_namespace_utils
NsResolver
namespaces_tuple
_namespace_utils
NamespacesTuple
locals
rebuild_ns
globals
globalns
parent_namespace
rebuild_ns
return
self
_init_core_attrs
ns_resolver
ns_resolver
force
True
raise_errors
raise_errors
validate_python
validate_python
object
Any
strict
bool
None
None
from_attributes
bool
None
None
context
dict
str
Any
None
None
experimental_allow_partial
bool
Literal
"off"
"on"
"trailing-strings"
False
by_alias
bool
None
None
by_name
bool
None
None
Validate a Python object against the model.
Parameters:
Name
Type
Description
Default
object
Any
The Python object to validate against the model.
required
strict
bool
| None
Whether to strictly check types.
None
from_attributes
bool
| None
Whether to extract data from object attributes.
None
context
dict
str
Any
] | None
Additional context to pass to the validator.
None
experimental_allow_partial
bool
Literal
['off', 'on', 'trailing-strings']
Experimental
whether to enable
partial validation
, e.g. to process streams.
* False / 'off': Default behavior, no partial validation.
* True / 'on': Enable partial validation.
* 'trailing-strings': Enable partial validation and allow trailing strings in the input.
False
by_alias
bool
| None
Whether to use the field's alias when validating against the provided input data.
None
by_name
bool
| None
Whether to use the field's name when validating against the provided input data.
None
Note
When using
TypeAdapter
with a Pydantic
dataclass
, the use of the
from_attributes
argument is not supported.
Returns:
Type
Description
The validated object.
Source code in
pydantic/type_adapter.py
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
def
validate_python
self
object
Any
strict
bool
None
None
from_attributes
bool
None
None
context
dict
str
Any
None
None
experimental_allow_partial
bool
Literal
'off'
'on'
'trailing-strings'
False
by_alias
bool
None
None
by_name
bool
None
None
"""Validate a Python object against the model.
Args:
object: The Python object to validate against the model.
strict: Whether to strictly check types.
from_attributes: Whether to extract data from object attributes.
context: Additional context to pass to the validator.
experimental_allow_partial: **Experimental** whether to enable
[partial validation](../concepts/experimental.md#partial-validation), e.g. to process streams.
* False / 'off': Default behavior, no partial validation.
* True / 'on': Enable partial validation.
* 'trailing-strings': Enable partial validation and allow trailing strings in the input.
by_alias: Whether to use the field's alias when validating against the provided input data.
by_name: Whether to use the field's name when validating against the provided input data.
!!! note
When using `TypeAdapter` with a Pydantic `dataclass`, the use of the `from_attributes`
argument is not supported.
Returns:
The validated object.
"""
by_alias
False
and
by_name
not
True
raise
PydanticUserError
'At least one of `by_alias` or `by_name` must be set to True.'
code
'validate-by-alias-and-name-false'
return
self
validator
validate_python
object
strict
strict
from_attributes
from_attributes
context
context
allow_partial
experimental_allow_partial
by_alias
by_alias
by_name
by_name
validate_json
validate_json
data
str
bytes
bytearray
strict
bool
None
None
context
dict
str
Any
None
None
experimental_allow_partial
bool
Literal
"off"
"on"
"trailing-strings"
False
by_alias
bool
None
None
by_name
bool
None
None
Usage Documentation
JSON Parsing
Validate a JSON string or bytes against the model.
Parameters:
Name
Type
Description
Default
data
str
bytes
bytearray
The JSON data to validate against the model.
required
strict
bool
| None
Whether to strictly check types.
None
context
dict
str
Any
] | None
Additional context to use during validation.
None
experimental_allow_partial
bool
Literal
['off', 'on', 'trailing-strings']
Experimental
whether to enable
partial validation
, e.g. to process streams.
* False / 'off': Default behavior, no partial validation.
* True / 'on': Enable partial validation.
* 'trailing-strings': Enable partial validation and allow trailing strings in the input.
False
by_alias
bool
| None
Whether to use the field's alias when validating against the provided input data.
None
by_name
bool
| None
Whether to use the field's name when validating against the provided input data.
None
Returns:
Type
Description
The validated object.
Source code in
pydantic/type_adapter.py
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
def
validate_json
self
data
str
bytes
bytearray
strict
bool
None
None
context
dict
str
Any
None
None
experimental_allow_partial
bool
Literal
'off'
'on'
'trailing-strings'
False
by_alias
bool
None
None
by_name
bool
None
None
"""!!! abstract "Usage Documentation"
[JSON Parsing](../concepts/json.md#json-parsing)
Validate a JSON string or bytes against the model.
Args:
data: The JSON data to validate against the model.
strict: Whether to strictly check types.
context: Additional context to use during validation.
experimental_allow_partial: **Experimental** whether to enable
[partial validation](../concepts/experimental.md#partial-validation), e.g. to process streams.
* False / 'off': Default behavior, no partial validation.
* True / 'on': Enable partial validation.
* 'trailing-strings': Enable partial validation and allow trailing strings in the input.
by_alias: Whether to use the field's alias when validating against the provided input data.
by_name: Whether to use the field's name when validating against the provided input data.
Returns:
The validated object.
"""
by_alias
False
and
by_name
not
True
raise
PydanticUserError
'At least one of `by_alias` or `by_name` must be set to True.'
code
'validate-by-alias-and-name-false'
return
self
validator
validate_json
data
strict
strict
context
context
allow_partial
experimental_allow_partial
by_alias
by_alias
by_name
by_name
validate_strings
validate_strings
obj
Any
strict
bool
None
None
context
dict
str
Any
None
None
experimental_allow_partial
bool
Literal
"off"
"on"
"trailing-strings"
False
by_alias
bool
None
None
by_name
bool
None
None
Validate object contains string data against the model.
Parameters:
Name
Type
Description
Default
obj
Any
The object contains string data to validate.
required
strict
bool
| None
Whether to strictly check types.
None
context
dict
str
Any
] | None
Additional context to use during validation.
None
experimental_allow_partial
bool
Literal
['off', 'on', 'trailing-strings']
Experimental
whether to enable
partial validation
, e.g. to process streams.
* False / 'off': Default behavior, no partial validation.
* True / 'on': Enable partial validation.
* 'trailing-strings': Enable partial validation and allow trailing strings in the input.
False
by_alias
bool
| None
Whether to use the field's alias when validating against the provided input data.
None
by_name
bool
| None
Whether to use the field's name when validating against the provided input data.
None
Returns:
Type
Description
The validated object.
Source code in
pydantic/type_adapter.py
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
def
validate_strings
self
obj
Any
strict
bool
None
None
context
dict
str
Any
None
None
experimental_allow_partial
bool
Literal
'off'
'on'
'trailing-strings'
False
by_alias
bool
None
None
by_name
bool
None
None
"""Validate object contains string data against the model.
Args:
obj: The object contains string data to validate.
strict: Whether to strictly check types.
context: Additional context to use during validation.
experimental_allow_partial: **Experimental** whether to enable
[partial validation](../concepts/experimental.md#partial-validation), e.g. to process streams.
* False / 'off': Default behavior, no partial validation.
* True / 'on': Enable partial validation.
* 'trailing-strings': Enable partial validation and allow trailing strings in the input.
by_alias: Whether to use the field's alias when validating against the provided input data.
by_name: Whether to use the field's name when validating against the provided input data.
Returns:
The validated object.
"""
by_alias
False
and
by_name
not
True
raise
PydanticUserError
'At least one of `by_alias` or `by_name` must be set to True.'
code
'validate-by-alias-and-name-false'
return
self
validator
validate_strings
obj
strict
strict
context
context
allow_partial
experimental_allow_partial
by_alias
by_alias
by_name
by_name
get_default_value
get_default_value
strict
bool
None
None
context
dict
str
Any
None
None
Some
None
Get the default value for the wrapped type.
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether to strictly check types.
None
context
dict
str
Any
] | None
Additional context to pass to the validator.
None
Returns:
Type
Description
Some
] | None
The default value wrapped in a
Some
if there is one or None if not.
Source code in
pydantic/type_adapter.py
520
521
522
523
524
525
526
527
528
529
530
def
get_default_value
self
strict
bool
None
None
context
dict
str
Any
None
None
Some
None
"""Get the default value for the wrapped type.
Args:
strict: Whether to strictly check types.
context: Additional context to pass to the validator.
Returns:
The default value wrapped in a `Some` if there is one or None if not.
"""
return
self
validator
get_default_value
strict
strict
context
context
dump_python
dump_python
instance
mode
Literal
"json"
"python"
"python"
include
IncEx
None
None
exclude
IncEx
None
None
by_alias
bool
None
None
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
round_trip
bool
False
warnings
bool
Literal
"none"
"warn"
"error"
True
fallback
Callable
Any
Any
None
None
serialize_as_any
bool
False
context
dict
str
Any
None
None
Any
Dump an instance of the adapted type to a Python object.
Parameters:
Name
Type
Description
Default
instance
The Python object to serialize.
required
mode
Literal
['json', 'python']
The output format.
'python'
include
IncEx
| None
Fields to include in the output.
None
exclude
IncEx
| None
Fields to exclude from the output.
None
by_alias
bool
| None
Whether to use alias names for field names.
None
exclude_unset
bool
Whether to exclude unset fields.
False
exclude_defaults
bool
Whether to exclude fields with default values.
False
exclude_none
bool
Whether to exclude fields with None values.
False
round_trip
bool
Whether to output the serialized data in a way that is compatible with deserialization.
False
warnings
bool
Literal
['none', 'warn', 'error']
How to handle serialization errors. False/"none" ignores them, True/"warn" logs errors,
"error" raises a
PydanticSerializationError
True
fallback
Callable
Any
Any
] | None
A function to call when an unknown value is encountered. If not provided,
PydanticSerializationError
error is raised.
None
serialize_as_any
bool
Whether to serialize fields with duck-typing serialization behavior.
False
context
dict
str
Any
] | None
Additional context to pass to the serializer.
None
Returns:
Type
Description
Any
The serialized object.
Source code in
pydantic/type_adapter.py
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
581
582
583
584
585
586
def
dump_python
self
instance
mode
Literal
'json'
'python'
'python'
include
IncEx
None
None
exclude
IncEx
None
None
by_alias
bool
None
None
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
round_trip
bool
False
warnings
bool
Literal
'none'
'warn'
'error'
True
fallback
Callable
Any
Any
None
None
serialize_as_any
bool
False
context
dict
str
Any
None
None
Any
"""Dump an instance of the adapted type to a Python object.
Args:
instance: The Python object to serialize.
mode: The output format.
include: Fields to include in the output.
exclude: Fields to exclude from the output.
by_alias: Whether to use alias names for field names.
exclude_unset: Whether to exclude unset fields.
exclude_defaults: Whether to exclude fields with default values.
exclude_none: Whether to exclude fields with None values.
round_trip: Whether to output the serialized data in a way that is compatible with deserialization.
warnings: How to handle serialization errors. False/"none" ignores them, True/"warn" logs errors,
"error" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].
fallback: A function to call when an unknown value is encountered. If not provided,
a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.
serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.
context: Additional context to pass to the serializer.
Returns:
The serialized object.
"""
return
self
serializer
to_python
instance
mode
mode
by_alias
by_alias
include
include
exclude
exclude
exclude_unset
exclude_unset
exclude_defaults
exclude_defaults
exclude_none
exclude_none
round_trip
round_trip
warnings
warnings
fallback
fallback
serialize_as_any
serialize_as_any
context
context
dump_json
dump_json
instance
indent
int
None
None
include
IncEx
None
None
exclude
IncEx
None
None
by_alias
bool
None
None
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
round_trip
bool
False
warnings
bool
Literal
"none"
"warn"
"error"
True
fallback
Callable
Any
Any
None
None
serialize_as_any
bool
False
context
dict
str
Any
None
None
bytes
Usage Documentation
JSON Serialization
Serialize an instance of the adapted type to JSON.
Parameters:
Name
Type
Description
Default
instance
The instance to be serialized.
required
indent
int
| None
Number of spaces for JSON indentation.
None
include
IncEx
| None
Fields to include.
None
exclude
IncEx
| None
Fields to exclude.
None
by_alias
bool
| None
Whether to use alias names for field names.
None
exclude_unset
bool
Whether to exclude unset fields.
False
exclude_defaults
bool
Whether to exclude fields with default values.
False
exclude_none
bool
Whether to exclude fields with a value of
None
False
round_trip
bool
Whether to serialize and deserialize the instance to ensure round-tripping.
False
warnings
bool
Literal
['none', 'warn', 'error']
How to handle serialization errors. False/"none" ignores them, True/"warn" logs errors,
"error" raises a
PydanticSerializationError
True
fallback
Callable
Any
Any
] | None
A function to call when an unknown value is encountered. If not provided,
PydanticSerializationError
error is raised.
None
serialize_as_any
bool
Whether to serialize fields with duck-typing serialization behavior.
False
context
dict
str
Any
] | None
Additional context to pass to the serializer.
None
Returns:
Type
Description
bytes
The JSON representation of the given instance as bytes.
Source code in
pydantic/type_adapter.py
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
639
640
641
642
643
644
645
def
dump_json
self
instance
indent
int
None
None
include
IncEx
None
None
exclude
IncEx
None
None
by_alias
bool
None
None
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
round_trip
bool
False
warnings
bool
Literal
'none'
'warn'
'error'
True
fallback
Callable
Any
Any
None
None
serialize_as_any
bool
False
context
dict
str
Any
None
None
bytes
"""!!! abstract "Usage Documentation"
[JSON Serialization](../concepts/json.md#json-serialization)
Serialize an instance of the adapted type to JSON.
Args:
instance: The instance to be serialized.
indent: Number of spaces for JSON indentation.
include: Fields to include.
exclude: Fields to exclude.
by_alias: Whether to use alias names for field names.
exclude_unset: Whether to exclude unset fields.
exclude_defaults: Whether to exclude fields with default values.
exclude_none: Whether to exclude fields with a value of `None`.
round_trip: Whether to serialize and deserialize the instance to ensure round-tripping.
warnings: How to handle serialization errors. False/"none" ignores them, True/"warn" logs errors,
"error" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].
fallback: A function to call when an unknown value is encountered. If not provided,
a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.
serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.
context: Additional context to pass to the serializer.
Returns:
The JSON representation of the given instance as bytes.
"""
return
self
serializer
to_json
instance
indent
indent
include
include
exclude
exclude
by_alias
by_alias
exclude_unset
exclude_unset
exclude_defaults
exclude_defaults
exclude_none
exclude_none
round_trip
round_trip
warnings
warnings
fallback
fallback
serialize_as_any
serialize_as_any
context
context
json_schema
json_schema
by_alias
bool
True
ref_template
str
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
GenerateJsonSchema
mode
JsonSchemaMode
"validation"
dict
str
Any
Generate a JSON schema for the adapted type.
Parameters:
Name
Type
Description
Default
by_alias
bool
Whether to use alias names for field names.
True
ref_template
str
The format string used for generating $ref strings.
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
The generator class used for creating the schema.
GenerateJsonSchema
mode
JsonSchemaMode
The mode to use for schema generation.
'validation'
Returns:
Type
Description
dict
str
Any
The JSON schema for the model as a dictionary.
Source code in
pydantic/type_adapter.py
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
def
json_schema
self
by_alias
bool
True
ref_template
str
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
GenerateJsonSchema
mode
JsonSchemaMode
'validation'
dict
str
Any
"""Generate a JSON schema for the adapted type.
Args:
by_alias: Whether to use alias names for field names.
ref_template: The format string used for generating $ref strings.
schema_generator: The generator class used for creating the schema.
mode: The mode to use for schema generation.
Returns:
The JSON schema for the model as a dictionary.
"""
schema_generator_instance
schema_generator
by_alias
by_alias
ref_template
ref_template
isinstance
self
core_schema
_mock_val_ser
MockCoreSchema
self
core_schema
rebuild
assert
not
isinstance
self
core_schema
_mock_val_ser
MockCoreSchema
'this is a bug! please report it'
return
schema_generator_instance
generate
self
core_schema
mode
mode
json_schemas
staticmethod
json_schemas
inputs
Iterable
tuple
JsonSchemaKeyT
JsonSchemaMode
TypeAdapter
Any
by_alias
bool
True
title
str
None
None
description
str
None
None
ref_template
str
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
GenerateJsonSchema
tuple
dict
tuple
JsonSchemaKeyT
JsonSchemaMode
JsonSchemaValue
JsonSchemaValue
Generate a JSON schema including definitions from multiple type adapters.
Parameters:
Name
Type
Description
Default
inputs
Iterable
tuple
JsonSchemaKeyT
JsonSchemaMode
TypeAdapter
Any
]]]
Inputs to schema generation. The first two items will form the keys of the (first)
output mapping; the type adapters will provide the core schemas that get converted into
definitions in the output JSON schema.
required
by_alias
bool
Whether to use alias names.
True
title
str
| None
The title for the schema.
None
description
str
| None
The description for the schema.
None
ref_template
str
The format string used for generating $ref strings.
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
The generator class used for creating the schema.
GenerateJsonSchema
Returns:
Type
Description
tuple
dict
tuple
JsonSchemaKeyT
JsonSchemaMode
JsonSchemaValue
JsonSchemaValue
A tuple where:
The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and
whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have
JsonRef references to definitions that are defined in the second returned element.)
The second element is a JSON schema containing all definitions referenced in the first returned
element, along with the optional title and description keys.
Source code in
pydantic/type_adapter.py
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
@staticmethod
def
json_schemas
inputs
Iterable
tuple
JsonSchemaKeyT
JsonSchemaMode
TypeAdapter
Any
]]],
by_alias
bool
True
title
str
None
None
description
str
None
None
ref_template
str
DEFAULT_REF_TEMPLATE
schema_generator
type
GenerateJsonSchema
GenerateJsonSchema
tuple
dict
tuple
JsonSchemaKeyT
JsonSchemaMode
JsonSchemaValue
JsonSchemaValue
"""Generate a JSON schema including definitions from multiple type adapters.
Args:
inputs: Inputs to schema generation. The first two items will form the keys of the (first)
output mapping; the type adapters will provide the core schemas that get converted into
definitions in the output JSON schema.
by_alias: Whether to use alias names.
title: The title for the schema.
description: The description for the schema.
ref_template: The format string used for generating $ref strings.
schema_generator: The generator class used for creating the schema.
Returns:
A tuple where:
- The first element is a dictionary whose keys are tuples of JSON schema key type and JSON mode, and
whose values are the JSON schema corresponding to that pair of inputs. (These schemas may have
JsonRef references to definitions that are defined in the second returned element.)
- The second element is a JSON schema containing all definitions referenced in the first returned
element, along with the optional title and description keys.
"""
schema_generator_instance
schema_generator
by_alias
by_alias
ref_template
ref_template
inputs_
for
key
mode
adapter
inputs
# This is the same pattern we follow for model json schemas - we attempt a core schema rebuild if we detect a mock
isinstance
adapter
core_schema
_mock_val_ser
MockCoreSchema
adapter
core_schema
rebuild
assert
not
isinstance
adapter
core_schema
_mock_val_ser
MockCoreSchema
'this is a bug! please report it'
inputs_
append
key
mode
adapter
core_schema
json_schemas_map
definitions
schema_generator_instance
generate_definitions
inputs_
json_schema
dict
str
Any
definitions
json_schema
'$defs'
definitions
title
json_schema
'title'
title
description
json_schema
'description'
description
return
json_schemas_map
json_schema
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 034_pydantic_extra_types_ulid.txt ---
ULID
The
pydantic_extra_types.ULID
module provides the [
ULID
] data type.
This class depends on the [python-ulid] package, which is a validate by the
ULID-spec
ULID
dataclass
ULID
ulid
ULID
Bases:
Representation
A wrapper around
python-ulid
package, which
is a validate by the
ULID-spec
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 025_pydantic_extra_types_timezone_name.txt ---
Timezone Name
Time zone name validation and serialization module.
TimeZoneName
Bases:
str
TimeZoneName is a custom string subclass for validating and serializing timezone names.
The TimeZoneName class uses the IANA Time Zone Database for validation.
It supports both strict and non-strict modes for timezone name validation.
Examples:
Some examples of using the TimeZoneName class:
Normal usage:
from
pydantic_extra_types.timezone_name
import
TimeZoneName
from
pydantic
import
BaseModel
class
Location
BaseModel
city
str
timezone
TimeZoneName
loc
Location
city
"New York"
timezone
"America/New_York"
print
loc
timezone
America
New_York
Non-strict mode:
from
pydantic_extra_types.timezone_name
import
TimeZoneName
timezone_name_settings
@timezone_name_settings
strict
False
class
TZNonStrict
TimeZoneName
pass
TZNonStrict
"america/new_york"
print
america
new_york
get_timezones
get_timezones
Set
str
Determine the timezone provider and return available timezones.
Source code in
pydantic_extra_types/timezone_name.py
def
get_timezones
Set
str
"""Determine the timezone provider and return available timezones."""
_is_available
'zoneinfo'
and
_is_available
'tzdata'
# pragma: no cover
return
_tz_provider_from_zone_info
elif
_is_available
'pytz'
# pragma: no cover
sys
version_info
_warn_about_pytz_usage
return
_tz_provider_from_pytz
else
# pragma: no cover
sys
version_info
raise
ImportError
'No pytz module found. Please install it with "pip install pytz"'
raise
ImportError
'No timezone provider found. Please install tzdata with "pip install tzdata"'
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 018_networks.txt ---
Network Types
The networks module contains types for common network-related fields.
MAX_EMAIL_LENGTH
module-attribute
MAX_EMAIL_LENGTH
2048
Maximum length for an email.
A somewhat arbitrary but very generous number compared to what is allowed by most implementations.
UrlConstraints
dataclass
UrlConstraints
max_length
int
None
None
allowed_schemes
list
str
None
None
host_required
bool
None
None
default_host
str
None
None
default_port
int
None
None
default_path
str
None
None
Url constraints.
Attributes:
Name
Type
Description
max_length
int
| None
The maximum length of the url. Defaults to
None
allowed_schemes
list
str
] | None
The allowed schemes. Defaults to
None
host_required
bool
| None
Whether the host is required. Defaults to
None
default_host
str
| None
The default host. Defaults to
None
default_port
int
| None
The default port. Defaults to
None
default_path
str
| None
The default path. Defaults to
None
defined_constraints
property
defined_constraints
dict
str
Any
Fetch a key / value mapping of constraints to values that are not None. Used for core schema updates.
AnyUrl
AnyUrl
url
str
Url
_BaseUrl
Bases:
_BaseUrl
Base type for all URLs.
Any scheme allowed
Top-level domain (TLD) not required
Host not required
Assuming an input URL of
http://samuel:pass@example.com:8000/the/path/?query=here#fragment=is;this=bit
the types export the following properties:
scheme
: the URL scheme (
http
), always set.
host
: the URL host (
example.com
username
: optional username if included (
samuel
password
: optional password if included (
pass
port
: optional port (
8000
path
: optional path (
/the/path/
query
: optional URL query (for example,
GET
arguments or "search string", such as
query=here
fragment
: optional fragment (
fragment=is;this=bit
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
AnyHttpUrl
AnyHttpUrl
url
str
Url
_BaseUrl
Bases:
AnyUrl
A type that will accept any http or https URL.
TLD not required
Host not required
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
HttpUrl
HttpUrl
url
str
Url
_BaseUrl
Bases:
AnyUrl
A type that will accept any http or https URL.
TLD not required
Host not required
Max length 2083
from
pydantic
import
BaseModel
HttpUrl
ValidationError
class
MyModel
BaseModel
url
HttpUrl
MyModel
url
'http://www.example.com'
print
url
#> http://www.example.com/
try
MyModel
url
'ftp://invalid.url'
except
ValidationError
print
'''
1 validation error for MyModel
url
URL scheme should be 'http' or 'https' [type=url_scheme, input_value='ftp://invalid.url', input_type=str]
'''
try
MyModel
url
'not a url'
except
ValidationError
print
'''
1 validation error for MyModel
url
Input should be a valid URL, relative URL without a base [type=url_parsing, input_value='not a url', input_type=str]
'''
"International domains" (e.g. a URL where the host or TLD includes non-ascii characters) will be encoded via
punycode
(see
this article
for a good description of why this is important):
from
pydantic
import
BaseModel
HttpUrl
class
MyModel
BaseModel
url
HttpUrl
MyModel
url
'http://punyÂ£code.com'
print
url
#> http://xn--punycode-eja.com/
MyModel
url
'https://www.Ð°ÑÑÓÐµ.com/'
print
url
#> https://www.xn--80ak6aa92e.com/
MyModel
url
'https://www.example.ç å®/'
print
url
#> https://www.example.xn--pbt977c/
Underscores in Hostnames
In Pydantic, underscores are allowed in all parts of a domain except the TLD.
Technically this might be wrong - in theory the hostname cannot have underscores, but subdomains can.
To explain this; consider the following two cases:
exam_ple.co.uk
: the hostname is
exam_ple
, which should not be allowed since it contains an underscore.
foo_bar.example.com
the hostname is
example
, which should be allowed since the underscore is in the subdomain.
Without having an exhaustive list of TLDs, it would be impossible to differentiate between these two. Therefore
underscores are allowed, but you can always do further validation in a validator if desired.
Also, Chrome, Firefox, and Safari all currently accept
http://exam_ple.com
as a URL, so we're in good
(or at least big) company.
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
AnyWebsocketUrl
AnyWebsocketUrl
url
str
Url
_BaseUrl
Bases:
AnyUrl
A type that will accept any ws or wss URL.
TLD not required
Host not required
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
WebsocketUrl
WebsocketUrl
url
str
Url
_BaseUrl
Bases:
AnyUrl
A type that will accept any ws or wss URL.
TLD not required
Host not required
Max length 2083
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
FileUrl
FileUrl
url
str
Url
_BaseUrl
Bases:
AnyUrl
A type that will accept any file URL.
Host not required
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
FtpUrl
FtpUrl
url
str
Url
_BaseUrl
Bases:
AnyUrl
A type that will accept ftp URL.
TLD not required
Host not required
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
PostgresDsn
PostgresDsn
url
str
MultiHostUrl
_BaseMultiHostUrl
Bases:
_BaseMultiHostUrl
A type that will accept any Postgres DSN.
User info required
TLD not required
Host required
Supports multiple hosts
If further validation is required, these properties can be used by validators to enforce specific behaviour:
from
pydantic
import
BaseModel
HttpUrl
PostgresDsn
ValidationError
field_validator
class
MyModel
BaseModel
url
HttpUrl
MyModel
url
'http://www.example.com'
# the repr() method for a url will display all properties of the url
print
repr
url
#> HttpUrl('http://www.example.com/')
print
url
scheme
#> http
print
url
host
#> www.example.com
print
url
port
#> 80
class
MyDatabaseModel
BaseModel
PostgresDsn
@field_validator
'db'
def
check_db_name
cls
assert
path
and
len
path
'database must be provided'
return
MyDatabaseModel
'postgres://user:pass@localhost:5432/foobar'
print
#> postgres://user:pass@localhost:5432/foobar
try
MyDatabaseModel
'postgres://user:pass@localhost:5432'
except
ValidationError
print
'''
1 validation error for MyDatabaseModel
Assertion failed, database must be provided
assert (None)
+ where None = PostgresDsn('postgres://user:pass@localhost:5432').path [type=assertion_error, input_value='postgres://user:pass@localhost:5432', input_type=str]
'''
Source code in
pydantic/networks.py
347
348
def
__init__
self
url
str
_CoreMultiHostUrl
_BaseMultiHostUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
host
property
host
str
The required URL host.
CockroachDsn
CockroachDsn
url
str
Url
_BaseUrl
Bases:
AnyUrl
A type that will accept any Cockroach DSN.
User info required
TLD not required
Host required
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
host
property
host
str
The required URL host.
AmqpDsn
AmqpDsn
url
str
Url
_BaseUrl
Bases:
AnyUrl
A type that will accept any AMQP DSN.
User info required
TLD not required
Host not required
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
RedisDsn
RedisDsn
url
str
Url
_BaseUrl
Bases:
AnyUrl
A type that will accept any Redis DSN.
User info required
TLD not required
Host required (e.g.,
rediss://:pass@localhost
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
host
property
host
str
The required URL host.
MongoDsn
MongoDsn
url
str
MultiHostUrl
_BaseMultiHostUrl
Bases:
_BaseMultiHostUrl
A type that will accept any MongoDB DSN.
User info not required
Database name not required
Port not required
User info may be passed without user part (e.g.,
mongodb://mongodb0.example.com:27017
Source code in
pydantic/networks.py
347
348
def
__init__
self
url
str
_CoreMultiHostUrl
_BaseMultiHostUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
KafkaDsn
KafkaDsn
url
str
Url
_BaseUrl
Bases:
AnyUrl
A type that will accept any Kafka DSN.
User info required
TLD not required
Host not required
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
NatsDsn
NatsDsn
url
str
MultiHostUrl
_BaseMultiHostUrl
Bases:
_BaseMultiHostUrl
A type that will accept any NATS DSN.
NATS is a connective technology built for the ever increasingly hyper-connected world.
It is a single technology that enables applications to securely communicate across
any combination of cloud vendors, on-premise, edge, web and mobile, and devices.
More: https://nats.io
Source code in
pydantic/networks.py
347
348
def
__init__
self
url
str
_CoreMultiHostUrl
_BaseMultiHostUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
MySQLDsn
MySQLDsn
url
str
Url
_BaseUrl
Bases:
AnyUrl
A type that will accept any MySQL DSN.
User info required
TLD not required
Host not required
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
MariaDBDsn
MariaDBDsn
url
str
Url
_BaseUrl
Bases:
AnyUrl
A type that will accept any MariaDB DSN.
User info required
TLD not required
Host not required
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
ClickHouseDsn
ClickHouseDsn
url
str
Url
_BaseUrl
Bases:
AnyUrl
A type that will accept any ClickHouse DSN.
User info required
TLD not required
Host not required
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
SnowflakeDsn
SnowflakeDsn
url
str
Url
_BaseUrl
Bases:
AnyUrl
A type that will accept any Snowflake DSN.
User info required
TLD not required
Host required
Source code in
pydantic/networks.py
127
128
def
__init__
self
url
str
_CoreUrl
_BaseUrl
None
self
_url
_build_type_adapter
self
__class__
validate_python
url
_url
host
property
host
str
The required URL host.
EmailStr
Info
To use this type, you need to install the optional
email-validator
package:
pip
install
email-validator
Validate email addresses.
from
pydantic
import
BaseModel
EmailStr
class
Model
BaseModel
email
EmailStr
print
Model
email
'contact@mail.com'
#> email='contact@mail.com'
NameEmail
NameEmail
name
str
email
str
Bases:
Representation
Info
To use this type, you need to install the optional
email-validator
package:
pip
install
email-validator
Validate a name and email address combination, as specified by
RFC 5322
The
NameEmail
has two properties:
name
and
email
In case the
name
is not provided, it's inferred from the email address.
from
pydantic
import
BaseModel
NameEmail
class
User
BaseModel
email
NameEmail
user
User
email
'Fred Bloggs <fred.bloggs@example.com>'
print
user
email
#> Fred Bloggs <fred.bloggs@example.com>
print
user
email
name
#> Fred Bloggs
user
User
email
'fred.bloggs@example.com'
print
user
email
#> fred.bloggs <fred.bloggs@example.com>
print
user
email
name
#> fred.bloggs
Source code in
pydantic/networks.py
1040
1041
1042
def
__init__
self
name
str
email
str
self
name
name
self
email
email
IPvAnyAddress
Validate an IPv4 or IPv6 address.
from
pydantic
import
BaseModel
from
pydantic.networks
import
IPvAnyAddress
class
IpModel
BaseModel
IPvAnyAddress
print
IpModel
'127.0.0.1'
#> ip=IPv4Address('127.0.0.1')
try
IpModel
'http://www.example.com'
except
ValueError
print
errors
())
'''
'type': 'ip_any_address',
'loc': ('ip',),
'msg': 'value is not a valid IPv4 or IPv6 address',
'input': 'http://www.example.com',
'''
IPvAnyInterface
Validate an IPv4 or IPv6 interface.
IPvAnyNetwork
Validate an IPv4 or IPv6 network.
validate_email
validate_email
value
str
tuple
str
str
Email address validation using
email-validator
Returns:
Type
Description
tuple
str
str
A tuple containing the local part of the email (or the name for "pretty" email addresses)
and the normalized email.
Raises:
Type
Description
PydanticCustomError
If the email is invalid.
Note
Note that:
Raw IP address (literal) domain parts are not allowed.
"John Doe <local_part@domain.com>"
style "pretty" email addresses are processed.
Spaces are striped from the beginning and end of addresses, but no error is raised.
Source code in
pydantic/networks.py
1264
1265
1266
1267
1268
1269
1270
1271
1272
1273
1274
1275
1276
1277
1278
1279
1280
1281
1282
1283
1284
1285
1286
1287
1288
1289
1290
1291
1292
1293
1294
1295
1296
1297
1298
1299
1300
1301
1302
1303
1304
1305
1306
1307
1308
1309
def
validate_email
value
str
tuple
str
str
"""Email address validation using [email-validator](https://pypi.org/project/email-validator/).
Returns:
A tuple containing the local part of the email (or the name for "pretty" email addresses)
and the normalized email.
Raises:
PydanticCustomError: If the email is invalid.
Note:
Note that:
* Raw IP address (literal) domain parts are not allowed.
* `"John Doe <local_part@domain.com>"` style "pretty" email addresses are processed.
* Spaces are striped from the beginning and end of addresses, but no error is raised.
"""
email_validator
None
import_email_validator
len
value
MAX_EMAIL_LENGTH
raise
PydanticCustomError
'value_error'
'value is not a valid email address:
{reason}
'reason'
'Length must not exceed
MAX_EMAIL_LENGTH
characters'
pretty_email_regex
fullmatch
value
name
str
None
None
unquoted_name
quoted_name
value
groups
name
unquoted_name
quoted_name
email
value
strip
try
parts
email_validator
validate_email
email
check_deliverability
False
except
email_validator
EmailNotValidError
raise
PydanticCustomError
'value_error'
'value is not a valid email address:
{reason}
'reason'
str
args
])}
from
email
parts
normalized
assert
email
not
None
name
name
parts
local_part
return
name
email
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 002_types_pydantic_types_NonPositiveFloat.txt ---
Pydantic Types
pydantic.types
The types module contains custom types used by pydantic.
StrictBool
module-attribute
StrictBool
Annotated
bool
Strict
()]
A boolean that must be either
True
False
PositiveInt
module-attribute
PositiveInt
Annotated
int
An integer that must be greater than zero.
from
pydantic
import
BaseModel
PositiveInt
ValidationError
class
Model
BaseModel
positive_int
PositiveInt
Model
positive_int
print
repr
#> Model(positive_int=1)
try
Model
positive_int
except
ValidationError
print
errors
())
'''
'type': 'greater_than',
'loc': ('positive_int',),
'msg': 'Input should be greater than 0',
'input': -1,
'ctx': {'gt': 0},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
NegativeInt
module-attribute
NegativeInt
Annotated
int
An integer that must be less than zero.
from
pydantic
import
BaseModel
NegativeInt
ValidationError
class
Model
BaseModel
negative_int
NegativeInt
Model
negative_int
print
repr
#> Model(negative_int=-1)
try
Model
negative_int
except
ValidationError
print
errors
())
'''
'type': 'less_than',
'loc': ('negative_int',),
'msg': 'Input should be less than 0',
'input': 1,
'ctx': {'lt': 0},
'url': 'https://errors.pydantic.dev/2/v/less_than',
'''
NonPositiveInt
module-attribute
NonPositiveInt
Annotated
int
An integer that must be less than or equal to zero.
from
pydantic
import
BaseModel
NonPositiveInt
ValidationError
class
Model
BaseModel
non_positive_int
NonPositiveInt
Model
non_positive_int
print
repr
#> Model(non_positive_int=0)
try
Model
non_positive_int
except
ValidationError
print
errors
())
'''
'type': 'less_than_equal',
'loc': ('non_positive_int',),
'msg': 'Input should be less than or equal to 0',
'input': 1,
'ctx': {'le': 0},
'url': 'https://errors.pydantic.dev/2/v/less_than_equal',
'''
NonNegativeInt
module-attribute
NonNegativeInt
Annotated
int
An integer that must be greater than or equal to zero.
from
pydantic
import
BaseModel
NonNegativeInt
ValidationError
class
Model
BaseModel
non_negative_int
NonNegativeInt
Model
non_negative_int
print
repr
#> Model(non_negative_int=0)
try
Model
non_negative_int
except
ValidationError
print
errors
())
'''
'type': 'greater_than_equal',
'loc': ('non_negative_int',),
'msg': 'Input should be greater than or equal to 0',
'input': -1,
'ctx': {'ge': 0},
'url': 'https://errors.pydantic.dev/2/v/greater_than_equal',
'''
StrictInt
module-attribute
StrictInt
Annotated
int
Strict
()]
An integer that must be validated in strict mode.
from
pydantic
import
BaseModel
StrictInt
ValidationError
class
StrictIntModel
BaseModel
strict_int
StrictInt
try
StrictIntModel
strict_int
3.14159
except
ValidationError
print
'''
1 validation error for StrictIntModel
strict_int
Input should be a valid integer [type=int_type, input_value=3.14159, input_type=float]
'''
PositiveFloat
module-attribute
PositiveFloat
Annotated
float
A float that must be greater than zero.
from
pydantic
import
BaseModel
PositiveFloat
ValidationError
class
Model
BaseModel
positive_float
PositiveFloat
Model
positive_float
1.0
print
repr
#> Model(positive_float=1.0)
try
Model
positive_float
1.0
except
ValidationError
print
errors
())
'''
'type': 'greater_than',
'loc': ('positive_float',),
'msg': 'Input should be greater than 0',
'input': -1.0,
'ctx': {'gt': 0.0},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
NegativeFloat
module-attribute
NegativeFloat
Annotated
float
A float that must be less than zero.
from
pydantic
import
BaseModel
NegativeFloat
ValidationError
class
Model
BaseModel
negative_float
NegativeFloat
Model
negative_float
1.0
print
repr
#> Model(negative_float=-1.0)
try
Model
negative_float
1.0
except
ValidationError
print
errors
())
'''
'type': 'less_than',
'loc': ('negative_float',),
'msg': 'Input should be less than 0',
'input': 1.0,
'ctx': {'lt': 0.0},
'url': 'https://errors.pydantic.dev/2/v/less_than',
'''
NonPositiveFloat
module-attribute
NonPositiveFloat
Annotated
float
A float that must be less than or equal to zero.
from
pydantic
import
BaseModel
NonPositiveFloat
ValidationError
class
Model
BaseModel
non_positive_float
NonPositiveFloat
Model
non_positive_float
0.0
print
repr
#> Model(non_positive_float=0.0)
try
Model
non_positive_float
1.0
except
ValidationError
print
errors
())
'''
'type': 'less_than_equal',
'loc': ('non_positive_float',),
'msg': 'Input should be less than or equal to 0',
'input': 1.0,
'ctx': {'le': 0.0},
'url': 'https://errors.pydantic.dev/2/v/less_than_equal',
'''
NonNegativeFloat
module-attribute
NonNegativeFloat
Annotated
float
A float that must be greater than or equal to zero.
from
pydantic
import
BaseModel
NonNegativeFloat
ValidationError
class
Model
BaseModel
non_negative_float
NonNegativeFloat
Model
non_negative_float
0.0
print
repr
#> Model(non_negative_float=0.0)
try
Model
non_negative_float
1.0
except
ValidationError
print
errors
())
'''
'type': 'greater_than_equal',
'loc': ('non_negative_float',),
'msg': 'Input should be greater than or equal to 0',
'input': -1.0,
'ctx': {'ge': 0.0},
'url': 'https://errors.pydantic.dev/2/v/greater_than_equal',
'''
StrictFloat
module-attribute
StrictFloat
Annotated
float
Strict
True
A float that must be validated in strict mode.
from
pydantic
import
BaseModel
StrictFloat
ValidationError
class
StrictFloatModel
BaseModel
strict_float
StrictFloat
try
StrictFloatModel
strict_float
'1.0'
except
ValidationError
print
'''
1 validation error for StrictFloatModel
strict_float
Input should be a valid number [type=float_type, input_value='1.0', input_type=str]
'''
FiniteFloat
module-attribute
FiniteFloat
Annotated
float
AllowInfNan
False
A float that must be finite (not
-inf
inf
, or
nan
from
pydantic
import
BaseModel
FiniteFloat
class
Model
BaseModel
finite
FiniteFloat
Model
finite
1.0
print
#> finite=1.0
StrictBytes
module-attribute
StrictBytes
Annotated
bytes
Strict
()]
A bytes that must be validated in strict mode.
StrictStr
module-attribute
StrictStr
Annotated
str
Strict
()]
A string that must be validated in strict mode.
UUID1
module-attribute
UUID1
Annotated
UUID
UuidVersion
UUID
that must be version 1.
import
uuid
from
pydantic
import
UUID1
BaseModel
class
Model
BaseModel
uuid1
UUID1
Model
uuid1
uuid
uuid1
())
UUID3
module-attribute
UUID3
Annotated
UUID
UuidVersion
UUID
that must be version 3.
import
uuid
from
pydantic
import
UUID3
BaseModel
class
Model
BaseModel
uuid3
UUID3
Model
uuid3
uuid
uuid3
uuid
NAMESPACE_DNS
'pydantic.org'
UUID4
module-attribute
UUID4
Annotated
UUID
UuidVersion
UUID
that must be version 4.
import
uuid
from
pydantic
import
UUID4
BaseModel
class
Model
BaseModel
uuid4
UUID4
Model
uuid4
uuid
uuid4
())
UUID5
module-attribute
UUID5
Annotated
UUID
UuidVersion
UUID
that must be version 5.
import
uuid
from
pydantic
import
UUID5
BaseModel
class
Model
BaseModel
uuid5
UUID5
Model
uuid5
uuid
uuid5
uuid
NAMESPACE_DNS
'pydantic.org'
UUID6
module-attribute
UUID6
Annotated
UUID
UuidVersion
UUID
that must be version 6.
import
uuid
from
pydantic
import
UUID6
BaseModel
class
Model
BaseModel
uuid6
UUID6
Model
uuid6
uuid
UUID
'1efea953-c2d6-6790-aa0a-69db8c87df97'
UUID7
module-attribute
UUID7
Annotated
UUID
UuidVersion
UUID
that must be version 7.
import
uuid
from
pydantic
import
UUID7
BaseModel
class
Model
BaseModel
uuid7
UUID7
Model
uuid7
uuid
UUID
'0194fdcb-1c47-7a09-b52c-561154de0b4a'
UUID8
module-attribute
UUID8
Annotated
UUID
UuidVersion
UUID
that must be version 8.
import
uuid
from
pydantic
import
UUID8
BaseModel
class
Model
BaseModel
uuid8
UUID8
Model
uuid8
uuid
UUID
'81a0b92e-6078-8551-9c81-8ccb666bdab8'
FilePath
module-attribute
FilePath
Annotated
Path
PathType
'file'
A path that must point to a file.
from
pathlib
import
Path
from
pydantic
import
BaseModel
FilePath
ValidationError
class
Model
BaseModel
FilePath
path
Path
'text.txt'
path
touch
Model
'text.txt'
print
model_dump
())
#> {'f': PosixPath('text.txt')}
path
unlink
path
Path
'directory'
path
mkdir
exist_ok
True
try
Model
'directory'
# directory
except
ValidationError
print
'''
1 validation error for Model
Path does not point to a file [type=path_not_file, input_value='directory', input_type=str]
'''
path
rmdir
try
Model
'not-exists-file'
except
ValidationError
print
'''
1 validation error for Model
Path does not point to a file [type=path_not_file, input_value='not-exists-file', input_type=str]
'''
DirectoryPath
module-attribute
DirectoryPath
Annotated
Path
PathType
'dir'
A path that must point to a directory.
from
pathlib
import
Path
from
pydantic
import
BaseModel
DirectoryPath
ValidationError
class
Model
BaseModel
DirectoryPath
path
Path
'directory/'
path
mkdir
Model
'directory/'
print
model_dump
())
#> {'f': PosixPath('directory')}
path
rmdir
path
Path
'file.txt'
path
touch
try
Model
'file.txt'
# file
except
ValidationError
print
'''
1 validation error for Model
Path does not point to a directory [type=path_not_directory, input_value='file.txt', input_type=str]
'''
path
unlink
try
Model
'not-exists-directory'
except
ValidationError
print
'''
1 validation error for Model
Path does not point to a directory [type=path_not_directory, input_value='not-exists-directory', input_type=str]
'''
NewPath
module-attribute
NewPath
Annotated
Path
PathType
'new'
A path for a new file or directory that must not already exist. The parent directory must already exist.
SocketPath
module-attribute
SocketPath
Annotated
Path
PathType
'socket'
A path to an existing socket file
Base64Bytes
module-attribute
Base64Bytes
Annotated
bytes
EncodedBytes
encoder
Base64Encoder
A bytes type that is encoded and decoded using the standard (non-URL-safe) base64 encoder.
Note
Under the hood,
Base64Bytes
uses the standard library
base64.b64encode
and
base64.b64decode
functions.
As a result, attempting to decode url-safe base64 data using the
Base64Bytes
type may fail or produce an incorrect
decoding.
Warning
In versions of Pydantic prior to v2.10,
Base64Bytes
used
base64.encodebytes
and
base64.decodebytes
functions. According to the
base64 documentation
these methods are considered legacy implementation, and thus, Pydantic v2.10+ now uses the modern
base64.b64encode
and
base64.b64decode
functions.
If you'd still like to use these legacy encoders / decoders, you can achieve this by creating a custom annotated type,
like follows:
import
base64
from
typing
import
Annotated
Literal
from
pydantic_core
import
PydanticCustomError
from
pydantic
import
EncodedBytes
EncoderProtocol
class
LegacyBase64Encoder
EncoderProtocol
@classmethod
def
decode
cls
data
bytes
bytes
try
return
base64
decodebytes
data
except
ValueError
raise
PydanticCustomError
'base64_decode'
"Base64 decoding error: '
{error}
'error'
str
)},
@classmethod
def
encode
cls
value
bytes
bytes
return
base64
encodebytes
value
@classmethod
def
get_json_format
cls
Literal
'base64'
return
'base64'
LegacyBase64Bytes
Annotated
bytes
EncodedBytes
encoder
LegacyBase64Encoder
from
pydantic
import
Base64Bytes
BaseModel
ValidationError
class
Model
BaseModel
base64_bytes
Base64Bytes
# Initialize the model with base64 data
Model
base64_bytes
'VGhpcyBpcyB0aGUgd2F5'
# Access decoded value
print
base64_bytes
#> b'This is the way'
# Serialize into the base64 form
print
model_dump
())
#> {'base64_bytes': b'VGhpcyBpcyB0aGUgd2F5'}
# Validate base64 data
try
print
Model
base64_bytes
'undecodable'
base64_bytes
except
ValidationError
print
'''
1 validation error for Model
base64_bytes
Base64 decoding error: 'Incorrect padding' [type=base64_decode, input_value=b'undecodable', input_type=bytes]
'''
Base64Str
module-attribute
Base64Str
Annotated
str
EncodedStr
encoder
Base64Encoder
A str type that is encoded and decoded using the standard (non-URL-safe) base64 encoder.
Note
Under the hood,
Base64Str
uses the standard library
base64.b64encode
and
base64.b64decode
functions.
As a result, attempting to decode url-safe base64 data using the
Base64Str
type may fail or produce an incorrect
decoding.
Warning
In versions of Pydantic prior to v2.10,
Base64Str
used
base64.encodebytes
and
base64.decodebytes
functions. According to the
base64 documentation
these methods are considered legacy implementation, and thus, Pydantic v2.10+ now uses the modern
base64.b64encode
and
base64.b64decode
functions.
See the
Base64Bytes
type for more information on how to
replicate the old behavior with the legacy encoders / decoders.
from
pydantic
import
Base64Str
BaseModel
ValidationError
class
Model
BaseModel
base64_str
Base64Str
# Initialize the model with base64 data
Model
base64_str
'VGhlc2UgYXJlbid0IHRoZSBkcm9pZHMgeW91J3JlIGxvb2tpbmcgZm9y'
# Access decoded value
print
base64_str
#> These aren't the droids you're looking for
# Serialize into the base64 form
print
model_dump
())
#> {'base64_str': 'VGhlc2UgYXJlbid0IHRoZSBkcm9pZHMgeW91J3JlIGxvb2tpbmcgZm9y'}
# Validate base64 data
try
print
Model
base64_str
'undecodable'
base64_str
except
ValidationError
print
'''
1 validation error for Model
base64_str
Base64 decoding error: 'Incorrect padding' [type=base64_decode, input_value='undecodable', input_type=str]
'''
Base64UrlBytes
module-attribute
Base64UrlBytes
Annotated
bytes
EncodedBytes
encoder
Base64UrlEncoder
A bytes type that is encoded and decoded using the URL-safe base64 encoder.
Note
Under the hood,
Base64UrlBytes
use standard library
base64.urlsafe_b64encode
and
base64.urlsafe_b64decode
functions.
As a result, the
Base64UrlBytes
type can be used to faithfully decode "vanilla" base64 data
(using
'+'
and
'/'
from
pydantic
import
Base64UrlBytes
BaseModel
class
Model
BaseModel
base64url_bytes
Base64UrlBytes
# Initialize the model with base64 data
Model
base64url_bytes
'SHc_dHc-TXc=='
print
#> base64url_bytes=b'Hw?tw>Mw'
Base64UrlStr
module-attribute
Base64UrlStr
Annotated
str
EncodedStr
encoder
Base64UrlEncoder
A str type that is encoded and decoded using the URL-safe base64 encoder.
Note
Under the hood,
Base64UrlStr
use standard library
base64.urlsafe_b64encode
and
base64.urlsafe_b64decode
functions.
As a result, the
Base64UrlStr
type can be used to faithfully decode "vanilla" base64 data (using
'+'
and
'/'
from
pydantic
import
Base64UrlStr
BaseModel
class
Model
BaseModel
base64url_str
Base64UrlStr
# Initialize the model with base64 data
Model
base64url_str
'SHc_dHc-TXc=='
print
#> base64url_str='Hw?tw>Mw'
JsonValue
module-attribute
JsonValue
TypeAlias
Union
list
"JsonValue"
dict
str
"JsonValue"
str
bool
int
float
None
JsonValue
is used to represent a value that can be serialized to JSON.
It may be one of:
list['JsonValue']
dict[str, 'JsonValue']
str
bool
int
float
None
The following example demonstrates how to use
JsonValue
to validate JSON data,
and what kind of errors to expect when input data is not json serializable.
import
json
from
pydantic
import
BaseModel
JsonValue
ValidationError
class
Model
BaseModel
JsonValue
valid_json_data
'j'
'a'
'b'
'c'
'd'
None
]}}}}
invalid_json_data
'j'
'a'
'b'
...
}}}
print
repr
Model
model_validate
valid_json_data
)))
#> Model(j={'a': {'b': {'c': 1, 'd': [2, None]}}})
print
repr
Model
model_validate_json
json
dumps
valid_json_data
))))
#> Model(j={'a': {'b': {'c': 1, 'd': [2, None]}}})
try
Model
model_validate
invalid_json_data
except
ValidationError
print
'''
1 validation error for Model
j.dict.a.dict.b
input was not a valid JSON value [type=invalid-json-value, input_value=Ellipsis, input_type=ellipsis]
'''
OnErrorOmit
module-attribute
OnErrorOmit
Annotated
_OnErrorOmit
When used as an item in a list, the key type in a dict, optional values of a TypedDict, etc.
this annotation omits the item from the iteration if there is any error validating it.
That is, instead of a
ValidationError
being propagated up and the entire iterable being discarded
any invalid items are discarded and the valid ones are returned.
Strict
dataclass
Bases:
PydanticMetadata
BaseMetadata
Usage Documentation
Strict Mode with
Annotated
Strict
A field metadata class to indicate that a field should be validated in strict mode.
Use this class as an annotation via
Annotated
, as seen below.
Attributes:
Name
Type
Description
strict
bool
Whether to validate the field in strict mode.
Example
from
typing
import
Annotated
from
pydantic.types
import
Strict
StrictBool
Annotated
bool
Strict
()]
Source code in
pydantic/types.py
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
@_dataclasses
dataclass
class
Strict
_fields
PydanticMetadata
BaseMetadata
"""!!! abstract "Usage Documentation"
[Strict Mode with `Annotated` `Strict`](../concepts/strict_mode.md#strict-mode-with-annotated-strict)
A field metadata class to indicate that a field should be validated in strict mode.
Use this class as an annotation via [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated), as seen below.
Attributes:
strict: Whether to validate the field in strict mode.
Example:
```python
from typing import Annotated
from pydantic.types import Strict
StrictBool = Annotated[bool, Strict()]
```
"""
strict
bool
True
def
__hash__
self
int
return
hash
self
strict
AllowInfNan
dataclass
Bases:
PydanticMetadata
A field metadata class to indicate that a field should allow
-inf
inf
, and
nan
Use this class as an annotation via
Annotated
, as seen below.
Attributes:
Name
Type
Description
allow_inf_nan
bool
Whether to allow
-inf
inf
, and
nan
. Defaults to
True
Example
from
typing
import
Annotated
from
pydantic.types
import
AllowInfNan
LaxFloat
Annotated
float
AllowInfNan
()]
Source code in
pydantic/types.py
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
@_dataclasses
dataclass
class
AllowInfNan
_fields
PydanticMetadata
"""A field metadata class to indicate that a field should allow `-inf`, `inf`, and `nan`.
Use this class as an annotation via [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated), as seen below.
Attributes:
allow_inf_nan: Whether to allow `-inf`, `inf`, and `nan`. Defaults to `True`.
Example:
```python
from typing import Annotated
from pydantic.types import AllowInfNan
LaxFloat = Annotated[float, AllowInfNan()]
```
"""
allow_inf_nan
bool
True
def
__hash__
self
int
return
hash
self
allow_inf_nan
StringConstraints
dataclass
Bases:
GroupedMetadata
Usage Documentation
StringConstraints
A field metadata class to apply constraints to
str
types.
Use this class as an annotation via
Annotated
, as seen below.
Attributes:
Name
Type
Description
strip_whitespace
bool
| None
Whether to remove leading and trailing whitespace.
to_upper
bool
| None
Whether to convert the string to uppercase.
to_lower
bool
| None
Whether to convert the string to lowercase.
strict
bool
| None
Whether to validate the string in strict mode.
min_length
int
| None
The minimum length of the string.
max_length
int
| None
The maximum length of the string.
pattern
str
Pattern
str
] | None
A regex pattern that the string must match.
Example
from
typing
import
Annotated
from
pydantic.types
import
StringConstraints
ConstrainedStr
Annotated
str
StringConstraints
min_length
max_length
Source code in
pydantic/types.py
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
@_dataclasses
dataclass
frozen
True
class
StringConstraints
annotated_types
GroupedMetadata
"""!!! abstract "Usage Documentation"
[`StringConstraints`](../concepts/fields.md#string-constraints)
A field metadata class to apply constraints to `str` types.
Use this class as an annotation via [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated), as seen below.
Attributes:
strip_whitespace: Whether to remove leading and trailing whitespace.
to_upper: Whether to convert the string to uppercase.
to_lower: Whether to convert the string to lowercase.
strict: Whether to validate the string in strict mode.
min_length: The minimum length of the string.
max_length: The maximum length of the string.
pattern: A regex pattern that the string must match.
Example:
```python
from typing import Annotated
from pydantic.types import StringConstraints
ConstrainedStr = Annotated[str, StringConstraints(min_length=1, max_length=10)]
```
"""
strip_whitespace
bool
None
None
to_upper
bool
None
None
to_lower
bool
None
None
strict
bool
None
None
min_length
int
None
None
max_length
int
None
None
pattern
str
Pattern
str
None
None
def
__iter__
self
Iterator
BaseMetadata
self
min_length
not
None
yield
MinLen
self
min_length
self
max_length
not
None
yield
MaxLen
self
max_length
self
strict
not
None
yield
Strict
self
strict
self
strip_whitespace
not
None
self
pattern
not
None
self
to_lower
not
None
self
to_upper
not
None
yield
_fields
pydantic_general_metadata
strip_whitespace
self
strip_whitespace
to_upper
self
to_upper
to_lower
self
to_lower
pattern
self
pattern
ImportString
A type that can be used to import a Python object from a string.
ImportString
expects a string and loads the Python object importable at that dotted path.
Attributes of modules may be separated from the module by
, e.g. if
'math:cos'
is provided,
the resulting field value would be the function
cos
. If a
is used and both an attribute and submodule
are present at the same path, the module will be preferred.
On model instantiation, pointers will be evaluated and imported. There is
some nuance to this behavior, demonstrated in the examples below.
import
math
from
pydantic
import
BaseModel
Field
ImportString
ValidationError
class
ImportThings
BaseModel
obj
ImportString
# A string value will cause an automatic import
my_cos
ImportThings
obj
'math.cos'
# You can use the imported function as you would expect
cos_of_0
my_cos
obj
assert
cos_of_0
# A string whose value cannot be imported will raise an error
try
ImportThings
obj
'foo.bar'
except
ValidationError
print
'''
1 validation error for ImportThings
obj
Invalid python path: No module named 'foo.bar' [type=import_error, input_value='foo.bar', input_type=str]
'''
# Actual python objects can be assigned as well
my_cos
ImportThings
obj
math
cos
my_cos_2
ImportThings
obj
'math.cos'
my_cos_3
ImportThings
obj
'math:cos'
assert
my_cos
my_cos_2
my_cos_3
# You can set default field value either as Python object:
class
ImportThingsDefaultPyObj
BaseModel
obj
ImportString
math
cos
# or as a string value (but only if used with `validate_default=True`)
class
ImportThingsDefaultString
BaseModel
obj
ImportString
Field
default
'math.cos'
validate_default
True
my_cos_default1
ImportThingsDefaultPyObj
my_cos_default2
ImportThingsDefaultString
assert
my_cos_default1
obj
my_cos_default2
obj
math
cos
# note: this will not work!
class
ImportThingsMissingValidateDefault
BaseModel
obj
ImportString
'math.cos'
my_cos_default3
ImportThingsMissingValidateDefault
assert
my_cos_default3
obj
'math.cos'
# just string, not evaluated
Serializing an
ImportString
type to json is also possible.
from
pydantic
import
BaseModel
ImportString
class
ImportThings
BaseModel
obj
ImportString
# Create an instance
ImportThings
obj
'math.cos'
print
#> obj=<built-in function cos>
print
model_dump_json
())
#> {"obj":"math.cos"}
Source code in
pydantic/types.py
913
914
915
916
917
918
919
920
921
922
923
924
925
926
927
928
929
930
931
932
933
934
935
936
937
938
939
940
941
942
943
944
945
946
947
948
949
950
951
952
953
954
955
956
957
958
959
960
961
962
963
964
965
966
967
968
969
970
971
972
973
974
975
976
977
978
979
980
981
982
983
984
985
986
987
988
989
990
991
992
993
994
995
996
997
998
999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
1026
1027
1028
1029
1030
1031
1032
1033
1034
1035
class
ImportString
"""A type that can be used to import a Python object from a string.
`ImportString` expects a string and loads the Python object importable at that dotted path.
Attributes of modules may be separated from the module by `:` or `.`, e.g. if `'math:cos'` is provided,
the resulting field value would be the function `cos`. If a `.` is used and both an attribute and submodule
are present at the same path, the module will be preferred.
On model instantiation, pointers will be evaluated and imported. There is
some nuance to this behavior, demonstrated in the examples below.
```python
import math
from pydantic import BaseModel, Field, ImportString, ValidationError
class ImportThings(BaseModel):
obj: ImportString
# A string value will cause an automatic import
my_cos = ImportThings(obj='math.cos')
# You can use the imported function as you would expect
cos_of_0 = my_cos.obj(0)
assert cos_of_0 == 1
# A string whose value cannot be imported will raise an error
try:
ImportThings(obj='foo.bar')
except ValidationError as e:
print(e)
'''
1 validation error for ImportThings
obj
Invalid python path: No module named 'foo.bar' [type=import_error, input_value='foo.bar', input_type=str]
'''
# Actual python objects can be assigned as well
my_cos = ImportThings(obj=math.cos)
my_cos_2 = ImportThings(obj='math.cos')
my_cos_3 = ImportThings(obj='math:cos')
assert my_cos == my_cos_2 == my_cos_3
# You can set default field value either as Python object:
class ImportThingsDefaultPyObj(BaseModel):
obj: ImportString = math.cos
# or as a string value (but only if used with `validate_default=True`)
class ImportThingsDefaultString(BaseModel):
obj: ImportString = Field(default='math.cos', validate_default=True)
my_cos_default1 = ImportThingsDefaultPyObj()
my_cos_default2 = ImportThingsDefaultString()
assert my_cos_default1.obj == my_cos_default2.obj == math.cos
# note: this will not work!
class ImportThingsMissingValidateDefault(BaseModel):
obj: ImportString = 'math.cos'
my_cos_default3 = ImportThingsMissingValidateDefault()
assert my_cos_default3.obj == 'math.cos'  # just string, not evaluated
```
Serializing an `ImportString` type to json is also possible.
```python
from pydantic import BaseModel, ImportString
class ImportThings(BaseModel):
obj: ImportString
# Create an instance
m = ImportThings(obj='math.cos')
print(m)
#> obj=<built-in function cos>
print(m.model_dump_json())
#> {"obj":"math.cos"}
```
"""
@classmethod
def
__class_getitem__
cls
item
AnyType
AnyType
return
Annotated
item
cls
()]
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
serializer
core_schema
plain_serializer_function_ser_schema
cls
_serialize
when_used
'json'
cls
source
# Treat bare usage of ImportString (`schema is None`) as the same as ImportString[Any]
return
core_schema
no_info_plain_validator_function
function
_validators
import_string
serialization
serializer
else
return
core_schema
no_info_before_validator_function
function
_validators
import_string
schema
handler
source
serialization
serializer
@classmethod
def
__get_pydantic_json_schema__
cls
CoreSchema
handler
GetJsonSchemaHandler
JsonSchemaValue
return
handler
core_schema
str_schema
())
@staticmethod
def
_serialize
Any
str
isinstance
ModuleType
return
__name__
elif
hasattr
'__module__'
and
hasattr
'__name__'
return
__module__
__name__
# Handle special cases for sys.XXX streams
# if we see more of these, we should consider a more general solution
elif
hasattr
'name'
name
'<stdout>'
return
'sys.stdout'
elif
name
'<stdin>'
return
'sys.stdin'
elif
name
'<stderr>'
return
'sys.stderr'
else
return
def
__repr__
self
str
return
'ImportString'
UuidVersion
dataclass
A field metadata class to indicate a
UUID
version.
Use this class as an annotation via
Annotated
, as seen below.
Attributes:
Name
Type
Description
uuid_version
Literal
[1, 3, 4, 5, 6, 7, 8]
The version of the UUID. Must be one of 1, 3, 4, 5, or 7.
Example
from
typing
import
Annotated
from
uuid
import
UUID
from
pydantic.types
import
UuidVersion
UUID1
Annotated
UUID
UuidVersion
Source code in
pydantic/types.py
1138
1139
1140
1141
1142
1143
1144
1145
1146
1147
1148
1149
1150
1151
1152
1153
1154
1155
1156
1157
1158
1159
1160
1161
1162
1163
1164
1165
1166
1167
1168
1169
1170
1171
1172
1173
1174
1175
1176
1177
1178
1179
1180
@_dataclasses
dataclass
_internal_dataclass
slots_true
class
UuidVersion
"""A field metadata class to indicate a [UUID](https://docs.python.org/3/library/uuid.html) version.
Use this class as an annotation via [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated), as seen below.
Attributes:
uuid_version: The version of the UUID. Must be one of 1, 3, 4, 5, or 7.
Example:
```python
from typing import Annotated
from uuid import UUID
from pydantic.types import UuidVersion
UUID1 = Annotated[UUID, UuidVersion(1)]
```
"""
uuid_version
Literal
def
__get_pydantic_json_schema__
self
core_schema
core_schema
CoreSchema
handler
GetJsonSchemaHandler
JsonSchemaValue
field_schema
handler
core_schema
field_schema
pop
'anyOf'
None
# remove the bytes/str union
field_schema
update
type
'string'
format
'uuid
self
uuid_version
return
field_schema
def
__get_pydantic_core_schema__
self
source
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
isinstance
self
source
# used directly as a type
return
core_schema
uuid_schema
version
self
uuid_version
else
# update existing schema with self.uuid_version
schema
handler
source
_check_annotated_type
schema
'type'
'uuid'
self
__class__
__name__
schema
'version'
self
uuid_version
# type: ignore
return
schema
def
__hash__
self
int
return
hash
type
self
uuid_version
Json
A special type wrapper which loads JSON before parsing.
You can use the
Json
data type to make Pydantic first load a raw JSON string before
validating the loaded data into the parametrized type:
from
typing
import
Any
from
pydantic
import
BaseModel
Json
ValidationError
class
AnyJsonModel
BaseModel
json_obj
Json
Any
class
ConstrainedJsonModel
BaseModel
json_obj
Json
list
int
print
AnyJsonModel
json_obj
'{"b": 1}'
#> json_obj={'b': 1}
print
ConstrainedJsonModel
json_obj
'[1, 2, 3]'
#> json_obj=[1, 2, 3]
try
ConstrainedJsonModel
json_obj
except
ValidationError
print
'''
1 validation error for ConstrainedJsonModel
json_obj
JSON input should be string, bytes or bytearray [type=json_type, input_value=12, input_type=int]
'''
try
ConstrainedJsonModel
json_obj
'[a, b]'
except
ValidationError
print
'''
1 validation error for ConstrainedJsonModel
json_obj
Invalid JSON: expected value at line 1 column 2 [type=json_invalid, input_value='[a, b]', input_type=str]
'''
try
ConstrainedJsonModel
json_obj
'["a", "b"]'
except
ValidationError
print
'''
2 validation errors for ConstrainedJsonModel
json_obj.0
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]
json_obj.1
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='b', input_type=str]
'''
When you dump the model using
model_dump
model_dump_json
, the dumped value will be the result of validation,
not the original JSON string. However, you can use the argument
round_trip=True
to get the original JSON string back:
from
pydantic
import
BaseModel
Json
class
ConstrainedJsonModel
BaseModel
json_obj
Json
list
int
print
ConstrainedJsonModel
json_obj
'[1, 2, 3]'
model_dump_json
())
#> {"json_obj":[1,2,3]}
print
ConstrainedJsonModel
json_obj
'[1, 2, 3]'
model_dump_json
round_trip
True
#> {"json_obj":"[1,2,3]"}
Source code in
pydantic/types.py
1442
1443
1444
1445
1446
1447
1448
1449
1450
1451
1452
1453
1454
1455
1456
1457
1458
1459
1460
1461
1462
1463
1464
1465
1466
1467
1468
1469
1470
1471
1472
1473
1474
1475
1476
1477
1478
1479
1480
1481
1482
1483
1484
1485
1486
1487
1488
1489
1490
1491
1492
1493
1494
1495
1496
1497
1498
1499
1500
1501
1502
1503
1504
1505
1506
1507
1508
1509
1510
1511
1512
1513
1514
1515
1516
1517
1518
1519
1520
1521
1522
1523
1524
1525
1526
1527
1528
1529
1530
1531
1532
1533
class
Json
"""A special type wrapper which loads JSON before parsing.
You can use the `Json` data type to make Pydantic first load a raw JSON string before
validating the loaded data into the parametrized type:
```python
from typing import Any
from pydantic import BaseModel, Json, ValidationError
class AnyJsonModel(BaseModel):
json_obj: Json[Any]
class ConstrainedJsonModel(BaseModel):
json_obj: Json[list[int]]
print(AnyJsonModel(json_obj='{"b": 1}'))
#> json_obj={'b': 1}
print(ConstrainedJsonModel(json_obj='[1, 2, 3]'))
#> json_obj=[1, 2, 3]
try:
ConstrainedJsonModel(json_obj=12)
except ValidationError as e:
print(e)
'''
1 validation error for ConstrainedJsonModel
json_obj
JSON input should be string, bytes or bytearray [type=json_type, input_value=12, input_type=int]
'''
try:
ConstrainedJsonModel(json_obj='[a, b]')
except ValidationError as e:
print(e)
'''
1 validation error for ConstrainedJsonModel
json_obj
Invalid JSON: expected value at line 1 column 2 [type=json_invalid, input_value='[a, b]', input_type=str]
'''
try:
ConstrainedJsonModel(json_obj='["a", "b"]')
except ValidationError as e:
print(e)
'''
2 validation errors for ConstrainedJsonModel
json_obj.0
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]
json_obj.1
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='b', input_type=str]
'''
```
When you dump the model using `model_dump` or `model_dump_json`, the dumped value will be the result of validation,
not the original JSON string. However, you can use the argument `round_trip=True` to get the original JSON string back:
```python
from pydantic import BaseModel, Json
class ConstrainedJsonModel(BaseModel):
json_obj: Json[list[int]]
print(ConstrainedJsonModel(json_obj='[1, 2, 3]').model_dump_json())
#> {"json_obj":[1,2,3]}
print(
ConstrainedJsonModel(json_obj='[1, 2, 3]').model_dump_json(round_trip=True)
#> {"json_obj":"[1,2,3]"}
```
"""
@classmethod
def
__class_getitem__
cls
item
AnyType
AnyType
return
Annotated
item
cls
()]
@classmethod
def
__get_pydantic_core_schema__
cls
source
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
cls
source
return
core_schema
json_schema
None
else
return
core_schema
json_schema
handler
source
def
__repr__
self
str
return
'Json'
def
__hash__
self
int
return
hash
type
self
def
__eq__
self
other
Any
bool
return
type
other
type
self
Secret
Bases:
_SecretBase
SecretType
A generic base class used for defining a field with sensitive information that you do not want to be visible in logging or tracebacks.
You may either directly parametrize
Secret
with a type, or subclass from
Secret
with a parametrized type. The benefit of subclassing
is that you can define a custom
_display
method, which will be used for
repr()
and
str()
methods. The examples below demonstrate both
ways of using
Secret
to create a new secret type.
Directly parametrizing
Secret
with a type:
from
pydantic
import
BaseModel
Secret
SecretBool
Secret
bool
class
Model
BaseModel
secret_bool
SecretBool
Model
secret_bool
True
print
model_dump
())
#> {'secret_bool': Secret('**********')}
print
model_dump_json
())
#> {"secret_bool":"**********"}
print
secret_bool
get_secret_value
())
#> True
Subclassing from parametrized
Secret
from
datetime
import
date
from
pydantic
import
BaseModel
Secret
class
SecretDate
Secret
date
]):
def
_display
self
str
return
'****/**/**'
class
Model
BaseModel
secret_date
SecretDate
Model
secret_date
date
2022
print
model_dump
())
#> {'secret_date': SecretDate('****/**/**')}
print
model_dump_json
())
#> {"secret_date":"****/**/**"}
print
secret_date
get_secret_value
())
#> 2022-01-01
The value returned by the
_display
method will be used for
repr()
and
str()
You can enforce constraints on the underlying type through annotations:
For example:
from
typing
import
Annotated
from
pydantic
import
BaseModel
Field
Secret
ValidationError
SecretPosInt
Secret
Annotated
int
Field
strict
True
)]]
class
Model
BaseModel
sensitive_int
SecretPosInt
Model
sensitive_int
print
model_dump
())
#> {'sensitive_int': Secret('**********')}
try
Model
sensitive_int
except
ValidationError
exc_info
print
exc_info
errors
include_url
False
include_input
False
'''
'type': 'greater_than',
'loc': ('sensitive_int',),
'msg': 'Input should be greater than 0',
'ctx': {'gt': 0},
'''
try
Model
sensitive_int
'42'
except
ValidationError
exc_info
print
exc_info
errors
include_url
False
include_input
False
'''
'type': 'int_type',
'loc': ('sensitive_int',),
'msg': 'Input should be a valid integer',
'''
Source code in
pydantic/types.py
1576
1577
1578
1579
1580
1581
1582
1583
1584
1585
1586
1587
1588
1589
1590
1591
1592
1593
1594
1595
1596
1597
1598
1599
1600
1601
1602
1603
1604
1605
1606
1607
1608
1609
1610
1611
1612
1613
1614
1615
1616
1617
1618
1619
1620
1621
1622
1623
1624
1625
1626
1627
1628
1629
1630
1631
1632
1633
1634
1635
1636
1637
1638
1639
1640
1641
1642
1643
1644
1645
1646
1647
1648
1649
1650
1651
1652
1653
1654
1655
1656
1657
1658
1659
1660
1661
1662
1663
1664
1665
1666
1667
1668
1669
1670
1671
1672
1673
1674
1675
1676
1677
1678
1679
1680
1681
1682
1683
1684
1685
1686
1687
1688
1689
1690
1691
1692
1693
1694
1695
1696
1697
1698
1699
1700
1701
1702
1703
1704
1705
1706
1707
1708
1709
1710
1711
1712
1713
1714
1715
1716
1717
1718
1719
1720
1721
1722
1723
1724
1725
1726
1727
1728
1729
1730
1731
1732
1733
class
Secret
_SecretBase
SecretType
]):
"""A generic base class used for defining a field with sensitive information that you do not want to be visible in logging or tracebacks.
You may either directly parametrize `Secret` with a type, or subclass from `Secret` with a parametrized type. The benefit of subclassing
is that you can define a custom `_display` method, which will be used for `repr()` and `str()` methods. The examples below demonstrate both
ways of using `Secret` to create a new secret type.
1. Directly parametrizing `Secret` with a type:
```python
from pydantic import BaseModel, Secret
SecretBool = Secret[bool]
class Model(BaseModel):
secret_bool: SecretBool
m = Model(secret_bool=True)
print(m.model_dump())
#> {'secret_bool': Secret('**********')}
print(m.model_dump_json())
#> {"secret_bool":"**********"}
print(m.secret_bool.get_secret_value())
#> True
```
2. Subclassing from parametrized `Secret`:
```python
from datetime import date
from pydantic import BaseModel, Secret
class SecretDate(Secret[date]):
def _display(self) -> str:
return '****/**/**'
class Model(BaseModel):
secret_date: SecretDate
m = Model(secret_date=date(2022, 1, 1))
print(m.model_dump())
#> {'secret_date': SecretDate('****/**/**')}
print(m.model_dump_json())
#> {"secret_date":"****/**/**"}
print(m.secret_date.get_secret_value())
#> 2022-01-01
```
The value returned by the `_display` method will be used for `repr()` and `str()`.
You can enforce constraints on the underlying type through annotations:
For example:
```python
from typing import Annotated
from pydantic import BaseModel, Field, Secret, ValidationError
SecretPosInt = Secret[Annotated[int, Field(gt=0, strict=True)]]
class Model(BaseModel):
sensitive_int: SecretPosInt
m = Model(sensitive_int=42)
print(m.model_dump())
#> {'sensitive_int': Secret('**********')}
try:
m = Model(sensitive_int=-42)  # (1)!
except ValidationError as exc_info:
print(exc_info.errors(include_url=False, include_input=False))
'''
'type': 'greater_than',
'loc': ('sensitive_int',),
'msg': 'Input should be greater than 0',
'ctx': {'gt': 0},
'''
try:
m = Model(sensitive_int='42')  # (2)!
except ValidationError as exc_info:
print(exc_info.errors(include_url=False, include_input=False))
'''
'type': 'int_type',
'loc': ('sensitive_int',),
'msg': 'Input should be a valid integer',
'''
```
1. The input value is not greater than 0, so it raises a validation error.
2. The input value is not an integer, so it raises a validation error because the `SecretPosInt` type has strict mode enabled.
"""
def
_display
self
str
bytes
return
'**********'
self
get_secret_value
else
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
inner_type
None
# if origin_type is Secret, then cls is a GenericAlias, and we can extract the inner type directly
origin_type
get_origin
source
origin_type
not
None
inner_type
get_args
source
# otherwise, we need to get the inner type from the base class
else
bases
getattr
cls
'__orig_bases__'
getattr
cls
'__bases__'
[]))
for
base
bases
get_origin
base
Secret
inner_type
get_args
base
bases
inner_type
None
raise
TypeError
"Can't get secret type from
cls
__name__
. "
'Please use Secret[<type>], or subclass from Secret[<type>] instead.'
inner_schema
handler
generate_schema
inner_type
# type: ignore
def
validate_secret_value
value
handler
Secret
SecretType
isinstance
value
Secret
value
value
get_secret_value
validated_inner
handler
value
return
cls
validated_inner
return
core_schema
json_or_python_schema
python_schema
core_schema
no_info_wrap_validator_function
validate_secret_value
inner_schema
json_schema
core_schema
no_info_after_validator_function
lambda
cls
inner_schema
serialization
core_schema
plain_serializer_function_ser_schema
_serialize_secret
info_arg
True
when_used
'always'
__pydantic_serializer__
SchemaSerializer
core_schema
any_schema
serialization
core_schema
plain_serializer_function_ser_schema
_serialize_secret
info_arg
True
when_used
'always'
SecretStr
Bases:
_SecretField
str
A string used for storing sensitive information that you do not want to be visible in logging or tracebacks.
When the secret value is nonempty, it is displayed as
'**********'
instead of the underlying value in
calls to
repr()
and
str()
. If the value
empty, it is displayed as
from
pydantic
import
BaseModel
SecretStr
class
User
BaseModel
username
str
password
SecretStr
user
User
username
'scolvin'
password
'password1'
print
user
#> username='scolvin' password=SecretStr('**********')
print
user
password
get_secret_value
())
#> password1
print
SecretStr
'password'
SecretStr
)))
#> (SecretStr('**********'), SecretStr(''))
As seen above, by default,
SecretStr
(and
SecretBytes
will be serialized as
**********
when serializing to json.
You can use the
field_serializer
to dump the
secret as plain-text when serializing to json.
from
pydantic
import
BaseModel
SecretBytes
SecretStr
field_serializer
class
Model
BaseModel
password
SecretStr
password_bytes
SecretBytes
@field_serializer
'password'
'password_bytes'
when_used
'json'
def
dump_secret
self
return
get_secret_value
model
Model
password
'IAmSensitive'
password_bytes
'IAmSensitiveBytes'
print
model
#> password=SecretStr('**********') password_bytes=SecretBytes(b'**********')
print
model
password
#> **********
print
model
model_dump
())
'''
'password': SecretStr('**********'),
'password_bytes': SecretBytes(b'**********'),
'''
print
model
model_dump_json
())
#> {"password":"IAmSensitive","password_bytes":"IAmSensitiveBytes"}
Source code in
pydantic/types.py
1806
1807
1808
1809
1810
1811
1812
1813
1814
1815
1816
1817
1818
1819
1820
1821
1822
1823
1824
1825
1826
1827
1828
1829
1830
1831
1832
1833
1834
1835
1836
1837
1838
1839
1840
1841
1842
1843
1844
1845
1846
1847
1848
1849
1850
1851
1852
1853
1854
1855
1856
1857
1858
1859
1860
1861
1862
1863
1864
1865
1866
1867
1868
1869
1870
class
SecretStr
_SecretField
str
]):
"""A string used for storing sensitive information that you do not want to be visible in logging or tracebacks.
When the secret value is nonempty, it is displayed as `'**********'` instead of the underlying value in
calls to `repr()` and `str()`. If the value _is_ empty, it is displayed as `''`.
```python
from pydantic import BaseModel, SecretStr
class User(BaseModel):
username: str
password: SecretStr
user = User(username='scolvin', password='password1')
print(user)
#> username='scolvin' password=SecretStr('**********')
print(user.password.get_secret_value())
#> password1
print((SecretStr('password'), SecretStr('')))
#> (SecretStr('**********'), SecretStr(''))
```
As seen above, by default, [`SecretStr`][pydantic.types.SecretStr] (and [`SecretBytes`][pydantic.types.SecretBytes])
will be serialized as `**********` when serializing to json.
You can use the [`field_serializer`][pydantic.functional_serializers.field_serializer] to dump the
secret as plain-text when serializing to json.
```python
from pydantic import BaseModel, SecretBytes, SecretStr, field_serializer
class Model(BaseModel):
password: SecretStr
password_bytes: SecretBytes
@field_serializer('password', 'password_bytes', when_used='json')
def dump_secret(self, v):
return v.get_secret_value()
model = Model(password='IAmSensitive', password_bytes=b'IAmSensitiveBytes')
print(model)
#> password=SecretStr('**********') password_bytes=SecretBytes(b'**********')
print(model.password)
#> **********
print(model.model_dump())
'''
'password': SecretStr('**********'),
'password_bytes': SecretBytes(b'**********'),
'''
print(model.model_dump_json())
#> {"password":"IAmSensitive","password_bytes":"IAmSensitiveBytes"}
```
"""
_inner_schema
ClassVar
CoreSchema
core_schema
str_schema
_error_kind
ClassVar
str
'string_type'
def
__len__
self
int
return
len
self
_secret_value
def
_display
self
str
return
_secret_display
self
_secret_value
SecretBytes
Bases:
_SecretField
bytes
A bytes used for storing sensitive information that you do not want to be visible in logging or tracebacks.
It displays
b'**********'
instead of the string value on
repr()
and
str()
calls.
When the secret value is nonempty, it is displayed as
b'**********'
instead of the underlying value in
calls to
repr()
and
str()
. If the value
empty, it is displayed as
b''
from
pydantic
import
BaseModel
SecretBytes
class
User
BaseModel
username
str
password
SecretBytes
user
User
username
'scolvin'
password
'password1'
#> username='scolvin' password=SecretBytes(b'**********')
print
user
password
get_secret_value
())
#> b'password1'
print
SecretBytes
'password'
SecretBytes
)))
#> (SecretBytes(b'**********'), SecretBytes(b''))
Source code in
pydantic/types.py
1873
1874
1875
1876
1877
1878
1879
1880
1881
1882
1883
1884
1885
1886
1887
1888
1889
1890
1891
1892
1893
1894
1895
1896
1897
1898
1899
1900
1901
1902
1903
class
SecretBytes
_SecretField
bytes
]):
"""A bytes used for storing sensitive information that you do not want to be visible in logging or tracebacks.
It displays `b'**********'` instead of the string value on `repr()` and `str()` calls.
When the secret value is nonempty, it is displayed as `b'**********'` instead of the underlying value in
calls to `repr()` and `str()`. If the value _is_ empty, it is displayed as `b''`.
```python
from pydantic import BaseModel, SecretBytes
class User(BaseModel):
username: str
password: SecretBytes
user = User(username='scolvin', password=b'password1')
#> username='scolvin' password=SecretBytes(b'**********')
print(user.password.get_secret_value())
#> b'password1'
print((SecretBytes(b'password'), SecretBytes(b'')))
#> (SecretBytes(b'**********'), SecretBytes(b''))
```
"""
_inner_schema
ClassVar
CoreSchema
core_schema
bytes_schema
_error_kind
ClassVar
str
'bytes_type'
def
__len__
self
int
return
len
self
_secret_value
def
_display
self
bytes
return
_secret_display
self
_secret_value
encode
PaymentCardNumber
Bases:
str
Based on: https://en.wikipedia.org/wiki/Payment_card_number.
Source code in
pydantic/types.py
1919
1920
1921
1922
1923
1924
1925
1926
1927
1928
1929
1930
1931
1932
1933
1934
1935
1936
1937
1938
1939
1940
1941
1942
1943
1944
1945
1946
1947
1948
1949
1950
1951
1952
1953
1954
1955
1956
1957
1958
1959
1960
1961
1962
1963
1964
1965
1966
1967
1968
1969
1970
1971
1972
1973
1974
1975
1976
1977
1978
1979
1980
1981
1982
1983
1984
1985
1986
1987
1988
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
2014
2015
2016
2017
2018
2019
2020
2021
2022
2023
2024
@deprecated
'The `PaymentCardNumber` class is deprecated, use `pydantic_extra_types` instead. '
'See https://docs.pydantic.dev/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.'
category
PydanticDeprecatedSince20
class
PaymentCardNumber
str
"""Based on: https://en.wikipedia.org/wiki/Payment_card_number."""
strip_whitespace
ClassVar
bool
True
min_length
ClassVar
int
max_length
ClassVar
int
bin
str
last4
str
brand
PaymentCardBrand
def
__init__
self
card_number
str
self
validate_digits
card_number
card_number
self
validate_luhn_check_digit
card_number
self
bin
card_number
self
last4
card_number
self
brand
self
validate_brand
card_number
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
return
core_schema
with_info_after_validator_function
cls
validate
core_schema
str_schema
min_length
cls
min_length
max_length
cls
max_length
strip_whitespace
cls
strip_whitespace
@classmethod
def
validate
cls
input_value
str
core_schema
ValidationInfo
PaymentCardNumber
"""Validate the card number and return a `PaymentCardNumber` instance."""
return
cls
input_value
@property
def
masked
self
str
"""Mask all but the last 4 digits of the card number.
Returns:
A masked card number string.
"""
num_masked
len
self
# len(bin) + len(last4) == 10
return
self
bin
"*"
num_masked
self
last4
@classmethod
def
validate_digits
cls
card_number
str
None
"""Validate that the card number is all digits."""
not
card_number
isdigit
():
raise
PydanticCustomError
'payment_card_number_digits'
'Card number is not all digits'
@classmethod
def
validate_luhn_check_digit
cls
card_number
str
str
"""Based on: https://en.wikipedia.org/wiki/Luhn_algorithm."""
sum_
int
card_number
length
len
card_number
parity
length
for
range
length
digit
int
card_number
parity
digit
digit
digit
sum_
digit
valid
sum_
not
valid
raise
PydanticCustomError
'payment_card_number_luhn'
'Card number is not luhn valid'
return
card_number
@staticmethod
def
validate_brand
card_number
str
PaymentCardBrand
"""Validate length based on BIN for major brands:
https://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN).
"""
card_number
'4'
brand
PaymentCardBrand
visa
elif
int
card_number
brand
PaymentCardBrand
mastercard
elif
card_number
'34'
'37'
brand
PaymentCardBrand
amex
else
brand
PaymentCardBrand
other
required_length
None
int
str
None
brand
PaymentCardBrand
mastercard
required_length
valid
len
card_number
required_length
elif
brand
PaymentCardBrand
visa
required_length
'13, 16 or 19'
valid
len
card_number
elif
brand
PaymentCardBrand
amex
required_length
valid
len
card_number
required_length
else
valid
True
not
valid
raise
PydanticCustomError
'payment_card_number_brand'
'Length for a
{brand}
card must be
{required_length}
'brand'
brand
'required_length'
required_length
return
brand
masked
property
masked
str
Mask all but the last 4 digits of the card number.
Returns:
Type
Description
str
A masked card number string.
validate
classmethod
validate
input_value
str
ValidationInfo
PaymentCardNumber
Validate the card number and return a
PaymentCardNumber
instance.
Source code in
pydantic/types.py
1952
1953
1954
1955
@classmethod
def
validate
cls
input_value
str
core_schema
ValidationInfo
PaymentCardNumber
"""Validate the card number and return a `PaymentCardNumber` instance."""
return
cls
input_value
validate_digits
classmethod
validate_digits
card_number
str
None
Validate that the card number is all digits.
Source code in
pydantic/types.py
1967
1968
1969
1970
1971
@classmethod
def
validate_digits
cls
card_number
str
None
"""Validate that the card number is all digits."""
not
card_number
isdigit
():
raise
PydanticCustomError
'payment_card_number_digits'
'Card number is not all digits'
validate_luhn_check_digit
classmethod
validate_luhn_check_digit
card_number
str
str
Based on: https://en.wikipedia.org/wiki/Luhn_algorithm.
Source code in
pydantic/types.py
1973
1974
1975
1976
1977
1978
1979
1980
1981
1982
1983
1984
1985
1986
1987
1988
1989
@classmethod
def
validate_luhn_check_digit
cls
card_number
str
str
"""Based on: https://en.wikipedia.org/wiki/Luhn_algorithm."""
sum_
int
card_number
length
len
card_number
parity
length
for
range
length
digit
int
card_number
parity
digit
digit
digit
sum_
digit
valid
sum_
not
valid
raise
PydanticCustomError
'payment_card_number_luhn'
'Card number is not luhn valid'
return
card_number
validate_brand
staticmethod
validate_brand
card_number
str
PaymentCardBrand
Validate length based on BIN for major brands:
https://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN).
Source code in
pydantic/types.py
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
2014
2015
2016
2017
2018
2019
2020
2021
2022
2023
2024
@staticmethod
def
validate_brand
card_number
str
PaymentCardBrand
"""Validate length based on BIN for major brands:
https://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN).
"""
card_number
'4'
brand
PaymentCardBrand
visa
elif
int
card_number
brand
PaymentCardBrand
mastercard
elif
card_number
'34'
'37'
brand
PaymentCardBrand
amex
else
brand
PaymentCardBrand
other
required_length
None
int
str
None
brand
PaymentCardBrand
mastercard
required_length
valid
len
card_number
required_length
elif
brand
PaymentCardBrand
visa
required_length
'13, 16 or 19'
valid
len
card_number
elif
brand
PaymentCardBrand
amex
required_length
valid
len
card_number
required_length
else
valid
True
not
valid
raise
PydanticCustomError
'payment_card_number_brand'
'Length for a
{brand}
card must be
{required_length}
'brand'
brand
'required_length'
required_length
return
brand
ByteSize
Bases:
int
Converts a string representing a number of bytes with units (such as
'1KB'
'11.5MiB'
) into an integer.
You can use the
ByteSize
data type to (case-insensitively) convert a string representation of a number of bytes into
an integer, and also to print out human-readable strings representing a number of bytes.
In conformance with
IEC 80000-13 Standard
we interpret
'1KB'
to mean 1000 bytes,
and
'1KiB'
to mean 1024 bytes. In general, including a middle
'i'
will cause the unit to be interpreted as a power of 2,
rather than a power of 10 (so, for example,
'1 MB'
is treated as
1_000_000
bytes, whereas
'1 MiB'
is treated as
1_048_576
bytes).
Info
Note that
will be parsed as "1 byte" and not "1 bit".
from
pydantic
import
BaseModel
ByteSize
class
MyModel
BaseModel
size
ByteSize
print
MyModel
size
52000
size
#> 52000
print
MyModel
size
'3000 KiB'
size
#> 3072000
MyModel
size
'50 PB'
print
size
human_readable
())
#> 44.4PiB
print
size
human_readable
decimal
True
#> 50.0PB
print
size
human_readable
separator
' '
#> 44.4 PiB
print
size
'TiB'
#> 45474.73508864641
Source code in
pydantic/types.py
2030
2031
2032
2033
2034
2035
2036
2037
2038
2039
2040
2041
2042
2043
2044
2045
2046
2047
2048
2049
2050
2051
2052
2053
2054
2055
2056
2057
2058
2059
2060
2061
2062
2063
2064
2065
2066
2067
2068
2069
2070
2071
2072
2073
2074
2075
2076
2077
2078
2079
2080
2081
2082
2083
2084
2085
2086
2087
2088
2089
2090
2091
2092
2093
2094
2095
2096
2097
2098
2099
2100
2101
2102
2103
2104
2105
2106
2107
2108
2109
2110
2111
2112
2113
2114
2115
2116
2117
2118
2119
2120
2121
2122
2123
2124
2125
2126
2127
2128
2129
2130
2131
2132
2133
2134
2135
2136
2137
2138
2139
2140
2141
2142
2143
2144
2145
2146
2147
2148
2149
2150
2151
2152
2153
2154
2155
2156
2157
2158
2159
2160
2161
2162
2163
2164
2165
2166
2167
2168
2169
2170
2171
2172
2173
2174
2175
2176
2177
2178
2179
2180
2181
2182
2183
2184
2185
2186
2187
class
ByteSize
int
"""Converts a string representing a number of bytes with units (such as `'1KB'` or `'11.5MiB'`) into an integer.
You can use the `ByteSize` data type to (case-insensitively) convert a string representation of a number of bytes into
an integer, and also to print out human-readable strings representing a number of bytes.
In conformance with [IEC 80000-13 Standard](https://en.wikipedia.org/wiki/ISO/IEC_80000) we interpret `'1KB'` to mean 1000 bytes,
and `'1KiB'` to mean 1024 bytes. In general, including a middle `'i'` will cause the unit to be interpreted as a power of 2,
rather than a power of 10 (so, for example, `'1 MB'` is treated as `1_000_000` bytes, whereas `'1 MiB'` is treated as `1_048_576` bytes).
!!! info
Note that `1b` will be parsed as "1 byte" and not "1 bit".
```python
from pydantic import BaseModel, ByteSize
class MyModel(BaseModel):
size: ByteSize
print(MyModel(size=52000).size)
#> 52000
print(MyModel(size='3000 KiB').size)
#> 3072000
m = MyModel(size='50 PB')
print(m.size.human_readable())
#> 44.4PiB
print(m.size.human_readable(decimal=True))
#> 50.0PB
print(m.size.human_readable(separator=' '))
#> 44.4 PiB
print(m.size.to('TiB'))
#> 45474.73508864641
```
"""
byte_sizes
'b'
'kb'
'mb'
'gb'
'tb'
'pb'
'eb'
'kib'
'mib'
'gib'
'tib'
'pib'
'eib'
'bit'
'kbit'
'mbit'
'gbit'
'tbit'
'pbit'
'ebit'
'kibit'
'mibit'
'gibit'
'tibit'
'pibit'
'eibit'
byte_sizes
update
lower
()[
for
byte_sizes
items
'i'
not
byte_string_pattern
'^\s*(\d*\.?\d+)\s*(\w+)?'
byte_string_re
compile
byte_string_pattern
IGNORECASE
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
return
core_schema
with_info_after_validator_function
function
cls
_validate
schema
core_schema
union_schema
core_schema
str_schema
pattern
cls
byte_string_pattern
core_schema
int_schema
custom_error_type
'byte_size'
custom_error_message
'could not parse value and unit from byte string'
serialization
core_schema
plain_serializer_function_ser_schema
int
return_schema
core_schema
int_schema
@classmethod
def
_validate
cls
input_value
Any
core_schema
ValidationInfo
ByteSize
try
return
cls
int
input_value
except
ValueError
pass
str_match
cls
byte_string_re
match
str
input_value
str_match
None
raise
PydanticCustomError
'byte_size'
'could not parse value and unit from byte string'
scalar
unit
str_match
groups
unit
None
unit
'b'
try
unit_mult
cls
byte_sizes
unit
lower
()]
except
KeyError
raise
PydanticCustomError
'byte_size_unit'
'could not interpret byte unit:
{unit}
'unit'
unit
return
cls
int
float
scalar
unit_mult
def
human_readable
self
decimal
bool
False
separator
str
str
"""Converts a byte size to a human readable string.
Args:
decimal: If True, use decimal units (e.g. 1000 bytes per KB). If False, use binary units
(e.g. 1024 bytes per KiB).
separator: A string used to split the value and unit. Defaults to an empty string ('').
Returns:
A human readable string representation of the byte size.
"""
decimal
divisor
1000
units
'B'
'KB'
'MB'
'GB'
'TB'
'PB'
final_unit
'EB'
else
divisor
1024
units
'B'
'KiB'
'MiB'
'GiB'
'TiB'
'PiB'
final_unit
'EiB'
num
float
self
for
unit
units
abs
num
divisor
unit
'B'
return
num
0.0f
separator
unit
else
return
num
0.1f
separator
unit
num
divisor
return
num
0.1f
separator
final_unit
def
self
unit
str
float
"""Converts a byte size to another unit, including both byte and bit units.
Args:
unit: The unit to convert to. Must be one of the following: B, KB, MB, GB, TB, PB, EB,
KiB, MiB, GiB, TiB, PiB, EiB (byte units) and
bit, kbit, mbit, gbit, tbit, pbit, ebit,
kibit, mibit, gibit, tibit, pibit, eibit (bit units).
Returns:
The byte size in the new unit.
"""
try
unit_div
self
byte_sizes
unit
lower
()]
except
KeyError
raise
PydanticCustomError
'byte_size_unit'
'Could not interpret byte unit:
{unit}
'unit'
unit
return
self
unit_div
human_readable
human_readable
decimal
bool
False
separator
str
str
Converts a byte size to a human readable string.
Parameters:
Name
Type
Description
Default
decimal
bool
If True, use decimal units (e.g. 1000 bytes per KB). If False, use binary units
(e.g. 1024 bytes per KiB).
False
separator
str
A string used to split the value and unit. Defaults to an empty string ('').
Returns:
Type
Description
str
A human readable string representation of the byte size.
Source code in
pydantic/types.py
2139
2140
2141
2142
2143
2144
2145
2146
2147
2148
2149
2150
2151
2152
2153
2154
2155
2156
2157
2158
2159
2160
2161
2162
2163
2164
2165
2166
2167
2168
def
human_readable
self
decimal
bool
False
separator
str
str
"""Converts a byte size to a human readable string.
Args:
decimal: If True, use decimal units (e.g. 1000 bytes per KB). If False, use binary units
(e.g. 1024 bytes per KiB).
separator: A string used to split the value and unit. Defaults to an empty string ('').
Returns:
A human readable string representation of the byte size.
"""
decimal
divisor
1000
units
'B'
'KB'
'MB'
'GB'
'TB'
'PB'
final_unit
'EB'
else
divisor
1024
units
'B'
'KiB'
'MiB'
'GiB'
'TiB'
'PiB'
final_unit
'EiB'
num
float
self
for
unit
units
abs
num
divisor
unit
'B'
return
num
0.0f
separator
unit
else
return
num
0.1f
separator
unit
num
divisor
return
num
0.1f
separator
final_unit
unit
str
float
Converts a byte size to another unit, including both byte and bit units.
Parameters:
Name
Type
Description
Default
unit
str
The unit to convert to. Must be one of the following: B, KB, MB, GB, TB, PB, EB,
KiB, MiB, GiB, TiB, PiB, EiB (byte units) and
bit, kbit, mbit, gbit, tbit, pbit, ebit,
kibit, mibit, gibit, tibit, pibit, eibit (bit units).
required
Returns:
Type
Description
float
The byte size in the new unit.
Source code in
pydantic/types.py
2170
2171
2172
2173
2174
2175
2176
2177
2178
2179
2180
2181
2182
2183
2184
2185
2186
2187
def
self
unit
str
float
"""Converts a byte size to another unit, including both byte and bit units.
Args:
unit: The unit to convert to. Must be one of the following: B, KB, MB, GB, TB, PB, EB,
KiB, MiB, GiB, TiB, PiB, EiB (byte units) and
bit, kbit, mbit, gbit, tbit, pbit, ebit,
kibit, mibit, gibit, tibit, pibit, eibit (bit units).
Returns:
The byte size in the new unit.
"""
try
unit_div
self
byte_sizes
unit
lower
()]
except
KeyError
raise
PydanticCustomError
'byte_size_unit'
'Could not interpret byte unit:
{unit}
'unit'
unit
return
self
unit_div
PastDate
A date in the past.
Source code in
pydantic/types.py
2203
2204
2205
2206
2207
2208
2209
2210
2211
2212
2213
2214
2215
2216
2217
2218
2219
2220
class
PastDate
"""A date in the past."""
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
cls
source
# used directly as a type
return
core_schema
date_schema
now_op
'past'
else
schema
handler
source
_check_annotated_type
schema
'type'
'date'
cls
__name__
schema
'now_op'
'past'
return
schema
def
__repr__
self
str
return
'PastDate'
FutureDate
A date in the future.
Source code in
pydantic/types.py
2222
2223
2224
2225
2226
2227
2228
2229
2230
2231
2232
2233
2234
2235
2236
2237
2238
2239
class
FutureDate
"""A date in the future."""
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
cls
source
# used directly as a type
return
core_schema
date_schema
now_op
'future'
else
schema
handler
source
_check_annotated_type
schema
'type'
'date'
cls
__name__
schema
'now_op'
'future'
return
schema
def
__repr__
self
str
return
'FutureDate'
AwareDatetime
A datetime that requires timezone info.
Source code in
pydantic/types.py
2279
2280
2281
2282
2283
2284
2285
2286
2287
2288
2289
2290
2291
2292
2293
2294
2295
2296
class
AwareDatetime
"""A datetime that requires timezone info."""
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
cls
source
# used directly as a type
return
core_schema
datetime_schema
tz_constraint
'aware'
else
schema
handler
source
_check_annotated_type
schema
'type'
'datetime'
cls
__name__
schema
'tz_constraint'
'aware'
return
schema
def
__repr__
self
str
return
'AwareDatetime'
NaiveDatetime
A datetime that doesn't require timezone info.
Source code in
pydantic/types.py
2298
2299
2300
2301
2302
2303
2304
2305
2306
2307
2308
2309
2310
2311
2312
2313
2314
2315
class
NaiveDatetime
"""A datetime that doesn't require timezone info."""
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
cls
source
# used directly as a type
return
core_schema
datetime_schema
tz_constraint
'naive'
else
schema
handler
source
_check_annotated_type
schema
'type'
'datetime'
cls
__name__
schema
'tz_constraint'
'naive'
return
schema
def
__repr__
self
str
return
'NaiveDatetime'
PastDatetime
A datetime that must be in the past.
Source code in
pydantic/types.py
2317
2318
2319
2320
2321
2322
2323
2324
2325
2326
2327
2328
2329
2330
2331
2332
2333
2334
class
PastDatetime
"""A datetime that must be in the past."""
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
cls
source
# used directly as a type
return
core_schema
datetime_schema
now_op
'past'
else
schema
handler
source
_check_annotated_type
schema
'type'
'datetime'
cls
__name__
schema
'now_op'
'past'
return
schema
def
__repr__
self
str
return
'PastDatetime'
FutureDatetime
A datetime that must be in the future.
Source code in
pydantic/types.py
2336
2337
2338
2339
2340
2341
2342
2343
2344
2345
2346
2347
2348
2349
2350
2351
2352
2353
class
FutureDatetime
"""A datetime that must be in the future."""
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
cls
source
# used directly as a type
return
core_schema
datetime_schema
now_op
'future'
else
schema
handler
source
_check_annotated_type
schema
'type'
'datetime'
cls
__name__
schema
'now_op'
'future'
return
schema
def
__repr__
self
str
return
'FutureDatetime'
EncoderProtocol
Bases:
Protocol
Protocol for encoding and decoding data to and from bytes.
Source code in
pydantic/types.py
2359
2360
2361
2362
2363
2364
2365
2366
2367
2368
2369
2370
2371
2372
2373
2374
2375
2376
2377
2378
2379
2380
2381
2382
2383
2384
2385
2386
2387
2388
2389
2390
2391
2392
2393
class
EncoderProtocol
Protocol
"""Protocol for encoding and decoding data to and from bytes."""
@classmethod
def
decode
cls
data
bytes
bytes
"""Decode the data using the encoder.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
...
@classmethod
def
encode
cls
value
bytes
bytes
"""Encode the data using the encoder.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
...
@classmethod
def
get_json_format
cls
str
"""Get the JSON format for the encoded data.
Returns:
The JSON format for the encoded data.
"""
...
decode
classmethod
decode
data
bytes
bytes
Decode the data using the encoder.
Parameters:
Name
Type
Description
Default
data
bytes
The data to decode.
required
Returns:
Type
Description
bytes
The decoded data.
Source code in
pydantic/types.py
2362
2363
2364
2365
2366
2367
2368
2369
2370
2371
2372
@classmethod
def
decode
cls
data
bytes
bytes
"""Decode the data using the encoder.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
...
encode
classmethod
encode
value
bytes
bytes
Encode the data using the encoder.
Parameters:
Name
Type
Description
Default
value
bytes
The data to encode.
required
Returns:
Type
Description
bytes
The encoded data.
Source code in
pydantic/types.py
2374
2375
2376
2377
2378
2379
2380
2381
2382
2383
2384
@classmethod
def
encode
cls
value
bytes
bytes
"""Encode the data using the encoder.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
...
get_json_format
classmethod
get_json_format
str
Get the JSON format for the encoded data.
Returns:
Type
Description
str
The JSON format for the encoded data.
Source code in
pydantic/types.py
2386
2387
2388
2389
2390
2391
2392
2393
@classmethod
def
get_json_format
cls
str
"""Get the JSON format for the encoded data.
Returns:
The JSON format for the encoded data.
"""
...
Base64Encoder
Bases:
EncoderProtocol
Standard (non-URL-safe) Base64 encoder.
Source code in
pydantic/types.py
2396
2397
2398
2399
2400
2401
2402
2403
2404
2405
2406
2407
2408
2409
2410
2411
2412
2413
2414
2415
2416
2417
2418
2419
2420
2421
2422
2423
2424
2425
2426
2427
2428
2429
2430
2431
2432
2433
class
Base64Encoder
EncoderProtocol
"""Standard (non-URL-safe) Base64 encoder."""
@classmethod
def
decode
cls
data
bytes
bytes
"""Decode the data from base64 encoded bytes to original bytes data.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
try
return
base64
b64decode
data
except
ValueError
raise
PydanticCustomError
'base64_decode'
"Base64 decoding error: '
{error}
'error'
str
)})
@classmethod
def
encode
cls
value
bytes
bytes
"""Encode the data from bytes to a base64 encoded bytes.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
base64
b64encode
value
@classmethod
def
get_json_format
cls
Literal
'base64'
"""Get the JSON format for the encoded data.
Returns:
The JSON format for the encoded data.
"""
return
'base64'
decode
classmethod
decode
data
bytes
bytes
Decode the data from base64 encoded bytes to original bytes data.
Parameters:
Name
Type
Description
Default
data
bytes
The data to decode.
required
Returns:
Type
Description
bytes
The decoded data.
Source code in
pydantic/types.py
2399
2400
2401
2402
2403
2404
2405
2406
2407
2408
2409
2410
2411
2412
@classmethod
def
decode
cls
data
bytes
bytes
"""Decode the data from base64 encoded bytes to original bytes data.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
try
return
base64
b64decode
data
except
ValueError
raise
PydanticCustomError
'base64_decode'
"Base64 decoding error: '
{error}
'error'
str
)})
encode
classmethod
encode
value
bytes
bytes
Encode the data from bytes to a base64 encoded bytes.
Parameters:
Name
Type
Description
Default
value
bytes
The data to encode.
required
Returns:
Type
Description
bytes
The encoded data.
Source code in
pydantic/types.py
2414
2415
2416
2417
2418
2419
2420
2421
2422
2423
2424
@classmethod
def
encode
cls
value
bytes
bytes
"""Encode the data from bytes to a base64 encoded bytes.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
base64
b64encode
value
get_json_format
classmethod
get_json_format
Literal
'base64'
Get the JSON format for the encoded data.
Returns:
Type
Description
Literal
['base64']
The JSON format for the encoded data.
Source code in
pydantic/types.py
2426
2427
2428
2429
2430
2431
2432
2433
@classmethod
def
get_json_format
cls
Literal
'base64'
"""Get the JSON format for the encoded data.
Returns:
The JSON format for the encoded data.
"""
return
'base64'
Base64UrlEncoder
Bases:
EncoderProtocol
URL-safe Base64 encoder.
Source code in
pydantic/types.py
2436
2437
2438
2439
2440
2441
2442
2443
2444
2445
2446
2447
2448
2449
2450
2451
2452
2453
2454
2455
2456
2457
2458
2459
2460
2461
2462
2463
2464
2465
2466
2467
2468
2469
2470
2471
2472
2473
class
Base64UrlEncoder
EncoderProtocol
"""URL-safe Base64 encoder."""
@classmethod
def
decode
cls
data
bytes
bytes
"""Decode the data from base64 encoded bytes to original bytes data.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
try
return
base64
urlsafe_b64decode
data
except
ValueError
raise
PydanticCustomError
'base64_decode'
"Base64 decoding error: '
{error}
'error'
str
)})
@classmethod
def
encode
cls
value
bytes
bytes
"""Encode the data from bytes to a base64 encoded bytes.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
base64
urlsafe_b64encode
value
@classmethod
def
get_json_format
cls
Literal
'base64url'
"""Get the JSON format for the encoded data.
Returns:
The JSON format for the encoded data.
"""
return
'base64url'
decode
classmethod
decode
data
bytes
bytes
Decode the data from base64 encoded bytes to original bytes data.
Parameters:
Name
Type
Description
Default
data
bytes
The data to decode.
required
Returns:
Type
Description
bytes
The decoded data.
Source code in
pydantic/types.py
2439
2440
2441
2442
2443
2444
2445
2446
2447
2448
2449
2450
2451
2452
@classmethod
def
decode
cls
data
bytes
bytes
"""Decode the data from base64 encoded bytes to original bytes data.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
try
return
base64
urlsafe_b64decode
data
except
ValueError
raise
PydanticCustomError
'base64_decode'
"Base64 decoding error: '
{error}
'error'
str
)})
encode
classmethod
encode
value
bytes
bytes
Encode the data from bytes to a base64 encoded bytes.
Parameters:
Name
Type
Description
Default
value
bytes
The data to encode.
required
Returns:
Type
Description
bytes
The encoded data.
Source code in
pydantic/types.py
2454
2455
2456
2457
2458
2459
2460
2461
2462
2463
2464
@classmethod
def
encode
cls
value
bytes
bytes
"""Encode the data from bytes to a base64 encoded bytes.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
base64
urlsafe_b64encode
value
get_json_format
classmethod
get_json_format
Literal
'base64url'
Get the JSON format for the encoded data.
Returns:
Type
Description
Literal
['base64url']
The JSON format for the encoded data.
Source code in
pydantic/types.py
2466
2467
2468
2469
2470
2471
2472
2473
@classmethod
def
get_json_format
cls
Literal
'base64url'
"""Get the JSON format for the encoded data.
Returns:
The JSON format for the encoded data.
"""
return
'base64url'
EncodedBytes
dataclass
A bytes type that is encoded and decoded using the specified encoder.
EncodedBytes
needs an encoder that implements
EncoderProtocol
to operate.
from
typing
import
Annotated
from
pydantic
import
BaseModel
EncodedBytes
EncoderProtocol
ValidationError
class
MyEncoder
EncoderProtocol
@classmethod
def
decode
cls
data
bytes
bytes
data
'**undecodable**'
raise
ValueError
'Cannot decode data'
return
data
@classmethod
def
encode
cls
value
bytes
bytes
return
'**encoded**: '
value
@classmethod
def
get_json_format
cls
str
return
'my-encoder'
MyEncodedBytes
Annotated
bytes
EncodedBytes
encoder
MyEncoder
class
Model
BaseModel
my_encoded_bytes
MyEncodedBytes
# Initialize the model with encoded data
Model
my_encoded_bytes
'**encoded**: some bytes'
# Access decoded value
print
my_encoded_bytes
#> b'some bytes'
# Serialize into the encoded form
print
model_dump
())
#> {'my_encoded_bytes': b'**encoded**: some bytes'}
# Validate encoded data
try
Model
my_encoded_bytes
'**undecodable**'
except
ValidationError
print
'''
1 validation error for Model
my_encoded_bytes
Value error, Cannot decode data [type=value_error, input_value=b'**undecodable**', input_type=bytes]
'''
Source code in
pydantic/types.py
2476
2477
2478
2479
2480
2481
2482
2483
2484
2485
2486
2487
2488
2489
2490
2491
2492
2493
2494
2495
2496
2497
2498
2499
2500
2501
2502
2503
2504
2505
2506
2507
2508
2509
2510
2511
2512
2513
2514
2515
2516
2517
2518
2519
2520
2521
2522
2523
2524
2525
2526
2527
2528
2529
2530
2531
2532
2533
2534
2535
2536
2537
2538
2539
2540
2541
2542
2543
2544
2545
2546
2547
2548
2549
2550
2551
2552
2553
2554
2555
2556
2557
2558
2559
2560
2561
2562
2563
2564
2565
2566
2567
2568
2569
2570
2571
2572
@_dataclasses
dataclass
_internal_dataclass
slots_true
class
EncodedBytes
"""A bytes type that is encoded and decoded using the specified encoder.
`EncodedBytes` needs an encoder that implements `EncoderProtocol` to operate.
```python
from typing import Annotated
from pydantic import BaseModel, EncodedBytes, EncoderProtocol, ValidationError
class MyEncoder(EncoderProtocol):
@classmethod
def decode(cls, data: bytes) -> bytes:
if data == b'**undecodable**':
raise ValueError('Cannot decode data')
return data[13:]
@classmethod
def encode(cls, value: bytes) -> bytes:
return b'**encoded**: ' + value
@classmethod
def get_json_format(cls) -> str:
return 'my-encoder'
MyEncodedBytes = Annotated[bytes, EncodedBytes(encoder=MyEncoder)]
class Model(BaseModel):
my_encoded_bytes: MyEncodedBytes
# Initialize the model with encoded data
m = Model(my_encoded_bytes=b'**encoded**: some bytes')
# Access decoded value
print(m.my_encoded_bytes)
#> b'some bytes'
# Serialize into the encoded form
print(m.model_dump())
#> {'my_encoded_bytes': b'**encoded**: some bytes'}
# Validate encoded data
try:
Model(my_encoded_bytes=b'**undecodable**')
except ValidationError as e:
print(e)
'''
1 validation error for Model
my_encoded_bytes
Value error, Cannot decode data [type=value_error, input_value=b'**undecodable**', input_type=bytes]
'''
```
"""
encoder
type
EncoderProtocol
def
__get_pydantic_json_schema__
self
core_schema
core_schema
CoreSchema
handler
GetJsonSchemaHandler
JsonSchemaValue
field_schema
handler
core_schema
field_schema
update
type
'string'
format
self
encoder
get_json_format
())
return
field_schema
def
__get_pydantic_core_schema__
self
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
schema
handler
source
_check_annotated_type
schema
'type'
'bytes'
self
__class__
__name__
return
core_schema
with_info_after_validator_function
function
self
decode
schema
schema
serialization
core_schema
plain_serializer_function_ser_schema
function
self
encode
def
decode
self
data
bytes
core_schema
ValidationInfo
bytes
"""Decode the data using the specified encoder.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
return
self
encoder
decode
data
def
encode
self
value
bytes
bytes
"""Encode the data using the specified encoder.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
self
encoder
encode
value
def
__hash__
self
int
return
hash
self
encoder
decode
decode
data
bytes
ValidationInfo
bytes
Decode the data using the specified encoder.
Parameters:
Name
Type
Description
Default
data
bytes
The data to decode.
required
Returns:
Type
Description
bytes
The decoded data.
Source code in
pydantic/types.py
2549
2550
2551
2552
2553
2554
2555
2556
2557
2558
def
decode
self
data
bytes
core_schema
ValidationInfo
bytes
"""Decode the data using the specified encoder.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
return
self
encoder
decode
data
encode
encode
value
bytes
bytes
Encode the data using the specified encoder.
Parameters:
Name
Type
Description
Default
value
bytes
The data to encode.
required
Returns:
Type
Description
bytes
The encoded data.
Source code in
pydantic/types.py
2560
2561
2562
2563
2564
2565
2566
2567
2568
2569
def
encode
self
value
bytes
bytes
"""Encode the data using the specified encoder.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
self
encoder
encode
value
EncodedStr
dataclass
A str type that is encoded and decoded using the specified encoder.
EncodedStr
needs an encoder that implements
EncoderProtocol
to operate.
from
typing
import
Annotated
from
pydantic
import
BaseModel
EncodedStr
EncoderProtocol
ValidationError
class
MyEncoder
EncoderProtocol
@classmethod
def
decode
cls
data
bytes
bytes
data
'**undecodable**'
raise
ValueError
'Cannot decode data'
return
data
@classmethod
def
encode
cls
value
bytes
bytes
return
'**encoded**: '
value
@classmethod
def
get_json_format
cls
str
return
'my-encoder'
MyEncodedStr
Annotated
str
EncodedStr
encoder
MyEncoder
class
Model
BaseModel
my_encoded_str
MyEncodedStr
# Initialize the model with encoded data
Model
my_encoded_str
'**encoded**: some str'
# Access decoded value
print
my_encoded_str
#> some str
# Serialize into the encoded form
print
model_dump
())
#> {'my_encoded_str': '**encoded**: some str'}
# Validate encoded data
try
Model
my_encoded_str
'**undecodable**'
except
ValidationError
print
'''
1 validation error for Model
my_encoded_str
Value error, Cannot decode data [type=value_error, input_value='**undecodable**', input_type=str]
'''
Source code in
pydantic/types.py
2575
2576
2577
2578
2579
2580
2581
2582
2583
2584
2585
2586
2587
2588
2589
2590
2591
2592
2593
2594
2595
2596
2597
2598
2599
2600
2601
2602
2603
2604
2605
2606
2607
2608
2609
2610
2611
2612
2613
2614
2615
2616
2617
2618
2619
2620
2621
2622
2623
2624
2625
2626
2627
2628
2629
2630
2631
2632
2633
2634
2635
2636
2637
2638
2639
2640
2641
2642
2643
2644
2645
2646
2647
2648
2649
2650
2651
2652
2653
2654
2655
2656
2657
2658
2659
2660
2661
2662
2663
2664
2665
2666
2667
2668
2669
2670
2671
@_dataclasses
dataclass
_internal_dataclass
slots_true
class
EncodedStr
"""A str type that is encoded and decoded using the specified encoder.
`EncodedStr` needs an encoder that implements `EncoderProtocol` to operate.
```python
from typing import Annotated
from pydantic import BaseModel, EncodedStr, EncoderProtocol, ValidationError
class MyEncoder(EncoderProtocol):
@classmethod
def decode(cls, data: bytes) -> bytes:
if data == b'**undecodable**':
raise ValueError('Cannot decode data')
return data[13:]
@classmethod
def encode(cls, value: bytes) -> bytes:
return b'**encoded**: ' + value
@classmethod
def get_json_format(cls) -> str:
return 'my-encoder'
MyEncodedStr = Annotated[str, EncodedStr(encoder=MyEncoder)]
class Model(BaseModel):
my_encoded_str: MyEncodedStr
# Initialize the model with encoded data
m = Model(my_encoded_str='**encoded**: some str')
# Access decoded value
print(m.my_encoded_str)
#> some str
# Serialize into the encoded form
print(m.model_dump())
#> {'my_encoded_str': '**encoded**: some str'}
# Validate encoded data
try:
Model(my_encoded_str='**undecodable**')
except ValidationError as e:
print(e)
'''
1 validation error for Model
my_encoded_str
Value error, Cannot decode data [type=value_error, input_value='**undecodable**', input_type=str]
'''
```
"""
encoder
type
EncoderProtocol
def
__get_pydantic_json_schema__
self
core_schema
core_schema
CoreSchema
handler
GetJsonSchemaHandler
JsonSchemaValue
field_schema
handler
core_schema
field_schema
update
type
'string'
format
self
encoder
get_json_format
())
return
field_schema
def
__get_pydantic_core_schema__
self
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
schema
handler
source
_check_annotated_type
schema
'type'
'str'
self
__class__
__name__
return
core_schema
with_info_after_validator_function
function
self
decode_str
schema
schema
serialization
core_schema
plain_serializer_function_ser_schema
function
self
encode_str
def
decode_str
self
data
str
core_schema
ValidationInfo
str
"""Decode the data using the specified encoder.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
return
self
encoder
decode
data
encode
())
decode
def
encode_str
self
value
str
str
"""Encode the data using the specified encoder.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
self
encoder
encode
value
encode
())
decode
# noqa: UP008
def
__hash__
self
int
return
hash
self
encoder
decode_str
decode_str
data
str
ValidationInfo
str
Decode the data using the specified encoder.
Parameters:
Name
Type
Description
Default
data
str
The data to decode.
required
Returns:
Type
Description
str
The decoded data.
Source code in
pydantic/types.py
2648
2649
2650
2651
2652
2653
2654
2655
2656
2657
def
decode_str
self
data
str
core_schema
ValidationInfo
str
"""Decode the data using the specified encoder.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
return
self
encoder
decode
data
encode
())
decode
encode_str
encode_str
value
str
str
Encode the data using the specified encoder.
Parameters:
Name
Type
Description
Default
value
str
The data to encode.
required
Returns:
Type
Description
str
The encoded data.
Source code in
pydantic/types.py
2659
2660
2661
2662
2663
2664
2665
2666
2667
2668
def
encode_str
self
value
str
str
"""Encode the data using the specified encoder.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
self
encoder
encode
value
encode
())
decode
# noqa: UP008
GetPydanticSchema
dataclass
Usage Documentation
Using
GetPydanticSchema
to Reduce Boilerplate
A convenience class for creating an annotation that provides pydantic custom type hooks.
This class is intended to eliminate the need to create a custom "marker" which defines the
__get_pydantic_core_schema__
and
__get_pydantic_json_schema__
custom hook methods.
For example, to have a field treated by type checkers as
int
, but by pydantic as
Any
, you can do:
from
typing
import
Annotated
Any
from
pydantic
import
BaseModel
GetPydanticSchema
HandleAsAny
GetPydanticSchema
lambda
Any
class
Model
BaseModel
Annotated
int
HandleAsAny
# pydantic sees `x: Any`
print
repr
Model
'abc'
#> 'abc'
Source code in
pydantic/types.py
2847
2848
2849
2850
2851
2852
2853
2854
2855
2856
2857
2858
2859
2860
2861
2862
2863
2864
2865
2866
2867
2868
2869
2870
2871
2872
2873
2874
2875
2876
2877
2878
2879
2880
2881
2882
2883
2884
2885
2886
2887
2888
2889
2890
2891
@_dataclasses
dataclass
_internal_dataclass
slots_true
class
GetPydanticSchema
"""!!! abstract "Usage Documentation"
[Using `GetPydanticSchema` to Reduce Boilerplate](../concepts/types.md#using-getpydanticschema-to-reduce-boilerplate)
A convenience class for creating an annotation that provides pydantic custom type hooks.
This class is intended to eliminate the need to create a custom "marker" which defines the
`__get_pydantic_core_schema__` and `__get_pydantic_json_schema__` custom hook methods.
For example, to have a field treated by type checkers as `int`, but by pydantic as `Any`, you can do:
```python
from typing import Annotated, Any
from pydantic import BaseModel, GetPydanticSchema
HandleAsAny = GetPydanticSchema(lambda _s, h: h(Any))
class Model(BaseModel):
x: Annotated[int, HandleAsAny]  # pydantic sees `x: Any`
print(repr(Model(x='abc').x))
#> 'abc'
```
"""
get_pydantic_core_schema
Callable
Any
GetCoreSchemaHandler
CoreSchema
None
None
get_pydantic_json_schema
Callable
Any
GetJsonSchemaHandler
JsonSchemaValue
None
None
# Note: we may want to consider adding a convenience staticmethod `def for_type(type_: Any) -> GetPydanticSchema:`
# which returns `GetPydanticSchema(lambda _s, h: h(type_))`
not
TYPE_CHECKING
# We put `__getattr__` in a non-TYPE_CHECKING block because otherwise, mypy allows arbitrary attribute access
def
__getattr__
self
item
str
Any
"""Use this rather than defining `__get_pydantic_core_schema__` etc. to reduce the number of nested calls."""
item
'__get_pydantic_core_schema__'
and
self
get_pydantic_core_schema
return
self
get_pydantic_core_schema
elif
item
'__get_pydantic_json_schema__'
and
self
get_pydantic_json_schema
return
self
get_pydantic_json_schema
else
return
object
__getattribute__
self
item
__hash__
object
__hash__
Tag
dataclass
Provides a way to specify the expected tag to use for a case of a (callable) discriminated union.
Also provides a way to label a union case in error messages.
When using a callable
Discriminator
, attach a
Tag
to each case in the
Union
to specify the tag that
should be used to identify that case. For example, in the below example, the
Tag
is used to specify that
get_discriminator_value
returns
'apple'
, the input should be validated as an
ApplePie
, and if it
returns
'pumpkin'
, the input should be validated as a
PumpkinPie
The primary role of the
Tag
here is to map the return value from the callable
Discriminator
function to
the appropriate member of the
Union
in question.
from
typing
import
Annotated
Any
Literal
Union
from
pydantic
import
BaseModel
Discriminator
Tag
class
Pie
BaseModel
time_to_cook
int
num_ingredients
int
class
ApplePie
Pie
fruit
Literal
'apple'
'apple'
class
PumpkinPie
Pie
filling
Literal
'pumpkin'
'pumpkin'
def
get_discriminator_value
Any
str
isinstance
dict
return
get
'fruit'
get
'filling'
return
getattr
'fruit'
getattr
'filling'
None
class
ThanksgivingDinner
BaseModel
dessert
Annotated
Union
Annotated
ApplePie
Tag
'apple'
)],
Annotated
PumpkinPie
Tag
'pumpkin'
)],
Discriminator
get_discriminator_value
apple_variation
ThanksgivingDinner
model_validate
'dessert'
'fruit'
'apple'
'time_to_cook'
'num_ingredients'
print
repr
apple_variation
'''
ThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))
'''
pumpkin_variation
ThanksgivingDinner
model_validate
'dessert'
'filling'
'pumpkin'
'time_to_cook'
'num_ingredients'
print
repr
pumpkin_variation
'''
ThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))
'''
Note
You must specify a
Tag
for every case in a
Tag
that is associated with a
callable
Discriminator
. Failing to do so will result in a
PydanticUserError
with code
callable-discriminator-no-tag
See the
Discriminated Unions
concepts docs for more details on how to use
Tag
Source code in
pydantic/types.py
2894
2895
2896
2897
2898
2899
2900
2901
2902
2903
2904
2905
2906
2907
2908
2909
2910
2911
2912
2913
2914
2915
2916
2917
2918
2919
2920
2921
2922
2923
2924
2925
2926
2927
2928
2929
2930
2931
2932
2933
2934
2935
2936
2937
2938
2939
2940
2941
2942
2943
2944
2945
2946
2947
2948
2949
2950
2951
2952
2953
2954
2955
2956
2957
2958
2959
2960
2961
2962
2963
2964
2965
2966
2967
2968
2969
2970
2971
2972
2973
2974
2975
2976
@_dataclasses
dataclass
_internal_dataclass
slots_true
frozen
True
class
Tag
"""Provides a way to specify the expected tag to use for a case of a (callable) discriminated union.
Also provides a way to label a union case in error messages.
When using a callable `Discriminator`, attach a `Tag` to each case in the `Union` to specify the tag that
should be used to identify that case. For example, in the below example, the `Tag` is used to specify that
if `get_discriminator_value` returns `'apple'`, the input should be validated as an `ApplePie`, and if it
returns `'pumpkin'`, the input should be validated as a `PumpkinPie`.
The primary role of the `Tag` here is to map the return value from the callable `Discriminator` function to
the appropriate member of the `Union` in question.
```python
from typing import Annotated, Any, Literal, Union
from pydantic import BaseModel, Discriminator, Tag
class Pie(BaseModel):
time_to_cook: int
num_ingredients: int
class ApplePie(Pie):
fruit: Literal['apple'] = 'apple'
class PumpkinPie(Pie):
filling: Literal['pumpkin'] = 'pumpkin'
def get_discriminator_value(v: Any) -> str:
if isinstance(v, dict):
return v.get('fruit', v.get('filling'))
return getattr(v, 'fruit', getattr(v, 'filling', None))
class ThanksgivingDinner(BaseModel):
dessert: Annotated[
Union[
Annotated[ApplePie, Tag('apple')],
Annotated[PumpkinPie, Tag('pumpkin')],
Discriminator(get_discriminator_value),
apple_variation = ThanksgivingDinner.model_validate(
{'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}
print(repr(apple_variation))
'''
ThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))
'''
pumpkin_variation = ThanksgivingDinner.model_validate(
'dessert': {
'filling': 'pumpkin',
'time_to_cook': 40,
'num_ingredients': 6,
print(repr(pumpkin_variation))
'''
ThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))
'''
```
!!! note
You must specify a `Tag` for every case in a `Tag` that is associated with a
callable `Discriminator`. Failing to do so will result in a `PydanticUserError` with code
[`callable-discriminator-no-tag`](../errors/usage_errors.md#callable-discriminator-no-tag).
See the [Discriminated Unions] concepts docs for more details on how to use `Tag`s.
[Discriminated Unions]: ../concepts/unions.md#discriminated-unions
"""
tag
str
def
__get_pydantic_core_schema__
self
source_type
Any
handler
GetCoreSchemaHandler
CoreSchema
schema
handler
source_type
metadata
cast
'CoreMetadata'
schema
setdefault
'metadata'
{}))
metadata
'pydantic_internal_union_tag_key'
self
tag
return
schema
Discriminator
dataclass
Usage Documentation
Discriminated Unions with
Callable
Discriminator
Provides a way to use a custom callable as the way to extract the value of a union discriminator.
This allows you to get validation behavior like you'd get from
Field(discriminator=<field_name>)
but without needing to have a single shared field across all the union choices. This also makes it
possible to handle unions of models and primitive types with discriminated-union-style validation errors.
Finally, this allows you to use a custom callable as the way to identify which member of a union a value
belongs to, while still seeing all the performance benefits of a discriminated union.
Consider this example, which is much more performant with the use of
Discriminator
and thus a
TaggedUnion
than it would be as a normal
Union
from
typing
import
Annotated
Any
Literal
Union
from
pydantic
import
BaseModel
Discriminator
Tag
class
Pie
BaseModel
time_to_cook
int
num_ingredients
int
class
ApplePie
Pie
fruit
Literal
'apple'
'apple'
class
PumpkinPie
Pie
filling
Literal
'pumpkin'
'pumpkin'
def
get_discriminator_value
Any
str
isinstance
dict
return
get
'fruit'
get
'filling'
return
getattr
'fruit'
getattr
'filling'
None
class
ThanksgivingDinner
BaseModel
dessert
Annotated
Union
Annotated
ApplePie
Tag
'apple'
)],
Annotated
PumpkinPie
Tag
'pumpkin'
)],
Discriminator
get_discriminator_value
apple_variation
ThanksgivingDinner
model_validate
'dessert'
'fruit'
'apple'
'time_to_cook'
'num_ingredients'
print
repr
apple_variation
'''
ThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))
'''
pumpkin_variation
ThanksgivingDinner
model_validate
'dessert'
'filling'
'pumpkin'
'time_to_cook'
'num_ingredients'
print
repr
pumpkin_variation
'''
ThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))
'''
See the
Discriminated Unions
concepts docs for more details on how to use
Discriminator
Source code in
pydantic/types.py
2979
2980
2981
2982
2983
2984
2985
2986
2987
2988
2989
2990
2991
2992
2993
2994
2995
2996
2997
2998
2999
3000
3001
3002
3003
3004
3005
3006
3007
3008
3009
3010
3011
3012
3013
3014
3015
3016
3017
3018
3019
3020
3021
3022
3023
3024
3025
3026
3027
3028
3029
3030
3031
3032
3033
3034
3035
3036
3037
3038
3039
3040
3041
3042
3043
3044
3045
3046
3047
3048
3049
3050
3051
3052
3053
3054
3055
3056
3057
3058
3059
3060
3061
3062
3063
3064
3065
3066
3067
3068
3069
3070
3071
3072
3073
3074
3075
3076
3077
3078
3079
3080
3081
3082
3083
3084
3085
3086
3087
3088
3089
3090
3091
3092
3093
3094
3095
3096
3097
3098
3099
3100
3101
3102
3103
3104
3105
3106
3107
3108
3109
3110
3111
3112
3113
3114
3115
3116
3117
3118
3119
3120
3121
3122
3123
3124
3125
3126
@_dataclasses
dataclass
_internal_dataclass
slots_true
frozen
True
class
Discriminator
"""!!! abstract "Usage Documentation"
[Discriminated Unions with `Callable` `Discriminator`](../concepts/unions.md#discriminated-unions-with-callable-discriminator)
Provides a way to use a custom callable as the way to extract the value of a union discriminator.
This allows you to get validation behavior like you'd get from `Field(discriminator=<field_name>)`,
but without needing to have a single shared field across all the union choices. This also makes it
possible to handle unions of models and primitive types with discriminated-union-style validation errors.
Finally, this allows you to use a custom callable as the way to identify which member of a union a value
belongs to, while still seeing all the performance benefits of a discriminated union.
Consider this example, which is much more performant with the use of `Discriminator` and thus a `TaggedUnion`
than it would be as a normal `Union`.
```python
from typing import Annotated, Any, Literal, Union
from pydantic import BaseModel, Discriminator, Tag
class Pie(BaseModel):
time_to_cook: int
num_ingredients: int
class ApplePie(Pie):
fruit: Literal['apple'] = 'apple'
class PumpkinPie(Pie):
filling: Literal['pumpkin'] = 'pumpkin'
def get_discriminator_value(v: Any) -> str:
if isinstance(v, dict):
return v.get('fruit', v.get('filling'))
return getattr(v, 'fruit', getattr(v, 'filling', None))
class ThanksgivingDinner(BaseModel):
dessert: Annotated[
Union[
Annotated[ApplePie, Tag('apple')],
Annotated[PumpkinPie, Tag('pumpkin')],
Discriminator(get_discriminator_value),
apple_variation = ThanksgivingDinner.model_validate(
{'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}
print(repr(apple_variation))
'''
ThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))
'''
pumpkin_variation = ThanksgivingDinner.model_validate(
'dessert': {
'filling': 'pumpkin',
'time_to_cook': 40,
'num_ingredients': 6,
print(repr(pumpkin_variation))
'''
ThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))
'''
```
See the [Discriminated Unions] concepts docs for more details on how to use `Discriminator`s.
[Discriminated Unions]: ../concepts/unions.md#discriminated-unions
"""
discriminator
str
Callable
Any
Hashable
"""The callable or field name for discriminating the type in a tagged union.
A `Callable` discriminator must extract the value of the discriminator from the input.
A `str` discriminator must be the name of a field to discriminate against.
"""
custom_error_type
str
None
None
"""Type to use in [custom errors](../errors/errors.md) replacing the standard discriminated union
validation errors.
"""
custom_error_message
str
None
None
"""Message to use in custom errors."""
custom_error_context
dict
str
int
str
float
None
None
"""Context to use in custom errors."""
def
__get_pydantic_core_schema__
self
source_type
Any
handler
GetCoreSchemaHandler
CoreSchema
not
is_union_origin
get_origin
source_type
)):
raise
TypeError
type
self
__name__
must be used with a Union type, not
source_type
isinstance
self
discriminator
str
from
pydantic
import
Field
return
handler
Annotated
source_type
Field
discriminator
self
discriminator
)])
else
original_schema
handler
source_type
return
self
_convert_schema
original_schema
def
_convert_schema
self
original_schema
core_schema
CoreSchema
core_schema
TaggedUnionSchema
original_schema
'type'
'union'
# This likely indicates that the schema was a single-item union that was simplified.
# In this case, we do the same thing we do in
# `pydantic._internal._discriminated_union._ApplyInferredDiscriminator._apply_to_root`, namely,
# package the generated schema back into a single-item union.
original_schema
core_schema
union_schema
original_schema
tagged_union_choices
for
choice
original_schema
'choices'
tag
None
isinstance
choice
tuple
choice
tag
choice
metadata
cast
'CoreMetadata | None'
choice
get
'metadata'
metadata
not
None
tag
metadata
get
'pydantic_internal_union_tag_key'
tag
tag
None
raise
PydanticUserError
'`Tag` not provided for choice
choice
used with `Discriminator`'
code
'callable-discriminator-no-tag'
tagged_union_choices
tag
choice
# Have to do these verbose checks to ensure falsy values ('' and {}) don't get ignored
custom_error_type
self
custom_error_type
custom_error_type
None
custom_error_type
original_schema
get
'custom_error_type'
custom_error_message
self
custom_error_message
custom_error_message
None
custom_error_message
original_schema
get
'custom_error_message'
custom_error_context
self
custom_error_context
custom_error_context
None
custom_error_context
original_schema
get
'custom_error_context'
custom_error_type
original_schema
get
'custom_error_type'
custom_error_type
None
else
custom_error_type
return
core_schema
tagged_union_schema
tagged_union_choices
self
discriminator
custom_error_type
custom_error_type
custom_error_message
custom_error_message
custom_error_context
custom_error_context
strict
original_schema
get
'strict'
ref
original_schema
get
'ref'
metadata
original_schema
get
'metadata'
serialization
original_schema
get
'serialization'
discriminator
instance-attribute
discriminator
str
Callable
Any
Hashable
The callable or field name for discriminating the type in a tagged union.
Callable
discriminator must extract the value of the discriminator from the input.
str
discriminator must be the name of a field to discriminate against.
custom_error_type
class-attribute
instance-attribute
custom_error_type
str
None
None
Type to use in
custom errors
replacing the standard discriminated union
validation errors.
custom_error_message
class-attribute
instance-attribute
custom_error_message
str
None
None
Message to use in custom errors.
custom_error_context
class-attribute
instance-attribute
custom_error_context
dict
str
int
str
float
None
None
Context to use in custom errors.
FailFast
dataclass
Bases:
PydanticMetadata
BaseMetadata
FailFast
annotation can be used to specify that validation should stop at the first error.
This can be useful when you want to validate a large amount of data and you only need to know if it's valid or not.
You might want to enable this setting if you want to validate your data faster (basically, if you use this,
validation will be more performant with the caveat that you get less information).
from
typing
import
Annotated
from
pydantic
import
BaseModel
FailFast
ValidationError
class
Model
BaseModel
Annotated
list
int
FailFast
()]
# This will raise a single error for the first invalid value and stop validation
try
obj
Model
'a'
'b'
'c'
except
ValidationError
print
'''
1 validation error for Model
x.2
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]
'''
Source code in
pydantic/types.py
3255
3256
3257
3258
3259
3260
3261
3262
3263
3264
3265
3266
3267
3268
3269
3270
3271
3272
3273
3274
3275
3276
3277
3278
3279
3280
3281
3282
3283
3284
3285
@_dataclasses
dataclass
class
FailFast
_fields
PydanticMetadata
BaseMetadata
"""A `FailFast` annotation can be used to specify that validation should stop at the first error.
This can be useful when you want to validate a large amount of data and you only need to know if it's valid or not.
You might want to enable this setting if you want to validate your data faster (basically, if you use this,
validation will be more performant with the caveat that you get less information).
```python
from typing import Annotated
from pydantic import BaseModel, FailFast, ValidationError
class Model(BaseModel):
x: Annotated[list[int], FailFast()]
# This will raise a single error for the first invalid value and stop validation
try:
obj = Model(x=[1, 2, 'a', 4, 5, 'b', 7, 8, 9, 'c'])
except ValidationError as e:
print(e)
'''
1 validation error for Model
x.2
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]
'''
```
"""
fail_fast
bool
True
conint
conint
strict
bool
None
None
int
None
None
int
None
None
int
None
None
int
None
None
multiple_of
int
None
None
type
int
Discouraged
This function is
discouraged
in favor of using
Annotated
with
Field
instead.
This function will be
deprecated
in Pydantic 3.0.
The reason is that
conint
returns a type, which doesn't play well with static analysis tools.
Don't do this
Do this
from
pydantic
import
BaseModel
conint
class
Foo
BaseModel
bar
conint
strict
True
from
typing
import
Annotated
from
pydantic
import
BaseModel
Field
class
Foo
BaseModel
bar
Annotated
int
Field
strict
True
A wrapper around
int
that allows for additional constraints.
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether to validate the integer in strict mode. Defaults to
None
None
int
| None
The value must be greater than this.
None
int
| None
The value must be greater than or equal to this.
None
int
| None
The value must be less than this.
None
int
| None
The value must be less than or equal to this.
None
multiple_of
int
| None
The value must be a multiple of this.
None
Returns:
Type
Description
type
int
The wrapped integer type.
from
pydantic
import
BaseModel
ValidationError
conint
class
ConstrainedExample
BaseModel
constrained_int
conint
ConstrainedExample
constrained_int
print
repr
#> ConstrainedExample(constrained_int=2)
try
ConstrainedExample
constrained_int
except
ValidationError
print
errors
())
'''
'type': 'greater_than',
'loc': ('constrained_int',),
'msg': 'Input should be greater than 1',
'input': 0,
'ctx': {'gt': 1},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
Source code in
pydantic/types.py
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
def
conint
strict
bool
None
None
int
None
None
int
None
None
int
None
None
int
None
None
multiple_of
int
None
None
type
int
"""
!!! warning "Discouraged"
This function is **discouraged** in favor of using
[`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with
[`Field`][pydantic.fields.Field] instead.
This function will be **deprecated** in Pydantic 3.0.
The reason is that `conint` returns a type, which doesn't play well with static analysis tools.
=== ":x: Don't do this"
```python
from pydantic import BaseModel, conint
class Foo(BaseModel):
bar: conint(strict=True, gt=0)
```
=== ":white_check_mark: Do this"
```python
from typing import Annotated
from pydantic import BaseModel, Field
class Foo(BaseModel):
bar: Annotated[int, Field(strict=True, gt=0)]
```
A wrapper around `int` that allows for additional constraints.
Args:
strict: Whether to validate the integer in strict mode. Defaults to `None`.
gt: The value must be greater than this.
ge: The value must be greater than or equal to this.
lt: The value must be less than this.
le: The value must be less than or equal to this.
multiple_of: The value must be a multiple of this.
Returns:
The wrapped integer type.
```python
from pydantic import BaseModel, ValidationError, conint
class ConstrainedExample(BaseModel):
constrained_int: conint(gt=1)
m = ConstrainedExample(constrained_int=2)
print(repr(m))
#> ConstrainedExample(constrained_int=2)
try:
ConstrainedExample(constrained_int=0)
except ValidationError as e:
print(e.errors())
'''
'type': 'greater_than',
'loc': ('constrained_int',),
'msg': 'Input should be greater than 1',
'input': 0,
'ctx': {'gt': 1},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
```
"""
# noqa: D212
return
Annotated
# pyright: ignore[reportReturnType]
int
Strict
strict
strict
not
None
else
None
annotated_types
Interval
annotated_types
MultipleOf
multiple_of
multiple_of
not
None
else
None
confloat
confloat
strict
bool
None
None
float
None
None
float
None
None
float
None
None
float
None
None
multiple_of
float
None
None
allow_inf_nan
bool
None
None
type
float
Discouraged
This function is
discouraged
in favor of using
Annotated
with
Field
instead.
This function will be
deprecated
in Pydantic 3.0.
The reason is that
confloat
returns a type, which doesn't play well with static analysis tools.
Don't do this
Do this
from
pydantic
import
BaseModel
confloat
class
Foo
BaseModel
bar
confloat
strict
True
from
typing
import
Annotated
from
pydantic
import
BaseModel
Field
class
Foo
BaseModel
bar
Annotated
float
Field
strict
True
A wrapper around
float
that allows for additional constraints.
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether to validate the float in strict mode.
None
float
| None
The value must be greater than this.
None
float
| None
The value must be greater than or equal to this.
None
float
| None
The value must be less than this.
None
float
| None
The value must be less than or equal to this.
None
multiple_of
float
| None
The value must be a multiple of this.
None
allow_inf_nan
bool
| None
Whether to allow
-inf
inf
, and
nan
None
Returns:
Type
Description
type
float
The wrapped float type.
from
pydantic
import
BaseModel
ValidationError
confloat
class
ConstrainedExample
BaseModel
constrained_float
confloat
1.0
ConstrainedExample
constrained_float
1.1
print
repr
#> ConstrainedExample(constrained_float=1.1)
try
ConstrainedExample
constrained_float
0.9
except
ValidationError
print
errors
())
'''
'type': 'greater_than',
'loc': ('constrained_float',),
'msg': 'Input should be greater than 1',
'input': 0.9,
'ctx': {'gt': 1.0},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
Source code in
pydantic/types.py
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
def
confloat
strict
bool
None
None
float
None
None
float
None
None
float
None
None
float
None
None
multiple_of
float
None
None
allow_inf_nan
bool
None
None
type
float
"""
!!! warning "Discouraged"
This function is **discouraged** in favor of using
[`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with
[`Field`][pydantic.fields.Field] instead.
This function will be **deprecated** in Pydantic 3.0.
The reason is that `confloat` returns a type, which doesn't play well with static analysis tools.
=== ":x: Don't do this"
```python
from pydantic import BaseModel, confloat
class Foo(BaseModel):
bar: confloat(strict=True, gt=0)
```
=== ":white_check_mark: Do this"
```python
from typing import Annotated
from pydantic import BaseModel, Field
class Foo(BaseModel):
bar: Annotated[float, Field(strict=True, gt=0)]
```
A wrapper around `float` that allows for additional constraints.
Args:
strict: Whether to validate the float in strict mode.
gt: The value must be greater than this.
ge: The value must be greater than or equal to this.
lt: The value must be less than this.
le: The value must be less than or equal to this.
multiple_of: The value must be a multiple of this.
allow_inf_nan: Whether to allow `-inf`, `inf`, and `nan`.
Returns:
The wrapped float type.
```python
from pydantic import BaseModel, ValidationError, confloat
class ConstrainedExample(BaseModel):
constrained_float: confloat(gt=1.0)
m = ConstrainedExample(constrained_float=1.1)
print(repr(m))
#> ConstrainedExample(constrained_float=1.1)
try:
ConstrainedExample(constrained_float=0.9)
except ValidationError as e:
print(e.errors())
'''
'type': 'greater_than',
'loc': ('constrained_float',),
'msg': 'Input should be greater than 1',
'input': 0.9,
'ctx': {'gt': 1.0},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
```
"""
# noqa: D212
return
Annotated
# pyright: ignore[reportReturnType]
float
Strict
strict
strict
not
None
else
None
annotated_types
Interval
annotated_types
MultipleOf
multiple_of
multiple_of
not
None
else
None
AllowInfNan
allow_inf_nan
allow_inf_nan
not
None
else
None
conbytes
conbytes
min_length
int
None
None
max_length
int
None
None
strict
bool
None
None
type
bytes
A wrapper around
bytes
that allows for additional constraints.
Parameters:
Name
Type
Description
Default
min_length
int
| None
The minimum length of the bytes.
None
max_length
int
| None
The maximum length of the bytes.
None
strict
bool
| None
Whether to validate the bytes in strict mode.
None
Returns:
Type
Description
type
bytes
The wrapped bytes type.
Source code in
pydantic/types.py
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
def
conbytes
min_length
int
None
None
max_length
int
None
None
strict
bool
None
None
type
bytes
"""A wrapper around `bytes` that allows for additional constraints.
Args:
min_length: The minimum length of the bytes.
max_length: The maximum length of the bytes.
strict: Whether to validate the bytes in strict mode.
Returns:
The wrapped bytes type.
"""
return
Annotated
# pyright: ignore[reportReturnType]
bytes
Strict
strict
strict
not
None
else
None
annotated_types
Len
min_length
max_length
constr
constr
strip_whitespace
bool
None
None
to_upper
bool
None
None
to_lower
bool
None
None
strict
bool
None
None
min_length
int
None
None
max_length
int
None
None
pattern
str
Pattern
str
None
None
type
str
Discouraged
This function is
discouraged
in favor of using
Annotated
with
StringConstraints
instead.
This function will be
deprecated
in Pydantic 3.0.
The reason is that
constr
returns a type, which doesn't play well with static analysis tools.
Don't do this
Do this
from
pydantic
import
BaseModel
constr
class
Foo
BaseModel
bar
constr
strip_whitespace
True
to_upper
True
pattern
'^[A-Z]+$'
from
typing
import
Annotated
from
pydantic
import
BaseModel
StringConstraints
class
Foo
BaseModel
bar
Annotated
str
StringConstraints
strip_whitespace
True
to_upper
True
pattern
'^[A-Z]+$'
A wrapper around
str
that allows for additional constraints.
from
pydantic
import
BaseModel
constr
class
Foo
BaseModel
bar
constr
strip_whitespace
True
to_upper
True
foo
Foo
bar
' hello '
print
foo
#> bar='HELLO'
Parameters:
Name
Type
Description
Default
strip_whitespace
bool
| None
Whether to remove leading and trailing whitespace.
None
to_upper
bool
| None
Whether to turn all characters to uppercase.
None
to_lower
bool
| None
Whether to turn all characters to lowercase.
None
strict
bool
| None
Whether to validate the string in strict mode.
None
min_length
int
| None
The minimum length of the string.
None
max_length
int
| None
The maximum length of the string.
None
pattern
str
Pattern
str
] | None
A regex pattern to validate the string against.
None
Returns:
Type
Description
type
str
The wrapped string type.
Source code in
pydantic/types.py
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
def
constr
strip_whitespace
bool
None
None
to_upper
bool
None
None
to_lower
bool
None
None
strict
bool
None
None
min_length
int
None
None
max_length
int
None
None
pattern
str
Pattern
str
None
None
type
str
"""
!!! warning "Discouraged"
This function is **discouraged** in favor of using
[`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with
[`StringConstraints`][pydantic.types.StringConstraints] instead.
This function will be **deprecated** in Pydantic 3.0.
The reason is that `constr` returns a type, which doesn't play well with static analysis tools.
=== ":x: Don't do this"
```python
from pydantic import BaseModel, constr
class Foo(BaseModel):
bar: constr(strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$')
```
=== ":white_check_mark: Do this"
```python
from typing import Annotated
from pydantic import BaseModel, StringConstraints
class Foo(BaseModel):
bar: Annotated[
str,
StringConstraints(
strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$'
```
A wrapper around `str` that allows for additional constraints.
```python
from pydantic import BaseModel, constr
class Foo(BaseModel):
bar: constr(strip_whitespace=True, to_upper=True)
foo = Foo(bar='  hello  ')
print(foo)
#> bar='HELLO'
```
Args:
strip_whitespace: Whether to remove leading and trailing whitespace.
to_upper: Whether to turn all characters to uppercase.
to_lower: Whether to turn all characters to lowercase.
strict: Whether to validate the string in strict mode.
min_length: The minimum length of the string.
max_length: The maximum length of the string.
pattern: A regex pattern to validate the string against.
Returns:
The wrapped string type.
"""
# noqa: D212
return
Annotated
# pyright: ignore[reportReturnType]
str
StringConstraints
strip_whitespace
strip_whitespace
to_upper
to_upper
to_lower
to_lower
strict
strict
min_length
min_length
max_length
max_length
pattern
pattern
conset
conset
item_type
type
HashableItemType
min_length
int
None
None
max_length
int
None
None
type
set
HashableItemType
A wrapper around
typing.Set
that allows for additional constraints.
Parameters:
Name
Type
Description
Default
item_type
type
HashableItemType
The type of the items in the set.
required
min_length
int
| None
The minimum length of the set.
None
max_length
int
| None
The maximum length of the set.
None
Returns:
Type
Description
type
set
HashableItemType
The wrapped set type.
Source code in
pydantic/types.py
839
840
841
842
843
844
845
846
847
848
849
850
851
852
def
conset
item_type
type
HashableItemType
min_length
int
None
None
max_length
int
None
None
type
set
HashableItemType
]]:
"""A wrapper around `typing.Set` that allows for additional constraints.
Args:
item_type: The type of the items in the set.
min_length: The minimum length of the set.
max_length: The maximum length of the set.
Returns:
The wrapped set type.
"""
return
Annotated
set
item_type
annotated_types
Len
min_length
max_length
# pyright: ignore[reportReturnType]
confrozenset
confrozenset
item_type
type
HashableItemType
min_length
int
None
None
max_length
int
None
None
type
frozenset
HashableItemType
A wrapper around
typing.FrozenSet
that allows for additional constraints.
Parameters:
Name
Type
Description
Default
item_type
type
HashableItemType
The type of the items in the frozenset.
required
min_length
int
| None
The minimum length of the frozenset.
None
max_length
int
| None
The maximum length of the frozenset.
None
Returns:
Type
Description
type
frozenset
HashableItemType
The wrapped frozenset type.
Source code in
pydantic/types.py
855
856
857
858
859
860
861
862
863
864
865
866
867
868
def
confrozenset
item_type
type
HashableItemType
min_length
int
None
None
max_length
int
None
None
type
frozenset
HashableItemType
]]:
"""A wrapper around `typing.FrozenSet` that allows for additional constraints.
Args:
item_type: The type of the items in the frozenset.
min_length: The minimum length of the frozenset.
max_length: The maximum length of the frozenset.
Returns:
The wrapped frozenset type.
"""
return
Annotated
frozenset
item_type
annotated_types
Len
min_length
max_length
# pyright: ignore[reportReturnType]
conlist
conlist
item_type
type
AnyItemType
min_length
int
None
None
max_length
int
None
None
unique_items
bool
None
None
type
list
AnyItemType
A wrapper around
list
that adds validation.
Parameters:
Name
Type
Description
Default
item_type
type
AnyItemType
The type of the items in the list.
required
min_length
int
| None
The minimum length of the list. Defaults to None.
None
max_length
int
| None
The maximum length of the list. Defaults to None.
None
unique_items
bool
| None
Whether the items in the list must be unique. Defaults to None.
Warning
The
unique_items
parameter is deprecated, use
Set
instead.
See
this issue
for more details.
None
Returns:
Type
Description
type
list
AnyItemType
The wrapped list type.
Source code in
pydantic/types.py
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899
900
901
902
903
def
conlist
item_type
type
AnyItemType
min_length
int
None
None
max_length
int
None
None
unique_items
bool
None
None
type
list
AnyItemType
]]:
"""A wrapper around [`list`][] that adds validation.
Args:
item_type: The type of the items in the list.
min_length: The minimum length of the list. Defaults to None.
max_length: The maximum length of the list. Defaults to None.
unique_items: Whether the items in the list must be unique. Defaults to None.
!!! warning Deprecated
The `unique_items` parameter is deprecated, use `Set` instead.
See [this issue](https://github.com/pydantic/pydantic-core/issues/296) for more details.
Returns:
The wrapped list type.
"""
unique_items
not
None
raise
PydanticUserError
'`unique_items` is removed, use `Set` instead'
'(this feature is discussed in https://github.com/pydantic/pydantic-core/issues/296)'
code
'removed-kwargs'
return
Annotated
list
item_type
annotated_types
Len
min_length
max_length
# pyright: ignore[reportReturnType]
condecimal
condecimal
strict
bool
None
None
int
Decimal
None
None
int
Decimal
None
None
int
Decimal
None
None
int
Decimal
None
None
multiple_of
int
Decimal
None
None
max_digits
int
None
None
decimal_places
int
None
None
allow_inf_nan
bool
None
None
type
Decimal
Discouraged
This function is
discouraged
in favor of using
Annotated
with
Field
instead.
This function will be
deprecated
in Pydantic 3.0.
The reason is that
condecimal
returns a type, which doesn't play well with static analysis tools.
Don't do this
Do this
from
pydantic
import
BaseModel
condecimal
class
Foo
BaseModel
bar
condecimal
strict
True
allow_inf_nan
True
from
decimal
import
Decimal
from
typing
import
Annotated
from
pydantic
import
BaseModel
Field
class
Foo
BaseModel
bar
Annotated
Decimal
Field
strict
True
allow_inf_nan
True
A wrapper around Decimal that adds validation.
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether to validate the value in strict mode. Defaults to
None
None
int
Decimal
| None
The value must be greater than this. Defaults to
None
None
int
Decimal
| None
The value must be greater than or equal to this. Defaults to
None
None
int
Decimal
| None
The value must be less than this. Defaults to
None
None
int
Decimal
| None
The value must be less than or equal to this. Defaults to
None
None
multiple_of
int
Decimal
| None
The value must be a multiple of this. Defaults to
None
None
max_digits
int
| None
The maximum number of digits. Defaults to
None
None
decimal_places
int
| None
The number of decimal places. Defaults to
None
None
allow_inf_nan
bool
| None
Whether to allow infinity and NaN. Defaults to
None
None
from
decimal
import
Decimal
from
pydantic
import
BaseModel
ValidationError
condecimal
class
ConstrainedExample
BaseModel
constrained_decimal
condecimal
Decimal
'1.0'
ConstrainedExample
constrained_decimal
Decimal
'1.1'
print
repr
#> ConstrainedExample(constrained_decimal=Decimal('1.1'))
try
ConstrainedExample
constrained_decimal
Decimal
'0.9'
except
ValidationError
print
errors
())
'''
'type': 'greater_than',
'loc': ('constrained_decimal',),
'msg': 'Input should be greater than 1.0',
'input': Decimal('0.9'),
'ctx': {'gt': Decimal('1.0')},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
Source code in
pydantic/types.py
1041
1042
1043
1044
1045
1046
1047
1048
1049
1050
1051
1052
1053
1054
1055
1056
1057
1058
1059
1060
1061
1062
1063
1064
1065
1066
1067
1068
1069
1070
1071
1072
1073
1074
1075
1076
1077
1078
1079
1080
1081
1082
1083
1084
1085
1086
1087
1088
1089
1090
1091
1092
1093
1094
1095
1096
1097
1098
1099
1100
1101
1102
1103
1104
1105
1106
1107
1108
1109
1110
1111
1112
1113
1114
1115
1116
1117
1118
1119
1120
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
def
condecimal
strict
bool
None
None
int
Decimal
None
None
int
Decimal
None
None
int
Decimal
None
None
int
Decimal
None
None
multiple_of
int
Decimal
None
None
max_digits
int
None
None
decimal_places
int
None
None
allow_inf_nan
bool
None
None
type
Decimal
"""
!!! warning "Discouraged"
This function is **discouraged** in favor of using
[`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with
[`Field`][pydantic.fields.Field] instead.
This function will be **deprecated** in Pydantic 3.0.
The reason is that `condecimal` returns a type, which doesn't play well with static analysis tools.
=== ":x: Don't do this"
```python
from pydantic import BaseModel, condecimal
class Foo(BaseModel):
bar: condecimal(strict=True, allow_inf_nan=True)
```
=== ":white_check_mark: Do this"
```python
from decimal import Decimal
from typing import Annotated
from pydantic import BaseModel, Field
class Foo(BaseModel):
bar: Annotated[Decimal, Field(strict=True, allow_inf_nan=True)]
```
A wrapper around Decimal that adds validation.
Args:
strict: Whether to validate the value in strict mode. Defaults to `None`.
gt: The value must be greater than this. Defaults to `None`.
ge: The value must be greater than or equal to this. Defaults to `None`.
lt: The value must be less than this. Defaults to `None`.
le: The value must be less than or equal to this. Defaults to `None`.
multiple_of: The value must be a multiple of this. Defaults to `None`.
max_digits: The maximum number of digits. Defaults to `None`.
decimal_places: The number of decimal places. Defaults to `None`.
allow_inf_nan: Whether to allow infinity and NaN. Defaults to `None`.
```python
from decimal import Decimal
from pydantic import BaseModel, ValidationError, condecimal
class ConstrainedExample(BaseModel):
constrained_decimal: condecimal(gt=Decimal('1.0'))
m = ConstrainedExample(constrained_decimal=Decimal('1.1'))
print(repr(m))
#> ConstrainedExample(constrained_decimal=Decimal('1.1'))
try:
ConstrainedExample(constrained_decimal=Decimal('0.9'))
except ValidationError as e:
print(e.errors())
'''
'type': 'greater_than',
'loc': ('constrained_decimal',),
'msg': 'Input should be greater than 1.0',
'input': Decimal('0.9'),
'ctx': {'gt': Decimal('1.0')},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
```
"""
# noqa: D212
return
Annotated
# pyright: ignore[reportReturnType]
Decimal
Strict
strict
strict
not
None
else
None
annotated_types
Interval
annotated_types
MultipleOf
multiple_of
multiple_of
not
None
else
None
_fields
pydantic_general_metadata
max_digits
max_digits
decimal_places
decimal_places
AllowInfNan
allow_inf_nan
allow_inf_nan
not
None
else
None
condate
condate
strict
bool
None
None
date
None
None
date
None
None
date
None
None
date
None
None
type
date
A wrapper for date that adds constraints.
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether to validate the date value in strict mode. Defaults to
None
None
date
| None
The value must be greater than this. Defaults to
None
None
date
| None
The value must be greater than or equal to this. Defaults to
None
None
date
| None
The value must be less than this. Defaults to
None
None
date
| None
The value must be less than or equal to this. Defaults to
None
None
Returns:
Type
Description
type
date
A date type with the specified constraints.
Source code in
pydantic/types.py
2242
2243
2244
2245
2246
2247
2248
2249
2250
2251
2252
2253
2254
2255
2256
2257
2258
2259
2260
2261
2262
2263
2264
2265
2266
def
condate
strict
bool
None
None
date
None
None
date
None
None
date
None
None
date
None
None
type
date
"""A wrapper for date that adds constraints.
Args:
strict: Whether to validate the date value in strict mode. Defaults to `None`.
gt: The value must be greater than this. Defaults to `None`.
ge: The value must be greater than or equal to this. Defaults to `None`.
lt: The value must be less than this. Defaults to `None`.
le: The value must be less than or equal to this. Defaults to `None`.
Returns:
A date type with the specified constraints.
"""
return
Annotated
# pyright: ignore[reportReturnType]
date
Strict
strict
strict
not
None
else
None
annotated_types
Interval
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 013_pydantic_extra_types_color.txt ---
Color
Color definitions are used as per the CSS3
CSS Color Module Level 3
specification.
A few colors have multiple names referring to the sames colors, eg.
grey
and
gray
aqua
and
cyan
In these cases the
last
color when sorted alphabetically takes preferences,
eg.
Color((0, 255, 255)).as_named() == 'cyan'
because "cyan" comes after "aqua".
RGBA
RGBA
float
float
float
alpha
float
None
Internal use only as a representation of a color.
Source code in
pydantic_extra_types/color.py
def
__init__
self
float
float
float
alpha
float
None
self
self
self
self
alpha
alpha
self
_tuple
tuple
float
float
float
float
None
alpha
Color
Color
value
ColorType
Bases:
Representation
Represents a color.
Source code in
pydantic_extra_types/color.py
def
__init__
self
value
ColorType
None
self
_rgba
RGBA
self
_original
ColorType
isinstance
value
tuple
list
)):
self
_rgba
parse_tuple
value
elif
isinstance
value
str
self
_rgba
parse_str
value
elif
isinstance
value
Color
self
_rgba
value
_rgba
value
value
_original
else
raise
PydanticCustomError
'color_error'
'value is not a valid color: value must be a tuple, list or string'
# if we've got here value must be a valid color
self
_original
value
original
original
ColorType
Original value passed to
Color
Source code in
pydantic_extra_types/color.py
103
104
105
106
107
def
original
self
ColorType
"""
Original value passed to `Color`.
"""
return
self
_original
as_named
as_named
fallback
bool
False
str
Returns the name of the color if it can be found in
COLORS_BY_VALUE
dictionary,
otherwise returns the hexadecimal representation of the color or raises
ValueError
Parameters:
Name
Type
Description
Default
fallback
bool
If True, falls back to returning the hexadecimal representation of
the color instead of raising a ValueError when no named color is found.
False
Returns:
Type
Description
str
The name of the color, or the hexadecimal representation of the color.
Raises:
Type
Description
ValueError
When no named color is found and fallback is
False
Source code in
pydantic_extra_types/color.py
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
def
as_named
self
fallback
bool
False
str
"""
Returns the name of the color if it can be found in `COLORS_BY_VALUE` dictionary,
otherwise returns the hexadecimal representation of the color or raises `ValueError`.
Args:
fallback: If True, falls back to returning the hexadecimal representation of
the color instead of raising a ValueError when no named color is found.
Returns:
The name of the color, or the hexadecimal representation of the color.
Raises:
ValueError: When no named color is found and fallback is `False`.
"""
self
_rgba
alpha
not
None
return
self
as_hex
rgb
cast
Tuple
int
int
int
self
as_rgb_tuple
())
rgb
COLORS_BY_VALUE
return
COLORS_BY_VALUE
rgb
else
fallback
return
self
as_hex
else
raise
ValueError
'no named color found, use fallback=True, as_hex() or as_rgb()'
as_hex
as_hex
format
Literal
'short'
'long'
'short'
str
Returns the hexadecimal representation of the color.
Hex string representing the color can be 3, 4, 6, or 8 characters depending on whether the string
a "short" representation of the color is possible and whether there's an alpha channel.
Returns:
Type
Description
str
The hexadecimal representation of the color.
Source code in
pydantic_extra_types/color.py
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
def
as_hex
self
format
Literal
'short'
'long'
'short'
str
"""Returns the hexadecimal representation of the color.
Hex string representing the color can be 3, 4, 6, or 8 characters depending on whether the string
a "short" representation of the color is possible and whether there's an alpha channel.
Returns:
The hexadecimal representation of the color.
"""
values
float_to_255
for
self
_rgba
self
_rgba
alpha
not
None
values
append
float_to_255
self
_rgba
alpha
as_hex
join
02x
for
values
format
'short'
and
all
repeat_colors
for
values
as_hex
join
as_hex
for
range
len
as_hex
return
as_hex
as_rgb
as_rgb
str
Color as an
rgb(<r>, <g>, <b>)
rgba(<r>, <g>, <b>, <a>)
string.
Source code in
pydantic_extra_types/color.py
154
155
156
157
158
159
160
161
162
163
164
def
as_rgb
self
str
"""
Color as an `rgb(<r>, <g>, <b>)` or `rgba(<r>, <g>, <b>, <a>)` string.
"""
self
_rgba
alpha
None
return
'rgb(
float_to_255
self
_rgba
float_to_255
self
_rgba
float_to_255
self
_rgba
else
return
'rgba(
float_to_255
self
_rgba
float_to_255
self
_rgba
float_to_255
self
_rgba
, '
round
self
_alpha_float
(),
as_rgb_tuple
as_rgb_tuple
alpha
bool
None
None
ColorTuple
Returns the color as an RGB or RGBA tuple.
Parameters:
Name
Type
Description
Default
alpha
bool
| None
Whether to include the alpha channel. There are three options for this input:
None
(default): Include alpha only if it's set. (e.g. not
None
True
: Always include alpha.
False
: Always omit alpha.
None
Returns:
Type
Description
ColorTuple
A tuple that contains the values of the red, green, and blue channels in the range 0 to 255.
If alpha is included, it is in the range 0 to 1.
Source code in
pydantic_extra_types/color.py
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
def
as_rgb_tuple
self
alpha
bool
None
None
ColorTuple
"""
Returns the color as an RGB or RGBA tuple.
Args:
alpha: Whether to include the alpha channel. There are three options for this input:
- `None` (default): Include alpha only if it's set. (e.g. not `None`)
- `True`: Always include alpha.
- `False`: Always omit alpha.
Returns:
A tuple that contains the values of the red, green, and blue channels in the range 0 to 255.
If alpha is included, it is in the range 0 to 1.
"""
float_to_255
for
self
_rgba
alpha
None
and
self
_rgba
alpha
None
alpha
not
None
and
not
alpha
return
else
return
self
_alpha_float
as_hsl
as_hsl
str
Color as an
hsl(<h>, <s>, <l>)
hsl(<h>, <s>, <l>, <a>)
string.
Source code in
pydantic_extra_types/color.py
187
188
189
190
191
192
193
194
195
196
def
as_hsl
self
str
"""
Color as an `hsl(<h>, <s>, <l>)` or `hsl(<h>, <s>, <l>, <a>)` string.
"""
self
_rgba
alpha
None
self
as_hsl_tuple
alpha
False
# type: ignore
return
'hsl(
360
0.0f
0.0%
0.0%
else
self
as_hsl_tuple
alpha
True
# type: ignore
return
'hsl(
360
0.0f
0.0%
0.0%
round
as_hsl_tuple
as_hsl_tuple
alpha
bool
None
None
HslColorTuple
Returns the color as an HSL or HSLA tuple.
Parameters:
Name
Type
Description
Default
alpha
bool
| None
Whether to include the alpha channel.
None
(default): Include the alpha channel only if it's set (e.g. not
None
True
: Always include alpha.
False
: Always omit alpha.
None
Returns:
Type
Description
HslColorTuple
The color as a tuple of hue, saturation, lightness, and alpha (if included).
All elements are in the range 0 to 1.
Note
This is HSL as used in HTML and most other places, not HLS as used in Python's
colorsys
Source code in
pydantic_extra_types/color.py
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
def
as_hsl_tuple
self
alpha
bool
None
None
HslColorTuple
"""
Returns the color as an HSL or HSLA tuple.
Args:
alpha: Whether to include the alpha channel.
- `None` (default): Include the alpha channel only if it's set (e.g. not `None`).
- `True`: Always include alpha.
- `False`: Always omit alpha.
Returns:
The color as a tuple of hue, saturation, lightness, and alpha (if included).
All elements are in the range 0 to 1.
Note:
This is HSL as used in HTML and most other places, not HLS as used in Python's `colorsys`.
"""
rgb_to_hls
self
_rgba
self
_rgba
self
_rgba
alpha
None
self
_rgba
alpha
None
return
else
return
self
_alpha_float
return
self
_alpha_float
())
alpha
else
parse_tuple
parse_tuple
value
tuple
Any
...
RGBA
Parse a tuple or list to get RGBA values.
Parameters:
Name
Type
Description
Default
value
tuple
Any
, ...]
A tuple or list.
required
Returns:
Type
Description
RGBA
RGBA
tuple parsed from the input tuple.
Raises:
Type
Description
PydanticCustomError
If tuple is not valid.
Source code in
pydantic_extra_types/color.py
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
def
parse_tuple
value
tuple
Any
...
RGBA
"""Parse a tuple or list to get RGBA values.
Args:
value: A tuple or list.
Returns:
An `RGBA` tuple parsed from the input tuple.
Raises:
PydanticCustomError: If tuple is not valid.
"""
len
value
parse_color_value
for
value
return
RGBA
None
elif
len
value
parse_color_value
for
value
return
RGBA
parse_float_alpha
value
]))
else
raise
PydanticCustomError
'color_error'
'value is not a valid color: tuples must have length 3 or 4'
parse_str
parse_str
value
str
RGBA
Parse a string representing a color to an RGBA tuple.
Possible formats for the input string include:
named color, see
COLORS_BY_NAME
hex short eg.
<prefix>fff
(prefix can be
or nothing)
hex long eg.
<prefix>ffffff
(prefix can be
or nothing)
rgb(<r>, <g>, <b>)
rgba(<r>, <g>, <b>, <a>)
transparent
Parameters:
Name
Type
Description
Default
value
str
A string representing a color.
required
Returns:
Type
Description
RGBA
RGBA
tuple parsed from the input string.
Raises:
Type
Description
ValueError
If the input string cannot be parsed to an RGBA tuple.
Source code in
pydantic_extra_types/color.py
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
def
parse_str
value
str
RGBA
"""
Parse a string representing a color to an RGBA tuple.
Possible formats for the input string include:
* named color, see `COLORS_BY_NAME`
* hex short eg. `<prefix>fff` (prefix can be `#`, `0x` or nothing)
* hex long eg. `<prefix>ffffff` (prefix can be `#`, `0x` or nothing)
* `rgb(<r>, <g>, <b>)`
* `rgba(<r>, <g>, <b>, <a>)`
* `transparent`
Args:
value: A string representing a color.
Returns:
An `RGBA` tuple parsed from the input string.
Raises:
ValueError: If the input string cannot be parsed to an RGBA tuple.
"""
value_lower
value
lower
value_lower
COLORS_BY_NAME
COLORS_BY_NAME
value_lower
return
ints_to_rgba
None
fullmatch
r_hex_short
value_lower
rgb
groups
int
for
rgb
alpha
int
255
else
None
return
ints_to_rgba
alpha
fullmatch
r_hex_long
value_lower
rgb
groups
int
for
rgb
alpha
int
255
else
None
return
ints_to_rgba
alpha
fullmatch
r_rgb
value_lower
fullmatch
r_rgb_v4_style
value_lower
return
ints_to_rgba
groups
())
# type: ignore
fullmatch
r_hsl
value_lower
fullmatch
r_hsl_v4_style
value_lower
return
parse_hsl
groups
())
# type: ignore
value_lower
'transparent'
return
RGBA
raise
PydanticCustomError
'color_error'
'value is not a valid color: string not recognised as a valid color'
ints_to_rgba
ints_to_rgba
int
str
int
str
int
str
alpha
float
None
None
RGBA
Converts integer or string values for RGB color and an optional alpha value to an
RGBA
object.
Parameters:
Name
Type
Description
Default
int
str
An integer or string representing the red color value.
required
int
str
An integer or string representing the green color value.
required
int
str
An integer or string representing the blue color value.
required
alpha
float
| None
A float representing the alpha value. Defaults to None.
None
Returns:
Type
Description
RGBA
An instance of the
RGBA
class with the corresponding color and alpha values.
Source code in
pydantic_extra_types/color.py
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
def
ints_to_rgba
int
str
int
str
int
str
alpha
float
None
None
RGBA
"""
Converts integer or string values for RGB color and an optional alpha value to an `RGBA` object.
Args:
r: An integer or string representing the red color value.
g: An integer or string representing the green color value.
b: An integer or string representing the blue color value.
alpha: A float representing the alpha value. Defaults to None.
Returns:
An instance of the `RGBA` class with the corresponding color and alpha values.
"""
return
RGBA
parse_color_value
parse_color_value
parse_color_value
parse_float_alpha
alpha
parse_color_value
parse_color_value
value
int
str
max_val
int
255
float
Parse the color value provided and return a number between 0 and 1.
Parameters:
Name
Type
Description
Default
value
int
str
An integer or string color value.
required
max_val
int
Maximum range value. Defaults to 255.
255
Raises:
Type
Description
PydanticCustomError
If the value is not a valid color.
Returns:
Type
Description
float
A number between 0 and 1.
Source code in
pydantic_extra_types/color.py
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
def
parse_color_value
value
int
str
max_val
int
255
float
"""
Parse the color value provided and return a number between 0 and 1.
Args:
value: An integer or string color value.
max_val: Maximum range value. Defaults to 255.
Raises:
PydanticCustomError: If the value is not a valid color.
Returns:
A number between 0 and 1.
"""
try
color
float
value
except
ValueError
raise
PydanticCustomError
'color_error'
'value is not a valid color: color values must be a valid number'
from
color
max_val
return
color
max_val
else
raise
PydanticCustomError
'color_error'
'value is not a valid color: color values must be in the range 0 to
{max_val}
'max_val'
max_val
parse_float_alpha
parse_float_alpha
value
None
str
float
int
float
None
Parse an alpha value checking it's a valid float in the range 0 to 1.
Parameters:
Name
Type
Description
Default
value
None |
str
float
int
The input value to parse.
required
Returns:
Type
Description
float
| None
The parsed value as a float, or
None
if the value was None or equal 1.
Raises:
Type
Description
PydanticCustomError
If the input value cannot be successfully parsed as a float in the expected range.
Source code in
pydantic_extra_types/color.py
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
def
parse_float_alpha
value
None
str
float
int
float
None
"""
Parse an alpha value checking it's a valid float in the range 0 to 1.
Args:
value: The input value to parse.
Returns:
The parsed value as a float, or `None` if the value was None or equal 1.
Raises:
PydanticCustomError: If the input value cannot be successfully parsed as a float in the expected range.
"""
value
None
return
None
try
isinstance
value
str
and
value
endswith
'%'
alpha
float
value
100
else
alpha
float
value
except
ValueError
raise
PydanticCustomError
'color_error'
'value is not a valid color: alpha values must be a valid float'
from
math
isclose
alpha
return
None
elif
alpha
return
alpha
else
raise
PydanticCustomError
'color_error'
'value is not a valid color: alpha values must be in the range 0 to 1'
parse_hsl
parse_hsl
str
h_units
str
sat
str
light
str
alpha
float
None
None
RGBA
Parse raw hue, saturation, lightness, and alpha values and convert to RGBA.
Parameters:
Name
Type
Description
Default
str
The hue value.
required
h_units
str
The unit for hue value.
required
sat
str
The saturation value.
required
light
str
The lightness value.
required
alpha
float
| None
Alpha value.
None
Returns:
Type
Description
RGBA
An instance of
RGBA
Source code in
pydantic_extra_types/color.py
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
def
parse_hsl
str
h_units
str
sat
str
light
str
alpha
float
None
None
RGBA
"""
Parse raw hue, saturation, lightness, and alpha values and convert to RGBA.
Args:
h: The hue value.
h_units: The unit for hue value.
sat: The saturation value.
light: The lightness value.
alpha: Alpha value.
Returns:
An instance of `RGBA`.
"""
s_value
l_value
parse_color_value
sat
100
parse_color_value
light
100
h_value
float
h_units
None
'deg'
h_value
h_value
360
360
elif
h_units
'rad'
h_value
h_value
rads
rads
else
# turns
h_value
hls_to_rgb
h_value
l_value
s_value
return
RGBA
parse_float_alpha
alpha
float_to_255
float_to_255
float
int
Converts a float value between 0 and 1 (inclusive) to an integer between 0 and 255 (inclusive).
Parameters:
Name
Type
Description
Default
float
The float value to be converted. Must be between 0 and 1 (inclusive).
required
Returns:
Type
Description
int
The integer equivalent of the given float value rounded to the nearest whole number.
Source code in
pydantic_extra_types/color.py
456
457
458
459
460
461
462
463
464
465
466
def
float_to_255
float
int
"""
Converts a float value between 0 and 1 (inclusive) to an integer between 0 and 255 (inclusive).
Args:
c: The float value to be converted. Must be between 0 and 1 (inclusive).
Returns:
The integer equivalent of the given float value rounded to the nearest whole number.
"""
return
round
255
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 031_pydantic_extra_types_pendulum_dt.txt ---
Pendulum
Native Pendulum DateTime object implementation. This is a copy of the Pendulum DateTime object, but with a Pydantic
CoreSchema implementation. This allows Pydantic to validate the DateTime object.
DateTime
Bases:
DateTime
pendulum.DateTime
object. At runtime, this type decomposes into pendulum.DateTime automatically.
This type exists because Pydantic throws a fit on unknown types.
from
pydantic
import
BaseModel
from
pydantic_extra_types.pendulum_dt
import
DateTime
class
test_model
BaseModel
DateTime
print
test_model
'2021-01-01T00:00:00+00:00'
#> test_model(dt=DateTime(2021, 1, 1, 0, 0, 0, tzinfo=FixedTimezone(0, name="+00:00")))
Date
Bases:
Date
pendulum.Date
object. At runtime, this type decomposes into pendulum.Date automatically.
This type exists because Pydantic throws a fit on unknown types.
from
pydantic
import
BaseModel
from
pydantic_extra_types.pendulum_dt
import
Date
class
test_model
BaseModel
Date
print
test_model
'2021-01-01'
#> test_model(dt=Date(2021, 1, 1))
Duration
Bases:
Duration
pendulum.Duration
object. At runtime, this type decomposes into pendulum.Duration automatically.
This type exists because Pydantic throws a fit on unknown types.
from
pydantic
import
BaseModel
from
pydantic_extra_types.pendulum_dt
import
Duration
class
test_model
BaseModel
delta_t
Duration
print
test_model
delta_t
'P1DT25H'
#> test_model(delta_t=Duration(days=2, hours=1))
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 033_pydantic_extra_types_script_code.txt ---
Script Code
script definitions that are based on the
ISO 15924
ISO_15924
Bases:
str
ISO_15924 parses script in the
ISO 15924
format.
from
pydantic
import
BaseModel
from
pydantic_extra_types.language_code
import
ISO_15924
class
Script
BaseModel
alpha_4
ISO_15924
script
Script
alpha_4
'Java'
print
lang
# > script='Java'
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 008_pydantic_extra_types_language_code.txt ---
Language
Language definitions that are based on the
ISO 639-3
ISO 639-5
LanguageInfo
dataclass
LanguageInfo
alpha2
Union
str
None
alpha3
str
name
str
LanguageInfo is a dataclass that contains the language information.
Parameters:
Name
Type
Description
Default
alpha2
Union
str
, None]
The language code in the
ISO 639-1 alpha-2
format.
required
alpha3
str
The language code in the
ISO 639-3 alpha-3
format.
required
name
str
The language name.
required
LanguageAlpha2
Bases:
str
LanguageAlpha2 parses languages codes in the
ISO 639-1 alpha-2
format.
from
pydantic
import
BaseModel
from
pydantic_extra_types.language_code
import
LanguageAlpha2
class
Movie
BaseModel
audio_lang
LanguageAlpha2
subtitles_lang
LanguageAlpha2
movie
Movie
audio_lang
'de'
subtitles_lang
'fr'
print
movie
#> audio_lang='de' subtitles_lang='fr'
alpha3
property
alpha3
str
The language code in the
ISO 639-3 alpha-3
format.
name
property
name
str
The language name.
LanguageName
Bases:
str
LanguageName parses languages names listed in the
ISO 639-3 standard
format.
from
pydantic
import
BaseModel
from
pydantic_extra_types.language_code
import
LanguageName
class
Movie
BaseModel
audio_lang
LanguageName
subtitles_lang
LanguageName
movie
Movie
audio_lang
'Dutch'
subtitles_lang
'Mandarin Chinese'
print
movie
#> audio_lang='Dutch' subtitles_lang='Mandarin Chinese'
alpha2
property
alpha2
Union
str
None
The language code in the
ISO 639-1 alpha-2
format. Does not exist for all languages.
alpha3
property
alpha3
str
The language code in the
ISO 639-3 alpha-3
format.
ISO639_3
Bases:
str
ISO639_3 parses Language in the
ISO 639-3 alpha-3
format.
from
pydantic
import
BaseModel
from
pydantic_extra_types.language_code
import
ISO639_3
class
Language
BaseModel
alpha_3
ISO639_3
lang
Language
alpha_3
'ssr'
print
lang
# > alpha_3='ssr'
ISO639_5
Bases:
str
ISO639_5 parses Language in the
ISO 639-5 alpha-3
format.
from
pydantic
import
BaseModel
from
pydantic_extra_types.language_code
import
ISO639_5
class
Language
BaseModel
alpha_3
ISO639_5
lang
Language
alpha_3
'gem'
print
lang
# > alpha_3='gem'
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 009_pydantic_core_schema_pydantic_core_core_schema_SerializationInfo_context.txt ---
pydantic_core.core_schema
This module contains definitions to build schemas which
pydantic_core
can
validate and serialize.
WhenUsed
module-attribute
WhenUsed
Literal
"always"
"unless-none"
"json"
"json-unless-none"
Values have the following meanings:
'always'
means always use
'unless-none'
means use unless the value is
None
'json'
means use when serializing to JSON
'json-unless-none'
means use when serializing to JSON and the value is not
None
CoreConfig
Bases:
TypedDict
Base class for schema configuration options.
Attributes:
Name
Type
Description
title
str
The name of the configuration.
strict
bool
Whether the configuration should strictly adhere to specified rules.
extra_fields_behavior
ExtraBehavior
The behavior for handling extra fields.
typed_dict_total
bool
Whether the TypedDict should be considered total. Default is
True
from_attributes
bool
Whether to use attributes for models, dataclasses, and tagged union keys.
loc_by_alias
bool
Whether to use the used alias (or first alias for "field required" errors) instead of
field_names
to construct error
loc
s. Default is
True
revalidate_instances
Literal
['always', 'never', 'subclass-instances']
Whether instances of models and dataclasses should re-validate. Default is 'never'.
validate_default
bool
Whether to validate default values during validation. Default is
False
str_max_length
int
The maximum length for string fields.
str_min_length
int
The minimum length for string fields.
str_strip_whitespace
bool
Whether to strip whitespace from string fields.
str_to_lower
bool
Whether to convert string fields to lowercase.
str_to_upper
bool
Whether to convert string fields to uppercase.
allow_inf_nan
bool
Whether to allow infinity and NaN values for float fields. Default is
True
ser_json_timedelta
Literal
['iso8601', 'float']
The serialization option for
timedelta
values. Default is 'iso8601'.
ser_json_bytes
Literal
['utf8', 'base64', 'hex']
The serialization option for
bytes
values. Default is 'utf8'.
ser_json_inf_nan
Literal
['null', 'constants', 'strings']
The serialization option for infinity and NaN values
in float fields. Default is 'null'.
val_json_bytes
Literal
['utf8', 'base64', 'hex']
The validation option for
bytes
values, complementing ser_json_bytes. Default is 'utf8'.
hide_input_in_errors
bool
Whether to hide input data from
ValidationError
representation.
validation_error_cause
bool
Whether to add user-python excs to the
cause
of a ValidationError.
Requires exceptiongroup backport pre Python 3.11.
coerce_numbers_to_str
bool
Whether to enable coercion of any
Number
type to
str
(not applicable in
strict
mode).
regex_engine
Literal
['rust-regex', 'python-re']
The regex engine to use for regex pattern validation. Default is 'rust-regex'. See
StringSchema
cache_strings
Union
bool
Literal
['all', 'keys', 'none']]
Whether to cache strings. Default is
True
True
'all'
is required to cache strings
during general validation since validators don't know if they're in a key or a value.
validate_by_alias
bool
Whether to use the field's alias when validating against the provided input data. Default is
True
validate_by_name
bool
Whether to use the field's name when validating against the provided input data. Default is
False
. Replacement for
populate_by_name
serialize_by_alias
bool
Whether to serialize by alias. Default is
False
, expected to change to
True
in V3.
SerializationInfo
Bases:
Protocol
context
property
context
Any
None
Current serialization context.
ValidationInfo
Bases:
Protocol
Argument passed to validation functions.
context
property
context
Any
None
Current validation context.
config
property
config
CoreConfig
None
The CoreConfig that applies to this validation.
mode
property
mode
Literal
'python'
'json'
The type of input data we are currently validating
data
property
data
dict
str
Any
The data being validated for this model.
field_name
property
field_name
str
None
The name of the current field being validated if this validator is
attached to a model field.
simple_ser_schema
simple_ser_schema
type
ExpectedSerializationTypes
SimpleSerSchema
Returns a schema for serialization with a custom type.
Parameters:
Name
Type
Description
Default
type
ExpectedSerializationTypes
The type to use for serialization
required
Source code in
pydantic_core/core_schema.py
230
231
232
233
234
235
236
237
def
simple_ser_schema
type
ExpectedSerializationTypes
SimpleSerSchema
"""
Returns a schema for serialization with a custom type.
Args:
type: The type to use for serialization
"""
return
SimpleSerSchema
type
type
plain_serializer_function_ser_schema
plain_serializer_function_ser_schema
function
SerializerFunction
is_field_serializer
bool
None
None
info_arg
bool
None
None
return_schema
CoreSchema
None
None
when_used
WhenUsed
"always"
PlainSerializerFunctionSerSchema
Returns a schema for serialization with a function, can be either a "general" or "field" function.
Parameters:
Name
Type
Description
Default
function
SerializerFunction
The function to use for serialization
required
is_field_serializer
bool
| None
Whether the serializer is for a field, e.g. takes
model
as the first argument,
and
info
includes
field_name
None
info_arg
bool
| None
Whether the function takes an
info
argument
None
return_schema
CoreSchema
| None
Schema to use for serializing return value
None
when_used
WhenUsed
When the function should be called
'always'
Source code in
pydantic_core/core_schema.py
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
def
plain_serializer_function_ser_schema
function
SerializerFunction
is_field_serializer
bool
None
None
info_arg
bool
None
None
return_schema
CoreSchema
None
None
when_used
WhenUsed
'always'
PlainSerializerFunctionSerSchema
"""
Returns a schema for serialization with a function, can be either a "general" or "field" function.
Args:
function: The function to use for serialization
is_field_serializer: Whether the serializer is for a field, e.g. takes `model` as the first argument,
and `info` includes `field_name`
info_arg: Whether the function takes an `info` argument
return_schema: Schema to use for serializing return value
when_used: When the function should be called
"""
when_used
'always'
# just to avoid extra elements in schema, and to use the actual default defined in rust
when_used
None
# type: ignore
return
_dict_not_none
type
'function-plain'
function
function
is_field_serializer
is_field_serializer
info_arg
info_arg
return_schema
return_schema
when_used
when_used
wrap_serializer_function_ser_schema
wrap_serializer_function_ser_schema
function
WrapSerializerFunction
is_field_serializer
bool
None
None
info_arg
bool
None
None
schema
CoreSchema
None
None
return_schema
CoreSchema
None
None
when_used
WhenUsed
"always"
WrapSerializerFunctionSerSchema
Returns a schema for serialization with a wrap function, can be either a "general" or "field" function.
Parameters:
Name
Type
Description
Default
function
WrapSerializerFunction
The function to use for serialization
required
is_field_serializer
bool
| None
Whether the serializer is for a field, e.g. takes
model
as the first argument,
and
info
includes
field_name
None
info_arg
bool
| None
Whether the function takes an
info
argument
None
schema
CoreSchema
| None
The schema to use for the inner serialization
None
return_schema
CoreSchema
| None
Schema to use for serializing return value
None
when_used
WhenUsed
When the function should be called
'always'
Source code in
pydantic_core/core_schema.py
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
def
wrap_serializer_function_ser_schema
function
WrapSerializerFunction
is_field_serializer
bool
None
None
info_arg
bool
None
None
schema
CoreSchema
None
None
return_schema
CoreSchema
None
None
when_used
WhenUsed
'always'
WrapSerializerFunctionSerSchema
"""
Returns a schema for serialization with a wrap function, can be either a "general" or "field" function.
Args:
function: The function to use for serialization
is_field_serializer: Whether the serializer is for a field, e.g. takes `model` as the first argument,
and `info` includes `field_name`
info_arg: Whether the function takes an `info` argument
schema: The schema to use for the inner serialization
return_schema: Schema to use for serializing return value
when_used: When the function should be called
"""
when_used
'always'
# just to avoid extra elements in schema, and to use the actual default defined in rust
when_used
None
# type: ignore
return
_dict_not_none
type
'function-wrap'
function
function
is_field_serializer
is_field_serializer
info_arg
info_arg
schema
schema
return_schema
return_schema
when_used
when_used
format_ser_schema
format_ser_schema
formatting_string
str
when_used
WhenUsed
"json-unless-none"
FormatSerSchema
Returns a schema for serialization using python's
format
method.
Parameters:
Name
Type
Description
Default
formatting_string
str
String defining the format to use
required
when_used
WhenUsed
Same meaning as for [general_function_plain_ser_schema], but with a different default
'json-unless-none'
Source code in
pydantic_core/core_schema.py
378
379
380
381
382
383
384
385
386
387
388
389
def
format_ser_schema
formatting_string
str
when_used
WhenUsed
'json-unless-none'
FormatSerSchema
"""
Returns a schema for serialization using python's `format` method.
Args:
formatting_string: String defining the format to use
when_used: Same meaning as for [general_function_plain_ser_schema], but with a different default
"""
when_used
'json-unless-none'
# just to avoid extra elements in schema, and to use the actual default defined in rust
when_used
None
# type: ignore
return
_dict_not_none
type
'format'
formatting_string
formatting_string
when_used
when_used
to_string_ser_schema
to_string_ser_schema
when_used
WhenUsed
"json-unless-none"
ToStringSerSchema
Returns a schema for serialization using python's
str()
__str__
method.
Parameters:
Name
Type
Description
Default
when_used
WhenUsed
Same meaning as for [general_function_plain_ser_schema], but with a different default
'json-unless-none'
Source code in
pydantic_core/core_schema.py
397
398
399
400
401
402
403
404
405
406
407
408
def
to_string_ser_schema
when_used
WhenUsed
'json-unless-none'
ToStringSerSchema
"""
Returns a schema for serialization using python's `str()` / `__str__` method.
Args:
when_used: Same meaning as for [general_function_plain_ser_schema], but with a different default
"""
dict
type
'to-string'
when_used
'json-unless-none'
# just to avoid extra elements in schema, and to use the actual default defined in rust
'when_used'
when_used
return
# type: ignore
model_ser_schema
model_ser_schema
cls
type
Any
schema
CoreSchema
ModelSerSchema
Returns a schema for serialization using a model.
Parameters:
Name
Type
Description
Default
cls
type
Any
The expected class type, used to generate warnings if the wrong type is passed
required
schema
CoreSchema
Internal schema to use to serialize the model dict
required
Source code in
pydantic_core/core_schema.py
417
418
419
420
421
422
423
424
425
def
model_ser_schema
cls
type
Any
schema
CoreSchema
ModelSerSchema
"""
Returns a schema for serialization using a model.
Args:
cls: The expected class type, used to generate warnings if the wrong type is passed
schema: Internal schema to use to serialize the model dict
"""
return
ModelSerSchema
type
'model'
cls
cls
schema
schema
invalid_schema
invalid_schema
ref
str
None
None
metadata
dict
str
Any
None
None
InvalidSchema
Returns an invalid schema, used to indicate that a schema is invalid.
Returns a schema that matches any value, e.g.:
Parameters:
Name
Type
Description
Default
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
Source code in
pydantic_core/core_schema.py
447
448
449
450
451
452
453
454
455
456
457
458
def
invalid_schema
ref
str
None
None
metadata
dict
str
Any
None
None
InvalidSchema
"""
Returns an invalid schema, used to indicate that a schema is invalid.
Returns a schema that matches any value, e.g.:
Args:
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
"""
return
_dict_not_none
type
'invalid'
ref
ref
metadata
metadata
computed_field
computed_field
property_name
str
return_schema
CoreSchema
alias
str
None
None
metadata
dict
str
Any
None
None
ComputedField
ComputedFields are properties of a model or dataclass that are included in serialization.
Parameters:
Name
Type
Description
Default
property_name
str
The name of the property on the model or dataclass
required
return_schema
CoreSchema
The schema used for the type returned by the computed field
required
alias
str
| None
The name to use in the serialized output
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
Source code in
pydantic_core/core_schema.py
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
def
computed_field
property_name
str
return_schema
CoreSchema
alias
str
None
None
metadata
dict
str
Any
None
None
ComputedField
"""
ComputedFields are properties of a model or dataclass that are included in serialization.
Args:
property_name: The name of the property on the model or dataclass
return_schema: The schema used for the type returned by the computed field
alias: The name to use in the serialized output
metadata: Any other information you want to include with the schema, not used by pydantic-core
"""
return
_dict_not_none
type
'computed-field'
property_name
property_name
return_schema
return_schema
alias
alias
metadata
metadata
any_schema
any_schema
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
AnySchema
Returns a schema that matches any value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
any_schema
SchemaValidator
schema
assert
validate_python
Parameters:
Name
Type
Description
Default
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
def
any_schema
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
AnySchema
"""
Returns a schema that matches any value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.any_schema()
v = SchemaValidator(schema)
assert v.validate_python(1) == 1
```
Args:
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'any'
ref
ref
metadata
metadata
serialization
serialization
none_schema
none_schema
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
NoneSchema
Returns a schema that matches a None value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
none_schema
SchemaValidator
schema
assert
validate_python
None
None
Parameters:
Name
Type
Description
Default
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
def
none_schema
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
NoneSchema
"""
Returns a schema that matches a None value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.none_schema()
v = SchemaValidator(schema)
assert v.validate_python(None) is None
```
Args:
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'none'
ref
ref
metadata
metadata
serialization
serialization
bool_schema
bool_schema
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
BoolSchema
Returns a schema that matches a bool value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
bool_schema
SchemaValidator
schema
assert
validate_python
'True'
True
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether the value should be a bool or a value that can be converted to a bool
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
def
bool_schema
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
BoolSchema
"""
Returns a schema that matches a bool value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.bool_schema()
v = SchemaValidator(schema)
assert v.validate_python('True') is True
```
Args:
strict: Whether the value should be a bool or a value that can be converted to a bool
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'bool'
strict
strict
ref
ref
metadata
metadata
serialization
serialization
int_schema
int_schema
multiple_of
int
None
None
int
None
None
int
None
None
int
None
None
int
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
IntSchema
Returns a schema that matches a int value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
int_schema
multiple_of
SchemaValidator
schema
assert
validate_python
'4'
Parameters:
Name
Type
Description
Default
multiple_of
int
| None
The value must be a multiple of this number
None
int
| None
The value must be less than or equal to this number
None
int
| None
The value must be greater than or equal to this number
None
int
| None
The value must be strictly less than this number
None
int
| None
The value must be strictly greater than this number
None
strict
bool
| None
Whether the value should be a int or a value that can be converted to a int
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
def
int_schema
multiple_of
int
None
None
int
None
None
int
None
None
int
None
None
int
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
IntSchema
"""
Returns a schema that matches a int value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.int_schema(multiple_of=2, le=6, ge=2)
v = SchemaValidator(schema)
assert v.validate_python('4') == 4
```
Args:
multiple_of: The value must be a multiple of this number
le: The value must be less than or equal to this number
ge: The value must be greater than or equal to this number
lt: The value must be strictly less than this number
gt: The value must be strictly greater than this number
strict: Whether the value should be a int or a value that can be converted to a int
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'int'
multiple_of
multiple_of
strict
strict
ref
ref
metadata
metadata
serialization
serialization
float_schema
float_schema
allow_inf_nan
bool
None
None
multiple_of
float
None
None
float
None
None
float
None
None
float
None
None
float
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
FloatSchema
Returns a schema that matches a float value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
float_schema
0.8
0.2
SchemaValidator
schema
assert
validate_python
'0.5'
0.5
Parameters:
Name
Type
Description
Default
allow_inf_nan
bool
| None
Whether to allow inf and nan values
None
multiple_of
float
| None
The value must be a multiple of this number
None
float
| None
The value must be less than or equal to this number
None
float
| None
The value must be greater than or equal to this number
None
float
| None
The value must be strictly less than this number
None
float
| None
The value must be strictly greater than this number
None
strict
bool
| None
Whether the value should be a float or a value that can be converted to a float
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
697
698
699
700
701
def
float_schema
allow_inf_nan
bool
None
None
multiple_of
float
None
None
float
None
None
float
None
None
float
None
None
float
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
FloatSchema
"""
Returns a schema that matches a float value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.float_schema(le=0.8, ge=0.2)
v = SchemaValidator(schema)
assert v.validate_python('0.5') == 0.5
```
Args:
allow_inf_nan: Whether to allow inf and nan values
multiple_of: The value must be a multiple of this number
le: The value must be less than or equal to this number
ge: The value must be greater than or equal to this number
lt: The value must be strictly less than this number
gt: The value must be strictly greater than this number
strict: Whether the value should be a float or a value that can be converted to a float
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'float'
allow_inf_nan
allow_inf_nan
multiple_of
multiple_of
strict
strict
ref
ref
metadata
metadata
serialization
serialization
decimal_schema
decimal_schema
allow_inf_nan
bool
None
None
multiple_of
Decimal
None
None
Decimal
None
None
Decimal
None
None
Decimal
None
None
Decimal
None
None
max_digits
int
None
None
decimal_places
int
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
DecimalSchema
Returns a schema that matches a decimal value, e.g.:
from
decimal
import
Decimal
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
decimal_schema
0.8
0.2
SchemaValidator
schema
assert
validate_python
'0.5'
Decimal
'0.5'
Parameters:
Name
Type
Description
Default
allow_inf_nan
bool
| None
Whether to allow inf and nan values
None
multiple_of
Decimal
| None
The value must be a multiple of this number
None
Decimal
| None
The value must be less than or equal to this number
None
Decimal
| None
The value must be greater than or equal to this number
None
Decimal
| None
The value must be strictly less than this number
None
Decimal
| None
The value must be strictly greater than this number
None
max_digits
int
| None
The maximum number of decimal digits allowed
None
decimal_places
int
| None
The maximum number of decimal places allowed
None
strict
bool
| None
Whether the value should be a float or a value that can be converted to a float
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
def
decimal_schema
allow_inf_nan
bool
None
None
multiple_of
Decimal
None
None
Decimal
None
None
Decimal
None
None
Decimal
None
None
Decimal
None
None
max_digits
int
None
None
decimal_places
int
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
DecimalSchema
"""
Returns a schema that matches a decimal value, e.g.:
```py
from decimal import Decimal
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.decimal_schema(le=0.8, ge=0.2)
v = SchemaValidator(schema)
assert v.validate_python('0.5') == Decimal('0.5')
```
Args:
allow_inf_nan: Whether to allow inf and nan values
multiple_of: The value must be a multiple of this number
le: The value must be less than or equal to this number
ge: The value must be greater than or equal to this number
lt: The value must be strictly less than this number
gt: The value must be strictly greater than this number
max_digits: The maximum number of decimal digits allowed
decimal_places: The maximum number of decimal places allowed
strict: Whether the value should be a float or a value that can be converted to a float
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'decimal'
max_digits
max_digits
decimal_places
decimal_places
multiple_of
multiple_of
allow_inf_nan
allow_inf_nan
strict
strict
ref
ref
metadata
metadata
serialization
serialization
complex_schema
complex_schema
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
ComplexSchema
Returns a schema that matches a complex value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
complex_schema
SchemaValidator
schema
assert
validate_python
'1+2j'
complex
assert
validate_python
complex
complex
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether the value should be a complex object instance or a value that can be converted to a complex object
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
def
complex_schema
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
ComplexSchema
"""
Returns a schema that matches a complex value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.complex_schema()
v = SchemaValidator(schema)
assert v.validate_python('1+2j') == complex(1, 2)
assert v.validate_python(complex(1, 2)) == complex(1, 2)
```
Args:
strict: Whether the value should be a complex object instance or a value that can be converted to a complex object
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'complex'
strict
strict
ref
ref
metadata
metadata
serialization
serialization
str_schema
str_schema
pattern
str
Pattern
str
None
None
max_length
int
None
None
min_length
int
None
None
strip_whitespace
bool
None
None
to_lower
bool
None
None
to_upper
bool
None
None
regex_engine
Literal
"rust-regex"
"python-re"
None
None
strict
bool
None
None
coerce_numbers_to_str
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
StringSchema
Returns a schema that matches a string value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
str_schema
max_length
min_length
SchemaValidator
schema
assert
validate_python
'hello'
'hello'
Parameters:
Name
Type
Description
Default
pattern
str
Pattern
str
] | None
A regex pattern that the value must match
None
max_length
int
| None
The value must be at most this length
None
min_length
int
| None
The value must be at least this length
None
strip_whitespace
bool
| None
Whether to strip whitespace from the value
None
to_lower
bool
| None
Whether to convert the value to lowercase
None
to_upper
bool
| None
Whether to convert the value to uppercase
None
regex_engine
Literal
['rust-regex', 'python-re'] | None
The regex engine to use for pattern validation. Default is 'rust-regex'.
rust-regex
uses the
regex
Rust
crate, which is non-backtracking and therefore more DDoS
resistant, but does not support all regex features.
python-re
use the
module,
which supports all regex features, but may be slower.
None
strict
bool
| None
Whether the value should be a string or a value that can be converted to a string
None
coerce_numbers_to_str
bool
| None
Whether to enable coercion of any
Number
type to
str
(not applicable in
strict
mode).
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
def
str_schema
pattern
str
Pattern
str
None
None
max_length
int
None
None
min_length
int
None
None
strip_whitespace
bool
None
None
to_lower
bool
None
None
to_upper
bool
None
None
regex_engine
Literal
'rust-regex'
'python-re'
None
None
strict
bool
None
None
coerce_numbers_to_str
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
StringSchema
"""
Returns a schema that matches a string value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.str_schema(max_length=10, min_length=2)
v = SchemaValidator(schema)
assert v.validate_python('hello') == 'hello'
```
Args:
pattern: A regex pattern that the value must match
max_length: The value must be at most this length
min_length: The value must be at least this length
strip_whitespace: Whether to strip whitespace from the value
to_lower: Whether to convert the value to lowercase
to_upper: Whether to convert the value to uppercase
regex_engine: The regex engine to use for pattern validation. Default is 'rust-regex'.
- `rust-regex` uses the [`regex`](https://docs.rs/regex) Rust
crate, which is non-backtracking and therefore more DDoS
resistant, but does not support all regex features.
- `python-re` use the [`re`](https://docs.python.org/3/library/re.html) module,
which supports all regex features, but may be slower.
strict: Whether the value should be a string or a value that can be converted to a string
coerce_numbers_to_str: Whether to enable coercion of any `Number` type to `str` (not applicable in `strict` mode).
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'str'
pattern
pattern
max_length
max_length
min_length
min_length
strip_whitespace
strip_whitespace
to_lower
to_lower
to_upper
to_upper
regex_engine
regex_engine
strict
strict
coerce_numbers_to_str
coerce_numbers_to_str
ref
ref
metadata
metadata
serialization
serialization
bytes_schema
bytes_schema
max_length
int
None
None
min_length
int
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
BytesSchema
Returns a schema that matches a bytes value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
bytes_schema
max_length
min_length
SchemaValidator
schema
assert
validate_python
'hello'
'hello'
Parameters:
Name
Type
Description
Default
max_length
int
| None
The value must be at most this length
None
min_length
int
| None
The value must be at least this length
None
strict
bool
| None
Whether the value should be a bytes or a value that can be converted to a bytes
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
908
909
910
911
912
913
914
915
916
917
918
919
920
921
922
923
924
925
926
927
928
929
930
931
932
933
934
935
936
937
938
939
940
941
942
943
944
def
bytes_schema
max_length
int
None
None
min_length
int
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
BytesSchema
"""
Returns a schema that matches a bytes value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.bytes_schema(max_length=10, min_length=2)
v = SchemaValidator(schema)
assert v.validate_python(b'hello') == b'hello'
```
Args:
max_length: The value must be at most this length
min_length: The value must be at least this length
strict: Whether the value should be a bytes or a value that can be converted to a bytes
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'bytes'
max_length
max_length
min_length
min_length
strict
strict
ref
ref
metadata
metadata
serialization
serialization
date_schema
date_schema
strict
bool
None
None
date
None
None
date
None
None
date
None
None
date
None
None
now_op
Literal
"past"
"future"
None
None
now_utc_offset
int
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
DateSchema
Returns a schema that matches a date value, e.g.:
from
datetime
import
date
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
date_schema
date
2020
date
2019
SchemaValidator
schema
assert
validate_python
date
2019
date
2019
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether the value should be a date or a value that can be converted to a date
None
date
| None
The value must be less than or equal to this date
None
date
| None
The value must be greater than or equal to this date
None
date
| None
The value must be strictly less than this date
None
date
| None
The value must be strictly greater than this date
None
now_op
Literal
['past', 'future'] | None
The value must be in the past or future relative to the current date
None
now_utc_offset
int
| None
The value must be in the past or future relative to the current date with this utc offset
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
963
964
965
966
967
968
969
970
971
972
973
974
975
976
977
978
979
980
981
982
983
984
985
986
987
988
989
990
991
992
993
994
995
996
997
998
999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
def
date_schema
strict
bool
None
None
date
None
None
date
None
None
date
None
None
date
None
None
now_op
Literal
'past'
'future'
None
None
now_utc_offset
int
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
DateSchema
"""
Returns a schema that matches a date value, e.g.:
```py
from datetime import date
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.date_schema(le=date(2020, 1, 1), ge=date(2019, 1, 1))
v = SchemaValidator(schema)
assert v.validate_python(date(2019, 6, 1)) == date(2019, 6, 1)
```
Args:
strict: Whether the value should be a date or a value that can be converted to a date
le: The value must be less than or equal to this date
ge: The value must be greater than or equal to this date
lt: The value must be strictly less than this date
gt: The value must be strictly greater than this date
now_op: The value must be in the past or future relative to the current date
now_utc_offset: The value must be in the past or future relative to the current date with this utc offset
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'date'
strict
strict
now_op
now_op
now_utc_offset
now_utc_offset
ref
ref
metadata
metadata
serialization
serialization
time_schema
time_schema
strict
bool
None
None
time
None
None
time
None
None
time
None
None
time
None
None
tz_constraint
Literal
"aware"
"naive"
int
None
None
microseconds_precision
Literal
"truncate"
"error"
"truncate"
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
TimeSchema
Returns a schema that matches a time value, e.g.:
from
datetime
import
time
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
time_schema
time
time
SchemaValidator
schema
assert
validate_python
time
time
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether the value should be a time or a value that can be converted to a time
None
time
| None
The value must be less than or equal to this time
None
time
| None
The value must be greater than or equal to this time
None
time
| None
The value must be strictly less than this time
None
time
| None
The value must be strictly greater than this time
None
tz_constraint
Literal
['aware', 'naive'] |
int
| None
The value must be timezone aware or naive, or an int to indicate required tz offset
None
microseconds_precision
Literal
['truncate', 'error']
The behavior when seconds have more than 6 digits or microseconds is too large
'truncate'
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1029
1030
1031
1032
1033
1034
1035
1036
1037
1038
1039
1040
1041
1042
1043
1044
1045
1046
1047
1048
1049
1050
1051
1052
1053
1054
1055
1056
1057
1058
1059
1060
1061
1062
1063
1064
1065
1066
1067
1068
1069
1070
1071
1072
1073
1074
1075
1076
1077
1078
def
time_schema
strict
bool
None
None
time
None
None
time
None
None
time
None
None
time
None
None
tz_constraint
Literal
'aware'
'naive'
int
None
None
microseconds_precision
Literal
'truncate'
'error'
'truncate'
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
TimeSchema
"""
Returns a schema that matches a time value, e.g.:
```py
from datetime import time
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.time_schema(le=time(12, 0, 0), ge=time(6, 0, 0))
v = SchemaValidator(schema)
assert v.validate_python(time(9, 0, 0)) == time(9, 0, 0)
```
Args:
strict: Whether the value should be a time or a value that can be converted to a time
le: The value must be less than or equal to this time
ge: The value must be greater than or equal to this time
lt: The value must be strictly less than this time
gt: The value must be strictly greater than this time
tz_constraint: The value must be timezone aware or naive, or an int to indicate required tz offset
microseconds_precision: The behavior when seconds have more than 6 digits or microseconds is too large
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'time'
strict
strict
tz_constraint
tz_constraint
microseconds_precision
microseconds_precision
ref
ref
metadata
metadata
serialization
serialization
datetime_schema
datetime_schema
strict
bool
None
None
datetime
None
None
datetime
None
None
datetime
None
None
datetime
None
None
now_op
Literal
"past"
"future"
None
None
tz_constraint
Literal
"aware"
"naive"
int
None
None
now_utc_offset
int
None
None
microseconds_precision
Literal
"truncate"
"error"
"truncate"
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
DatetimeSchema
Returns a schema that matches a datetime value, e.g.:
from
datetime
import
datetime
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
datetime_schema
SchemaValidator
schema
now
datetime
now
assert
validate_python
str
now
now
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether the value should be a datetime or a value that can be converted to a datetime
None
datetime
| None
The value must be less than or equal to this datetime
None
datetime
| None
The value must be greater than or equal to this datetime
None
datetime
| None
The value must be strictly less than this datetime
None
datetime
| None
The value must be strictly greater than this datetime
None
now_op
Literal
['past', 'future'] | None
The value must be in the past or future relative to the current datetime
None
tz_constraint
Literal
['aware', 'naive'] |
int
| None
The value must be timezone aware or naive, or an int to indicate required tz offset
TODO: use of a tzinfo where offset changes based on the datetime is not yet supported
None
now_utc_offset
int
| None
The value must be in the past or future relative to the current datetime with this utc offset
None
microseconds_precision
Literal
['truncate', 'error']
The behavior when seconds have more than 6 digits or microseconds is too large
'truncate'
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1099
1100
1101
1102
1103
1104
1105
1106
1107
1108
1109
1110
1111
1112
1113
1114
1115
1116
1117
1118
1119
1120
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
1133
1134
1135
1136
1137
1138
1139
1140
1141
1142
1143
1144
1145
1146
1147
1148
1149
1150
1151
1152
1153
1154
1155
1156
def
datetime_schema
strict
bool
None
None
datetime
None
None
datetime
None
None
datetime
None
None
datetime
None
None
now_op
Literal
'past'
'future'
None
None
tz_constraint
Literal
'aware'
'naive'
int
None
None
now_utc_offset
int
None
None
microseconds_precision
Literal
'truncate'
'error'
'truncate'
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
DatetimeSchema
"""
Returns a schema that matches a datetime value, e.g.:
```py
from datetime import datetime
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.datetime_schema()
v = SchemaValidator(schema)
now = datetime.now()
assert v.validate_python(str(now)) == now
```
Args:
strict: Whether the value should be a datetime or a value that can be converted to a datetime
le: The value must be less than or equal to this datetime
ge: The value must be greater than or equal to this datetime
lt: The value must be strictly less than this datetime
gt: The value must be strictly greater than this datetime
now_op: The value must be in the past or future relative to the current datetime
tz_constraint: The value must be timezone aware or naive, or an int to indicate required tz offset
TODO: use of a tzinfo where offset changes based on the datetime is not yet supported
now_utc_offset: The value must be in the past or future relative to the current datetime with this utc offset
microseconds_precision: The behavior when seconds have more than 6 digits or microseconds is too large
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'datetime'
strict
strict
now_op
now_op
tz_constraint
tz_constraint
now_utc_offset
now_utc_offset
microseconds_precision
microseconds_precision
ref
ref
metadata
metadata
serialization
serialization
timedelta_schema
timedelta_schema
strict
bool
None
None
timedelta
None
None
timedelta
None
None
timedelta
None
None
timedelta
None
None
microseconds_precision
Literal
"truncate"
"error"
"truncate"
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
TimedeltaSchema
Returns a schema that matches a timedelta value, e.g.:
from
datetime
import
timedelta
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
timedelta_schema
timedelta
days
timedelta
days
SchemaValidator
schema
assert
validate_python
timedelta
hours
timedelta
hours
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether the value should be a timedelta or a value that can be converted to a timedelta
None
timedelta
| None
The value must be less than or equal to this timedelta
None
timedelta
| None
The value must be greater than or equal to this timedelta
None
timedelta
| None
The value must be strictly less than this timedelta
None
timedelta
| None
The value must be strictly greater than this timedelta
None
microseconds_precision
Literal
['truncate', 'error']
The behavior when seconds have more than 6 digits or microseconds is too large
'truncate'
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1172
1173
1174
1175
1176
1177
1178
1179
1180
1181
1182
1183
1184
1185
1186
1187
1188
1189
1190
1191
1192
1193
1194
1195
1196
1197
1198
1199
1200
1201
1202
1203
1204
1205
1206
1207
1208
1209
1210
1211
1212
1213
1214
1215
1216
1217
1218
def
timedelta_schema
strict
bool
None
None
timedelta
None
None
timedelta
None
None
timedelta
None
None
timedelta
None
None
microseconds_precision
Literal
'truncate'
'error'
'truncate'
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
TimedeltaSchema
"""
Returns a schema that matches a timedelta value, e.g.:
```py
from datetime import timedelta
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.timedelta_schema(le=timedelta(days=1), ge=timedelta(days=0))
v = SchemaValidator(schema)
assert v.validate_python(timedelta(hours=12)) == timedelta(hours=12)
```
Args:
strict: Whether the value should be a timedelta or a value that can be converted to a timedelta
le: The value must be less than or equal to this timedelta
ge: The value must be greater than or equal to this timedelta
lt: The value must be strictly less than this timedelta
gt: The value must be strictly greater than this timedelta
microseconds_precision: The behavior when seconds have more than 6 digits or microseconds is too large
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'timedelta'
strict
strict
microseconds_precision
microseconds_precision
ref
ref
metadata
metadata
serialization
serialization
literal_schema
literal_schema
expected
list
Any
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
LiteralSchema
Returns a schema that matches a literal value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
literal_schema
'hello'
'world'
SchemaValidator
schema
assert
validate_python
'hello'
'hello'
Parameters:
Name
Type
Description
Default
expected
list
Any
The value must be one of these values
required
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1229
1230
1231
1232
1233
1234
1235
1236
1237
1238
1239
1240
1241
1242
1243
1244
1245
1246
1247
1248
1249
1250
1251
1252
1253
def
literal_schema
expected
list
Any
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
LiteralSchema
"""
Returns a schema that matches a literal value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.literal_schema(['hello', 'world'])
v = SchemaValidator(schema)
assert v.validate_python('hello') == 'hello'
```
Args:
expected: The value must be one of these values
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'literal'
expected
expected
ref
ref
metadata
metadata
serialization
serialization
enum_schema
enum_schema
cls
Any
members
list
Any
sub_type
Literal
"str"
"int"
"float"
None
None
missing
Callable
Any
Any
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
EnumSchema
Returns a schema that matches an enum value, e.g.:
from
enum
import
Enum
from
pydantic_core
import
SchemaValidator
core_schema
class
Color
Enum
RED
GREEN
BLUE
schema
core_schema
enum_schema
Color
list
Color
__members__
values
()))
SchemaValidator
schema
assert
validate_python
Color
GREEN
Parameters:
Name
Type
Description
Default
cls
Any
The enum class
required
members
list
Any
The members of the enum, generally
list(MyEnum.__members__.values())
required
sub_type
Literal
['str', 'int', 'float'] | None
The type of the enum, either 'str' or 'int' or None for plain enums
None
missing
Callable
Any
Any
] | None
A function to use when the value is not found in the enum, from
_missing_
None
strict
bool
| None
Whether to use strict mode, defaults to False
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1268
1269
1270
1271
1272
1273
1274
1275
1276
1277
1278
1279
1280
1281
1282
1283
1284
1285
1286
1287
1288
1289
1290
1291
1292
1293
1294
1295
1296
1297
1298
1299
1300
1301
1302
1303
1304
1305
1306
1307
1308
1309
1310
1311
1312
1313
1314
1315
1316
def
enum_schema
cls
Any
members
list
Any
sub_type
Literal
'str'
'int'
'float'
None
None
missing
Callable
Any
Any
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
EnumSchema
"""
Returns a schema that matches an enum value, e.g.:
```py
from enum import Enum
from pydantic_core import SchemaValidator, core_schema
class Color(Enum):
RED = 1
GREEN = 2
BLUE = 3
schema = core_schema.enum_schema(Color, list(Color.__members__.values()))
v = SchemaValidator(schema)
assert v.validate_python(2) is Color.GREEN
```
Args:
cls: The enum class
members: The members of the enum, generally `list(MyEnum.__members__.values())`
sub_type: The type of the enum, either 'str' or 'int' or None for plain enums
missing: A function to use when the value is not found in the enum, from `_missing_`
strict: Whether to use strict mode, defaults to False
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'enum'
cls
cls
members
members
sub_type
sub_type
missing
missing
strict
strict
ref
ref
metadata
metadata
serialization
serialization
is_instance_schema
is_instance_schema
cls
Any
cls_repr
str
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
IsInstanceSchema
Returns a schema that checks if a value is an instance of a class, equivalent to python's
isinstance
method, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
class
pass
schema
core_schema
is_instance_schema
cls
SchemaValidator
schema
validate_python
())
Parameters:
Name
Type
Description
Default
cls
Any
The value must be an instance of this class
required
cls_repr
str
| None
If provided this string is used in the validator name instead of
repr(cls)
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1332
1333
1334
1335
1336
1337
1338
1339
1340
1341
1342
1343
1344
1345
1346
1347
1348
1349
1350
1351
1352
1353
1354
1355
1356
1357
1358
1359
1360
1361
1362
1363
def
is_instance_schema
cls
Any
cls_repr
str
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
IsInstanceSchema
"""
Returns a schema that checks if a value is an instance of a class, equivalent to python's `isinstance` method, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
class A:
pass
schema = core_schema.is_instance_schema(cls=A)
v = SchemaValidator(schema)
v.validate_python(A())
```
Args:
cls: The value must be an instance of this class
cls_repr: If provided this string is used in the validator name instead of `repr(cls)`
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'is-instance'
cls
cls
cls_repr
cls_repr
ref
ref
metadata
metadata
serialization
serialization
is_subclass_schema
is_subclass_schema
cls
type
Any
cls_repr
str
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
IsInstanceSchema
Returns a schema that checks if a value is a subtype of a class, equivalent to python's
issubclass
method, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
class
pass
class
pass
schema
core_schema
is_subclass_schema
cls
SchemaValidator
schema
validate_python
Parameters:
Name
Type
Description
Default
cls
type
Any
The value must be a subclass of this class
required
cls_repr
str
| None
If provided this string is used in the validator name instead of
repr(cls)
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1375
1376
1377
1378
1379
1380
1381
1382
1383
1384
1385
1386
1387
1388
1389
1390
1391
1392
1393
1394
1395
1396
1397
1398
1399
1400
1401
1402
1403
1404
1405
1406
1407
1408
1409
def
is_subclass_schema
cls
type
Any
cls_repr
str
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
IsInstanceSchema
"""
Returns a schema that checks if a value is a subtype of a class, equivalent to python's `issubclass` method, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
class A:
pass
class B(A):
pass
schema = core_schema.is_subclass_schema(cls=A)
v = SchemaValidator(schema)
v.validate_python(B)
```
Args:
cls: The value must be a subclass of this class
cls_repr: If provided this string is used in the validator name instead of `repr(cls)`
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'is-subclass'
cls
cls
cls_repr
cls_repr
ref
ref
metadata
metadata
serialization
serialization
callable_schema
callable_schema
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
CallableSchema
Returns a schema that checks if a value is callable, equivalent to python's
callable
method, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
callable_schema
SchemaValidator
schema
validate_python
min
Parameters:
Name
Type
Description
Default
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1419
1420
1421
1422
1423
1424
1425
1426
1427
1428
1429
1430
1431
1432
1433
1434
1435
1436
1437
1438
def
callable_schema
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
CallableSchema
"""
Returns a schema that checks if a value is callable, equivalent to python's `callable` method, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.callable_schema()
v = SchemaValidator(schema)
v.validate_python(min)
```
Args:
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'callable'
ref
ref
metadata
metadata
serialization
serialization
list_schema
list_schema
items_schema
CoreSchema
None
None
min_length
int
None
None
max_length
int
None
None
fail_fast
bool
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
IncExSeqOrElseSerSchema
None
None
ListSchema
Returns a schema that matches a list value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
list_schema
core_schema
int_schema
(),
min_length
max_length
SchemaValidator
schema
assert
validate_python
'4'
Parameters:
Name
Type
Description
Default
items_schema
CoreSchema
| None
The value must be a list of items that match this schema
None
min_length
int
| None
The value must be a list with at least this many items
None
max_length
int
| None
The value must be a list with at most this many items
None
fail_fast
bool
| None
Stop validation on the first error
None
strict
bool
| None
The value must be a list with exactly this many items
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
IncExSeqOrElseSerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1488
1489
1490
1491
1492
1493
1494
1495
1496
1497
1498
1499
1500
1501
1502
1503
1504
1505
1506
1507
1508
1509
1510
1511
1512
1513
1514
1515
1516
1517
1518
1519
1520
1521
1522
1523
1524
1525
1526
1527
1528
1529
1530
def
list_schema
items_schema
CoreSchema
None
None
min_length
int
None
None
max_length
int
None
None
fail_fast
bool
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
IncExSeqOrElseSerSchema
None
None
ListSchema
"""
Returns a schema that matches a list value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.list_schema(core_schema.int_schema(), min_length=0, max_length=10)
v = SchemaValidator(schema)
assert v.validate_python(['4']) == [4]
```
Args:
items_schema: The value must be a list of items that match this schema
min_length: The value must be a list with at least this many items
max_length: The value must be a list with at most this many items
fail_fast: Stop validation on the first error
strict: The value must be a list with exactly this many items
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'list'
items_schema
items_schema
min_length
min_length
max_length
max_length
fail_fast
fail_fast
strict
strict
ref
ref
metadata
metadata
serialization
serialization
tuple_positional_schema
tuple_positional_schema
items_schema
list
CoreSchema
extras_schema
CoreSchema
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
IncExSeqOrElseSerSchema
None
None
TupleSchema
Returns a schema that matches a tuple of schemas, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
tuple_positional_schema
core_schema
int_schema
(),
core_schema
str_schema
()]
SchemaValidator
schema
assert
validate_python
'hello'
'hello'
Parameters:
Name
Type
Description
Default
items_schema
list
CoreSchema
The value must be a tuple with items that match these schemas
required
extras_schema
CoreSchema
| None
The value must be a tuple with items that match this schema
This was inspired by JSON schema's
prefixItems
and
items
fields.
In python's
typing.Tuple
, you can't specify a type for "extra" items -- they must all be the same type
if the length is variable. So this field won't be set from a
typing.Tuple
annotation on a pydantic model.
None
strict
bool
| None
The value must be a tuple with exactly this many items
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
IncExSeqOrElseSerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1534
1535
1536
1537
1538
1539
1540
1541
1542
1543
1544
1545
1546
1547
1548
1549
1550
1551
1552
1553
1554
1555
1556
1557
1558
1559
1560
1561
1562
1563
1564
1565
1566
1567
1568
1569
1570
1571
1572
1573
1574
1575
1576
1577
1578
1579
def
tuple_positional_schema
items_schema
list
CoreSchema
extras_schema
CoreSchema
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
IncExSeqOrElseSerSchema
None
None
TupleSchema
"""
Returns a schema that matches a tuple of schemas, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.tuple_positional_schema(
[core_schema.int_schema(), core_schema.str_schema()]
v = SchemaValidator(schema)
assert v.validate_python((1, 'hello')) == (1, 'hello')
```
Args:
items_schema: The value must be a tuple with items that match these schemas
extras_schema: The value must be a tuple with items that match this schema
This was inspired by JSON schema's `prefixItems` and `items` fields.
In python's `typing.Tuple`, you can't specify a type for "extra" items -- they must all be the same type
if the length is variable. So this field won't be set from a `typing.Tuple` annotation on a pydantic model.
strict: The value must be a tuple with exactly this many items
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
extras_schema
not
None
variadic_item_index
len
items_schema
items_schema
items_schema
extras_schema
else
variadic_item_index
None
return
tuple_schema
items_schema
items_schema
variadic_item_index
variadic_item_index
strict
strict
ref
ref
metadata
metadata
serialization
serialization
tuple_variable_schema
tuple_variable_schema
items_schema
CoreSchema
None
None
min_length
int
None
None
max_length
int
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
IncExSeqOrElseSerSchema
None
None
TupleSchema
Returns a schema that matches a tuple of a given schema, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
tuple_variable_schema
items_schema
core_schema
int_schema
(),
min_length
max_length
SchemaValidator
schema
assert
validate_python
'1'
Parameters:
Name
Type
Description
Default
items_schema
CoreSchema
| None
The value must be a tuple with items that match this schema
None
min_length
int
| None
The value must be a tuple with at least this many items
None
max_length
int
| None
The value must be a tuple with at most this many items
None
strict
bool
| None
The value must be a tuple with exactly this many items
None
ref
str
| None
Optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
IncExSeqOrElseSerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1583
1584
1585
1586
1587
1588
1589
1590
1591
1592
1593
1594
1595
1596
1597
1598
1599
1600
1601
1602
1603
1604
1605
1606
1607
1608
1609
1610
1611
1612
1613
1614
1615
1616
1617
1618
1619
1620
1621
1622
1623
1624
def
tuple_variable_schema
items_schema
CoreSchema
None
None
min_length
int
None
None
max_length
int
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
IncExSeqOrElseSerSchema
None
None
TupleSchema
"""
Returns a schema that matches a tuple of a given schema, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.tuple_variable_schema(
items_schema=core_schema.int_schema(), min_length=0, max_length=10
v = SchemaValidator(schema)
assert v.validate_python(('1', 2, 3)) == (1, 2, 3)
```
Args:
items_schema: The value must be a tuple with items that match this schema
min_length: The value must be a tuple with at least this many items
max_length: The value must be a tuple with at most this many items
strict: The value must be a tuple with exactly this many items
ref: Optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
tuple_schema
items_schema
items_schema
any_schema
()],
variadic_item_index
min_length
min_length
max_length
max_length
strict
strict
ref
ref
metadata
metadata
serialization
serialization
tuple_schema
tuple_schema
items_schema
list
CoreSchema
variadic_item_index
int
None
None
min_length
int
None
None
max_length
int
None
None
fail_fast
bool
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
IncExSeqOrElseSerSchema
None
None
TupleSchema
Returns a schema that matches a tuple of schemas, with an optional variadic item, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
tuple_schema
core_schema
int_schema
(),
core_schema
str_schema
(),
core_schema
float_schema
()],
variadic_item_index
SchemaValidator
schema
assert
validate_python
'hello'
'world'
1.5
'hello'
'world'
1.5
Parameters:
Name
Type
Description
Default
items_schema
list
CoreSchema
The value must be a tuple with items that match these schemas
required
variadic_item_index
int
| None
The index of the schema in
items_schema
to be treated as variadic (following PEP 646)
None
min_length
int
| None
The value must be a tuple with at least this many items
None
max_length
int
| None
The value must be a tuple with at most this many items
None
fail_fast
bool
| None
Stop validation on the first error
None
strict
bool
| None
The value must be a tuple with exactly this many items
None
ref
str
| None
Optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
IncExSeqOrElseSerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1640
1641
1642
1643
1644
1645
1646
1647
1648
1649
1650
1651
1652
1653
1654
1655
1656
1657
1658
1659
1660
1661
1662
1663
1664
1665
1666
1667
1668
1669
1670
1671
1672
1673
1674
1675
1676
1677
1678
1679
1680
1681
1682
1683
1684
1685
1686
1687
1688
def
tuple_schema
items_schema
list
CoreSchema
variadic_item_index
int
None
None
min_length
int
None
None
max_length
int
None
None
fail_fast
bool
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
IncExSeqOrElseSerSchema
None
None
TupleSchema
"""
Returns a schema that matches a tuple of schemas, with an optional variadic item, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.tuple_schema(
[core_schema.int_schema(), core_schema.str_schema(), core_schema.float_schema()],
variadic_item_index=1,
v = SchemaValidator(schema)
assert v.validate_python((1, 'hello', 'world', 1.5)) == (1, 'hello', 'world', 1.5)
```
Args:
items_schema: The value must be a tuple with items that match these schemas
variadic_item_index: The index of the schema in `items_schema` to be treated as variadic (following PEP 646)
min_length: The value must be a tuple with at least this many items
max_length: The value must be a tuple with at most this many items
fail_fast: Stop validation on the first error
strict: The value must be a tuple with exactly this many items
ref: Optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'tuple'
items_schema
items_schema
variadic_item_index
variadic_item_index
min_length
min_length
max_length
max_length
fail_fast
fail_fast
strict
strict
ref
ref
metadata
metadata
serialization
serialization
set_schema
set_schema
items_schema
CoreSchema
None
None
min_length
int
None
None
max_length
int
None
None
fail_fast
bool
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
SetSchema
Returns a schema that matches a set of a given schema, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
set_schema
items_schema
core_schema
int_schema
(),
min_length
max_length
SchemaValidator
schema
assert
validate_python
'2'
Parameters:
Name
Type
Description
Default
items_schema
CoreSchema
| None
The value must be a set with items that match this schema
None
min_length
int
| None
The value must be a set with at least this many items
None
max_length
int
| None
The value must be a set with at most this many items
None
fail_fast
bool
| None
Stop validation on the first error
None
strict
bool
| None
The value must be a set with exactly this many items
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1703
1704
1705
1706
1707
1708
1709
1710
1711
1712
1713
1714
1715
1716
1717
1718
1719
1720
1721
1722
1723
1724
1725
1726
1727
1728
1729
1730
1731
1732
1733
1734
1735
1736
1737
1738
1739
1740
1741
1742
1743
1744
1745
1746
1747
def
set_schema
items_schema
CoreSchema
None
None
min_length
int
None
None
max_length
int
None
None
fail_fast
bool
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
SetSchema
"""
Returns a schema that matches a set of a given schema, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.set_schema(
items_schema=core_schema.int_schema(), min_length=0, max_length=10
v = SchemaValidator(schema)
assert v.validate_python({1, '2', 3}) == {1, 2, 3}
```
Args:
items_schema: The value must be a set with items that match this schema
min_length: The value must be a set with at least this many items
max_length: The value must be a set with at most this many items
fail_fast: Stop validation on the first error
strict: The value must be a set with exactly this many items
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'set'
items_schema
items_schema
min_length
min_length
max_length
max_length
fail_fast
fail_fast
strict
strict
ref
ref
metadata
metadata
serialization
serialization
frozenset_schema
frozenset_schema
items_schema
CoreSchema
None
None
min_length
int
None
None
max_length
int
None
None
fail_fast
bool
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
FrozenSetSchema
Returns a schema that matches a frozenset of a given schema, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
frozenset_schema
items_schema
core_schema
int_schema
(),
min_length
max_length
SchemaValidator
schema
assert
validate_python
frozenset
range
)))
frozenset
Parameters:
Name
Type
Description
Default
items_schema
CoreSchema
| None
The value must be a frozenset with items that match this schema
None
min_length
int
| None
The value must be a frozenset with at least this many items
None
max_length
int
| None
The value must be a frozenset with at most this many items
None
fail_fast
bool
| None
Stop validation on the first error
None
strict
bool
| None
The value must be a frozenset with exactly this many items
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1762
1763
1764
1765
1766
1767
1768
1769
1770
1771
1772
1773
1774
1775
1776
1777
1778
1779
1780
1781
1782
1783
1784
1785
1786
1787
1788
1789
1790
1791
1792
1793
1794
1795
1796
1797
1798
1799
1800
1801
1802
1803
1804
1805
1806
def
frozenset_schema
items_schema
CoreSchema
None
None
min_length
int
None
None
max_length
int
None
None
fail_fast
bool
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
FrozenSetSchema
"""
Returns a schema that matches a frozenset of a given schema, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.frozenset_schema(
items_schema=core_schema.int_schema(), min_length=0, max_length=10
v = SchemaValidator(schema)
assert v.validate_python(frozenset(range(3))) == frozenset({0, 1, 2})
```
Args:
items_schema: The value must be a frozenset with items that match this schema
min_length: The value must be a frozenset with at least this many items
max_length: The value must be a frozenset with at most this many items
fail_fast: Stop validation on the first error
strict: The value must be a frozenset with exactly this many items
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'frozenset'
items_schema
items_schema
min_length
min_length
max_length
max_length
fail_fast
fail_fast
strict
strict
ref
ref
metadata
metadata
serialization
serialization
generator_schema
generator_schema
items_schema
CoreSchema
None
None
min_length
int
None
None
max_length
int
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
IncExSeqOrElseSerSchema
None
None
GeneratorSchema
Returns a schema that matches a generator value, e.g.:
from
typing
import
Iterator
from
pydantic_core
import
SchemaValidator
core_schema
def
gen
Iterator
int
yield
schema
core_schema
generator_schema
items_schema
core_schema
int_schema
())
SchemaValidator
schema
validate_python
gen
())
Unlike other types, validated generators do not raise ValidationErrors eagerly,
but instead will raise a ValidationError when a violating value is actually read from the generator.
This is to ensure that "validated" generators retain the benefit of lazy evaluation.
Parameters:
Name
Type
Description
Default
items_schema
CoreSchema
| None
The value must be a generator with items that match this schema
None
min_length
int
| None
The value must be a generator that yields at least this many items
None
max_length
int
| None
The value must be a generator that yields at most this many items
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
IncExSeqOrElseSerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1819
1820
1821
1822
1823
1824
1825
1826
1827
1828
1829
1830
1831
1832
1833
1834
1835
1836
1837
1838
1839
1840
1841
1842
1843
1844
1845
1846
1847
1848
1849
1850
1851
1852
1853
1854
1855
1856
1857
1858
1859
1860
1861
1862
1863
def
generator_schema
items_schema
CoreSchema
None
None
min_length
int
None
None
max_length
int
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
IncExSeqOrElseSerSchema
None
None
GeneratorSchema
"""
Returns a schema that matches a generator value, e.g.:
```py
from typing import Iterator
from pydantic_core import SchemaValidator, core_schema
def gen() -> Iterator[int]:
yield 1
schema = core_schema.generator_schema(items_schema=core_schema.int_schema())
v = SchemaValidator(schema)
v.validate_python(gen())
```
Unlike other types, validated generators do not raise ValidationErrors eagerly,
but instead will raise a ValidationError when a violating value is actually read from the generator.
This is to ensure that "validated" generators retain the benefit of lazy evaluation.
Args:
items_schema: The value must be a generator with items that match this schema
min_length: The value must be a generator that yields at least this many items
max_length: The value must be a generator that yields at most this many items
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'generator'
items_schema
items_schema
min_length
min_length
max_length
max_length
ref
ref
metadata
metadata
serialization
serialization
dict_schema
dict_schema
keys_schema
CoreSchema
None
None
values_schema
CoreSchema
None
None
min_length
int
None
None
max_length
int
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
DictSchema
Returns a schema that matches a dict value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
dict_schema
keys_schema
core_schema
str_schema
(),
values_schema
core_schema
int_schema
SchemaValidator
schema
assert
validate_python
'a'
'1'
'b'
'a'
'b'
Parameters:
Name
Type
Description
Default
keys_schema
CoreSchema
| None
The value must be a dict with keys that match this schema
None
values_schema
CoreSchema
| None
The value must be a dict with values that match this schema
None
min_length
int
| None
The value must be a dict with at least this many items
None
max_length
int
| None
The value must be a dict with at most this many items
None
strict
bool
| None
Whether the keys and values should be validated with strict mode
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1894
1895
1896
1897
1898
1899
1900
1901
1902
1903
1904
1905
1906
1907
1908
1909
1910
1911
1912
1913
1914
1915
1916
1917
1918
1919
1920
1921
1922
1923
1924
1925
1926
1927
1928
1929
1930
1931
1932
1933
1934
1935
1936
1937
1938
def
dict_schema
keys_schema
CoreSchema
None
None
values_schema
CoreSchema
None
None
min_length
int
None
None
max_length
int
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
DictSchema
"""
Returns a schema that matches a dict value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.dict_schema(
keys_schema=core_schema.str_schema(), values_schema=core_schema.int_schema()
v = SchemaValidator(schema)
assert v.validate_python({'a': '1', 'b': 2}) == {'a': 1, 'b': 2}
```
Args:
keys_schema: The value must be a dict with keys that match this schema
values_schema: The value must be a dict with values that match this schema
min_length: The value must be a dict with at least this many items
max_length: The value must be a dict with at most this many items
strict: Whether the keys and values should be validated with strict mode
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'dict'
keys_schema
keys_schema
values_schema
values_schema
min_length
min_length
max_length
max_length
strict
strict
ref
ref
metadata
metadata
serialization
serialization
no_info_before_validator_function
no_info_before_validator_function
function
NoInfoValidatorFunction
schema
CoreSchema
ref
str
None
None
json_schema_input_schema
CoreSchema
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
BeforeValidatorFunctionSchema
Returns a schema that calls a validator function before validating, no
info
argument is provided, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
def
bytes
str
return
decode
'world'
func_schema
core_schema
no_info_before_validator_function
function
schema
core_schema
str_schema
schema
core_schema
typed_dict_schema
'a'
core_schema
typed_dict_field
func_schema
)})
SchemaValidator
schema
assert
validate_python
'a'
'hello '
'a'
'hello world'
Parameters:
Name
Type
Description
Default
function
NoInfoValidatorFunction
The validator function to call
required
schema
CoreSchema
The schema to validate the output of the validator function
required
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
json_schema_input_schema
CoreSchema
| None
The core schema to be used to generate the corresponding JSON Schema input type
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
1976
1977
1978
1979
1980
1981
1982
1983
1984
1985
1986
1987
1988
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
2014
2015
2016
2017
2018
2019
def
no_info_before_validator_function
function
NoInfoValidatorFunction
schema
CoreSchema
ref
str
None
None
json_schema_input_schema
CoreSchema
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
BeforeValidatorFunctionSchema
"""
Returns a schema that calls a validator function before validating, no `info` argument is provided, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
def fn(v: bytes) -> str:
return v.decode() + 'world'
func_schema = core_schema.no_info_before_validator_function(
function=fn, schema=core_schema.str_schema()
schema = core_schema.typed_dict_schema({'a': core_schema.typed_dict_field(func_schema)})
v = SchemaValidator(schema)
assert v.validate_python({'a': b'hello '}) == {'a': 'hello world'}
```
Args:
function: The validator function to call
schema: The schema to validate the output of the validator function
ref: optional unique identifier of the schema, used to reference the schema in other places
json_schema_input_schema: The core schema to be used to generate the corresponding JSON Schema input type
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'function-before'
function
'type'
'no-info'
'function'
function
schema
schema
ref
ref
json_schema_input_schema
json_schema_input_schema
metadata
metadata
serialization
serialization
with_info_before_validator_function
with_info_before_validator_function
function
WithInfoValidatorFunction
schema
CoreSchema
field_name
str
None
None
ref
str
None
None
json_schema_input_schema
CoreSchema
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
BeforeValidatorFunctionSchema
Returns a schema that calls a validator function before validation, the function is called with
info
argument, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
def
bytes
info
core_schema
ValidationInfo
str
assert
info
data
not
None
assert
info
field_name
not
None
return
decode
'world'
func_schema
core_schema
with_info_before_validator_function
function
schema
core_schema
str_schema
(),
field_name
'a'
schema
core_schema
typed_dict_schema
'a'
core_schema
typed_dict_field
func_schema
)})
SchemaValidator
schema
assert
validate_python
'a'
'hello '
'a'
'hello world'
Parameters:
Name
Type
Description
Default
function
WithInfoValidatorFunction
The validator function to call
required
field_name
str
| None
The name of the field
None
schema
CoreSchema
The schema to validate the output of the validator function
required
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
json_schema_input_schema
CoreSchema
| None
The core schema to be used to generate the corresponding JSON Schema input type
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2022
2023
2024
2025
2026
2027
2028
2029
2030
2031
2032
2033
2034
2035
2036
2037
2038
2039
2040
2041
2042
2043
2044
2045
2046
2047
2048
2049
2050
2051
2052
2053
2054
2055
2056
2057
2058
2059
2060
2061
2062
2063
2064
2065
2066
2067
2068
2069
2070
def
with_info_before_validator_function
function
WithInfoValidatorFunction
schema
CoreSchema
field_name
str
None
None
ref
str
None
None
json_schema_input_schema
CoreSchema
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
BeforeValidatorFunctionSchema
"""
Returns a schema that calls a validator function before validation, the function is called with
an `info` argument, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
def fn(v: bytes, info: core_schema.ValidationInfo) -> str:
assert info.data is not None
assert info.field_name is not None
return v.decode() + 'world'
func_schema = core_schema.with_info_before_validator_function(
function=fn, schema=core_schema.str_schema(), field_name='a'
schema = core_schema.typed_dict_schema({'a': core_schema.typed_dict_field(func_schema)})
v = SchemaValidator(schema)
assert v.validate_python({'a': b'hello '}) == {'a': 'hello world'}
```
Args:
function: The validator function to call
field_name: The name of the field
schema: The schema to validate the output of the validator function
ref: optional unique identifier of the schema, used to reference the schema in other places
json_schema_input_schema: The core schema to be used to generate the corresponding JSON Schema input type
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'function-before'
function
_dict_not_none
type
'with-info'
function
function
field_name
field_name
schema
schema
ref
ref
json_schema_input_schema
json_schema_input_schema
metadata
metadata
serialization
serialization
no_info_after_validator_function
no_info_after_validator_function
function
NoInfoValidatorFunction
schema
CoreSchema
ref
str
None
None
json_schema_input_schema
CoreSchema
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
AfterValidatorFunctionSchema
Returns a schema that calls a validator function after validating, no
info
argument is provided, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
def
str
str
return
'world'
func_schema
core_schema
no_info_after_validator_function
core_schema
str_schema
())
schema
core_schema
typed_dict_schema
'a'
core_schema
typed_dict_field
func_schema
)})
SchemaValidator
schema
assert
validate_python
'a'
'hello '
'a'
'hello world'
Parameters:
Name
Type
Description
Default
function
NoInfoValidatorFunction
The validator function to call after the schema is validated
required
schema
CoreSchema
The schema to validate before the validator function
required
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
json_schema_input_schema
CoreSchema
| None
The core schema to be used to generate the corresponding JSON Schema input type
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2077
2078
2079
2080
2081
2082
2083
2084
2085
2086
2087
2088
2089
2090
2091
2092
2093
2094
2095
2096
2097
2098
2099
2100
2101
2102
2103
2104
2105
2106
2107
2108
2109
2110
2111
2112
2113
2114
2115
2116
2117
2118
def
no_info_after_validator_function
function
NoInfoValidatorFunction
schema
CoreSchema
ref
str
None
None
json_schema_input_schema
CoreSchema
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
AfterValidatorFunctionSchema
"""
Returns a schema that calls a validator function after validating, no `info` argument is provided, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
def fn(v: str) -> str:
return v + 'world'
func_schema = core_schema.no_info_after_validator_function(fn, core_schema.str_schema())
schema = core_schema.typed_dict_schema({'a': core_schema.typed_dict_field(func_schema)})
v = SchemaValidator(schema)
assert v.validate_python({'a': b'hello '}) == {'a': 'hello world'}
```
Args:
function: The validator function to call after the schema is validated
schema: The schema to validate before the validator function
ref: optional unique identifier of the schema, used to reference the schema in other places
json_schema_input_schema: The core schema to be used to generate the corresponding JSON Schema input type
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'function-after'
function
'type'
'no-info'
'function'
function
schema
schema
ref
ref
json_schema_input_schema
json_schema_input_schema
metadata
metadata
serialization
serialization
with_info_after_validator_function
with_info_after_validator_function
function
WithInfoValidatorFunction
schema
CoreSchema
field_name
str
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
AfterValidatorFunctionSchema
Returns a schema that calls a validator function after validation, the function is called with
info
argument, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
def
str
info
core_schema
ValidationInfo
str
assert
info
data
not
None
assert
info
field_name
not
None
return
'world'
func_schema
core_schema
with_info_after_validator_function
function
schema
core_schema
str_schema
(),
field_name
'a'
schema
core_schema
typed_dict_schema
'a'
core_schema
typed_dict_field
func_schema
)})
SchemaValidator
schema
assert
validate_python
'a'
'hello '
'a'
'hello world'
Parameters:
Name
Type
Description
Default
function
WithInfoValidatorFunction
The validator function to call after the schema is validated
required
schema
CoreSchema
The schema to validate before the validator function
required
field_name
str
| None
The name of the field this validators is applied to, if any
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2121
2122
2123
2124
2125
2126
2127
2128
2129
2130
2131
2132
2133
2134
2135
2136
2137
2138
2139
2140
2141
2142
2143
2144
2145
2146
2147
2148
2149
2150
2151
2152
2153
2154
2155
2156
2157
2158
2159
2160
2161
2162
2163
2164
2165
2166
def
with_info_after_validator_function
function
WithInfoValidatorFunction
schema
CoreSchema
field_name
str
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
AfterValidatorFunctionSchema
"""
Returns a schema that calls a validator function after validation, the function is called with
an `info` argument, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
def fn(v: str, info: core_schema.ValidationInfo) -> str:
assert info.data is not None
assert info.field_name is not None
return v + 'world'
func_schema = core_schema.with_info_after_validator_function(
function=fn, schema=core_schema.str_schema(), field_name='a'
schema = core_schema.typed_dict_schema({'a': core_schema.typed_dict_field(func_schema)})
v = SchemaValidator(schema)
assert v.validate_python({'a': b'hello '}) == {'a': 'hello world'}
```
Args:
function: The validator function to call after the schema is validated
schema: The schema to validate before the validator function
field_name: The name of the field this validators is applied to, if any
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'function-after'
function
_dict_not_none
type
'with-info'
function
function
field_name
field_name
schema
schema
ref
ref
metadata
metadata
serialization
serialization
no_info_wrap_validator_function
no_info_wrap_validator_function
function
NoInfoWrapValidatorFunction
schema
CoreSchema
ref
str
None
None
json_schema_input_schema
CoreSchema
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
WrapValidatorFunctionSchema
Returns a schema which calls a function with a
validator
callable argument which can
optionally be used to call inner validation with the function logic, this is much like the
"onion" implementation of middleware in many popular web frameworks, no
info
argument is passed, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
def
str
validator
core_schema
ValidatorFunctionWrapHandler
str
return
validator
input_value
'world'
schema
core_schema
no_info_wrap_validator_function
function
schema
core_schema
str_schema
SchemaValidator
schema
assert
validate_python
'hello '
'hello world'
Parameters:
Name
Type
Description
Default
function
NoInfoWrapValidatorFunction
The validator function to call
required
schema
CoreSchema
The schema to validate the output of the validator function
required
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
json_schema_input_schema
CoreSchema
| None
The core schema to be used to generate the corresponding JSON Schema input type
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2206
2207
2208
2209
2210
2211
2212
2213
2214
2215
2216
2217
2218
2219
2220
2221
2222
2223
2224
2225
2226
2227
2228
2229
2230
2231
2232
2233
2234
2235
2236
2237
2238
2239
2240
2241
2242
2243
2244
2245
2246
2247
2248
2249
2250
2251
2252
def
no_info_wrap_validator_function
function
NoInfoWrapValidatorFunction
schema
CoreSchema
ref
str
None
None
json_schema_input_schema
CoreSchema
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
WrapValidatorFunctionSchema
"""
Returns a schema which calls a function with a `validator` callable argument which can
optionally be used to call inner validation with the function logic, this is much like the
"onion" implementation of middleware in many popular web frameworks, no `info` argument is passed, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
def fn(
v: str,
validator: core_schema.ValidatorFunctionWrapHandler,
) -> str:
return validator(input_value=v) + 'world'
schema = core_schema.no_info_wrap_validator_function(
function=fn, schema=core_schema.str_schema()
v = SchemaValidator(schema)
assert v.validate_python('hello ') == 'hello world'
```
Args:
function: The validator function to call
schema: The schema to validate the output of the validator function
ref: optional unique identifier of the schema, used to reference the schema in other places
json_schema_input_schema: The core schema to be used to generate the corresponding JSON Schema input type
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'function-wrap'
function
'type'
'no-info'
'function'
function
schema
schema
json_schema_input_schema
json_schema_input_schema
ref
ref
metadata
metadata
serialization
serialization
with_info_wrap_validator_function
with_info_wrap_validator_function
function
WithInfoWrapValidatorFunction
schema
CoreSchema
field_name
str
None
None
json_schema_input_schema
CoreSchema
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
WrapValidatorFunctionSchema
Returns a schema which calls a function with a
validator
callable argument which can
optionally be used to call inner validation with the function logic, this is much like the
"onion" implementation of middleware in many popular web frameworks, an
info
argument is also passed, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
def
str
validator
core_schema
ValidatorFunctionWrapHandler
info
core_schema
ValidationInfo
str
return
validator
input_value
'world'
schema
core_schema
with_info_wrap_validator_function
function
schema
core_schema
str_schema
SchemaValidator
schema
assert
validate_python
'hello '
'hello world'
Parameters:
Name
Type
Description
Default
function
WithInfoWrapValidatorFunction
The validator function to call
required
schema
CoreSchema
The schema to validate the output of the validator function
required
field_name
str
| None
The name of the field this validators is applied to, if any
None
json_schema_input_schema
CoreSchema
| None
The core schema to be used to generate the corresponding JSON Schema input type
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2255
2256
2257
2258
2259
2260
2261
2262
2263
2264
2265
2266
2267
2268
2269
2270
2271
2272
2273
2274
2275
2276
2277
2278
2279
2280
2281
2282
2283
2284
2285
2286
2287
2288
2289
2290
2291
2292
2293
2294
2295
2296
2297
2298
2299
2300
2301
2302
2303
2304
def
with_info_wrap_validator_function
function
WithInfoWrapValidatorFunction
schema
CoreSchema
field_name
str
None
None
json_schema_input_schema
CoreSchema
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
WrapValidatorFunctionSchema
"""
Returns a schema which calls a function with a `validator` callable argument which can
optionally be used to call inner validation with the function logic, this is much like the
"onion" implementation of middleware in many popular web frameworks, an `info` argument is also passed, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
def fn(
v: str,
validator: core_schema.ValidatorFunctionWrapHandler,
info: core_schema.ValidationInfo,
) -> str:
return validator(input_value=v) + 'world'
schema = core_schema.with_info_wrap_validator_function(
function=fn, schema=core_schema.str_schema()
v = SchemaValidator(schema)
assert v.validate_python('hello ') == 'hello world'
```
Args:
function: The validator function to call
schema: The schema to validate the output of the validator function
field_name: The name of the field this validators is applied to, if any
json_schema_input_schema: The core schema to be used to generate the corresponding JSON Schema input type
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'function-wrap'
function
_dict_not_none
type
'with-info'
function
function
field_name
field_name
schema
schema
json_schema_input_schema
json_schema_input_schema
ref
ref
metadata
metadata
serialization
serialization
no_info_plain_validator_function
no_info_plain_validator_function
function
NoInfoValidatorFunction
ref
str
None
None
json_schema_input_schema
CoreSchema
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
PlainValidatorFunctionSchema
Returns a schema that uses the provided function for validation, no
info
argument is passed, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
def
str
str
assert
'hello'
return
'world'
schema
core_schema
no_info_plain_validator_function
function
SchemaValidator
schema
assert
validate_python
'hello '
'hello world'
Parameters:
Name
Type
Description
Default
function
NoInfoValidatorFunction
The validator function to call
required
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
json_schema_input_schema
CoreSchema
| None
The core schema to be used to generate the corresponding JSON Schema input type
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2316
2317
2318
2319
2320
2321
2322
2323
2324
2325
2326
2327
2328
2329
2330
2331
2332
2333
2334
2335
2336
2337
2338
2339
2340
2341
2342
2343
2344
2345
2346
2347
2348
2349
2350
2351
2352
2353
def
no_info_plain_validator_function
function
NoInfoValidatorFunction
ref
str
None
None
json_schema_input_schema
CoreSchema
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
PlainValidatorFunctionSchema
"""
Returns a schema that uses the provided function for validation, no `info` argument is passed, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
def fn(v: str) -> str:
assert 'hello' in v
return v + 'world'
schema = core_schema.no_info_plain_validator_function(function=fn)
v = SchemaValidator(schema)
assert v.validate_python('hello ') == 'hello world'
```
Args:
function: The validator function to call
ref: optional unique identifier of the schema, used to reference the schema in other places
json_schema_input_schema: The core schema to be used to generate the corresponding JSON Schema input type
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'function-plain'
function
'type'
'no-info'
'function'
function
ref
ref
json_schema_input_schema
json_schema_input_schema
metadata
metadata
serialization
serialization
with_info_plain_validator_function
with_info_plain_validator_function
function
WithInfoValidatorFunction
field_name
str
None
None
ref
str
None
None
json_schema_input_schema
CoreSchema
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
PlainValidatorFunctionSchema
Returns a schema that uses the provided function for validation, an
info
argument is passed, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
def
str
info
core_schema
ValidationInfo
str
assert
'hello'
return
'world'
schema
core_schema
with_info_plain_validator_function
function
SchemaValidator
schema
assert
validate_python
'hello '
'hello world'
Parameters:
Name
Type
Description
Default
function
WithInfoValidatorFunction
The validator function to call
required
field_name
str
| None
The name of the field this validators is applied to, if any
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
json_schema_input_schema
CoreSchema
| None
The core schema to be used to generate the corresponding JSON Schema input type
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2356
2357
2358
2359
2360
2361
2362
2363
2364
2365
2366
2367
2368
2369
2370
2371
2372
2373
2374
2375
2376
2377
2378
2379
2380
2381
2382
2383
2384
2385
2386
2387
2388
2389
2390
2391
2392
2393
2394
2395
def
with_info_plain_validator_function
function
WithInfoValidatorFunction
field_name
str
None
None
ref
str
None
None
json_schema_input_schema
CoreSchema
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
PlainValidatorFunctionSchema
"""
Returns a schema that uses the provided function for validation, an `info` argument is passed, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
def fn(v: str, info: core_schema.ValidationInfo) -> str:
assert 'hello' in v
return v + 'world'
schema = core_schema.with_info_plain_validator_function(function=fn)
v = SchemaValidator(schema)
assert v.validate_python('hello ') == 'hello world'
```
Args:
function: The validator function to call
field_name: The name of the field this validators is applied to, if any
ref: optional unique identifier of the schema, used to reference the schema in other places
json_schema_input_schema: The core schema to be used to generate the corresponding JSON Schema input type
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'function-plain'
function
_dict_not_none
type
'with-info'
function
function
field_name
field_name
ref
ref
json_schema_input_schema
json_schema_input_schema
metadata
metadata
serialization
serialization
with_default_schema
with_default_schema
schema
CoreSchema
default
Any
PydanticUndefined
default_factory
Union
Callable
[[],
Any
Callable
dict
str
Any
]],
Any
None
None
default_factory_takes_data
bool
None
None
on_error
Literal
"raise"
"omit"
"default"
None
None
validate_default
bool
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
WithDefaultSchema
Returns a schema that adds a default value to the given schema, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
with_default_schema
core_schema
str_schema
(),
default
'hello'
wrapper_schema
core_schema
typed_dict_schema
'a'
core_schema
typed_dict_field
schema
SchemaValidator
wrapper_schema
assert
validate_python
({})
validate_python
'a'
'hello'
Parameters:
Name
Type
Description
Default
schema
CoreSchema
The schema to add a default value to
required
default
Any
The default value to use
PydanticUndefined
default_factory
Union
Callable
[[],
Any
Callable
dict
str
Any
]],
Any
], None]
A callable that returns the default value to use
None
default_factory_takes_data
bool
| None
Whether the default factory takes a validated data argument
None
on_error
Literal
['raise', 'omit', 'default'] | None
What to do if the schema validation fails. One of 'raise', 'omit', 'default'
None
validate_default
bool
| None
Whether the default value should be validated
None
strict
bool
| None
Whether the underlying schema should be validated with strict mode
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2412
2413
2414
2415
2416
2417
2418
2419
2420
2421
2422
2423
2424
2425
2426
2427
2428
2429
2430
2431
2432
2433
2434
2435
2436
2437
2438
2439
2440
2441
2442
2443
2444
2445
2446
2447
2448
2449
2450
2451
2452
2453
2454
2455
2456
2457
2458
2459
2460
2461
2462
2463
2464
2465
def
with_default_schema
schema
CoreSchema
default
Any
PydanticUndefined
default_factory
Union
Callable
[[],
Any
Callable
dict
str
Any
]],
Any
None
None
default_factory_takes_data
bool
None
None
on_error
Literal
'raise'
'omit'
'default'
None
None
validate_default
bool
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
WithDefaultSchema
"""
Returns a schema that adds a default value to the given schema, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.with_default_schema(core_schema.str_schema(), default='hello')
wrapper_schema = core_schema.typed_dict_schema(
{'a': core_schema.typed_dict_field(schema)}
v = SchemaValidator(wrapper_schema)
assert v.validate_python({}) == v.validate_python({'a': 'hello'})
```
Args:
schema: The schema to add a default value to
default: The default value to use
default_factory: A callable that returns the default value to use
default_factory_takes_data: Whether the default factory takes a validated data argument
on_error: What to do if the schema validation fails. One of 'raise', 'omit', 'default'
validate_default: Whether the default value should be validated
strict: Whether the underlying schema should be validated with strict mode
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
_dict_not_none
type
'default'
schema
schema
default_factory
default_factory
default_factory_takes_data
default_factory_takes_data
on_error
on_error
validate_default
validate_default
strict
strict
ref
ref
metadata
metadata
serialization
serialization
default
not
PydanticUndefined
'default'
default
return
nullable_schema
nullable_schema
schema
CoreSchema
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
NullableSchema
Returns a schema that matches a nullable value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
nullable_schema
core_schema
str_schema
())
SchemaValidator
schema
assert
validate_python
None
None
Parameters:
Name
Type
Description
Default
schema
CoreSchema
The schema to wrap
required
strict
bool
| None
Whether the underlying schema should be validated with strict mode
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2477
2478
2479
2480
2481
2482
2483
2484
2485
2486
2487
2488
2489
2490
2491
2492
2493
2494
2495
2496
2497
2498
2499
2500
2501
2502
2503
2504
2505
def
nullable_schema
schema
CoreSchema
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
NullableSchema
"""
Returns a schema that matches a nullable value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.nullable_schema(core_schema.str_schema())
v = SchemaValidator(schema)
assert v.validate_python(None) is None
```
Args:
schema: The schema to wrap
strict: Whether the underlying schema should be validated with strict mode
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'nullable'
schema
schema
strict
strict
ref
ref
metadata
metadata
serialization
serialization
union_schema
union_schema
choices
list
CoreSchema
tuple
CoreSchema
str
]],
auto_collapse
bool
None
None
custom_error_type
str
None
None
custom_error_message
str
None
None
custom_error_context
dict
str
str
int
None
None
mode
Literal
"smart"
"left_to_right"
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
UnionSchema
Returns a schema that matches a union value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
union_schema
core_schema
str_schema
(),
core_schema
int_schema
()])
SchemaValidator
schema
assert
validate_python
'hello'
'hello'
assert
validate_python
Parameters:
Name
Type
Description
Default
choices
list
CoreSchema
tuple
CoreSchema
str
The schemas to match. If a tuple, the second item is used as the label for the case.
required
auto_collapse
bool
| None
whether to automatically collapse unions with one element to the inner validator, default true
None
custom_error_type
str
| None
The custom error type to use if the validation fails
None
custom_error_message
str
| None
The custom error message to use if the validation fails
None
custom_error_context
dict
str
str
int
] | None
The custom error context to use if the validation fails
None
mode
Literal
['smart', 'left_to_right'] | None
How to select which choice to return
smart
(default) will try to return the choice which is the closest match to the input value
left_to_right
will return the first choice in
choices
which succeeds validation
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2523
2524
2525
2526
2527
2528
2529
2530
2531
2532
2533
2534
2535
2536
2537
2538
2539
2540
2541
2542
2543
2544
2545
2546
2547
2548
2549
2550
2551
2552
2553
2554
2555
2556
2557
2558
2559
2560
2561
2562
2563
2564
2565
2566
2567
2568
2569
2570
2571
def
union_schema
choices
list
CoreSchema
tuple
CoreSchema
str
]],
auto_collapse
bool
None
None
custom_error_type
str
None
None
custom_error_message
str
None
None
custom_error_context
dict
str
str
int
None
None
mode
Literal
'smart'
'left_to_right'
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
UnionSchema
"""
Returns a schema that matches a union value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.union_schema([core_schema.str_schema(), core_schema.int_schema()])
v = SchemaValidator(schema)
assert v.validate_python('hello') == 'hello'
assert v.validate_python(1) == 1
```
Args:
choices: The schemas to match. If a tuple, the second item is used as the label for the case.
auto_collapse: whether to automatically collapse unions with one element to the inner validator, default true
custom_error_type: The custom error type to use if the validation fails
custom_error_message: The custom error message to use if the validation fails
custom_error_context: The custom error context to use if the validation fails
mode: How to select which choice to return
* `smart` (default) will try to return the choice which is the closest match to the input value
* `left_to_right` will return the first choice in `choices` which succeeds validation
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'union'
choices
choices
auto_collapse
auto_collapse
custom_error_type
custom_error_type
custom_error_message
custom_error_message
custom_error_context
custom_error_context
mode
mode
ref
ref
metadata
metadata
serialization
serialization
tagged_union_schema
tagged_union_schema
choices
dict
Any
CoreSchema
discriminator
str
list
str
int
list
list
str
int
Callable
Any
Any
custom_error_type
str
None
None
custom_error_message
str
None
None
custom_error_context
dict
str
int
str
float
None
None
strict
bool
None
None
from_attributes
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
TaggedUnionSchema
Returns a schema that matches a tagged union value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
apple_schema
core_schema
typed_dict_schema
'foo'
core_schema
typed_dict_field
core_schema
str_schema
()),
'bar'
core_schema
typed_dict_field
core_schema
int_schema
()),
banana_schema
core_schema
typed_dict_schema
'foo'
core_schema
typed_dict_field
core_schema
str_schema
()),
'spam'
core_schema
typed_dict_field
core_schema
list_schema
items_schema
core_schema
int_schema
())
schema
core_schema
tagged_union_schema
choices
'apple'
apple_schema
'banana'
banana_schema
discriminator
'foo'
SchemaValidator
schema
assert
validate_python
'foo'
'apple'
'bar'
'123'
'foo'
'apple'
'bar'
123
assert
validate_python
'foo'
'banana'
'spam'
]})
'foo'
'banana'
'spam'
Parameters:
Name
Type
Description
Default
choices
dict
Any
CoreSchema
The schemas to match
When retrieving a schema from
choices
using the discriminator value, if the value is a str,
it should be fed back into the
choices
map until a schema is obtained
(This approach is to prevent multiple ownership of a single schema in Rust)
required
discriminator
str
list
str
int
] |
list
list
str
int
]] |
Callable
Any
Any
The discriminator to use to determine the schema to use
* If
discriminator
is a str, it is the name of the attribute to use as the discriminator
* If
discriminator
is a list of int/str, it should be used as a "path" to access the discriminator
* If
discriminator
is a list of lists, each inner list is a path, and the first path that exists is used
* If
discriminator
is a callable, it should return the discriminator when called on the value to validate;
the callable can return
None
to indicate that there is no matching discriminator present on the input
required
custom_error_type
str
| None
The custom error type to use if the validation fails
None
custom_error_message
str
| None
The custom error message to use if the validation fails
None
custom_error_context
dict
str
int
str
float
] | None
The custom error context to use if the validation fails
None
strict
bool
| None
Whether the underlying schemas should be validated with strict mode
None
from_attributes
bool
| None
Whether to use the attributes of the object to retrieve the discriminator value
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2588
2589
2590
2591
2592
2593
2594
2595
2596
2597
2598
2599
2600
2601
2602
2603
2604
2605
2606
2607
2608
2609
2610
2611
2612
2613
2614
2615
2616
2617
2618
2619
2620
2621
2622
2623
2624
2625
2626
2627
2628
2629
2630
2631
2632
2633
2634
2635
2636
2637
2638
2639
2640
2641
2642
2643
2644
2645
2646
2647
2648
2649
2650
2651
2652
2653
2654
2655
2656
2657
2658
2659
2660
2661
2662
2663
2664
2665
2666
2667
2668
def
tagged_union_schema
choices
dict
Any
CoreSchema
discriminator
str
list
str
int
list
list
str
int
Callable
Any
Any
custom_error_type
str
None
None
custom_error_message
str
None
None
custom_error_context
dict
str
int
str
float
None
None
strict
bool
None
None
from_attributes
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
TaggedUnionSchema
"""
Returns a schema that matches a tagged union value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
apple_schema = core_schema.typed_dict_schema(
'foo': core_schema.typed_dict_field(core_schema.str_schema()),
'bar': core_schema.typed_dict_field(core_schema.int_schema()),
banana_schema = core_schema.typed_dict_schema(
'foo': core_schema.typed_dict_field(core_schema.str_schema()),
'spam': core_schema.typed_dict_field(
core_schema.list_schema(items_schema=core_schema.int_schema())
schema = core_schema.tagged_union_schema(
choices={
'apple': apple_schema,
'banana': banana_schema,
discriminator='foo',
v = SchemaValidator(schema)
assert v.validate_python({'foo': 'apple', 'bar': '123'}) == {'foo': 'apple', 'bar': 123}
assert v.validate_python({'foo': 'banana', 'spam': [1, 2, 3]}) == {
'foo': 'banana',
'spam': [1, 2, 3],
```
Args:
choices: The schemas to match
When retrieving a schema from `choices` using the discriminator value, if the value is a str,
it should be fed back into the `choices` map until a schema is obtained
(This approach is to prevent multiple ownership of a single schema in Rust)
discriminator: The discriminator to use to determine the schema to use
* If `discriminator` is a str, it is the name of the attribute to use as the discriminator
* If `discriminator` is a list of int/str, it should be used as a "path" to access the discriminator
* If `discriminator` is a list of lists, each inner list is a path, and the first path that exists is used
* If `discriminator` is a callable, it should return the discriminator when called on the value to validate;
the callable can return `None` to indicate that there is no matching discriminator present on the input
custom_error_type: The custom error type to use if the validation fails
custom_error_message: The custom error message to use if the validation fails
custom_error_context: The custom error context to use if the validation fails
strict: Whether the underlying schemas should be validated with strict mode
from_attributes: Whether to use the attributes of the object to retrieve the discriminator value
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'tagged-union'
choices
choices
discriminator
discriminator
custom_error_type
custom_error_type
custom_error_message
custom_error_message
custom_error_context
custom_error_context
strict
strict
from_attributes
from_attributes
ref
ref
metadata
metadata
serialization
serialization
chain_schema
chain_schema
steps
list
CoreSchema
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
ChainSchema
Returns a schema that chains the provided validation schemas, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
def
str
info
core_schema
ValidationInfo
str
assert
'hello'
return
' world'
fn_schema
core_schema
with_info_plain_validator_function
function
schema
core_schema
chain_schema
fn_schema
fn_schema
fn_schema
core_schema
str_schema
()]
SchemaValidator
schema
assert
validate_python
'hello'
'hello world world world'
Parameters:
Name
Type
Description
Default
steps
list
CoreSchema
The schemas to chain
required
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2679
2680
2681
2682
2683
2684
2685
2686
2687
2688
2689
2690
2691
2692
2693
2694
2695
2696
2697
2698
2699
2700
2701
2702
2703
2704
2705
2706
2707
2708
2709
2710
def
chain_schema
steps
list
CoreSchema
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
ChainSchema
"""
Returns a schema that chains the provided validation schemas, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
def fn(v: str, info: core_schema.ValidationInfo) -> str:
assert 'hello' in v
return v + ' world'
fn_schema = core_schema.with_info_plain_validator_function(function=fn)
schema = core_schema.chain_schema(
[fn_schema, fn_schema, fn_schema, core_schema.str_schema()]
v = SchemaValidator(schema)
assert v.validate_python('hello') == 'hello world world world'
```
Args:
steps: The schemas to chain
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'chain'
steps
steps
ref
ref
metadata
metadata
serialization
serialization
lax_or_strict_schema
lax_or_strict_schema
lax_schema
CoreSchema
strict_schema
CoreSchema
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
LaxOrStrictSchema
Returns a schema that uses the lax or strict schema, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
def
str
info
core_schema
ValidationInfo
str
assert
'hello'
return
' world'
lax_schema
core_schema
int_schema
strict
False
strict_schema
core_schema
int_schema
strict
True
schema
core_schema
lax_or_strict_schema
lax_schema
lax_schema
strict_schema
strict_schema
strict
True
SchemaValidator
schema
assert
validate_python
123
123
schema
core_schema
lax_or_strict_schema
lax_schema
lax_schema
strict_schema
strict_schema
strict
False
SchemaValidator
schema
assert
validate_python
'123'
123
Parameters:
Name
Type
Description
Default
lax_schema
CoreSchema
The lax schema to use
required
strict_schema
CoreSchema
The strict schema to use
required
strict
bool
| None
Whether the strict schema should be used
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2723
2724
2725
2726
2727
2728
2729
2730
2731
2732
2733
2734
2735
2736
2737
2738
2739
2740
2741
2742
2743
2744
2745
2746
2747
2748
2749
2750
2751
2752
2753
2754
2755
2756
2757
2758
2759
2760
2761
2762
2763
2764
2765
2766
2767
2768
2769
2770
2771
2772
2773
2774
def
lax_or_strict_schema
lax_schema
CoreSchema
strict_schema
CoreSchema
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
LaxOrStrictSchema
"""
Returns a schema that uses the lax or strict schema, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
def fn(v: str, info: core_schema.ValidationInfo) -> str:
assert 'hello' in v
return v + ' world'
lax_schema = core_schema.int_schema(strict=False)
strict_schema = core_schema.int_schema(strict=True)
schema = core_schema.lax_or_strict_schema(
lax_schema=lax_schema, strict_schema=strict_schema, strict=True
v = SchemaValidator(schema)
assert v.validate_python(123) == 123
schema = core_schema.lax_or_strict_schema(
lax_schema=lax_schema, strict_schema=strict_schema, strict=False
v = SchemaValidator(schema)
assert v.validate_python('123') == 123
```
Args:
lax_schema: The lax schema to use
strict_schema: The strict schema to use
strict: Whether the strict schema should be used
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'lax-or-strict'
lax_schema
lax_schema
strict_schema
strict_schema
strict
strict
ref
ref
metadata
metadata
serialization
serialization
json_or_python_schema
json_or_python_schema
json_schema
CoreSchema
python_schema
CoreSchema
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
JsonOrPythonSchema
Returns a schema that uses the Json or Python schema depending on the input:
from
pydantic_core
import
SchemaValidator
ValidationError
core_schema
SchemaValidator
core_schema
json_or_python_schema
json_schema
core_schema
int_schema
(),
python_schema
core_schema
int_schema
strict
True
assert
validate_json
'"123"'
123
try
validate_python
'123'
except
ValidationError
pass
else
raise
AssertionError
'Validation should have failed'
Parameters:
Name
Type
Description
Default
json_schema
CoreSchema
The schema to use for Json inputs
required
python_schema
CoreSchema
The schema to use for Python inputs
required
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2786
2787
2788
2789
2790
2791
2792
2793
2794
2795
2796
2797
2798
2799
2800
2801
2802
2803
2804
2805
2806
2807
2808
2809
2810
2811
2812
2813
2814
2815
2816
2817
2818
2819
2820
2821
2822
2823
2824
2825
2826
2827
2828
2829
2830
2831
def
json_or_python_schema
json_schema
CoreSchema
python_schema
CoreSchema
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
JsonOrPythonSchema
"""
Returns a schema that uses the Json or Python schema depending on the input:
```py
from pydantic_core import SchemaValidator, ValidationError, core_schema
v = SchemaValidator(
core_schema.json_or_python_schema(
json_schema=core_schema.int_schema(),
python_schema=core_schema.int_schema(strict=True),
assert v.validate_json('"123"') == 123
try:
v.validate_python('123')
except ValidationError:
pass
else:
raise AssertionError('Validation should have failed')
```
Args:
json_schema: The schema to use for Json inputs
python_schema: The schema to use for Python inputs
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'json-or-python'
json_schema
json_schema
python_schema
python_schema
ref
ref
metadata
metadata
serialization
serialization
typed_dict_field
typed_dict_field
schema
CoreSchema
required
bool
None
None
validation_alias
str
list
str
int
list
list
str
int
None
None
serialization_alias
str
None
None
serialization_exclude
bool
None
None
metadata
dict
str
Any
None
None
TypedDictField
Returns a schema that matches a typed dict field, e.g.:
from
pydantic_core
import
core_schema
field
core_schema
typed_dict_field
schema
core_schema
int_schema
(),
required
True
Parameters:
Name
Type
Description
Default
schema
CoreSchema
The schema to use for the field
required
required
bool
| None
Whether the field is required, otherwise uses the value from
total
on the typed dict
None
validation_alias
str
list
str
int
] |
list
list
str
int
]] | None
The alias(es) to use to find the field in the validation data
None
serialization_alias
str
| None
The alias to use as a key when serializing
None
serialization_exclude
bool
| None
Whether to exclude the field when serializing
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
Source code in
pydantic_core/core_schema.py
2844
2845
2846
2847
2848
2849
2850
2851
2852
2853
2854
2855
2856
2857
2858
2859
2860
2861
2862
2863
2864
2865
2866
2867
2868
2869
2870
2871
2872
2873
2874
2875
2876
2877
2878
def
typed_dict_field
schema
CoreSchema
required
bool
None
None
validation_alias
str
list
str
int
list
list
str
int
None
None
serialization_alias
str
None
None
serialization_exclude
bool
None
None
metadata
dict
str
Any
None
None
TypedDictField
"""
Returns a schema that matches a typed dict field, e.g.:
```py
from pydantic_core import core_schema
field = core_schema.typed_dict_field(schema=core_schema.int_schema(), required=True)
```
Args:
schema: The schema to use for the field
required: Whether the field is required, otherwise uses the value from `total` on the typed dict
validation_alias: The alias(es) to use to find the field in the validation data
serialization_alias: The alias to use as a key when serializing
serialization_exclude: Whether to exclude the field when serializing
metadata: Any other information you want to include with the schema, not used by pydantic-core
"""
return
_dict_not_none
type
'typed-dict-field'
schema
schema
required
required
validation_alias
validation_alias
serialization_alias
serialization_alias
serialization_exclude
serialization_exclude
metadata
metadata
typed_dict_schema
typed_dict_schema
fields
dict
str
TypedDictField
cls
type
Any
None
None
cls_name
str
None
None
computed_fields
list
ComputedField
None
None
strict
bool
None
None
extras_schema
CoreSchema
None
None
extra_behavior
ExtraBehavior
None
None
total
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
config
CoreConfig
None
None
TypedDictSchema
Returns a schema that matches a typed dict, e.g.:
from
typing_extensions
import
TypedDict
from
pydantic_core
import
SchemaValidator
core_schema
class
MyTypedDict
TypedDict
str
wrapper_schema
core_schema
typed_dict_schema
'a'
core_schema
typed_dict_field
core_schema
str_schema
())},
cls
MyTypedDict
SchemaValidator
wrapper_schema
assert
validate_python
'a'
'hello'
'a'
'hello'
Parameters:
Name
Type
Description
Default
fields
dict
str
TypedDictField
The fields to use for the typed dict
required
cls
type
Any
] | None
The class to use for the typed dict
None
cls_name
str
| None
The name to use in error locations. Falls back to
cls.__name__
, or the validator name if no class
is provided.
None
computed_fields
list
ComputedField
] | None
Computed fields to use when serializing the model, only applies when directly inside a model
None
strict
bool
| None
Whether the typed dict is strict
None
extras_schema
CoreSchema
| None
The extra validator to use for the typed dict
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
extra_behavior
ExtraBehavior
| None
The extra behavior to use for the typed dict
None
total
bool
| None
Whether the typed dict is total, otherwise uses
typed_dict_total
from config
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
2898
2899
2900
2901
2902
2903
2904
2905
2906
2907
2908
2909
2910
2911
2912
2913
2914
2915
2916
2917
2918
2919
2920
2921
2922
2923
2924
2925
2926
2927
2928
2929
2930
2931
2932
2933
2934
2935
2936
2937
2938
2939
2940
2941
2942
2943
2944
2945
2946
2947
2948
2949
2950
2951
2952
2953
2954
2955
2956
2957
2958
2959
def
typed_dict_schema
fields
dict
str
TypedDictField
cls
type
Any
None
None
cls_name
str
None
None
computed_fields
list
ComputedField
None
None
strict
bool
None
None
extras_schema
CoreSchema
None
None
extra_behavior
ExtraBehavior
None
None
total
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
config
CoreConfig
None
None
TypedDictSchema
"""
Returns a schema that matches a typed dict, e.g.:
```py
from typing_extensions import TypedDict
from pydantic_core import SchemaValidator, core_schema
class MyTypedDict(TypedDict):
a: str
wrapper_schema = core_schema.typed_dict_schema(
{'a': core_schema.typed_dict_field(core_schema.str_schema())}, cls=MyTypedDict
v = SchemaValidator(wrapper_schema)
assert v.validate_python({'a': 'hello'}) == {'a': 'hello'}
```
Args:
fields: The fields to use for the typed dict
cls: The class to use for the typed dict
cls_name: The name to use in error locations. Falls back to `cls.__name__`, or the validator name if no class
is provided.
computed_fields: Computed fields to use when serializing the model, only applies when directly inside a model
strict: Whether the typed dict is strict
extras_schema: The extra validator to use for the typed dict
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
extra_behavior: The extra behavior to use for the typed dict
total: Whether the typed dict is total, otherwise uses `typed_dict_total` from config
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'typed-dict'
fields
fields
cls
cls
cls_name
cls_name
computed_fields
computed_fields
strict
strict
extras_schema
extras_schema
extra_behavior
extra_behavior
total
total
ref
ref
metadata
metadata
serialization
serialization
config
config
model_field
model_field
schema
CoreSchema
validation_alias
str
list
str
int
list
list
str
int
None
None
serialization_alias
str
None
None
serialization_exclude
bool
None
None
frozen
bool
None
None
metadata
dict
str
Any
None
None
ModelField
Returns a schema for a model field, e.g.:
from
pydantic_core
import
core_schema
field
core_schema
model_field
schema
core_schema
int_schema
())
Parameters:
Name
Type
Description
Default
schema
CoreSchema
The schema to use for the field
required
validation_alias
str
list
str
int
] |
list
list
str
int
]] | None
The alias(es) to use to find the field in the validation data
None
serialization_alias
str
| None
The alias to use as a key when serializing
None
serialization_exclude
bool
| None
Whether to exclude the field when serializing
None
frozen
bool
| None
Whether the field is frozen
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
Source code in
pydantic_core/core_schema.py
2972
2973
2974
2975
2976
2977
2978
2979
2980
2981
2982
2983
2984
2985
2986
2987
2988
2989
2990
2991
2992
2993
2994
2995
2996
2997
2998
2999
3000
3001
3002
3003
3004
3005
3006
def
model_field
schema
CoreSchema
validation_alias
str
list
str
int
list
list
str
int
None
None
serialization_alias
str
None
None
serialization_exclude
bool
None
None
frozen
bool
None
None
metadata
dict
str
Any
None
None
ModelField
"""
Returns a schema for a model field, e.g.:
```py
from pydantic_core import core_schema
field = core_schema.model_field(schema=core_schema.int_schema())
```
Args:
schema: The schema to use for the field
validation_alias: The alias(es) to use to find the field in the validation data
serialization_alias: The alias to use as a key when serializing
serialization_exclude: Whether to exclude the field when serializing
frozen: Whether the field is frozen
metadata: Any other information you want to include with the schema, not used by pydantic-core
"""
return
_dict_not_none
type
'model-field'
schema
schema
validation_alias
validation_alias
serialization_alias
serialization_alias
serialization_exclude
serialization_exclude
frozen
frozen
metadata
metadata
model_fields_schema
model_fields_schema
fields
dict
str
ModelField
model_name
str
None
None
computed_fields
list
ComputedField
None
None
strict
bool
None
None
extras_schema
CoreSchema
None
None
extras_keys_schema
CoreSchema
None
None
extra_behavior
ExtraBehavior
None
None
from_attributes
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
ModelFieldsSchema
Returns a schema that matches the fields of a Pydantic model, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
wrapper_schema
core_schema
model_fields_schema
'a'
core_schema
model_field
core_schema
str_schema
())}
SchemaValidator
wrapper_schema
print
validate_python
'a'
'hello'
}))
#> ({'a': 'hello'}, None, {'a'})
Parameters:
Name
Type
Description
Default
fields
dict
str
ModelField
The fields of the model
required
model_name
str
| None
The name of the model, used for error messages, defaults to "Model"
None
computed_fields
list
ComputedField
] | None
Computed fields to use when serializing the model, only applies when directly inside a model
None
strict
bool
| None
Whether the model is strict
None
extras_schema
CoreSchema
| None
The schema to use when validating extra input data
None
extras_keys_schema
CoreSchema
| None
The schema to use when validating the keys of extra input data
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
extra_behavior
ExtraBehavior
| None
The extra behavior to use for the model fields
None
from_attributes
bool
| None
Whether the model fields should be populated from attributes
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
3024
3025
3026
3027
3028
3029
3030
3031
3032
3033
3034
3035
3036
3037
3038
3039
3040
3041
3042
3043
3044
3045
3046
3047
3048
3049
3050
3051
3052
3053
3054
3055
3056
3057
3058
3059
3060
3061
3062
3063
3064
3065
3066
3067
3068
3069
3070
3071
3072
3073
3074
3075
3076
3077
3078
def
model_fields_schema
fields
dict
str
ModelField
model_name
str
None
None
computed_fields
list
ComputedField
None
None
strict
bool
None
None
extras_schema
CoreSchema
None
None
extras_keys_schema
CoreSchema
None
None
extra_behavior
ExtraBehavior
None
None
from_attributes
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
ModelFieldsSchema
"""
Returns a schema that matches the fields of a Pydantic model, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
wrapper_schema = core_schema.model_fields_schema(
{'a': core_schema.model_field(core_schema.str_schema())}
v = SchemaValidator(wrapper_schema)
print(v.validate_python({'a': 'hello'}))
#> ({'a': 'hello'}, None, {'a'})
```
Args:
fields: The fields of the model
model_name: The name of the model, used for error messages, defaults to "Model"
computed_fields: Computed fields to use when serializing the model, only applies when directly inside a model
strict: Whether the model is strict
extras_schema: The schema to use when validating extra input data
extras_keys_schema: The schema to use when validating the keys of extra input data
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
extra_behavior: The extra behavior to use for the model fields
from_attributes: Whether the model fields should be populated from attributes
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'model-fields'
fields
fields
model_name
model_name
computed_fields
computed_fields
strict
strict
extras_schema
extras_schema
extras_keys_schema
extras_keys_schema
extra_behavior
extra_behavior
from_attributes
from_attributes
ref
ref
metadata
metadata
serialization
serialization
model_schema
model_schema
cls
type
Any
schema
CoreSchema
generic_origin
type
Any
None
None
custom_init
bool
None
None
root_model
bool
None
None
post_init
str
None
None
revalidate_instances
Literal
"always"
"never"
"subclass-instances"
None
None
strict
bool
None
None
frozen
bool
None
None
extra_behavior
ExtraBehavior
None
None
config
CoreConfig
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
ModelSchema
A model schema generally contains a typed-dict schema.
It will run the typed dict validator, then create a new class
and set the dict and fields set returned from the typed dict validator
__dict__
and
__pydantic_fields_set__
respectively.
Example:
from
pydantic_core
import
CoreConfig
SchemaValidator
core_schema
class
MyModel
__slots__
'__dict__'
'__pydantic_fields_set__'
'__pydantic_extra__'
'__pydantic_private__'
schema
core_schema
model_schema
cls
MyModel
config
CoreConfig
str_max_length
schema
core_schema
model_fields_schema
fields
'a'
core_schema
model_field
core_schema
str_schema
())},
SchemaValidator
schema
assert
isinstance_python
'a'
'hello'
True
assert
isinstance_python
'a'
'too long'
False
Parameters:
Name
Type
Description
Default
cls
type
Any
The class to use for the model
required
schema
CoreSchema
The schema to use for the model
required
generic_origin
type
Any
] | None
The origin type used for this model, if it's a parametrized generic. Ex,
if this model schema represents
SomeModel[int]
, generic_origin is
SomeModel
None
custom_init
bool
| None
Whether the model has a custom init method
None
root_model
bool
| None
Whether the model is a
RootModel
None
post_init
str
| None
The call after init to use for the model
None
revalidate_instances
Literal
['always', 'never', 'subclass-instances'] | None
whether instances of models and dataclasses (including subclass instances)
should re-validate defaults to config.revalidate_instances, else 'never'
None
strict
bool
| None
Whether the model is strict
None
frozen
bool
| None
Whether the model is frozen
None
extra_behavior
ExtraBehavior
| None
The extra behavior to use for the model, used in serialization
None
config
CoreConfig
| None
The config to use for the model
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
3099
3100
3101
3102
3103
3104
3105
3106
3107
3108
3109
3110
3111
3112
3113
3114
3115
3116
3117
3118
3119
3120
3121
3122
3123
3124
3125
3126
3127
3128
3129
3130
3131
3132
3133
3134
3135
3136
3137
3138
3139
3140
3141
3142
3143
3144
3145
3146
3147
3148
3149
3150
3151
3152
3153
3154
3155
3156
3157
3158
3159
3160
3161
3162
3163
3164
3165
3166
3167
3168
3169
3170
3171
3172
3173
3174
3175
3176
3177
3178
3179
3180
3181
def
model_schema
cls
type
Any
schema
CoreSchema
generic_origin
type
Any
None
None
custom_init
bool
None
None
root_model
bool
None
None
post_init
str
None
None
revalidate_instances
Literal
'always'
'never'
'subclass-instances'
None
None
strict
bool
None
None
frozen
bool
None
None
extra_behavior
ExtraBehavior
None
None
config
CoreConfig
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
ModelSchema
"""
A model schema generally contains a typed-dict schema.
It will run the typed dict validator, then create a new class
and set the dict and fields set returned from the typed dict validator
to `__dict__` and `__pydantic_fields_set__` respectively.
Example:
```py
from pydantic_core import CoreConfig, SchemaValidator, core_schema
class MyModel:
__slots__ = (
'__dict__',
'__pydantic_fields_set__',
'__pydantic_extra__',
'__pydantic_private__',
schema = core_schema.model_schema(
cls=MyModel,
config=CoreConfig(str_max_length=5),
schema=core_schema.model_fields_schema(
fields={'a': core_schema.model_field(core_schema.str_schema())},
v = SchemaValidator(schema)
assert v.isinstance_python({'a': 'hello'}) is True
assert v.isinstance_python({'a': 'too long'}) is False
```
Args:
cls: The class to use for the model
schema: The schema to use for the model
generic_origin: The origin type used for this model, if it's a parametrized generic. Ex,
if this model schema represents `SomeModel[int]`, generic_origin is `SomeModel`
custom_init: Whether the model has a custom init method
root_model: Whether the model is a `RootModel`
post_init: The call after init to use for the model
revalidate_instances: whether instances of models and dataclasses (including subclass instances)
should re-validate defaults to config.revalidate_instances, else 'never'
strict: Whether the model is strict
frozen: Whether the model is frozen
extra_behavior: The extra behavior to use for the model, used in serialization
config: The config to use for the model
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'model'
cls
cls
generic_origin
generic_origin
schema
schema
custom_init
custom_init
root_model
root_model
post_init
post_init
revalidate_instances
revalidate_instances
strict
strict
frozen
frozen
extra_behavior
extra_behavior
config
config
ref
ref
metadata
metadata
serialization
serialization
dataclass_field
dataclass_field
name
str
schema
CoreSchema
kw_only
bool
None
None
init
bool
None
None
init_only
bool
None
None
validation_alias
str
list
str
int
list
list
str
int
None
None
serialization_alias
str
None
None
serialization_exclude
bool
None
None
metadata
dict
str
Any
None
None
frozen
bool
None
None
DataclassField
Returns a schema for a dataclass field, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
field
core_schema
dataclass_field
name
'a'
schema
core_schema
str_schema
(),
kw_only
False
schema
core_schema
dataclass_args_schema
'Foobar'
field
SchemaValidator
schema
assert
validate_python
'a'
'hello'
'a'
'hello'
None
Parameters:
Name
Type
Description
Default
name
str
The name to use for the argument parameter
required
schema
CoreSchema
The schema to use for the argument parameter
required
kw_only
bool
| None
Whether the field can be set with a positional argument as well as a keyword argument
None
init
bool
| None
Whether the field should be validated during initialization
None
init_only
bool
| None
Whether the field should be omitted from
__dict__
and passed to
__post_init__
None
validation_alias
str
list
str
int
] |
list
list
str
int
]] | None
The alias(es) to use to find the field in the validation data
None
serialization_alias
str
| None
The alias to use as a key when serializing
None
serialization_exclude
bool
| None
Whether to exclude the field when serializing
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
frozen
bool
| None
Whether the field is frozen
None
Source code in
pydantic_core/core_schema.py
3198
3199
3200
3201
3202
3203
3204
3205
3206
3207
3208
3209
3210
3211
3212
3213
3214
3215
3216
3217
3218
3219
3220
3221
3222
3223
3224
3225
3226
3227
3228
3229
3230
3231
3232
3233
3234
3235
3236
3237
3238
3239
3240
3241
3242
3243
3244
3245
3246
3247
3248
3249
def
dataclass_field
name
str
schema
CoreSchema
kw_only
bool
None
None
init
bool
None
None
init_only
bool
None
None
validation_alias
str
list
str
int
list
list
str
int
None
None
serialization_alias
str
None
None
serialization_exclude
bool
None
None
metadata
dict
str
Any
None
None
frozen
bool
None
None
DataclassField
"""
Returns a schema for a dataclass field, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
field = core_schema.dataclass_field(
name='a', schema=core_schema.str_schema(), kw_only=False
schema = core_schema.dataclass_args_schema('Foobar', [field])
v = SchemaValidator(schema)
assert v.validate_python({'a': 'hello'}) == ({'a': 'hello'}, None)
```
Args:
name: The name to use for the argument parameter
schema: The schema to use for the argument parameter
kw_only: Whether the field can be set with a positional argument as well as a keyword argument
init: Whether the field should be validated during initialization
init_only: Whether the field should be omitted from `__dict__` and passed to `__post_init__`
validation_alias: The alias(es) to use to find the field in the validation data
serialization_alias: The alias to use as a key when serializing
serialization_exclude: Whether to exclude the field when serializing
metadata: Any other information you want to include with the schema, not used by pydantic-core
frozen: Whether the field is frozen
"""
return
_dict_not_none
type
'dataclass-field'
name
name
schema
schema
kw_only
kw_only
init
init
init_only
init_only
validation_alias
validation_alias
serialization_alias
serialization_alias
serialization_exclude
serialization_exclude
metadata
metadata
frozen
frozen
dataclass_args_schema
dataclass_args_schema
dataclass_name
str
fields
list
DataclassField
computed_fields
list
ComputedField
None
None
collect_init_only
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
extra_behavior
ExtraBehavior
None
None
DataclassArgsSchema
Returns a schema for validating dataclass arguments, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
field_a
core_schema
dataclass_field
name
'a'
schema
core_schema
str_schema
(),
kw_only
False
field_b
core_schema
dataclass_field
name
'b'
schema
core_schema
bool_schema
(),
kw_only
False
schema
core_schema
dataclass_args_schema
'Foobar'
field_a
field_b
SchemaValidator
schema
assert
validate_python
'a'
'hello'
'b'
True
'a'
'hello'
'b'
True
None
Parameters:
Name
Type
Description
Default
dataclass_name
str
The name of the dataclass being validated
required
fields
list
DataclassField
The fields to use for the dataclass
required
computed_fields
list
ComputedField
] | None
Computed fields to use when serializing the dataclass
None
collect_init_only
bool
| None
Whether to collect init only fields into a dict to pass to
__post_init__
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
extra_behavior
ExtraBehavior
| None
How to handle extra fields
None
Source code in
pydantic_core/core_schema.py
3264
3265
3266
3267
3268
3269
3270
3271
3272
3273
3274
3275
3276
3277
3278
3279
3280
3281
3282
3283
3284
3285
3286
3287
3288
3289
3290
3291
3292
3293
3294
3295
3296
3297
3298
3299
3300
3301
3302
3303
3304
3305
3306
3307
3308
3309
3310
3311
3312
def
dataclass_args_schema
dataclass_name
str
fields
list
DataclassField
computed_fields
list
ComputedField
None
None
collect_init_only
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
extra_behavior
ExtraBehavior
None
None
DataclassArgsSchema
"""
Returns a schema for validating dataclass arguments, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
field_a = core_schema.dataclass_field(
name='a', schema=core_schema.str_schema(), kw_only=False
field_b = core_schema.dataclass_field(
name='b', schema=core_schema.bool_schema(), kw_only=False
schema = core_schema.dataclass_args_schema('Foobar', [field_a, field_b])
v = SchemaValidator(schema)
assert v.validate_python({'a': 'hello', 'b': True}) == ({'a': 'hello', 'b': True}, None)
```
Args:
dataclass_name: The name of the dataclass being validated
fields: The fields to use for the dataclass
computed_fields: Computed fields to use when serializing the dataclass
collect_init_only: Whether to collect init only fields into a dict to pass to `__post_init__`
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
extra_behavior: How to handle extra fields
"""
return
_dict_not_none
type
'dataclass-args'
dataclass_name
dataclass_name
fields
fields
computed_fields
computed_fields
collect_init_only
collect_init_only
ref
ref
metadata
metadata
serialization
serialization
extra_behavior
extra_behavior
dataclass_schema
dataclass_schema
cls
type
Any
schema
CoreSchema
fields
list
str
generic_origin
type
Any
None
None
cls_name
str
None
None
post_init
bool
None
None
revalidate_instances
Literal
"always"
"never"
"subclass-instances"
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
frozen
bool
None
None
slots
bool
None
None
config
CoreConfig
None
None
DataclassSchema
Returns a schema for a dataclass. As with
ModelSchema
, this schema can only be used as a field within
another schema, not as the root type.
Parameters:
Name
Type
Description
Default
cls
type
Any
The dataclass type, used to perform subclass checks
required
schema
CoreSchema
The schema to use for the dataclass fields
required
fields
list
str
Fields of the dataclass, this is used in serialization and in validation during re-validation
and while validating assignment
required
generic_origin
type
Any
] | None
The origin type used for this dataclass, if it's a parametrized generic. Ex,
if this model schema represents
SomeDataclass[int]
, generic_origin is
SomeDataclass
None
cls_name
str
| None
The name to use in error locs, etc; this is useful for generics (default:
cls.__name__
None
post_init
bool
| None
Whether to call
__post_init__
after validation
None
revalidate_instances
Literal
['always', 'never', 'subclass-instances'] | None
whether instances of models and dataclasses (including subclass instances)
should re-validate defaults to config.revalidate_instances, else 'never'
None
strict
bool
| None
Whether to require an exact instance of
cls
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
frozen
bool
| None
Whether the dataclass is frozen
None
slots
bool
| None
Whether
slots=True
on the dataclass, means each field is assigned independently, rather than
simply setting
__dict__
, default false
None
Source code in
pydantic_core/core_schema.py
3333
3334
3335
3336
3337
3338
3339
3340
3341
3342
3343
3344
3345
3346
3347
3348
3349
3350
3351
3352
3353
3354
3355
3356
3357
3358
3359
3360
3361
3362
3363
3364
3365
3366
3367
3368
3369
3370
3371
3372
3373
3374
3375
3376
3377
3378
3379
3380
3381
3382
3383
3384
3385
3386
3387
3388
3389
def
dataclass_schema
cls
type
Any
schema
CoreSchema
fields
list
str
generic_origin
type
Any
None
None
cls_name
str
None
None
post_init
bool
None
None
revalidate_instances
Literal
'always'
'never'
'subclass-instances'
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
frozen
bool
None
None
slots
bool
None
None
config
CoreConfig
None
None
DataclassSchema
"""
Returns a schema for a dataclass. As with `ModelSchema`, this schema can only be used as a field within
another schema, not as the root type.
Args:
cls: The dataclass type, used to perform subclass checks
schema: The schema to use for the dataclass fields
fields: Fields of the dataclass, this is used in serialization and in validation during re-validation
and while validating assignment
generic_origin: The origin type used for this dataclass, if it's a parametrized generic. Ex,
if this model schema represents `SomeDataclass[int]`, generic_origin is `SomeDataclass`
cls_name: The name to use in error locs, etc; this is useful for generics (default: `cls.__name__`)
post_init: Whether to call `__post_init__` after validation
revalidate_instances: whether instances of models and dataclasses (including subclass instances)
should re-validate defaults to config.revalidate_instances, else 'never'
strict: Whether to require an exact instance of `cls`
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
frozen: Whether the dataclass is frozen
slots: Whether `slots=True` on the dataclass, means each field is assigned independently, rather than
simply setting `__dict__`, default false
"""
return
_dict_not_none
type
'dataclass'
cls
cls
generic_origin
generic_origin
fields
fields
cls_name
cls_name
schema
schema
post_init
post_init
revalidate_instances
revalidate_instances
strict
strict
ref
ref
metadata
metadata
serialization
serialization
frozen
frozen
slots
slots
config
config
arguments_parameter
arguments_parameter
name
str
schema
CoreSchema
mode
Literal
"positional_only"
"positional_or_keyword"
"keyword_only"
None
None
alias
str
list
str
int
list
list
str
int
None
None
ArgumentsParameter
Returns a schema that matches an argument parameter, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
param
core_schema
arguments_parameter
name
'a'
schema
core_schema
str_schema
(),
mode
'positional_only'
schema
core_schema
arguments_schema
param
SchemaValidator
schema
assert
validate_python
'hello'
,))
'hello'
,),
{})
Parameters:
Name
Type
Description
Default
name
str
The name to use for the argument parameter
required
schema
CoreSchema
The schema to use for the argument parameter
required
mode
Literal
['positional_only', 'positional_or_keyword', 'keyword_only'] | None
The mode to use for the argument parameter
None
alias
str
list
str
int
] |
list
list
str
int
]] | None
The alias to use for the argument parameter
None
Source code in
pydantic_core/core_schema.py
3399
3400
3401
3402
3403
3404
3405
3406
3407
3408
3409
3410
3411
3412
3413
3414
3415
3416
3417
3418
3419
3420
3421
3422
3423
3424
3425
3426
def
arguments_parameter
name
str
schema
CoreSchema
mode
Literal
'positional_only'
'positional_or_keyword'
'keyword_only'
None
None
alias
str
list
str
int
list
list
str
int
None
None
ArgumentsParameter
"""
Returns a schema that matches an argument parameter, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
param = core_schema.arguments_parameter(
name='a', schema=core_schema.str_schema(), mode='positional_only'
schema = core_schema.arguments_schema([param])
v = SchemaValidator(schema)
assert v.validate_python(('hello',)) == (('hello',), {})
```
Args:
name: The name to use for the argument parameter
schema: The schema to use for the argument parameter
mode: The mode to use for the argument parameter
alias: The alias to use for the argument parameter
"""
return
_dict_not_none
name
name
schema
schema
mode
mode
alias
alias
arguments_schema
arguments_schema
arguments
list
ArgumentsParameter
validate_by_name
bool
None
None
validate_by_alias
bool
None
None
var_args_schema
CoreSchema
None
None
var_kwargs_mode
VarKwargsMode
None
None
var_kwargs_schema
CoreSchema
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
ArgumentsSchema
Returns a schema that matches an arguments schema, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
param_a
core_schema
arguments_parameter
name
'a'
schema
core_schema
str_schema
(),
mode
'positional_only'
param_b
core_schema
arguments_parameter
name
'b'
schema
core_schema
bool_schema
(),
mode
'positional_only'
schema
core_schema
arguments_schema
param_a
param_b
SchemaValidator
schema
assert
validate_python
'hello'
True
'hello'
True
{})
Parameters:
Name
Type
Description
Default
arguments
list
ArgumentsParameter
The arguments to use for the arguments schema
required
validate_by_name
bool
| None
Whether to populate by the parameter names, defaults to
False
None
validate_by_alias
bool
| None
Whether to populate by the parameter aliases, defaults to
True
None
var_args_schema
CoreSchema
| None
The variable args schema to use for the arguments schema
None
var_kwargs_mode
VarKwargsMode
| None
The validation mode to use for variadic keyword arguments. If
'uniform'
, every value of the
keyword arguments will be validated against the
var_kwargs_schema
schema. If
'unpacked-typed-dict'
the
var_kwargs_schema
argument must be a
typed_dict_schema
None
var_kwargs_schema
CoreSchema
| None
The variable kwargs schema to use for the arguments schema
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
3445
3446
3447
3448
3449
3450
3451
3452
3453
3454
3455
3456
3457
3458
3459
3460
3461
3462
3463
3464
3465
3466
3467
3468
3469
3470
3471
3472
3473
3474
3475
3476
3477
3478
3479
3480
3481
3482
3483
3484
3485
3486
3487
3488
3489
3490
3491
3492
3493
3494
3495
3496
3497
3498
def
arguments_schema
arguments
list
ArgumentsParameter
validate_by_name
bool
None
None
validate_by_alias
bool
None
None
var_args_schema
CoreSchema
None
None
var_kwargs_mode
VarKwargsMode
None
None
var_kwargs_schema
CoreSchema
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
ArgumentsSchema
"""
Returns a schema that matches an arguments schema, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
param_a = core_schema.arguments_parameter(
name='a', schema=core_schema.str_schema(), mode='positional_only'
param_b = core_schema.arguments_parameter(
name='b', schema=core_schema.bool_schema(), mode='positional_only'
schema = core_schema.arguments_schema([param_a, param_b])
v = SchemaValidator(schema)
assert v.validate_python(('hello', True)) == (('hello', True), {})
```
Args:
arguments: The arguments to use for the arguments schema
validate_by_name: Whether to populate by the parameter names, defaults to `False`.
validate_by_alias: Whether to populate by the parameter aliases, defaults to `True`.
var_args_schema: The variable args schema to use for the arguments schema
var_kwargs_mode: The validation mode to use for variadic keyword arguments. If `'uniform'`, every value of the
keyword arguments will be validated against the `var_kwargs_schema` schema. If `'unpacked-typed-dict'`,
the `var_kwargs_schema` argument must be a [`typed_dict_schema`][pydantic_core.core_schema.typed_dict_schema]
var_kwargs_schema: The variable kwargs schema to use for the arguments schema
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'arguments'
arguments_schema
arguments
validate_by_name
validate_by_name
validate_by_alias
validate_by_alias
var_args_schema
var_args_schema
var_kwargs_mode
var_kwargs_mode
var_kwargs_schema
var_kwargs_schema
ref
ref
metadata
metadata
serialization
serialization
arguments_v3_parameter
arguments_v3_parameter
name
str
schema
CoreSchema
mode
Literal
"positional_only"
"positional_or_keyword"
"keyword_only"
"var_args"
"var_kwargs_uniform"
"var_kwargs_unpacked_typed_dict"
None
None
alias
str
list
str
int
list
list
str
int
None
None
ArgumentsV3Parameter
Returns a schema that matches an argument parameter, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
param
core_schema
arguments_v3_parameter
name
'a'
schema
core_schema
str_schema
(),
mode
'positional_only'
schema
core_schema
arguments_v3_schema
param
SchemaValidator
schema
assert
validate_python
'a'
'hello'
'hello'
,),
{})
Parameters:
Name
Type
Description
Default
name
str
The name to use for the argument parameter
required
schema
CoreSchema
The schema to use for the argument parameter
required
mode
Literal
['positional_only', 'positional_or_keyword', 'keyword_only', 'var_args', 'var_kwargs_uniform', 'var_kwargs_unpacked_typed_dict'] | None
The mode to use for the argument parameter
None
alias
str
list
str
int
] |
list
list
str
int
]] | None
The alias to use for the argument parameter
None
Source code in
pydantic_core/core_schema.py
3515
3516
3517
3518
3519
3520
3521
3522
3523
3524
3525
3526
3527
3528
3529
3530
3531
3532
3533
3534
3535
3536
3537
3538
3539
3540
3541
3542
3543
3544
3545
3546
3547
3548
3549
3550
def
arguments_v3_parameter
name
str
schema
CoreSchema
mode
Literal
'positional_only'
'positional_or_keyword'
'keyword_only'
'var_args'
'var_kwargs_uniform'
'var_kwargs_unpacked_typed_dict'
None
None
alias
str
list
str
int
list
list
str
int
None
None
ArgumentsV3Parameter
"""
Returns a schema that matches an argument parameter, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
param = core_schema.arguments_v3_parameter(
name='a', schema=core_schema.str_schema(), mode='positional_only'
schema = core_schema.arguments_v3_schema([param])
v = SchemaValidator(schema)
assert v.validate_python({'a': 'hello'}) == (('hello',), {})
```
Args:
name: The name to use for the argument parameter
schema: The schema to use for the argument parameter
mode: The mode to use for the argument parameter
alias: The alias to use for the argument parameter
"""
return
_dict_not_none
name
name
schema
schema
mode
mode
alias
alias
arguments_v3_schema
arguments_v3_schema
arguments
list
ArgumentsV3Parameter
validate_by_name
bool
None
None
validate_by_alias
bool
None
None
extra_behavior
Literal
"forbid"
"ignore"
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
ArgumentsV3Schema
Returns a schema that matches an arguments schema, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
param_a
core_schema
arguments_v3_parameter
name
'a'
schema
core_schema
str_schema
(),
mode
'positional_only'
param_b
core_schema
arguments_v3_parameter
name
'kwargs'
schema
core_schema
bool_schema
(),
mode
'var_kwargs_uniform'
schema
core_schema
arguments_v3_schema
param_a
param_b
SchemaValidator
schema
assert
validate_python
'a'
'hi'
'kwargs'
'b'
True
}})
'hi'
,),
'b'
True
This schema is currently not used by other Pydantic components. In V3, it will most likely
become the default arguments schema for the
'call'
schema.
Parameters:
Name
Type
Description
Default
arguments
list
ArgumentsV3Parameter
The arguments to use for the arguments schema.
required
validate_by_name
bool
| None
Whether to populate by the parameter names, defaults to
False
None
validate_by_alias
bool
| None
Whether to populate by the parameter aliases, defaults to
True
None
extra_behavior
Literal
['forbid', 'ignore'] | None
The extra behavior to use.
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places.
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core.
None
serialization
SerSchema
| None
Custom serialization schema.
None
Source code in
pydantic_core/core_schema.py
3564
3565
3566
3567
3568
3569
3570
3571
3572
3573
3574
3575
3576
3577
3578
3579
3580
3581
3582
3583
3584
3585
3586
3587
3588
3589
3590
3591
3592
3593
3594
3595
3596
3597
3598
3599
3600
3601
3602
3603
3604
3605
3606
3607
3608
3609
3610
3611
3612
def
arguments_v3_schema
arguments
list
ArgumentsV3Parameter
validate_by_name
bool
None
None
validate_by_alias
bool
None
None
extra_behavior
Literal
'forbid'
'ignore'
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
ArgumentsV3Schema
"""
Returns a schema that matches an arguments schema, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
param_a = core_schema.arguments_v3_parameter(
name='a', schema=core_schema.str_schema(), mode='positional_only'
param_b = core_schema.arguments_v3_parameter(
name='kwargs', schema=core_schema.bool_schema(), mode='var_kwargs_uniform'
schema = core_schema.arguments_v3_schema([param_a, param_b])
v = SchemaValidator(schema)
assert v.validate_python({'a': 'hi', 'kwargs': {'b': True}}) == (('hi',), {'b': True})
```
This schema is currently not used by other Pydantic components. In V3, it will most likely
become the default arguments schema for the `'call'` schema.
Args:
arguments: The arguments to use for the arguments schema.
validate_by_name: Whether to populate by the parameter names, defaults to `False`.
validate_by_alias: Whether to populate by the parameter aliases, defaults to `True`.
extra_behavior: The extra behavior to use.
ref: optional unique identifier of the schema, used to reference the schema in other places.
metadata: Any other information you want to include with the schema, not used by pydantic-core.
serialization: Custom serialization schema.
"""
return
_dict_not_none
type
'arguments-v3'
arguments_schema
arguments
validate_by_name
validate_by_name
validate_by_alias
validate_by_alias
extra_behavior
extra_behavior
ref
ref
metadata
metadata
serialization
serialization
call_schema
call_schema
arguments
CoreSchema
function
Callable
...
Any
function_name
str
None
None
return_schema
CoreSchema
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
CallSchema
Returns a schema that matches an arguments schema, then calls a function, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
param_a
core_schema
arguments_parameter
name
'a'
schema
core_schema
str_schema
(),
mode
'positional_only'
param_b
core_schema
arguments_parameter
name
'b'
schema
core_schema
bool_schema
(),
mode
'positional_only'
args_schema
core_schema
arguments_schema
param_a
param_b
schema
core_schema
call_schema
arguments
args_schema
function
lambda
str
not
return_schema
core_schema
str_schema
(),
SchemaValidator
schema
assert
validate_python
(((
'hello'
True
)))
'helloFalse'
Parameters:
Name
Type
Description
Default
arguments
CoreSchema
The arguments to use for the arguments schema
required
function
Callable
[...,
Any
The function to use for the call schema
required
function_name
str
| None
The function name to use for the call schema, if not provided
function.__name__
is used
None
return_schema
CoreSchema
| None
The return schema to use for the call schema
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
3626
3627
3628
3629
3630
3631
3632
3633
3634
3635
3636
3637
3638
3639
3640
3641
3642
3643
3644
3645
3646
3647
3648
3649
3650
3651
3652
3653
3654
3655
3656
3657
3658
3659
3660
3661
3662
3663
3664
3665
3666
3667
3668
3669
3670
3671
3672
3673
3674
3675
3676
3677
def
call_schema
arguments
CoreSchema
function
Callable
...
Any
function_name
str
None
None
return_schema
CoreSchema
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
CallSchema
"""
Returns a schema that matches an arguments schema, then calls a function, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
param_a = core_schema.arguments_parameter(
name='a', schema=core_schema.str_schema(), mode='positional_only'
param_b = core_schema.arguments_parameter(
name='b', schema=core_schema.bool_schema(), mode='positional_only'
args_schema = core_schema.arguments_schema([param_a, param_b])
schema = core_schema.call_schema(
arguments=args_schema,
function=lambda a, b: a + str(not b),
return_schema=core_schema.str_schema(),
v = SchemaValidator(schema)
assert v.validate_python((('hello', True))) == 'helloFalse'
```
Args:
arguments: The arguments to use for the arguments schema
function: The function to use for the call schema
function_name: The function name to use for the call schema, if not provided `function.__name__` is used
return_schema: The return schema to use for the call schema
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'call'
arguments_schema
arguments
function
function
function_name
function_name
return_schema
return_schema
ref
ref
metadata
metadata
serialization
serialization
custom_error_schema
custom_error_schema
schema
CoreSchema
custom_error_type
str
custom_error_message
str
None
None
custom_error_context
dict
str
Any
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
CustomErrorSchema
Returns a schema that matches a custom error value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
custom_error_schema
schema
core_schema
int_schema
(),
custom_error_type
'MyError'
custom_error_message
'Error msg'
SchemaValidator
schema
validate_python
Parameters:
Name
Type
Description
Default
schema
CoreSchema
The schema to use for the custom error schema
required
custom_error_type
str
The custom error type to use for the custom error schema
required
custom_error_message
str
| None
The custom error message to use for the custom error schema
None
custom_error_context
dict
str
Any
] | None
The custom error context to use for the custom error schema
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
3691
3692
3693
3694
3695
3696
3697
3698
3699
3700
3701
3702
3703
3704
3705
3706
3707
3708
3709
3710
3711
3712
3713
3714
3715
3716
3717
3718
3719
3720
3721
3722
3723
3724
3725
3726
3727
3728
3729
3730
3731
3732
3733
3734
def
custom_error_schema
schema
CoreSchema
custom_error_type
str
custom_error_message
str
None
None
custom_error_context
dict
str
Any
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
CustomErrorSchema
"""
Returns a schema that matches a custom error value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.custom_error_schema(
schema=core_schema.int_schema(),
custom_error_type='MyError',
custom_error_message='Error msg',
v = SchemaValidator(schema)
v.validate_python(1)
```
Args:
schema: The schema to use for the custom error schema
custom_error_type: The custom error type to use for the custom error schema
custom_error_message: The custom error message to use for the custom error schema
custom_error_context: The custom error context to use for the custom error schema
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'custom-error'
schema
schema
custom_error_type
custom_error_type
custom_error_message
custom_error_message
custom_error_context
custom_error_context
ref
ref
metadata
metadata
serialization
serialization
json_schema
json_schema
schema
CoreSchema
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
JsonSchema
Returns a schema that matches a JSON value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
dict_schema
core_schema
model_fields_schema
'field_a'
core_schema
model_field
core_schema
str_schema
()),
'field_b'
core_schema
model_field
core_schema
bool_schema
()),
class
MyModel
__slots__
'__dict__'
'__pydantic_fields_set__'
'__pydantic_extra__'
'__pydantic_private__'
field_a
str
field_b
bool
json_schema
core_schema
json_schema
schema
dict_schema
schema
core_schema
model_schema
cls
MyModel
schema
json_schema
SchemaValidator
schema
validate_python
'{"field_a": "hello", "field_b": true}'
assert
isinstance
MyModel
Parameters:
Name
Type
Description
Default
schema
CoreSchema
| None
The schema to use for the JSON schema
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
3745
3746
3747
3748
3749
3750
3751
3752
3753
3754
3755
3756
3757
3758
3759
3760
3761
3762
3763
3764
3765
3766
3767
3768
3769
3770
3771
3772
3773
3774
3775
3776
3777
3778
3779
3780
3781
3782
3783
3784
3785
3786
3787
3788
def
json_schema
schema
CoreSchema
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
JsonSchema
"""
Returns a schema that matches a JSON value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
dict_schema = core_schema.model_fields_schema(
'field_a': core_schema.model_field(core_schema.str_schema()),
'field_b': core_schema.model_field(core_schema.bool_schema()),
class MyModel:
__slots__ = (
'__dict__',
'__pydantic_fields_set__',
'__pydantic_extra__',
'__pydantic_private__',
field_a: str
field_b: bool
json_schema = core_schema.json_schema(schema=dict_schema)
schema = core_schema.model_schema(cls=MyModel, schema=json_schema)
v = SchemaValidator(schema)
m = v.validate_python('{"field_a": "hello", "field_b": true}')
assert isinstance(m, MyModel)
```
Args:
schema: The schema to use for the JSON schema
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'json'
schema
schema
ref
ref
metadata
metadata
serialization
serialization
url_schema
url_schema
max_length
int
None
None
allowed_schemes
list
str
None
None
host_required
bool
None
None
default_host
str
None
None
default_port
int
None
None
default_path
str
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
UrlSchema
Returns a schema that matches a URL value, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
url_schema
SchemaValidator
schema
print
validate_python
'https://example.com'
#> https://example.com/
Parameters:
Name
Type
Description
Default
max_length
int
| None
The maximum length of the URL
None
allowed_schemes
list
str
] | None
The allowed URL schemes
None
host_required
bool
| None
Whether the URL must have a host
None
default_host
str
| None
The default host to use if the URL does not have a host
None
default_port
int
| None
The default port to use if the URL does not have a port
None
default_path
str
| None
The default path to use if the URL does not have a path
None
strict
bool
| None
Whether to use strict URL parsing
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
3805
3806
3807
3808
3809
3810
3811
3812
3813
3814
3815
3816
3817
3818
3819
3820
3821
3822
3823
3824
3825
3826
3827
3828
3829
3830
3831
3832
3833
3834
3835
3836
3837
3838
3839
3840
3841
3842
3843
3844
3845
3846
3847
3848
3849
3850
3851
3852
3853
3854
def
url_schema
max_length
int
None
None
allowed_schemes
list
str
None
None
host_required
bool
None
None
default_host
str
None
None
default_port
int
None
None
default_path
str
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
UrlSchema
"""
Returns a schema that matches a URL value, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.url_schema()
v = SchemaValidator(schema)
print(v.validate_python('https://example.com'))
#> https://example.com/
```
Args:
max_length: The maximum length of the URL
allowed_schemes: The allowed URL schemes
host_required: Whether the URL must have a host
default_host: The default host to use if the URL does not have a host
default_port: The default port to use if the URL does not have a port
default_path: The default path to use if the URL does not have a path
strict: Whether to use strict URL parsing
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'url'
max_length
max_length
allowed_schemes
allowed_schemes
host_required
host_required
default_host
default_host
default_port
default_port
default_path
default_path
strict
strict
ref
ref
metadata
metadata
serialization
serialization
multi_host_url_schema
multi_host_url_schema
max_length
int
None
None
allowed_schemes
list
str
None
None
host_required
bool
None
None
default_host
str
None
None
default_port
int
None
None
default_path
str
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
MultiHostUrlSchema
Returns a schema that matches a URL value with possibly multiple hosts, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
multi_host_url_schema
SchemaValidator
schema
print
validate_python
'redis://localhost,0.0.0.0,127.0.0.1'
#> redis://localhost,0.0.0.0,127.0.0.1
Parameters:
Name
Type
Description
Default
max_length
int
| None
The maximum length of the URL
None
allowed_schemes
list
str
] | None
The allowed URL schemes
None
host_required
bool
| None
Whether the URL must have a host
None
default_host
str
| None
The default host to use if the URL does not have a host
None
default_port
int
| None
The default port to use if the URL does not have a port
None
default_path
str
| None
The default path to use if the URL does not have a path
None
strict
bool
| None
Whether to use strict URL parsing
None
ref
str
| None
optional unique identifier of the schema, used to reference the schema in other places
None
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
3871
3872
3873
3874
3875
3876
3877
3878
3879
3880
3881
3882
3883
3884
3885
3886
3887
3888
3889
3890
3891
3892
3893
3894
3895
3896
3897
3898
3899
3900
3901
3902
3903
3904
3905
3906
3907
3908
3909
3910
3911
3912
3913
3914
3915
3916
3917
3918
3919
3920
def
multi_host_url_schema
max_length
int
None
None
allowed_schemes
list
str
None
None
host_required
bool
None
None
default_host
str
None
None
default_port
int
None
None
default_path
str
None
None
strict
bool
None
None
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
MultiHostUrlSchema
"""
Returns a schema that matches a URL value with possibly multiple hosts, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.multi_host_url_schema()
v = SchemaValidator(schema)
print(v.validate_python('redis://localhost,0.0.0.0,127.0.0.1'))
#> redis://localhost,0.0.0.0,127.0.0.1
```
Args:
max_length: The maximum length of the URL
allowed_schemes: The allowed URL schemes
host_required: Whether the URL must have a host
default_host: The default host to use if the URL does not have a host
default_port: The default port to use if the URL does not have a port
default_path: The default path to use if the URL does not have a path
strict: Whether to use strict URL parsing
ref: optional unique identifier of the schema, used to reference the schema in other places
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'multi-host-url'
max_length
max_length
allowed_schemes
allowed_schemes
host_required
host_required
default_host
default_host
default_port
default_port
default_path
default_path
strict
strict
ref
ref
metadata
metadata
serialization
serialization
definitions_schema
definitions_schema
schema
CoreSchema
definitions
list
CoreSchema
DefinitionsSchema
Build a schema that contains both an inner schema and a list of definitions which can be used
within the inner schema.
from
pydantic_core
import
SchemaValidator
core_schema
schema
core_schema
definitions_schema
core_schema
list_schema
core_schema
definition_reference_schema
'foobar'
)),
core_schema
int_schema
ref
'foobar'
)],
SchemaValidator
schema
assert
validate_python
'3'
Parameters:
Name
Type
Description
Default
schema
CoreSchema
The inner schema
required
definitions
list
CoreSchema
List of definitions which can be referenced within inner schema
required
Source code in
pydantic_core/core_schema.py
3931
3932
3933
3934
3935
3936
3937
3938
3939
3940
3941
3942
3943
3944
3945
3946
3947
3948
3949
3950
3951
def
definitions_schema
schema
CoreSchema
definitions
list
CoreSchema
DefinitionsSchema
"""
Build a schema that contains both an inner schema and a list of definitions which can be used
within the inner schema.
```py
from pydantic_core import SchemaValidator, core_schema
schema = core_schema.definitions_schema(
core_schema.list_schema(core_schema.definition_reference_schema('foobar')),
[core_schema.int_schema(ref='foobar')],
v = SchemaValidator(schema)
assert v.validate_python([1, 2, '3']) == [1, 2, 3]
```
Args:
schema: The inner schema
definitions: List of definitions which can be referenced within inner schema
"""
return
DefinitionsSchema
type
'definitions'
schema
schema
definitions
definitions
definition_reference_schema
definition_reference_schema
schema_ref
str
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
DefinitionReferenceSchema
Returns a schema that points to a schema stored in "definitions", this is useful for nested recursive
models and also when you want to define validators separately from the main schema, e.g.:
from
pydantic_core
import
SchemaValidator
core_schema
schema_definition
core_schema
definition_reference_schema
'list-schema'
schema
core_schema
definitions_schema
schema
schema_definition
definitions
core_schema
list_schema
items_schema
schema_definition
ref
'list-schema'
SchemaValidator
schema
assert
validate_python
([()])
[[]]
Parameters:
Name
Type
Description
Default
schema_ref
str
The schema ref to use for the definition reference schema
required
metadata
dict
str
Any
] | None
Any other information you want to include with the schema, not used by pydantic-core
None
serialization
SerSchema
| None
Custom serialization schema
None
Source code in
pydantic_core/core_schema.py
3962
3963
3964
3965
3966
3967
3968
3969
3970
3971
3972
3973
3974
3975
3976
3977
3978
3979
3980
3981
3982
3983
3984
3985
3986
3987
3988
3989
3990
3991
3992
3993
def
definition_reference_schema
schema_ref
str
ref
str
None
None
metadata
dict
str
Any
None
None
serialization
SerSchema
None
None
DefinitionReferenceSchema
"""
Returns a schema that points to a schema stored in "definitions", this is useful for nested recursive
models and also when you want to define validators separately from the main schema, e.g.:
```py
from pydantic_core import SchemaValidator, core_schema
schema_definition = core_schema.definition_reference_schema('list-schema')
schema = core_schema.definitions_schema(
schema=schema_definition,
definitions=[
core_schema.list_schema(items_schema=schema_definition, ref='list-schema'),
v = SchemaValidator(schema)
assert v.validate_python([()]) == [[]]
```
Args:
schema_ref: The schema ref to use for the definition reference schema
metadata: Any other information you want to include with the schema, not used by pydantic-core
serialization: Custom serialization schema
"""
return
_dict_not_none
type
'definition-ref'
schema_ref
schema_ref
ref
ref
metadata
metadata
serialization
serialization
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 019_validate_call.txt ---
Validate Call
Decorator for validating function calls.
validate_call
validate_call
config
ConfigDict
None
None
validate_return
bool
False
Callable
AnyCallableT
AnyCallableT
validate_call
func
AnyCallableT
AnyCallableT
validate_call
func
AnyCallableT
None
None
config
ConfigDict
None
None
validate_return
bool
False
AnyCallableT
Callable
AnyCallableT
AnyCallableT
Usage Documentation
Validation Decorator
Returns a decorated wrapper around the function that validates the arguments and, optionally, the return value.
Usage may be either as a plain decorator
@validate_call
or with arguments
@validate_call(...)
Parameters:
Name
Type
Description
Default
func
AnyCallableT
| None
The function to be decorated.
None
config
ConfigDict
| None
The configuration dictionary.
None
validate_return
bool
Whether to validate the return value.
False
Returns:
Type
Description
AnyCallableT
Callable
AnyCallableT
AnyCallableT
The decorated function.
Source code in
pydantic/validate_call_decorator.py
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
def
validate_call
func
AnyCallableT
None
None
config
ConfigDict
None
None
validate_return
bool
False
AnyCallableT
Callable
AnyCallableT
AnyCallableT
"""!!! abstract "Usage Documentation"
[Validation Decorator](../concepts/validation_decorator.md)
Returns a decorated wrapper around the function that validates the arguments and, optionally, the return value.
Usage may be either as a plain decorator `@validate_call` or with arguments `@validate_call(...)`.
Args:
func: The function to be decorated.
config: The configuration dictionary.
validate_return: Whether to validate the return value.
Returns:
The decorated function.
"""
parent_namespace
_typing_extra
parent_frame_namespace
def
validate
function
AnyCallableT
AnyCallableT
_check_function_type
function
validate_call_wrapper
_validate_call
ValidateCallWrapper
cast
_generate_schema
ValidateCallSupportedTypes
function
config
validate_return
parent_namespace
return
_validate_call
update_wrapper_attributes
function
validate_call_wrapper
__call__
# type: ignore
func
not
None
return
validate
func
else
return
validate
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 014_pydantic_extra_types_mac_address_pydantic_extra_types_mac_address_MacAddress.txt ---
Mac Address
The MAC address module provides functionality to parse and validate MAC addresses in different
formats, such as IEEE 802 MAC-48, EUI-48, EUI-64, or a 20-octet format.
MacAddress
Bases:
str
Represents a MAC address and provides methods for conversion, validation, and serialization.
from
pydantic
import
BaseModel
from
pydantic_extra_types.mac_address
import
MacAddress
class
Network
BaseModel
mac_address
MacAddress
network
Network
mac_address
"00:00:5e:00:53:01"
print
network
#> mac_address='00:00:5e:00:53:01'
validate_mac_address
staticmethod
validate_mac_address
value
bytes
str
Validate a MAC Address from the provided byte value.
Source code in
pydantic_extra_types/mac_address.py
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
@staticmethod
def
validate_mac_address
value
bytes
str
"""
Validate a MAC Address from the provided byte value.
"""
len
value
raise
PydanticCustomError
'mac_address_len'
'Length for a
{mac_address}
MAC address must be
{required_length}
'mac_address'
value
decode
(),
'required_length'
value
ord
':'
ord
'-'
)]:
len
value
raise
PydanticCustomError
'mac_address_format'
'Must have the format xx:xx:xx:xx:xx:xx or xx-xx-xx-xx-xx-xx'
len
value
not
raise
PydanticCustomError
'mac_address_format'
'Length for a
{mac_address}
MAC address must be
{required_length}
'mac_address'
value
decode
(),
'required_length'
)},
mac_address
bytearray
for
range
try
byte_value
int
value
mac_address
byte_value
except
ValueError
raise
PydanticCustomError
'mac_address_format'
'Unrecognized format'
from
elif
value
ord
'.'
len
value
raise
PydanticCustomError
'mac_address_format'
'Must have the format xx.xx.xx.xx.xx.xx'
len
value
not
raise
PydanticCustomError
'mac_address_format'
'Length for a
{mac_address}
MAC address must be
{required_length}
'mac_address'
value
decode
(),
'required_length'
)},
mac_address
bytearray
for
range
try
byte_value
int
value
mac_address
byte_value
byte_value
int
value
mac_address
byte_value
except
ValueError
raise
PydanticCustomError
'mac_address_format'
'Unrecognized format'
from
else
raise
PydanticCustomError
'mac_address_format'
'Unrecognized format'
return
':'
join
02x
for
mac_address
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 005_pydantic_core_pydantic_core_InitErrorDetails_type.txt ---
pydantic_core
__version__
module-attribute
__version__
str
SchemaValidator
SchemaValidator
schema
CoreSchema
config
CoreConfig
None
None
SchemaValidator
is the Python wrapper for
pydantic-core
's Rust validation logic, internally it owns one
CombinedValidator
which may in turn own more
CombinedValidator
s which make up the full schema validator.
Parameters:
Name
Type
Description
Default
schema
CoreSchema
The
CoreSchema
to use for validation.
required
config
CoreConfig
| None
Optionally a
CoreConfig
to configure validation.
None
title
property
title
str
The title of the schema, as used in the heading of
ValidationError.__str__()
validate_python
validate_python
input
Any
strict
bool
None
None
from_attributes
bool
None
None
context
Any
None
None
self_instance
Any
None
None
allow_partial
bool
Literal
"off"
"on"
"trailing-strings"
False
by_alias
bool
None
None
by_name
bool
None
None
Any
Validate a Python object against the schema and return the validated object.
Parameters:
Name
Type
Description
Default
input
Any
The Python object to validate.
required
strict
bool
| None
Whether to validate the object in strict mode.
None
, the value of
CoreConfig.strict
is used.
None
from_attributes
bool
| None
Whether to validate objects as inputs to models by extracting attributes.
None
, the value of
CoreConfig.from_attributes
is used.
None
context
Any
| None
The context to use for validation, this is passed to functional validators as
info.context
None
self_instance
Any
| None
An instance of a model set attributes on from validation, this is used when running
validation from the
__init__
method of a model.
None
allow_partial
bool
Literal
['off', 'on', 'trailing-strings']
Whether to allow partial validation; if
True
errors in the last element of sequences
and mappings are ignored.
'trailing-strings'
means any final unfinished JSON string is included in the result.
False
by_alias
bool
| None
Whether to use the field's alias when validating against the provided input data.
None
by_name
bool
| None
Whether to use the field's name when validating against the provided input data.
None
Raises:
Type
Description
ValidationError
If validation fails.
Exception
Other error types maybe raised if internal errors occur.
Returns:
Type
Description
Any
The validated object.
isinstance_python
isinstance_python
input
Any
strict
bool
None
None
from_attributes
bool
None
None
context
Any
None
None
self_instance
Any
None
None
by_alias
bool
None
None
by_name
bool
None
None
bool
Similar to
validate_python()
but returns a boolean.
Arguments match
validate_python()
. This method will not raise
ValidationError
s but will raise internal
errors.
Returns:
Type
Description
bool
True
if validation succeeds,
False
if validation fails.
validate_json
validate_json
input
str
bytes
bytearray
strict
bool
None
None
context
Any
None
None
self_instance
Any
None
None
allow_partial
bool
Literal
"off"
"on"
"trailing-strings"
False
by_alias
bool
None
None
by_name
bool
None
None
Any
Validate JSON data directly against the schema and return the validated Python object.
This method should be significantly faster than
validate_python(json.loads(json_data))
as it avoids the
need to create intermediate Python objects
It also handles constructing the correct Python type even in strict mode, where
validate_python(json.loads(json_data))
would fail validation.
Parameters:
Name
Type
Description
Default
input
str
bytes
bytearray
The JSON data to validate.
required
strict
bool
| None
Whether to validate the object in strict mode.
None
, the value of
CoreConfig.strict
is used.
None
context
Any
| None
The context to use for validation, this is passed to functional validators as
info.context
None
self_instance
Any
| None
An instance of a model set attributes on from validation.
None
allow_partial
bool
Literal
['off', 'on', 'trailing-strings']
Whether to allow partial validation; if
True
incomplete JSON will be parsed successfully
and errors in the last element of sequences and mappings are ignored.
'trailing-strings'
means any final unfinished JSON string is included in the result.
False
by_alias
bool
| None
Whether to use the field's alias when validating against the provided input data.
None
by_name
bool
| None
Whether to use the field's name when validating against the provided input data.
None
Raises:
Type
Description
ValidationError
If validation fails or if the JSON data is invalid.
Exception
Other error types maybe raised if internal errors occur.
Returns:
Type
Description
Any
The validated Python object.
validate_strings
validate_strings
input
_StringInput
strict
bool
None
None
context
Any
None
None
allow_partial
bool
Literal
"off"
"on"
"trailing-strings"
False
by_alias
bool
None
None
by_name
bool
None
None
Any
Validate a string against the schema and return the validated Python object.
This is similar to
validate_json
but applies to scenarios where the input will be a string but not
JSON data, e.g. URL fragments, query parameters, etc.
Parameters:
Name
Type
Description
Default
input
_StringInput
The input as a string, or bytes/bytearray if
strict=False
required
strict
bool
| None
Whether to validate the object in strict mode.
None
, the value of
CoreConfig.strict
is used.
None
context
Any
| None
The context to use for validation, this is passed to functional validators as
info.context
None
allow_partial
bool
Literal
['off', 'on', 'trailing-strings']
Whether to allow partial validation; if
True
errors in the last element of sequences
and mappings are ignored.
'trailing-strings'
means any final unfinished JSON string is included in the result.
False
by_alias
bool
| None
Whether to use the field's alias when validating against the provided input data.
None
by_name
bool
| None
Whether to use the field's name when validating against the provided input data.
None
Raises:
Type
Description
ValidationError
If validation fails or if the JSON data is invalid.
Exception
Other error types maybe raised if internal errors occur.
Returns:
Type
Description
Any
The validated Python object.
validate_assignment
validate_assignment
obj
Any
field_name
str
field_value
Any
strict
bool
None
None
from_attributes
bool
None
None
context
Any
None
None
by_alias
bool
None
None
by_name
bool
None
None
dict
str
Any
tuple
dict
str
Any
dict
str
Any
None
set
str
Validate an assignment to a field on a model.
Parameters:
Name
Type
Description
Default
obj
Any
The model instance being assigned to.
required
field_name
str
The name of the field to validate assignment for.
required
field_value
Any
The value to assign to the field.
required
strict
bool
| None
Whether to validate the object in strict mode.
None
, the value of
CoreConfig.strict
is used.
None
from_attributes
bool
| None
Whether to validate objects as inputs to models by extracting attributes.
None
, the value of
CoreConfig.from_attributes
is used.
None
context
Any
| None
The context to use for validation, this is passed to functional validators as
info.context
None
by_alias
bool
| None
Whether to use the field's alias when validating against the provided input data.
None
by_name
bool
| None
Whether to use the field's name when validating against the provided input data.
None
Raises:
Type
Description
ValidationError
If validation fails.
Exception
Other error types maybe raised if internal errors occur.
Returns:
Type
Description
dict
str
Any
] |
tuple
dict
str
Any
dict
str
Any
] | None,
set
str
Either the model dict or a tuple of
(model_data, model_extra, fields_set)
get_default_value
get_default_value
strict
bool
None
None
context
Any
None
Some
None
Get the default value for the schema, including running default value validation.
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether to validate the default value in strict mode.
None
, the value of
CoreConfig.strict
is used.
None
context
Any
The context to use for validation, this is passed to functional validators as
info.context
None
Raises:
Type
Description
ValidationError
If validation fails.
Exception
Other error types maybe raised if internal errors occur.
Returns:
Type
Description
Some
| None
None
if the schema has no default value, otherwise a
Some
containing the default.
SchemaSerializer
SchemaSerializer
schema
CoreSchema
config
CoreConfig
None
None
SchemaSerializer
is the Python wrapper for
pydantic-core
's Rust serialization logic, internally it owns one
CombinedSerializer
which may in turn own more
CombinedSerializer
s which make up the full schema serializer.
Parameters:
Name
Type
Description
Default
schema
CoreSchema
The
CoreSchema
to use for serialization.
required
config
CoreConfig
| None
Optionally a
CoreConfig
to to configure serialization.
None
to_python
to_python
value
Any
mode
str
None
None
include
_IncEx
None
None
exclude
_IncEx
None
None
by_alias
bool
None
None
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
round_trip
bool
False
warnings
bool
Literal
"none"
"warn"
"error"
True
fallback
Callable
Any
Any
None
None
serialize_as_any
bool
False
context
Any
None
None
Any
Serialize/marshal a Python object to a Python object including transforming and filtering data.
Parameters:
Name
Type
Description
Default
value
Any
The Python object to serialize.
required
mode
str
| None
The serialization mode to use, either
'python'
'json'
, defaults to
'python'
. In JSON mode,
all values are converted to JSON compatible types, e.g.
None
int
float
str
list
dict
None
include
_IncEx
| None
A set of fields to include, if
None
all fields are included.
None
exclude
_IncEx
| None
A set of fields to exclude, if
None
no fields are excluded.
None
by_alias
bool
| None
Whether to use the alias names of fields.
None
exclude_unset
bool
Whether to exclude fields that are not set,
e.g. are not included in
__pydantic_fields_set__
False
exclude_defaults
bool
Whether to exclude fields that are equal to their default value.
False
exclude_none
bool
Whether to exclude fields that have a value of
None
False
round_trip
bool
Whether to enable serialization and validation round-trip support.
False
warnings
bool
Literal
['none', 'warn', 'error']
How to handle invalid fields. False/"none" ignores them, True/"warn" logs errors,
"error" raises a
PydanticSerializationError
True
fallback
Callable
Any
Any
] | None
A function to call when an unknown value is encountered,
None
PydanticSerializationError
error is raised.
None
serialize_as_any
bool
Whether to serialize fields with duck-typing serialization behavior.
False
context
Any
| None
The context to use for serialization, this is passed to functional serializers as
info.context
None
Raises:
Type
Description
PydanticSerializationError
If serialization fails and no
fallback
function is provided.
Returns:
Type
Description
Any
The serialized Python object.
to_json
to_json
value
Any
indent
int
None
None
include
_IncEx
None
None
exclude
_IncEx
None
None
by_alias
bool
None
None
exclude_unset
bool
False
exclude_defaults
bool
False
exclude_none
bool
False
round_trip
bool
False
warnings
bool
Literal
"none"
"warn"
"error"
True
fallback
Callable
Any
Any
None
None
serialize_as_any
bool
False
context
Any
None
None
bytes
Serialize a Python object to JSON including transforming and filtering data.
Parameters:
Name
Type
Description
Default
value
Any
The Python object to serialize.
required
indent
int
| None
None
, the JSON will be compact, otherwise it will be pretty-printed with the indent provided.
None
include
_IncEx
| None
A set of fields to include, if
None
all fields are included.
None
exclude
_IncEx
| None
A set of fields to exclude, if
None
no fields are excluded.
None
by_alias
bool
| None
Whether to use the alias names of fields.
None
exclude_unset
bool
Whether to exclude fields that are not set,
e.g. are not included in
__pydantic_fields_set__
False
exclude_defaults
bool
Whether to exclude fields that are equal to their default value.
False
exclude_none
bool
Whether to exclude fields that have a value of
None
False
round_trip
bool
Whether to enable serialization and validation round-trip support.
False
warnings
bool
Literal
['none', 'warn', 'error']
How to handle invalid fields. False/"none" ignores them, True/"warn" logs errors,
"error" raises a
PydanticSerializationError
True
fallback
Callable
Any
Any
] | None
A function to call when an unknown value is encountered,
None
PydanticSerializationError
error is raised.
None
serialize_as_any
bool
Whether to serialize fields with duck-typing serialization behavior.
False
context
Any
| None
The context to use for serialization, this is passed to functional serializers as
info.context
None
Raises:
Type
Description
PydanticSerializationError
If serialization fails and no
fallback
function is provided.
Returns:
Type
Description
bytes
JSON bytes.
ValidationError
Bases:
ValueError
ValidationError
is the exception raised by
pydantic-core
when validation fails, it contains a list of errors
which detail why validation failed.
title
property
title
str
The title of the error, as used in the heading of
str(validation_error)
from_exception_data
classmethod
from_exception_data
title
str
line_errors
list
InitErrorDetails
input_type
Literal
"python"
"json"
"python"
hide_input
bool
False
Self
Python constructor for a Validation Error.
The API for constructing validation errors will probably change in the future,
hence the static method rather than
__init__
Parameters:
Name
Type
Description
Default
title
str
The title of the error, as used in the heading of
str(validation_error)
required
line_errors
list
InitErrorDetails
A list of
InitErrorDetails
which contain information
about errors that occurred during validation.
required
input_type
Literal
['python', 'json']
Whether the error is for a Python object or JSON.
'python'
hide_input
bool
Whether to hide the input value in the error message.
False
error_count
error_count
int
Returns:
Type
Description
int
The number of errors in the validation error.
errors
errors
include_url
bool
True
include_context
bool
True
include_input
bool
True
list
ErrorDetails
Details about each error in the validation error.
Parameters:
Name
Type
Description
Default
include_url
bool
Whether to include a URL to documentation on the error each error.
True
include_context
bool
Whether to include the context of each error.
True
include_input
bool
Whether to include the input value of each error.
True
Returns:
Type
Description
list
ErrorDetails
A list of
ErrorDetails
for each error in the validation error.
json
json
indent
int
None
None
include_url
bool
True
include_context
bool
True
include_input
bool
True
str
Same as
errors()
but returns a JSON string.
Parameters:
Name
Type
Description
Default
indent
int
| None
The number of spaces to indent the JSON by, or
None
for no indentation - compact JSON.
None
include_url
bool
Whether to include a URL to documentation on the error each error.
True
include_context
bool
Whether to include the context of each error.
True
include_input
bool
Whether to include the input value of each error.
True
Returns:
Type
Description
str
a JSON string.
ErrorDetails
Bases:
TypedDict
type
instance-attribute
type
str
The type of error that occurred, this is an identifier designed for
programmatic use that will change rarely or never.
type
is unique for each error message, and can hence be used as an identifier to build custom error messages.
loc
instance-attribute
loc
tuple
int
str
...
Tuple of strings and ints identifying where in the schema the error occurred.
msg
instance-attribute
msg
str
A human readable error message.
input
instance-attribute
input
Any
The input data at this
loc
that caused the error.
ctx
instance-attribute
ctx
NotRequired
dict
str
Any
Values which are required to render the error message, and could hence be useful in rendering custom error messages.
Also useful for passing custom error data forward.
url
instance-attribute
url
NotRequired
str
The documentation URL giving information about the error. No URL is available if
PydanticCustomError
is used.
InitErrorDetails
Bases:
TypedDict
type
instance-attribute
type
str
PydanticCustomError
The type of error that occurred, this should be a "slug" identifier that changes rarely or never.
loc
instance-attribute
loc
NotRequired
tuple
int
str
...
Tuple of strings and ints identifying where in the schema the error occurred.
input
instance-attribute
input
Any
The input data at this
loc
that caused the error.
ctx
instance-attribute
ctx
NotRequired
dict
str
Any
Values which are required to render the error message, and could hence be useful in rendering custom error messages.
Also useful for passing custom error data forward.
SchemaError
Bases:
Exception
Information about errors that occur while building a
SchemaValidator
SchemaSerializer
error_count
error_count
int
Returns:
Type
Description
int
The number of errors in the schema.
errors
errors
list
ErrorDetails
Returns:
Type
Description
list
ErrorDetails
A list of
ErrorDetails
for each error in the schema.
PydanticCustomError
PydanticCustomError
error_type
LiteralString
message_template
LiteralString
context
dict
str
Any
None
None
Bases:
ValueError
A custom exception providing flexible error handling for Pydantic validators.
You can raise this error in custom validators when you'd like flexibility in regards to the error type, message, and context.
Example
from
pydantic_core
import
PydanticCustomError
def
custom_validator
None
raise
PydanticCustomError
'custom_value_error'
'Value must be greater than
{value}
'value'
'extra_context'
'extra_data'
return
Parameters:
Name
Type
Description
Default
error_type
LiteralString
The error type.
required
message_template
LiteralString
The message template.
required
context
dict
str
Any
] | None
The data to inject into the message template.
None
context
property
context
dict
str
Any
None
Values which are required to render the error message, and could hence be useful in passing error data forward.
type
property
type
str
The error type associated with the error. For consistency with Pydantic, this is typically a snake_case string.
message_template
property
message_template
str
The message template associated with the error. This is a string that can be formatted with context variables in
{curly_braces}
message
message
str
The formatted message associated with the error. This presents as the message template with context variables appropriately injected.
PydanticKnownError
PydanticKnownError
error_type
ErrorType
context
dict
str
Any
None
None
Bases:
ValueError
A helper class for raising exceptions that mimic Pydantic's built-in exceptions, with more flexibility in regards to context.
Unlike
PydanticCustomError
, the
error_type
argument must be a known
ErrorType
Example
from
pydantic_core
import
PydanticKnownError
def
custom_validator
None
raise
PydanticKnownError
error_type
'greater_than'
context
'gt'
return
Parameters:
Name
Type
Description
Default
error_type
ErrorType
The error type.
required
context
dict
str
Any
] | None
The data to inject into the message template.
None
context
property
context
dict
str
Any
None
Values which are required to render the error message, and could hence be useful in passing error data forward.
type
property
type
ErrorType
The type of the error.
message_template
property
message_template
str
The message template associated with the provided error type. This is a string that can be formatted with context variables in
{curly_braces}
message
message
str
The formatted message associated with the error. This presents as the message template with context variables appropriately injected.
PydanticOmit
Bases:
Exception
An exception to signal that a field should be omitted from a generated result.
This could span from omitting a field from a JSON Schema to omitting a field from a serialized result.
Upcoming: more robust support for using PydanticOmit in custom serializers is still in development.
Right now, this is primarily used in the JSON Schema generation process.
Example
from
typing
import
Callable
from
pydantic_core
import
PydanticOmit
from
pydantic
import
BaseModel
from
pydantic.json_schema
import
GenerateJsonSchema
JsonSchemaValue
class
MyGenerateJsonSchema
GenerateJsonSchema
def
handle_invalid_for_json_schema
self
schema
error_info
JsonSchemaValue
raise
PydanticOmit
class
Predicate
BaseModel
name
str
'no-op'
func
Callable
lambda
instance_example
Predicate
validation_schema
instance_example
model_json_schema
schema_generator
MyGenerateJsonSchema
mode
'validation'
print
validation_schema
'''
{'properties': {'name': {'default': 'no-op', 'title': 'Name', 'type': 'string'}}, 'title': 'Predicate', 'type': 'object'}
'''
For a more in depth example / explanation, see the
customizing JSON schema
docs.
PydanticUseDefault
Bases:
Exception
An exception to signal that standard validation either failed or should be skipped, and the default value should be used instead.
This warning can be raised in custom valiation functions to redirect the flow of validation.
Example
from
pydantic_core
import
PydanticUseDefault
from
datetime
import
datetime
from
pydantic
import
BaseModel
field_validator
class
Event
BaseModel
name
str
'meeting'
time
datetime
@field_validator
'name'
mode
'plain'
def
name_must_be_present
cls
str
not
not
isinstance
str
raise
PydanticUseDefault
return
event1
Event
name
'party'
time
datetime
2024
print
repr
event1
# > Event(name='party', time=datetime.datetime(2024, 1, 1, 12, 0))
event2
Event
time
datetime
2024
print
repr
event2
# > Event(name='meeting', time=datetime.datetime(2024, 1, 1, 12, 0))
For an additional example, see the
validating partial json data
section of the Pydantic documentation.
PydanticSerializationError
PydanticSerializationError
message
str
Bases:
ValueError
An error raised when an issue occurs during serialization.
In custom serializers, this error can be used to indicate that serialization has failed.
Parameters:
Name
Type
Description
Default
message
str
The message associated with the error.
required
PydanticSerializationUnexpectedValue
PydanticSerializationUnexpectedValue
message
str
Bases:
ValueError
An error raised when an unexpected value is encountered during serialization.
This error is often caught and coerced into a warning, as
pydantic-core
generally makes a best attempt
at serializing values, in contrast with validation where errors are eagerly raised.
Example
from
pydantic
import
BaseModel
field_serializer
from
pydantic_core
import
PydanticSerializationUnexpectedValue
class
BasicPoint
BaseModel
int
int
@field_serializer
'*'
def
serialize
self
not
isinstance
int
raise
PydanticSerializationUnexpectedValue
'Expected type `int`, got
type
with value
return
point
BasicPoint
# some sort of mutation
point
'a'
print
point
model_dump
())
'''
UserWarning: Pydantic serializer warnings:
PydanticSerializationUnexpectedValue(Expected type `int`, got <class 'str'> with value a)
return self.__pydantic_serializer__.to_python(
{'x': 'a', 'y': 2}
'''
This is often used internally in
pydantic-core
when unexpected types are encountered during serialization,
but it can also be used by users in custom serializers, as seen above.
Parameters:
Name
Type
Description
Default
message
str
The message associated with the unexpected value.
required
Url
Url
url
str
Bases:
SupportsAllComparisons
A URL type, internal logic uses the
url rust crate
originally developed
by Mozilla.
MultiHostUrl
MultiHostUrl
url
str
Bases:
SupportsAllComparisons
A URL type with support for multiple hosts, as used by some databases for DSNs, e.g.
https://foo.com,bar.com/path
Internal URL logic uses the
url rust crate
originally developed
by Mozilla.
MultiHostHost
Bases:
TypedDict
A host part of a multi-host URL.
username
instance-attribute
username
str
None
The username part of this host, or
None
password
instance-attribute
password
str
None
The password part of this host, or
None
host
instance-attribute
host
str
None
The host part of this host, or
None
port
instance-attribute
port
int
None
The port part of this host, or
None
ArgsKwargs
ArgsKwargs
args
tuple
Any
...
kwargs
dict
str
Any
None
None
A construct used to store arguments and keyword arguments for a function call.
This data structure is generally used to store information for core schemas associated with functions (like in an arguments schema).
This data structure is also currently used for some validation against dataclasses.
Example
from
pydantic.dataclasses
import
dataclass
from
pydantic
import
model_validator
@dataclass
class
Model
int
int
@model_validator
mode
"before"
@classmethod
def
no_op_validator
cls
values
print
values
return
values
Model
#> ArgsKwargs((1,), {"b": 2})
Model
#> ArgsKwargs((1, 2), {})
Model
#> ArgsKwargs((), {"a": 1, "b": 2})
Parameters:
Name
Type
Description
Default
args
tuple
Any
, ...]
The arguments (inherently ordered) for a function call.
required
kwargs
dict
str
Any
] | None
The keyword arguments for a function call
None
args
property
args
tuple
Any
...
The arguments (inherently ordered) for a function call.
kwargs
property
kwargs
dict
str
Any
None
The keyword arguments for a function call.
Some
Bases:
Generic
Similar to Rust's
Option::Some
type, this
identifies a value as being present, and provides a way to access it.
Generally used in a union with
None
to different between "some value which could be None" and no value.
value
property
value
Returns the value wrapped by
Some
TzInfo
Bases:
tzinfo
pydantic-core
implementation of the abstract
datetime.tzinfo
class.
tzname
tzname
datetime
None
str
None
Return the time zone name corresponding to the
datetime
object
, as a string.
For more info, see
tzinfo.tzname
utcoffset
utcoffset
datetime
None
timedelta
None
Return offset of local time from UTC, as a
timedelta
object that is positive east of UTC. If local time is west of UTC, this should be negative.
More info can be found at
tzinfo.utcoffset
dst
dst
datetime
None
timedelta
None
Return the daylight saving time (DST) adjustment, as a
timedelta
object or
None
if DST information isnât known.
More info can be found at
tzinfo.dst
fromutc
fromutc
datetime
datetime
Adjust the date and time data associated datetime object
, returning an equivalent datetime in selfâs local time.
More info can be found at
tzinfo.fromutc
ErrorTypeInfo
Bases:
TypedDict
Gives information about errors.
type
instance-attribute
type
ErrorType
The type of error that occurred, this should a "slug" identifier that changes rarely or never.
message_template_python
instance-attribute
message_template_python
str
String template to render a human readable error message from using context, when the input is Python.
example_message_python
instance-attribute
example_message_python
str
Example of a human readable error message, when the input is Python.
message_template_json
instance-attribute
message_template_json
NotRequired
str
String template to render a human readable error message from using context, when the input is JSON data.
example_message_json
instance-attribute
example_message_json
NotRequired
str
Example of a human readable error message, when the input is JSON data.
example_context
instance-attribute
example_context
dict
str
Any
None
Example of context values.
to_json
to_json
value
Any
indent
int
None
None
include
_IncEx
None
None
exclude
_IncEx
None
None
by_alias
bool
True
exclude_none
bool
False
round_trip
bool
False
timedelta_mode
Literal
"iso8601"
"float"
"iso8601"
bytes_mode
Literal
"utf8"
"base64"
"hex"
"utf8"
inf_nan_mode
Literal
"null"
"constants"
"strings"
"constants"
serialize_unknown
bool
False
fallback
Callable
Any
Any
None
None
serialize_as_any
bool
False
context
Any
None
None
bytes
Serialize a Python object to JSON including transforming and filtering data.
This is effectively a standalone version of
SchemaSerializer.to_json
Parameters:
Name
Type
Description
Default
value
Any
The Python object to serialize.
required
indent
int
| None
None
, the JSON will be compact, otherwise it will be pretty-printed with the indent provided.
None
include
_IncEx
| None
A set of fields to include, if
None
all fields are included.
None
exclude
_IncEx
| None
A set of fields to exclude, if
None
no fields are excluded.
None
by_alias
bool
Whether to use the alias names of fields.
True
exclude_none
bool
Whether to exclude fields that have a value of
None
False
round_trip
bool
Whether to enable serialization and validation round-trip support.
False
timedelta_mode
Literal
['iso8601', 'float']
How to serialize
timedelta
objects, either
'iso8601'
'float'
'iso8601'
bytes_mode
Literal
['utf8', 'base64', 'hex']
How to serialize
bytes
objects, either
'utf8'
'base64'
, or
'hex'
'utf8'
inf_nan_mode
Literal
['null', 'constants', 'strings']
How to serialize
Infinity
-Infinity
and
NaN
values, either
'null'
'constants'
, or
'strings'
'constants'
serialize_unknown
bool
Attempt to serialize unknown types,
str(value)
will be used, if that fails
"<Unserializable {value_type} object>"
will be used.
False
fallback
Callable
Any
Any
] | None
A function to call when an unknown value is encountered,
None
PydanticSerializationError
error is raised.
None
serialize_as_any
bool
Whether to serialize fields with duck-typing serialization behavior.
False
context
Any
| None
The context to use for serialization, this is passed to functional serializers as
info.context
None
Raises:
Type
Description
PydanticSerializationError
If serialization fails and no
fallback
function is provided.
Returns:
Type
Description
bytes
JSON bytes.
from_json
from_json
data
str
bytes
bytearray
allow_inf_nan
bool
True
cache_strings
bool
Literal
"all"
"keys"
"none"
True
allow_partial
bool
Literal
"off"
"on"
"trailing-strings"
False
Any
Deserialize JSON data to a Python object.
This is effectively a faster version of
json.loads()
, with some extra functionality.
Parameters:
Name
Type
Description
Default
data
str
bytes
bytearray
The JSON data to deserialize.
required
allow_inf_nan
bool
Whether to allow
Infinity
-Infinity
and
NaN
values as
json.loads()
does by default.
True
cache_strings
bool
Literal
['all', 'keys', 'none']
Whether to cache strings to avoid constructing new Python objects,
this should have a significant impact on performance while increasing memory usage slightly,
all/True
means cache all strings,
keys
means cache only dict keys,
none/False
means no caching.
True
allow_partial
bool
Literal
['off', 'on', 'trailing-strings']
Whether to allow partial deserialization, if
True
JSON data is returned if the end of the
input is reached before the full object is deserialized, e.g.
["aa", "bb", "c
would return
['aa', 'bb']
'trailing-strings'
means any final unfinished JSON string is included in the result.
False
Raises:
Type
Description
ValueError
If deserialization fails.
Returns:
Type
Description
Any
The deserialized Python object.
to_jsonable_python
to_jsonable_python
value
Any
include
_IncEx
None
None
exclude
_IncEx
None
None
by_alias
bool
True
exclude_none
bool
False
round_trip
bool
False
timedelta_mode
Literal
"iso8601"
"float"
"iso8601"
bytes_mode
Literal
"utf8"
"base64"
"hex"
"utf8"
inf_nan_mode
Literal
"null"
"constants"
"strings"
"constants"
serialize_unknown
bool
False
fallback
Callable
Any
Any
None
None
serialize_as_any
bool
False
context
Any
None
None
Any
Serialize/marshal a Python object to a JSON-serializable Python object including transforming and filtering data.
This is effectively a standalone version of
SchemaSerializer.to_python(mode='json')
Parameters:
Name
Type
Description
Default
value
Any
The Python object to serialize.
required
include
_IncEx
| None
A set of fields to include, if
None
all fields are included.
None
exclude
_IncEx
| None
A set of fields to exclude, if
None
no fields are excluded.
None
by_alias
bool
Whether to use the alias names of fields.
True
exclude_none
bool
Whether to exclude fields that have a value of
None
False
round_trip
bool
Whether to enable serialization and validation round-trip support.
False
timedelta_mode
Literal
['iso8601', 'float']
How to serialize
timedelta
objects, either
'iso8601'
'float'
'iso8601'
bytes_mode
Literal
['utf8', 'base64', 'hex']
How to serialize
bytes
objects, either
'utf8'
'base64'
, or
'hex'
'utf8'
inf_nan_mode
Literal
['null', 'constants', 'strings']
How to serialize
Infinity
-Infinity
and
NaN
values, either
'null'
'constants'
, or
'strings'
'constants'
serialize_unknown
bool
Attempt to serialize unknown types,
str(value)
will be used, if that fails
"<Unserializable {value_type} object>"
will be used.
False
fallback
Callable
Any
Any
] | None
A function to call when an unknown value is encountered,
None
PydanticSerializationError
error is raised.
None
serialize_as_any
bool
Whether to serialize fields with duck-typing serialization behavior.
False
context
Any
| None
The context to use for serialization, this is passed to functional serializers as
info.context
None
Raises:
Type
Description
PydanticSerializationError
If serialization fails and no
fallback
function is provided.
Returns:
Type
Description
Any
The serialized Python object.
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!

--- 015_types___code_73_annotation_2.txt ---
Pydantic Types
pydantic.types
The types module contains custom types used by pydantic.
StrictBool
module-attribute
StrictBool
Annotated
bool
Strict
()]
A boolean that must be either
True
False
PositiveInt
module-attribute
PositiveInt
Annotated
int
An integer that must be greater than zero.
from
pydantic
import
BaseModel
PositiveInt
ValidationError
class
Model
BaseModel
positive_int
PositiveInt
Model
positive_int
print
repr
#> Model(positive_int=1)
try
Model
positive_int
except
ValidationError
print
errors
())
'''
'type': 'greater_than',
'loc': ('positive_int',),
'msg': 'Input should be greater than 0',
'input': -1,
'ctx': {'gt': 0},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
NegativeInt
module-attribute
NegativeInt
Annotated
int
An integer that must be less than zero.
from
pydantic
import
BaseModel
NegativeInt
ValidationError
class
Model
BaseModel
negative_int
NegativeInt
Model
negative_int
print
repr
#> Model(negative_int=-1)
try
Model
negative_int
except
ValidationError
print
errors
())
'''
'type': 'less_than',
'loc': ('negative_int',),
'msg': 'Input should be less than 0',
'input': 1,
'ctx': {'lt': 0},
'url': 'https://errors.pydantic.dev/2/v/less_than',
'''
NonPositiveInt
module-attribute
NonPositiveInt
Annotated
int
An integer that must be less than or equal to zero.
from
pydantic
import
BaseModel
NonPositiveInt
ValidationError
class
Model
BaseModel
non_positive_int
NonPositiveInt
Model
non_positive_int
print
repr
#> Model(non_positive_int=0)
try
Model
non_positive_int
except
ValidationError
print
errors
())
'''
'type': 'less_than_equal',
'loc': ('non_positive_int',),
'msg': 'Input should be less than or equal to 0',
'input': 1,
'ctx': {'le': 0},
'url': 'https://errors.pydantic.dev/2/v/less_than_equal',
'''
NonNegativeInt
module-attribute
NonNegativeInt
Annotated
int
An integer that must be greater than or equal to zero.
from
pydantic
import
BaseModel
NonNegativeInt
ValidationError
class
Model
BaseModel
non_negative_int
NonNegativeInt
Model
non_negative_int
print
repr
#> Model(non_negative_int=0)
try
Model
non_negative_int
except
ValidationError
print
errors
())
'''
'type': 'greater_than_equal',
'loc': ('non_negative_int',),
'msg': 'Input should be greater than or equal to 0',
'input': -1,
'ctx': {'ge': 0},
'url': 'https://errors.pydantic.dev/2/v/greater_than_equal',
'''
StrictInt
module-attribute
StrictInt
Annotated
int
Strict
()]
An integer that must be validated in strict mode.
from
pydantic
import
BaseModel
StrictInt
ValidationError
class
StrictIntModel
BaseModel
strict_int
StrictInt
try
StrictIntModel
strict_int
3.14159
except
ValidationError
print
'''
1 validation error for StrictIntModel
strict_int
Input should be a valid integer [type=int_type, input_value=3.14159, input_type=float]
'''
PositiveFloat
module-attribute
PositiveFloat
Annotated
float
A float that must be greater than zero.
from
pydantic
import
BaseModel
PositiveFloat
ValidationError
class
Model
BaseModel
positive_float
PositiveFloat
Model
positive_float
1.0
print
repr
#> Model(positive_float=1.0)
try
Model
positive_float
1.0
except
ValidationError
print
errors
())
'''
'type': 'greater_than',
'loc': ('positive_float',),
'msg': 'Input should be greater than 0',
'input': -1.0,
'ctx': {'gt': 0.0},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
NegativeFloat
module-attribute
NegativeFloat
Annotated
float
A float that must be less than zero.
from
pydantic
import
BaseModel
NegativeFloat
ValidationError
class
Model
BaseModel
negative_float
NegativeFloat
Model
negative_float
1.0
print
repr
#> Model(negative_float=-1.0)
try
Model
negative_float
1.0
except
ValidationError
print
errors
())
'''
'type': 'less_than',
'loc': ('negative_float',),
'msg': 'Input should be less than 0',
'input': 1.0,
'ctx': {'lt': 0.0},
'url': 'https://errors.pydantic.dev/2/v/less_than',
'''
NonPositiveFloat
module-attribute
NonPositiveFloat
Annotated
float
A float that must be less than or equal to zero.
from
pydantic
import
BaseModel
NonPositiveFloat
ValidationError
class
Model
BaseModel
non_positive_float
NonPositiveFloat
Model
non_positive_float
0.0
print
repr
#> Model(non_positive_float=0.0)
try
Model
non_positive_float
1.0
except
ValidationError
print
errors
())
'''
'type': 'less_than_equal',
'loc': ('non_positive_float',),
'msg': 'Input should be less than or equal to 0',
'input': 1.0,
'ctx': {'le': 0.0},
'url': 'https://errors.pydantic.dev/2/v/less_than_equal',
'''
NonNegativeFloat
module-attribute
NonNegativeFloat
Annotated
float
A float that must be greater than or equal to zero.
from
pydantic
import
BaseModel
NonNegativeFloat
ValidationError
class
Model
BaseModel
non_negative_float
NonNegativeFloat
Model
non_negative_float
0.0
print
repr
#> Model(non_negative_float=0.0)
try
Model
non_negative_float
1.0
except
ValidationError
print
errors
())
'''
'type': 'greater_than_equal',
'loc': ('non_negative_float',),
'msg': 'Input should be greater than or equal to 0',
'input': -1.0,
'ctx': {'ge': 0.0},
'url': 'https://errors.pydantic.dev/2/v/greater_than_equal',
'''
StrictFloat
module-attribute
StrictFloat
Annotated
float
Strict
True
A float that must be validated in strict mode.
from
pydantic
import
BaseModel
StrictFloat
ValidationError
class
StrictFloatModel
BaseModel
strict_float
StrictFloat
try
StrictFloatModel
strict_float
'1.0'
except
ValidationError
print
'''
1 validation error for StrictFloatModel
strict_float
Input should be a valid number [type=float_type, input_value='1.0', input_type=str]
'''
FiniteFloat
module-attribute
FiniteFloat
Annotated
float
AllowInfNan
False
A float that must be finite (not
-inf
inf
, or
nan
from
pydantic
import
BaseModel
FiniteFloat
class
Model
BaseModel
finite
FiniteFloat
Model
finite
1.0
print
#> finite=1.0
StrictBytes
module-attribute
StrictBytes
Annotated
bytes
Strict
()]
A bytes that must be validated in strict mode.
StrictStr
module-attribute
StrictStr
Annotated
str
Strict
()]
A string that must be validated in strict mode.
UUID1
module-attribute
UUID1
Annotated
UUID
UuidVersion
UUID
that must be version 1.
import
uuid
from
pydantic
import
UUID1
BaseModel
class
Model
BaseModel
uuid1
UUID1
Model
uuid1
uuid
uuid1
())
UUID3
module-attribute
UUID3
Annotated
UUID
UuidVersion
UUID
that must be version 3.
import
uuid
from
pydantic
import
UUID3
BaseModel
class
Model
BaseModel
uuid3
UUID3
Model
uuid3
uuid
uuid3
uuid
NAMESPACE_DNS
'pydantic.org'
UUID4
module-attribute
UUID4
Annotated
UUID
UuidVersion
UUID
that must be version 4.
import
uuid
from
pydantic
import
UUID4
BaseModel
class
Model
BaseModel
uuid4
UUID4
Model
uuid4
uuid
uuid4
())
UUID5
module-attribute
UUID5
Annotated
UUID
UuidVersion
UUID
that must be version 5.
import
uuid
from
pydantic
import
UUID5
BaseModel
class
Model
BaseModel
uuid5
UUID5
Model
uuid5
uuid
uuid5
uuid
NAMESPACE_DNS
'pydantic.org'
UUID6
module-attribute
UUID6
Annotated
UUID
UuidVersion
UUID
that must be version 6.
import
uuid
from
pydantic
import
UUID6
BaseModel
class
Model
BaseModel
uuid6
UUID6
Model
uuid6
uuid
UUID
'1efea953-c2d6-6790-aa0a-69db8c87df97'
UUID7
module-attribute
UUID7
Annotated
UUID
UuidVersion
UUID
that must be version 7.
import
uuid
from
pydantic
import
UUID7
BaseModel
class
Model
BaseModel
uuid7
UUID7
Model
uuid7
uuid
UUID
'0194fdcb-1c47-7a09-b52c-561154de0b4a'
UUID8
module-attribute
UUID8
Annotated
UUID
UuidVersion
UUID
that must be version 8.
import
uuid
from
pydantic
import
UUID8
BaseModel
class
Model
BaseModel
uuid8
UUID8
Model
uuid8
uuid
UUID
'81a0b92e-6078-8551-9c81-8ccb666bdab8'
FilePath
module-attribute
FilePath
Annotated
Path
PathType
'file'
A path that must point to a file.
from
pathlib
import
Path
from
pydantic
import
BaseModel
FilePath
ValidationError
class
Model
BaseModel
FilePath
path
Path
'text.txt'
path
touch
Model
'text.txt'
print
model_dump
())
#> {'f': PosixPath('text.txt')}
path
unlink
path
Path
'directory'
path
mkdir
exist_ok
True
try
Model
'directory'
# directory
except
ValidationError
print
'''
1 validation error for Model
Path does not point to a file [type=path_not_file, input_value='directory', input_type=str]
'''
path
rmdir
try
Model
'not-exists-file'
except
ValidationError
print
'''
1 validation error for Model
Path does not point to a file [type=path_not_file, input_value='not-exists-file', input_type=str]
'''
DirectoryPath
module-attribute
DirectoryPath
Annotated
Path
PathType
'dir'
A path that must point to a directory.
from
pathlib
import
Path
from
pydantic
import
BaseModel
DirectoryPath
ValidationError
class
Model
BaseModel
DirectoryPath
path
Path
'directory/'
path
mkdir
Model
'directory/'
print
model_dump
())
#> {'f': PosixPath('directory')}
path
rmdir
path
Path
'file.txt'
path
touch
try
Model
'file.txt'
# file
except
ValidationError
print
'''
1 validation error for Model
Path does not point to a directory [type=path_not_directory, input_value='file.txt', input_type=str]
'''
path
unlink
try
Model
'not-exists-directory'
except
ValidationError
print
'''
1 validation error for Model
Path does not point to a directory [type=path_not_directory, input_value='not-exists-directory', input_type=str]
'''
NewPath
module-attribute
NewPath
Annotated
Path
PathType
'new'
A path for a new file or directory that must not already exist. The parent directory must already exist.
SocketPath
module-attribute
SocketPath
Annotated
Path
PathType
'socket'
A path to an existing socket file
Base64Bytes
module-attribute
Base64Bytes
Annotated
bytes
EncodedBytes
encoder
Base64Encoder
A bytes type that is encoded and decoded using the standard (non-URL-safe) base64 encoder.
Note
Under the hood,
Base64Bytes
uses the standard library
base64.b64encode
and
base64.b64decode
functions.
As a result, attempting to decode url-safe base64 data using the
Base64Bytes
type may fail or produce an incorrect
decoding.
Warning
In versions of Pydantic prior to v2.10,
Base64Bytes
used
base64.encodebytes
and
base64.decodebytes
functions. According to the
base64 documentation
these methods are considered legacy implementation, and thus, Pydantic v2.10+ now uses the modern
base64.b64encode
and
base64.b64decode
functions.
If you'd still like to use these legacy encoders / decoders, you can achieve this by creating a custom annotated type,
like follows:
import
base64
from
typing
import
Annotated
Literal
from
pydantic_core
import
PydanticCustomError
from
pydantic
import
EncodedBytes
EncoderProtocol
class
LegacyBase64Encoder
EncoderProtocol
@classmethod
def
decode
cls
data
bytes
bytes
try
return
base64
decodebytes
data
except
ValueError
raise
PydanticCustomError
'base64_decode'
"Base64 decoding error: '
{error}
'error'
str
)},
@classmethod
def
encode
cls
value
bytes
bytes
return
base64
encodebytes
value
@classmethod
def
get_json_format
cls
Literal
'base64'
return
'base64'
LegacyBase64Bytes
Annotated
bytes
EncodedBytes
encoder
LegacyBase64Encoder
from
pydantic
import
Base64Bytes
BaseModel
ValidationError
class
Model
BaseModel
base64_bytes
Base64Bytes
# Initialize the model with base64 data
Model
base64_bytes
'VGhpcyBpcyB0aGUgd2F5'
# Access decoded value
print
base64_bytes
#> b'This is the way'
# Serialize into the base64 form
print
model_dump
())
#> {'base64_bytes': b'VGhpcyBpcyB0aGUgd2F5'}
# Validate base64 data
try
print
Model
base64_bytes
'undecodable'
base64_bytes
except
ValidationError
print
'''
1 validation error for Model
base64_bytes
Base64 decoding error: 'Incorrect padding' [type=base64_decode, input_value=b'undecodable', input_type=bytes]
'''
Base64Str
module-attribute
Base64Str
Annotated
str
EncodedStr
encoder
Base64Encoder
A str type that is encoded and decoded using the standard (non-URL-safe) base64 encoder.
Note
Under the hood,
Base64Str
uses the standard library
base64.b64encode
and
base64.b64decode
functions.
As a result, attempting to decode url-safe base64 data using the
Base64Str
type may fail or produce an incorrect
decoding.
Warning
In versions of Pydantic prior to v2.10,
Base64Str
used
base64.encodebytes
and
base64.decodebytes
functions. According to the
base64 documentation
these methods are considered legacy implementation, and thus, Pydantic v2.10+ now uses the modern
base64.b64encode
and
base64.b64decode
functions.
See the
Base64Bytes
type for more information on how to
replicate the old behavior with the legacy encoders / decoders.
from
pydantic
import
Base64Str
BaseModel
ValidationError
class
Model
BaseModel
base64_str
Base64Str
# Initialize the model with base64 data
Model
base64_str
'VGhlc2UgYXJlbid0IHRoZSBkcm9pZHMgeW91J3JlIGxvb2tpbmcgZm9y'
# Access decoded value
print
base64_str
#> These aren't the droids you're looking for
# Serialize into the base64 form
print
model_dump
())
#> {'base64_str': 'VGhlc2UgYXJlbid0IHRoZSBkcm9pZHMgeW91J3JlIGxvb2tpbmcgZm9y'}
# Validate base64 data
try
print
Model
base64_str
'undecodable'
base64_str
except
ValidationError
print
'''
1 validation error for Model
base64_str
Base64 decoding error: 'Incorrect padding' [type=base64_decode, input_value='undecodable', input_type=str]
'''
Base64UrlBytes
module-attribute
Base64UrlBytes
Annotated
bytes
EncodedBytes
encoder
Base64UrlEncoder
A bytes type that is encoded and decoded using the URL-safe base64 encoder.
Note
Under the hood,
Base64UrlBytes
use standard library
base64.urlsafe_b64encode
and
base64.urlsafe_b64decode
functions.
As a result, the
Base64UrlBytes
type can be used to faithfully decode "vanilla" base64 data
(using
'+'
and
'/'
from
pydantic
import
Base64UrlBytes
BaseModel
class
Model
BaseModel
base64url_bytes
Base64UrlBytes
# Initialize the model with base64 data
Model
base64url_bytes
'SHc_dHc-TXc=='
print
#> base64url_bytes=b'Hw?tw>Mw'
Base64UrlStr
module-attribute
Base64UrlStr
Annotated
str
EncodedStr
encoder
Base64UrlEncoder
A str type that is encoded and decoded using the URL-safe base64 encoder.
Note
Under the hood,
Base64UrlStr
use standard library
base64.urlsafe_b64encode
and
base64.urlsafe_b64decode
functions.
As a result, the
Base64UrlStr
type can be used to faithfully decode "vanilla" base64 data (using
'+'
and
'/'
from
pydantic
import
Base64UrlStr
BaseModel
class
Model
BaseModel
base64url_str
Base64UrlStr
# Initialize the model with base64 data
Model
base64url_str
'SHc_dHc-TXc=='
print
#> base64url_str='Hw?tw>Mw'
JsonValue
module-attribute
JsonValue
TypeAlias
Union
list
"JsonValue"
dict
str
"JsonValue"
str
bool
int
float
None
JsonValue
is used to represent a value that can be serialized to JSON.
It may be one of:
list['JsonValue']
dict[str, 'JsonValue']
str
bool
int
float
None
The following example demonstrates how to use
JsonValue
to validate JSON data,
and what kind of errors to expect when input data is not json serializable.
import
json
from
pydantic
import
BaseModel
JsonValue
ValidationError
class
Model
BaseModel
JsonValue
valid_json_data
'j'
'a'
'b'
'c'
'd'
None
]}}}}
invalid_json_data
'j'
'a'
'b'
...
}}}
print
repr
Model
model_validate
valid_json_data
)))
#> Model(j={'a': {'b': {'c': 1, 'd': [2, None]}}})
print
repr
Model
model_validate_json
json
dumps
valid_json_data
))))
#> Model(j={'a': {'b': {'c': 1, 'd': [2, None]}}})
try
Model
model_validate
invalid_json_data
except
ValidationError
print
'''
1 validation error for Model
j.dict.a.dict.b
input was not a valid JSON value [type=invalid-json-value, input_value=Ellipsis, input_type=ellipsis]
'''
OnErrorOmit
module-attribute
OnErrorOmit
Annotated
_OnErrorOmit
When used as an item in a list, the key type in a dict, optional values of a TypedDict, etc.
this annotation omits the item from the iteration if there is any error validating it.
That is, instead of a
ValidationError
being propagated up and the entire iterable being discarded
any invalid items are discarded and the valid ones are returned.
Strict
dataclass
Bases:
PydanticMetadata
BaseMetadata
Usage Documentation
Strict Mode with
Annotated
Strict
A field metadata class to indicate that a field should be validated in strict mode.
Use this class as an annotation via
Annotated
, as seen below.
Attributes:
Name
Type
Description
strict
bool
Whether to validate the field in strict mode.
Example
from
typing
import
Annotated
from
pydantic.types
import
Strict
StrictBool
Annotated
bool
Strict
()]
Source code in
pydantic/types.py
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
@_dataclasses
dataclass
class
Strict
_fields
PydanticMetadata
BaseMetadata
"""!!! abstract "Usage Documentation"
[Strict Mode with `Annotated` `Strict`](../concepts/strict_mode.md#strict-mode-with-annotated-strict)
A field metadata class to indicate that a field should be validated in strict mode.
Use this class as an annotation via [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated), as seen below.
Attributes:
strict: Whether to validate the field in strict mode.
Example:
```python
from typing import Annotated
from pydantic.types import Strict
StrictBool = Annotated[bool, Strict()]
```
"""
strict
bool
True
def
__hash__
self
int
return
hash
self
strict
AllowInfNan
dataclass
Bases:
PydanticMetadata
A field metadata class to indicate that a field should allow
-inf
inf
, and
nan
Use this class as an annotation via
Annotated
, as seen below.
Attributes:
Name
Type
Description
allow_inf_nan
bool
Whether to allow
-inf
inf
, and
nan
. Defaults to
True
Example
from
typing
import
Annotated
from
pydantic.types
import
AllowInfNan
LaxFloat
Annotated
float
AllowInfNan
()]
Source code in
pydantic/types.py
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
@_dataclasses
dataclass
class
AllowInfNan
_fields
PydanticMetadata
"""A field metadata class to indicate that a field should allow `-inf`, `inf`, and `nan`.
Use this class as an annotation via [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated), as seen below.
Attributes:
allow_inf_nan: Whether to allow `-inf`, `inf`, and `nan`. Defaults to `True`.
Example:
```python
from typing import Annotated
from pydantic.types import AllowInfNan
LaxFloat = Annotated[float, AllowInfNan()]
```
"""
allow_inf_nan
bool
True
def
__hash__
self
int
return
hash
self
allow_inf_nan
StringConstraints
dataclass
Bases:
GroupedMetadata
Usage Documentation
StringConstraints
A field metadata class to apply constraints to
str
types.
Use this class as an annotation via
Annotated
, as seen below.
Attributes:
Name
Type
Description
strip_whitespace
bool
| None
Whether to remove leading and trailing whitespace.
to_upper
bool
| None
Whether to convert the string to uppercase.
to_lower
bool
| None
Whether to convert the string to lowercase.
strict
bool
| None
Whether to validate the string in strict mode.
min_length
int
| None
The minimum length of the string.
max_length
int
| None
The maximum length of the string.
pattern
str
Pattern
str
] | None
A regex pattern that the string must match.
Example
from
typing
import
Annotated
from
pydantic.types
import
StringConstraints
ConstrainedStr
Annotated
str
StringConstraints
min_length
max_length
Source code in
pydantic/types.py
693
694
695
696
697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
@_dataclasses
dataclass
frozen
True
class
StringConstraints
annotated_types
GroupedMetadata
"""!!! abstract "Usage Documentation"
[`StringConstraints`](../concepts/fields.md#string-constraints)
A field metadata class to apply constraints to `str` types.
Use this class as an annotation via [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated), as seen below.
Attributes:
strip_whitespace: Whether to remove leading and trailing whitespace.
to_upper: Whether to convert the string to uppercase.
to_lower: Whether to convert the string to lowercase.
strict: Whether to validate the string in strict mode.
min_length: The minimum length of the string.
max_length: The maximum length of the string.
pattern: A regex pattern that the string must match.
Example:
```python
from typing import Annotated
from pydantic.types import StringConstraints
ConstrainedStr = Annotated[str, StringConstraints(min_length=1, max_length=10)]
```
"""
strip_whitespace
bool
None
None
to_upper
bool
None
None
to_lower
bool
None
None
strict
bool
None
None
min_length
int
None
None
max_length
int
None
None
pattern
str
Pattern
str
None
None
def
__iter__
self
Iterator
BaseMetadata
self
min_length
not
None
yield
MinLen
self
min_length
self
max_length
not
None
yield
MaxLen
self
max_length
self
strict
not
None
yield
Strict
self
strict
self
strip_whitespace
not
None
self
pattern
not
None
self
to_lower
not
None
self
to_upper
not
None
yield
_fields
pydantic_general_metadata
strip_whitespace
self
strip_whitespace
to_upper
self
to_upper
to_lower
self
to_lower
pattern
self
pattern
ImportString
A type that can be used to import a Python object from a string.
ImportString
expects a string and loads the Python object importable at that dotted path.
Attributes of modules may be separated from the module by
, e.g. if
'math:cos'
is provided,
the resulting field value would be the function
cos
. If a
is used and both an attribute and submodule
are present at the same path, the module will be preferred.
On model instantiation, pointers will be evaluated and imported. There is
some nuance to this behavior, demonstrated in the examples below.
import
math
from
pydantic
import
BaseModel
Field
ImportString
ValidationError
class
ImportThings
BaseModel
obj
ImportString
# A string value will cause an automatic import
my_cos
ImportThings
obj
'math.cos'
# You can use the imported function as you would expect
cos_of_0
my_cos
obj
assert
cos_of_0
# A string whose value cannot be imported will raise an error
try
ImportThings
obj
'foo.bar'
except
ValidationError
print
'''
1 validation error for ImportThings
obj
Invalid python path: No module named 'foo.bar' [type=import_error, input_value='foo.bar', input_type=str]
'''
# Actual python objects can be assigned as well
my_cos
ImportThings
obj
math
cos
my_cos_2
ImportThings
obj
'math.cos'
my_cos_3
ImportThings
obj
'math:cos'
assert
my_cos
my_cos_2
my_cos_3
# You can set default field value either as Python object:
class
ImportThingsDefaultPyObj
BaseModel
obj
ImportString
math
cos
# or as a string value (but only if used with `validate_default=True`)
class
ImportThingsDefaultString
BaseModel
obj
ImportString
Field
default
'math.cos'
validate_default
True
my_cos_default1
ImportThingsDefaultPyObj
my_cos_default2
ImportThingsDefaultString
assert
my_cos_default1
obj
my_cos_default2
obj
math
cos
# note: this will not work!
class
ImportThingsMissingValidateDefault
BaseModel
obj
ImportString
'math.cos'
my_cos_default3
ImportThingsMissingValidateDefault
assert
my_cos_default3
obj
'math.cos'
# just string, not evaluated
Serializing an
ImportString
type to json is also possible.
from
pydantic
import
BaseModel
ImportString
class
ImportThings
BaseModel
obj
ImportString
# Create an instance
ImportThings
obj
'math.cos'
print
#> obj=<built-in function cos>
print
model_dump_json
())
#> {"obj":"math.cos"}
Source code in
pydantic/types.py
913
914
915
916
917
918
919
920
921
922
923
924
925
926
927
928
929
930
931
932
933
934
935
936
937
938
939
940
941
942
943
944
945
946
947
948
949
950
951
952
953
954
955
956
957
958
959
960
961
962
963
964
965
966
967
968
969
970
971
972
973
974
975
976
977
978
979
980
981
982
983
984
985
986
987
988
989
990
991
992
993
994
995
996
997
998
999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
1026
1027
1028
1029
1030
1031
1032
1033
1034
1035
class
ImportString
"""A type that can be used to import a Python object from a string.
`ImportString` expects a string and loads the Python object importable at that dotted path.
Attributes of modules may be separated from the module by `:` or `.`, e.g. if `'math:cos'` is provided,
the resulting field value would be the function `cos`. If a `.` is used and both an attribute and submodule
are present at the same path, the module will be preferred.
On model instantiation, pointers will be evaluated and imported. There is
some nuance to this behavior, demonstrated in the examples below.
```python
import math
from pydantic import BaseModel, Field, ImportString, ValidationError
class ImportThings(BaseModel):
obj: ImportString
# A string value will cause an automatic import
my_cos = ImportThings(obj='math.cos')
# You can use the imported function as you would expect
cos_of_0 = my_cos.obj(0)
assert cos_of_0 == 1
# A string whose value cannot be imported will raise an error
try:
ImportThings(obj='foo.bar')
except ValidationError as e:
print(e)
'''
1 validation error for ImportThings
obj
Invalid python path: No module named 'foo.bar' [type=import_error, input_value='foo.bar', input_type=str]
'''
# Actual python objects can be assigned as well
my_cos = ImportThings(obj=math.cos)
my_cos_2 = ImportThings(obj='math.cos')
my_cos_3 = ImportThings(obj='math:cos')
assert my_cos == my_cos_2 == my_cos_3
# You can set default field value either as Python object:
class ImportThingsDefaultPyObj(BaseModel):
obj: ImportString = math.cos
# or as a string value (but only if used with `validate_default=True`)
class ImportThingsDefaultString(BaseModel):
obj: ImportString = Field(default='math.cos', validate_default=True)
my_cos_default1 = ImportThingsDefaultPyObj()
my_cos_default2 = ImportThingsDefaultString()
assert my_cos_default1.obj == my_cos_default2.obj == math.cos
# note: this will not work!
class ImportThingsMissingValidateDefault(BaseModel):
obj: ImportString = 'math.cos'
my_cos_default3 = ImportThingsMissingValidateDefault()
assert my_cos_default3.obj == 'math.cos'  # just string, not evaluated
```
Serializing an `ImportString` type to json is also possible.
```python
from pydantic import BaseModel, ImportString
class ImportThings(BaseModel):
obj: ImportString
# Create an instance
m = ImportThings(obj='math.cos')
print(m)
#> obj=<built-in function cos>
print(m.model_dump_json())
#> {"obj":"math.cos"}
```
"""
@classmethod
def
__class_getitem__
cls
item
AnyType
AnyType
return
Annotated
item
cls
()]
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
serializer
core_schema
plain_serializer_function_ser_schema
cls
_serialize
when_used
'json'
cls
source
# Treat bare usage of ImportString (`schema is None`) as the same as ImportString[Any]
return
core_schema
no_info_plain_validator_function
function
_validators
import_string
serialization
serializer
else
return
core_schema
no_info_before_validator_function
function
_validators
import_string
schema
handler
source
serialization
serializer
@classmethod
def
__get_pydantic_json_schema__
cls
CoreSchema
handler
GetJsonSchemaHandler
JsonSchemaValue
return
handler
core_schema
str_schema
())
@staticmethod
def
_serialize
Any
str
isinstance
ModuleType
return
__name__
elif
hasattr
'__module__'
and
hasattr
'__name__'
return
__module__
__name__
# Handle special cases for sys.XXX streams
# if we see more of these, we should consider a more general solution
elif
hasattr
'name'
name
'<stdout>'
return
'sys.stdout'
elif
name
'<stdin>'
return
'sys.stdin'
elif
name
'<stderr>'
return
'sys.stderr'
else
return
def
__repr__
self
str
return
'ImportString'
UuidVersion
dataclass
A field metadata class to indicate a
UUID
version.
Use this class as an annotation via
Annotated
, as seen below.
Attributes:
Name
Type
Description
uuid_version
Literal
[1, 3, 4, 5, 6, 7, 8]
The version of the UUID. Must be one of 1, 3, 4, 5, or 7.
Example
from
typing
import
Annotated
from
uuid
import
UUID
from
pydantic.types
import
UuidVersion
UUID1
Annotated
UUID
UuidVersion
Source code in
pydantic/types.py
1138
1139
1140
1141
1142
1143
1144
1145
1146
1147
1148
1149
1150
1151
1152
1153
1154
1155
1156
1157
1158
1159
1160
1161
1162
1163
1164
1165
1166
1167
1168
1169
1170
1171
1172
1173
1174
1175
1176
1177
1178
1179
1180
@_dataclasses
dataclass
_internal_dataclass
slots_true
class
UuidVersion
"""A field metadata class to indicate a [UUID](https://docs.python.org/3/library/uuid.html) version.
Use this class as an annotation via [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated), as seen below.
Attributes:
uuid_version: The version of the UUID. Must be one of 1, 3, 4, 5, or 7.
Example:
```python
from typing import Annotated
from uuid import UUID
from pydantic.types import UuidVersion
UUID1 = Annotated[UUID, UuidVersion(1)]
```
"""
uuid_version
Literal
def
__get_pydantic_json_schema__
self
core_schema
core_schema
CoreSchema
handler
GetJsonSchemaHandler
JsonSchemaValue
field_schema
handler
core_schema
field_schema
pop
'anyOf'
None
# remove the bytes/str union
field_schema
update
type
'string'
format
'uuid
self
uuid_version
return
field_schema
def
__get_pydantic_core_schema__
self
source
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
isinstance
self
source
# used directly as a type
return
core_schema
uuid_schema
version
self
uuid_version
else
# update existing schema with self.uuid_version
schema
handler
source
_check_annotated_type
schema
'type'
'uuid'
self
__class__
__name__
schema
'version'
self
uuid_version
# type: ignore
return
schema
def
__hash__
self
int
return
hash
type
self
uuid_version
Json
A special type wrapper which loads JSON before parsing.
You can use the
Json
data type to make Pydantic first load a raw JSON string before
validating the loaded data into the parametrized type:
from
typing
import
Any
from
pydantic
import
BaseModel
Json
ValidationError
class
AnyJsonModel
BaseModel
json_obj
Json
Any
class
ConstrainedJsonModel
BaseModel
json_obj
Json
list
int
print
AnyJsonModel
json_obj
'{"b": 1}'
#> json_obj={'b': 1}
print
ConstrainedJsonModel
json_obj
'[1, 2, 3]'
#> json_obj=[1, 2, 3]
try
ConstrainedJsonModel
json_obj
except
ValidationError
print
'''
1 validation error for ConstrainedJsonModel
json_obj
JSON input should be string, bytes or bytearray [type=json_type, input_value=12, input_type=int]
'''
try
ConstrainedJsonModel
json_obj
'[a, b]'
except
ValidationError
print
'''
1 validation error for ConstrainedJsonModel
json_obj
Invalid JSON: expected value at line 1 column 2 [type=json_invalid, input_value='[a, b]', input_type=str]
'''
try
ConstrainedJsonModel
json_obj
'["a", "b"]'
except
ValidationError
print
'''
2 validation errors for ConstrainedJsonModel
json_obj.0
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]
json_obj.1
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='b', input_type=str]
'''
When you dump the model using
model_dump
model_dump_json
, the dumped value will be the result of validation,
not the original JSON string. However, you can use the argument
round_trip=True
to get the original JSON string back:
from
pydantic
import
BaseModel
Json
class
ConstrainedJsonModel
BaseModel
json_obj
Json
list
int
print
ConstrainedJsonModel
json_obj
'[1, 2, 3]'
model_dump_json
())
#> {"json_obj":[1,2,3]}
print
ConstrainedJsonModel
json_obj
'[1, 2, 3]'
model_dump_json
round_trip
True
#> {"json_obj":"[1,2,3]"}
Source code in
pydantic/types.py
1442
1443
1444
1445
1446
1447
1448
1449
1450
1451
1452
1453
1454
1455
1456
1457
1458
1459
1460
1461
1462
1463
1464
1465
1466
1467
1468
1469
1470
1471
1472
1473
1474
1475
1476
1477
1478
1479
1480
1481
1482
1483
1484
1485
1486
1487
1488
1489
1490
1491
1492
1493
1494
1495
1496
1497
1498
1499
1500
1501
1502
1503
1504
1505
1506
1507
1508
1509
1510
1511
1512
1513
1514
1515
1516
1517
1518
1519
1520
1521
1522
1523
1524
1525
1526
1527
1528
1529
1530
1531
1532
1533
class
Json
"""A special type wrapper which loads JSON before parsing.
You can use the `Json` data type to make Pydantic first load a raw JSON string before
validating the loaded data into the parametrized type:
```python
from typing import Any
from pydantic import BaseModel, Json, ValidationError
class AnyJsonModel(BaseModel):
json_obj: Json[Any]
class ConstrainedJsonModel(BaseModel):
json_obj: Json[list[int]]
print(AnyJsonModel(json_obj='{"b": 1}'))
#> json_obj={'b': 1}
print(ConstrainedJsonModel(json_obj='[1, 2, 3]'))
#> json_obj=[1, 2, 3]
try:
ConstrainedJsonModel(json_obj=12)
except ValidationError as e:
print(e)
'''
1 validation error for ConstrainedJsonModel
json_obj
JSON input should be string, bytes or bytearray [type=json_type, input_value=12, input_type=int]
'''
try:
ConstrainedJsonModel(json_obj='[a, b]')
except ValidationError as e:
print(e)
'''
1 validation error for ConstrainedJsonModel
json_obj
Invalid JSON: expected value at line 1 column 2 [type=json_invalid, input_value='[a, b]', input_type=str]
'''
try:
ConstrainedJsonModel(json_obj='["a", "b"]')
except ValidationError as e:
print(e)
'''
2 validation errors for ConstrainedJsonModel
json_obj.0
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]
json_obj.1
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='b', input_type=str]
'''
```
When you dump the model using `model_dump` or `model_dump_json`, the dumped value will be the result of validation,
not the original JSON string. However, you can use the argument `round_trip=True` to get the original JSON string back:
```python
from pydantic import BaseModel, Json
class ConstrainedJsonModel(BaseModel):
json_obj: Json[list[int]]
print(ConstrainedJsonModel(json_obj='[1, 2, 3]').model_dump_json())
#> {"json_obj":[1,2,3]}
print(
ConstrainedJsonModel(json_obj='[1, 2, 3]').model_dump_json(round_trip=True)
#> {"json_obj":"[1,2,3]"}
```
"""
@classmethod
def
__class_getitem__
cls
item
AnyType
AnyType
return
Annotated
item
cls
()]
@classmethod
def
__get_pydantic_core_schema__
cls
source
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
cls
source
return
core_schema
json_schema
None
else
return
core_schema
json_schema
handler
source
def
__repr__
self
str
return
'Json'
def
__hash__
self
int
return
hash
type
self
def
__eq__
self
other
Any
bool
return
type
other
type
self
Secret
Bases:
_SecretBase
SecretType
A generic base class used for defining a field with sensitive information that you do not want to be visible in logging or tracebacks.
You may either directly parametrize
Secret
with a type, or subclass from
Secret
with a parametrized type. The benefit of subclassing
is that you can define a custom
_display
method, which will be used for
repr()
and
str()
methods. The examples below demonstrate both
ways of using
Secret
to create a new secret type.
Directly parametrizing
Secret
with a type:
from
pydantic
import
BaseModel
Secret
SecretBool
Secret
bool
class
Model
BaseModel
secret_bool
SecretBool
Model
secret_bool
True
print
model_dump
())
#> {'secret_bool': Secret('**********')}
print
model_dump_json
())
#> {"secret_bool":"**********"}
print
secret_bool
get_secret_value
())
#> True
Subclassing from parametrized
Secret
from
datetime
import
date
from
pydantic
import
BaseModel
Secret
class
SecretDate
Secret
date
]):
def
_display
self
str
return
'****/**/**'
class
Model
BaseModel
secret_date
SecretDate
Model
secret_date
date
2022
print
model_dump
())
#> {'secret_date': SecretDate('****/**/**')}
print
model_dump_json
())
#> {"secret_date":"****/**/**"}
print
secret_date
get_secret_value
())
#> 2022-01-01
The value returned by the
_display
method will be used for
repr()
and
str()
You can enforce constraints on the underlying type through annotations:
For example:
from
typing
import
Annotated
from
pydantic
import
BaseModel
Field
Secret
ValidationError
SecretPosInt
Secret
Annotated
int
Field
strict
True
)]]
class
Model
BaseModel
sensitive_int
SecretPosInt
Model
sensitive_int
print
model_dump
())
#> {'sensitive_int': Secret('**********')}
try
Model
sensitive_int
except
ValidationError
exc_info
print
exc_info
errors
include_url
False
include_input
False
'''
'type': 'greater_than',
'loc': ('sensitive_int',),
'msg': 'Input should be greater than 0',
'ctx': {'gt': 0},
'''
try
Model
sensitive_int
'42'
The input value is not an integer, so it raises a validation error because the
SecretPosInt
type has strict mode enabled.
except
ValidationError
exc_info
print
exc_info
errors
include_url
False
include_input
False
'''
'type': 'int_type',
'loc': ('sensitive_int',),
'msg': 'Input should be a valid integer',
'''
Source code in
pydantic/types.py
1576
1577
1578
1579
1580
1581
1582
1583
1584
1585
1586
1587
1588
1589
1590
1591
1592
1593
1594
1595
1596
1597
1598
1599
1600
1601
1602
1603
1604
1605
1606
1607
1608
1609
1610
1611
1612
1613
1614
1615
1616
1617
1618
1619
1620
1621
1622
1623
1624
1625
1626
1627
1628
1629
1630
1631
1632
1633
1634
1635
1636
1637
1638
1639
1640
1641
1642
1643
1644
1645
1646
1647
1648
1649
1650
1651
1652
1653
1654
1655
1656
1657
1658
1659
1660
1661
1662
1663
1664
1665
1666
1667
1668
1669
1670
1671
1672
1673
1674
1675
1676
1677
1678
1679
1680
1681
1682
1683
1684
1685
1686
1687
1688
1689
1690
1691
1692
1693
1694
1695
1696
1697
1698
1699
1700
1701
1702
1703
1704
1705
1706
1707
1708
1709
1710
1711
1712
1713
1714
1715
1716
1717
1718
1719
1720
1721
1722
1723
1724
1725
1726
1727
1728
1729
1730
1731
1732
1733
class
Secret
_SecretBase
SecretType
]):
"""A generic base class used for defining a field with sensitive information that you do not want to be visible in logging or tracebacks.
You may either directly parametrize `Secret` with a type, or subclass from `Secret` with a parametrized type. The benefit of subclassing
is that you can define a custom `_display` method, which will be used for `repr()` and `str()` methods. The examples below demonstrate both
ways of using `Secret` to create a new secret type.
1. Directly parametrizing `Secret` with a type:
```python
from pydantic import BaseModel, Secret
SecretBool = Secret[bool]
class Model(BaseModel):
secret_bool: SecretBool
m = Model(secret_bool=True)
print(m.model_dump())
#> {'secret_bool': Secret('**********')}
print(m.model_dump_json())
#> {"secret_bool":"**********"}
print(m.secret_bool.get_secret_value())
#> True
```
2. Subclassing from parametrized `Secret`:
```python
from datetime import date
from pydantic import BaseModel, Secret
class SecretDate(Secret[date]):
def _display(self) -> str:
return '****/**/**'
class Model(BaseModel):
secret_date: SecretDate
m = Model(secret_date=date(2022, 1, 1))
print(m.model_dump())
#> {'secret_date': SecretDate('****/**/**')}
print(m.model_dump_json())
#> {"secret_date":"****/**/**"}
print(m.secret_date.get_secret_value())
#> 2022-01-01
```
The value returned by the `_display` method will be used for `repr()` and `str()`.
You can enforce constraints on the underlying type through annotations:
For example:
```python
from typing import Annotated
from pydantic import BaseModel, Field, Secret, ValidationError
SecretPosInt = Secret[Annotated[int, Field(gt=0, strict=True)]]
class Model(BaseModel):
sensitive_int: SecretPosInt
m = Model(sensitive_int=42)
print(m.model_dump())
#> {'sensitive_int': Secret('**********')}
try:
m = Model(sensitive_int=-42)  # (1)!
except ValidationError as exc_info:
print(exc_info.errors(include_url=False, include_input=False))
'''
'type': 'greater_than',
'loc': ('sensitive_int',),
'msg': 'Input should be greater than 0',
'ctx': {'gt': 0},
'''
try:
m = Model(sensitive_int='42')  # (2)!
except ValidationError as exc_info:
print(exc_info.errors(include_url=False, include_input=False))
'''
'type': 'int_type',
'loc': ('sensitive_int',),
'msg': 'Input should be a valid integer',
'''
```
1. The input value is not greater than 0, so it raises a validation error.
2. The input value is not an integer, so it raises a validation error because the `SecretPosInt` type has strict mode enabled.
"""
def
_display
self
str
bytes
return
'**********'
self
get_secret_value
else
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
inner_type
None
# if origin_type is Secret, then cls is a GenericAlias, and we can extract the inner type directly
origin_type
get_origin
source
origin_type
not
None
inner_type
get_args
source
# otherwise, we need to get the inner type from the base class
else
bases
getattr
cls
'__orig_bases__'
getattr
cls
'__bases__'
[]))
for
base
bases
get_origin
base
Secret
inner_type
get_args
base
bases
inner_type
None
raise
TypeError
"Can't get secret type from
cls
__name__
. "
'Please use Secret[<type>], or subclass from Secret[<type>] instead.'
inner_schema
handler
generate_schema
inner_type
# type: ignore
def
validate_secret_value
value
handler
Secret
SecretType
isinstance
value
Secret
value
value
get_secret_value
validated_inner
handler
value
return
cls
validated_inner
return
core_schema
json_or_python_schema
python_schema
core_schema
no_info_wrap_validator_function
validate_secret_value
inner_schema
json_schema
core_schema
no_info_after_validator_function
lambda
cls
inner_schema
serialization
core_schema
plain_serializer_function_ser_schema
_serialize_secret
info_arg
True
when_used
'always'
__pydantic_serializer__
SchemaSerializer
core_schema
any_schema
serialization
core_schema
plain_serializer_function_ser_schema
_serialize_secret
info_arg
True
when_used
'always'
SecretStr
Bases:
_SecretField
str
A string used for storing sensitive information that you do not want to be visible in logging or tracebacks.
When the secret value is nonempty, it is displayed as
'**********'
instead of the underlying value in
calls to
repr()
and
str()
. If the value
empty, it is displayed as
from
pydantic
import
BaseModel
SecretStr
class
User
BaseModel
username
str
password
SecretStr
user
User
username
'scolvin'
password
'password1'
print
user
#> username='scolvin' password=SecretStr('**********')
print
user
password
get_secret_value
())
#> password1
print
SecretStr
'password'
SecretStr
)))
#> (SecretStr('**********'), SecretStr(''))
As seen above, by default,
SecretStr
(and
SecretBytes
will be serialized as
**********
when serializing to json.
You can use the
field_serializer
to dump the
secret as plain-text when serializing to json.
from
pydantic
import
BaseModel
SecretBytes
SecretStr
field_serializer
class
Model
BaseModel
password
SecretStr
password_bytes
SecretBytes
@field_serializer
'password'
'password_bytes'
when_used
'json'
def
dump_secret
self
return
get_secret_value
model
Model
password
'IAmSensitive'
password_bytes
'IAmSensitiveBytes'
print
model
#> password=SecretStr('**********') password_bytes=SecretBytes(b'**********')
print
model
password
#> **********
print
model
model_dump
())
'''
'password': SecretStr('**********'),
'password_bytes': SecretBytes(b'**********'),
'''
print
model
model_dump_json
())
#> {"password":"IAmSensitive","password_bytes":"IAmSensitiveBytes"}
Source code in
pydantic/types.py
1806
1807
1808
1809
1810
1811
1812
1813
1814
1815
1816
1817
1818
1819
1820
1821
1822
1823
1824
1825
1826
1827
1828
1829
1830
1831
1832
1833
1834
1835
1836
1837
1838
1839
1840
1841
1842
1843
1844
1845
1846
1847
1848
1849
1850
1851
1852
1853
1854
1855
1856
1857
1858
1859
1860
1861
1862
1863
1864
1865
1866
1867
1868
1869
1870
class
SecretStr
_SecretField
str
]):
"""A string used for storing sensitive information that you do not want to be visible in logging or tracebacks.
When the secret value is nonempty, it is displayed as `'**********'` instead of the underlying value in
calls to `repr()` and `str()`. If the value _is_ empty, it is displayed as `''`.
```python
from pydantic import BaseModel, SecretStr
class User(BaseModel):
username: str
password: SecretStr
user = User(username='scolvin', password='password1')
print(user)
#> username='scolvin' password=SecretStr('**********')
print(user.password.get_secret_value())
#> password1
print((SecretStr('password'), SecretStr('')))
#> (SecretStr('**********'), SecretStr(''))
```
As seen above, by default, [`SecretStr`][pydantic.types.SecretStr] (and [`SecretBytes`][pydantic.types.SecretBytes])
will be serialized as `**********` when serializing to json.
You can use the [`field_serializer`][pydantic.functional_serializers.field_serializer] to dump the
secret as plain-text when serializing to json.
```python
from pydantic import BaseModel, SecretBytes, SecretStr, field_serializer
class Model(BaseModel):
password: SecretStr
password_bytes: SecretBytes
@field_serializer('password', 'password_bytes', when_used='json')
def dump_secret(self, v):
return v.get_secret_value()
model = Model(password='IAmSensitive', password_bytes=b'IAmSensitiveBytes')
print(model)
#> password=SecretStr('**********') password_bytes=SecretBytes(b'**********')
print(model.password)
#> **********
print(model.model_dump())
'''
'password': SecretStr('**********'),
'password_bytes': SecretBytes(b'**********'),
'''
print(model.model_dump_json())
#> {"password":"IAmSensitive","password_bytes":"IAmSensitiveBytes"}
```
"""
_inner_schema
ClassVar
CoreSchema
core_schema
str_schema
_error_kind
ClassVar
str
'string_type'
def
__len__
self
int
return
len
self
_secret_value
def
_display
self
str
return
_secret_display
self
_secret_value
SecretBytes
Bases:
_SecretField
bytes
A bytes used for storing sensitive information that you do not want to be visible in logging or tracebacks.
It displays
b'**********'
instead of the string value on
repr()
and
str()
calls.
When the secret value is nonempty, it is displayed as
b'**********'
instead of the underlying value in
calls to
repr()
and
str()
. If the value
empty, it is displayed as
b''
from
pydantic
import
BaseModel
SecretBytes
class
User
BaseModel
username
str
password
SecretBytes
user
User
username
'scolvin'
password
'password1'
#> username='scolvin' password=SecretBytes(b'**********')
print
user
password
get_secret_value
())
#> b'password1'
print
SecretBytes
'password'
SecretBytes
)))
#> (SecretBytes(b'**********'), SecretBytes(b''))
Source code in
pydantic/types.py
1873
1874
1875
1876
1877
1878
1879
1880
1881
1882
1883
1884
1885
1886
1887
1888
1889
1890
1891
1892
1893
1894
1895
1896
1897
1898
1899
1900
1901
1902
1903
class
SecretBytes
_SecretField
bytes
]):
"""A bytes used for storing sensitive information that you do not want to be visible in logging or tracebacks.
It displays `b'**********'` instead of the string value on `repr()` and `str()` calls.
When the secret value is nonempty, it is displayed as `b'**********'` instead of the underlying value in
calls to `repr()` and `str()`. If the value _is_ empty, it is displayed as `b''`.
```python
from pydantic import BaseModel, SecretBytes
class User(BaseModel):
username: str
password: SecretBytes
user = User(username='scolvin', password=b'password1')
#> username='scolvin' password=SecretBytes(b'**********')
print(user.password.get_secret_value())
#> b'password1'
print((SecretBytes(b'password'), SecretBytes(b'')))
#> (SecretBytes(b'**********'), SecretBytes(b''))
```
"""
_inner_schema
ClassVar
CoreSchema
core_schema
bytes_schema
_error_kind
ClassVar
str
'bytes_type'
def
__len__
self
int
return
len
self
_secret_value
def
_display
self
bytes
return
_secret_display
self
_secret_value
encode
PaymentCardNumber
Bases:
str
Based on: https://en.wikipedia.org/wiki/Payment_card_number.
Source code in
pydantic/types.py
1919
1920
1921
1922
1923
1924
1925
1926
1927
1928
1929
1930
1931
1932
1933
1934
1935
1936
1937
1938
1939
1940
1941
1942
1943
1944
1945
1946
1947
1948
1949
1950
1951
1952
1953
1954
1955
1956
1957
1958
1959
1960
1961
1962
1963
1964
1965
1966
1967
1968
1969
1970
1971
1972
1973
1974
1975
1976
1977
1978
1979
1980
1981
1982
1983
1984
1985
1986
1987
1988
1989
1990
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
2014
2015
2016
2017
2018
2019
2020
2021
2022
2023
2024
@deprecated
'The `PaymentCardNumber` class is deprecated, use `pydantic_extra_types` instead. '
'See https://docs.pydantic.dev/latest/api/pydantic_extra_types_payment/#pydantic_extra_types.payment.PaymentCardNumber.'
category
PydanticDeprecatedSince20
class
PaymentCardNumber
str
"""Based on: https://en.wikipedia.org/wiki/Payment_card_number."""
strip_whitespace
ClassVar
bool
True
min_length
ClassVar
int
max_length
ClassVar
int
bin
str
last4
str
brand
PaymentCardBrand
def
__init__
self
card_number
str
self
validate_digits
card_number
card_number
self
validate_luhn_check_digit
card_number
self
bin
card_number
self
last4
card_number
self
brand
self
validate_brand
card_number
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
return
core_schema
with_info_after_validator_function
cls
validate
core_schema
str_schema
min_length
cls
min_length
max_length
cls
max_length
strip_whitespace
cls
strip_whitespace
@classmethod
def
validate
cls
input_value
str
core_schema
ValidationInfo
PaymentCardNumber
"""Validate the card number and return a `PaymentCardNumber` instance."""
return
cls
input_value
@property
def
masked
self
str
"""Mask all but the last 4 digits of the card number.
Returns:
A masked card number string.
"""
num_masked
len
self
# len(bin) + len(last4) == 10
return
self
bin
"*"
num_masked
self
last4
@classmethod
def
validate_digits
cls
card_number
str
None
"""Validate that the card number is all digits."""
not
card_number
isdigit
():
raise
PydanticCustomError
'payment_card_number_digits'
'Card number is not all digits'
@classmethod
def
validate_luhn_check_digit
cls
card_number
str
str
"""Based on: https://en.wikipedia.org/wiki/Luhn_algorithm."""
sum_
int
card_number
length
len
card_number
parity
length
for
range
length
digit
int
card_number
parity
digit
digit
digit
sum_
digit
valid
sum_
not
valid
raise
PydanticCustomError
'payment_card_number_luhn'
'Card number is not luhn valid'
return
card_number
@staticmethod
def
validate_brand
card_number
str
PaymentCardBrand
"""Validate length based on BIN for major brands:
https://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN).
"""
card_number
'4'
brand
PaymentCardBrand
visa
elif
int
card_number
brand
PaymentCardBrand
mastercard
elif
card_number
'34'
'37'
brand
PaymentCardBrand
amex
else
brand
PaymentCardBrand
other
required_length
None
int
str
None
brand
PaymentCardBrand
mastercard
required_length
valid
len
card_number
required_length
elif
brand
PaymentCardBrand
visa
required_length
'13, 16 or 19'
valid
len
card_number
elif
brand
PaymentCardBrand
amex
required_length
valid
len
card_number
required_length
else
valid
True
not
valid
raise
PydanticCustomError
'payment_card_number_brand'
'Length for a
{brand}
card must be
{required_length}
'brand'
brand
'required_length'
required_length
return
brand
masked
property
masked
str
Mask all but the last 4 digits of the card number.
Returns:
Type
Description
str
A masked card number string.
validate
classmethod
validate
input_value
str
ValidationInfo
PaymentCardNumber
Validate the card number and return a
PaymentCardNumber
instance.
Source code in
pydantic/types.py
1952
1953
1954
1955
@classmethod
def
validate
cls
input_value
str
core_schema
ValidationInfo
PaymentCardNumber
"""Validate the card number and return a `PaymentCardNumber` instance."""
return
cls
input_value
validate_digits
classmethod
validate_digits
card_number
str
None
Validate that the card number is all digits.
Source code in
pydantic/types.py
1967
1968
1969
1970
1971
@classmethod
def
validate_digits
cls
card_number
str
None
"""Validate that the card number is all digits."""
not
card_number
isdigit
():
raise
PydanticCustomError
'payment_card_number_digits'
'Card number is not all digits'
validate_luhn_check_digit
classmethod
validate_luhn_check_digit
card_number
str
str
Based on: https://en.wikipedia.org/wiki/Luhn_algorithm.
Source code in
pydantic/types.py
1973
1974
1975
1976
1977
1978
1979
1980
1981
1982
1983
1984
1985
1986
1987
1988
1989
@classmethod
def
validate_luhn_check_digit
cls
card_number
str
str
"""Based on: https://en.wikipedia.org/wiki/Luhn_algorithm."""
sum_
int
card_number
length
len
card_number
parity
length
for
range
length
digit
int
card_number
parity
digit
digit
digit
sum_
digit
valid
sum_
not
valid
raise
PydanticCustomError
'payment_card_number_luhn'
'Card number is not luhn valid'
return
card_number
validate_brand
staticmethod
validate_brand
card_number
str
PaymentCardBrand
Validate length based on BIN for major brands:
https://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN).
Source code in
pydantic/types.py
1991
1992
1993
1994
1995
1996
1997
1998
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
2011
2012
2013
2014
2015
2016
2017
2018
2019
2020
2021
2022
2023
2024
@staticmethod
def
validate_brand
card_number
str
PaymentCardBrand
"""Validate length based on BIN for major brands:
https://en.wikipedia.org/wiki/Payment_card_number#Issuer_identification_number_(IIN).
"""
card_number
'4'
brand
PaymentCardBrand
visa
elif
int
card_number
brand
PaymentCardBrand
mastercard
elif
card_number
'34'
'37'
brand
PaymentCardBrand
amex
else
brand
PaymentCardBrand
other
required_length
None
int
str
None
brand
PaymentCardBrand
mastercard
required_length
valid
len
card_number
required_length
elif
brand
PaymentCardBrand
visa
required_length
'13, 16 or 19'
valid
len
card_number
elif
brand
PaymentCardBrand
amex
required_length
valid
len
card_number
required_length
else
valid
True
not
valid
raise
PydanticCustomError
'payment_card_number_brand'
'Length for a
{brand}
card must be
{required_length}
'brand'
brand
'required_length'
required_length
return
brand
ByteSize
Bases:
int
Converts a string representing a number of bytes with units (such as
'1KB'
'11.5MiB'
) into an integer.
You can use the
ByteSize
data type to (case-insensitively) convert a string representation of a number of bytes into
an integer, and also to print out human-readable strings representing a number of bytes.
In conformance with
IEC 80000-13 Standard
we interpret
'1KB'
to mean 1000 bytes,
and
'1KiB'
to mean 1024 bytes. In general, including a middle
'i'
will cause the unit to be interpreted as a power of 2,
rather than a power of 10 (so, for example,
'1 MB'
is treated as
1_000_000
bytes, whereas
'1 MiB'
is treated as
1_048_576
bytes).
Info
Note that
will be parsed as "1 byte" and not "1 bit".
from
pydantic
import
BaseModel
ByteSize
class
MyModel
BaseModel
size
ByteSize
print
MyModel
size
52000
size
#> 52000
print
MyModel
size
'3000 KiB'
size
#> 3072000
MyModel
size
'50 PB'
print
size
human_readable
())
#> 44.4PiB
print
size
human_readable
decimal
True
#> 50.0PB
print
size
human_readable
separator
' '
#> 44.4 PiB
print
size
'TiB'
#> 45474.73508864641
Source code in
pydantic/types.py
2030
2031
2032
2033
2034
2035
2036
2037
2038
2039
2040
2041
2042
2043
2044
2045
2046
2047
2048
2049
2050
2051
2052
2053
2054
2055
2056
2057
2058
2059
2060
2061
2062
2063
2064
2065
2066
2067
2068
2069
2070
2071
2072
2073
2074
2075
2076
2077
2078
2079
2080
2081
2082
2083
2084
2085
2086
2087
2088
2089
2090
2091
2092
2093
2094
2095
2096
2097
2098
2099
2100
2101
2102
2103
2104
2105
2106
2107
2108
2109
2110
2111
2112
2113
2114
2115
2116
2117
2118
2119
2120
2121
2122
2123
2124
2125
2126
2127
2128
2129
2130
2131
2132
2133
2134
2135
2136
2137
2138
2139
2140
2141
2142
2143
2144
2145
2146
2147
2148
2149
2150
2151
2152
2153
2154
2155
2156
2157
2158
2159
2160
2161
2162
2163
2164
2165
2166
2167
2168
2169
2170
2171
2172
2173
2174
2175
2176
2177
2178
2179
2180
2181
2182
2183
2184
2185
2186
2187
class
ByteSize
int
"""Converts a string representing a number of bytes with units (such as `'1KB'` or `'11.5MiB'`) into an integer.
You can use the `ByteSize` data type to (case-insensitively) convert a string representation of a number of bytes into
an integer, and also to print out human-readable strings representing a number of bytes.
In conformance with [IEC 80000-13 Standard](https://en.wikipedia.org/wiki/ISO/IEC_80000) we interpret `'1KB'` to mean 1000 bytes,
and `'1KiB'` to mean 1024 bytes. In general, including a middle `'i'` will cause the unit to be interpreted as a power of 2,
rather than a power of 10 (so, for example, `'1 MB'` is treated as `1_000_000` bytes, whereas `'1 MiB'` is treated as `1_048_576` bytes).
!!! info
Note that `1b` will be parsed as "1 byte" and not "1 bit".
```python
from pydantic import BaseModel, ByteSize
class MyModel(BaseModel):
size: ByteSize
print(MyModel(size=52000).size)
#> 52000
print(MyModel(size='3000 KiB').size)
#> 3072000
m = MyModel(size='50 PB')
print(m.size.human_readable())
#> 44.4PiB
print(m.size.human_readable(decimal=True))
#> 50.0PB
print(m.size.human_readable(separator=' '))
#> 44.4 PiB
print(m.size.to('TiB'))
#> 45474.73508864641
```
"""
byte_sizes
'b'
'kb'
'mb'
'gb'
'tb'
'pb'
'eb'
'kib'
'mib'
'gib'
'tib'
'pib'
'eib'
'bit'
'kbit'
'mbit'
'gbit'
'tbit'
'pbit'
'ebit'
'kibit'
'mibit'
'gibit'
'tibit'
'pibit'
'eibit'
byte_sizes
update
lower
()[
for
byte_sizes
items
'i'
not
byte_string_pattern
'^\s*(\d*\.?\d+)\s*(\w+)?'
byte_string_re
compile
byte_string_pattern
IGNORECASE
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
return
core_schema
with_info_after_validator_function
function
cls
_validate
schema
core_schema
union_schema
core_schema
str_schema
pattern
cls
byte_string_pattern
core_schema
int_schema
custom_error_type
'byte_size'
custom_error_message
'could not parse value and unit from byte string'
serialization
core_schema
plain_serializer_function_ser_schema
int
return_schema
core_schema
int_schema
@classmethod
def
_validate
cls
input_value
Any
core_schema
ValidationInfo
ByteSize
try
return
cls
int
input_value
except
ValueError
pass
str_match
cls
byte_string_re
match
str
input_value
str_match
None
raise
PydanticCustomError
'byte_size'
'could not parse value and unit from byte string'
scalar
unit
str_match
groups
unit
None
unit
'b'
try
unit_mult
cls
byte_sizes
unit
lower
()]
except
KeyError
raise
PydanticCustomError
'byte_size_unit'
'could not interpret byte unit:
{unit}
'unit'
unit
return
cls
int
float
scalar
unit_mult
def
human_readable
self
decimal
bool
False
separator
str
str
"""Converts a byte size to a human readable string.
Args:
decimal: If True, use decimal units (e.g. 1000 bytes per KB). If False, use binary units
(e.g. 1024 bytes per KiB).
separator: A string used to split the value and unit. Defaults to an empty string ('').
Returns:
A human readable string representation of the byte size.
"""
decimal
divisor
1000
units
'B'
'KB'
'MB'
'GB'
'TB'
'PB'
final_unit
'EB'
else
divisor
1024
units
'B'
'KiB'
'MiB'
'GiB'
'TiB'
'PiB'
final_unit
'EiB'
num
float
self
for
unit
units
abs
num
divisor
unit
'B'
return
num
0.0f
separator
unit
else
return
num
0.1f
separator
unit
num
divisor
return
num
0.1f
separator
final_unit
def
self
unit
str
float
"""Converts a byte size to another unit, including both byte and bit units.
Args:
unit: The unit to convert to. Must be one of the following: B, KB, MB, GB, TB, PB, EB,
KiB, MiB, GiB, TiB, PiB, EiB (byte units) and
bit, kbit, mbit, gbit, tbit, pbit, ebit,
kibit, mibit, gibit, tibit, pibit, eibit (bit units).
Returns:
The byte size in the new unit.
"""
try
unit_div
self
byte_sizes
unit
lower
()]
except
KeyError
raise
PydanticCustomError
'byte_size_unit'
'Could not interpret byte unit:
{unit}
'unit'
unit
return
self
unit_div
human_readable
human_readable
decimal
bool
False
separator
str
str
Converts a byte size to a human readable string.
Parameters:
Name
Type
Description
Default
decimal
bool
If True, use decimal units (e.g. 1000 bytes per KB). If False, use binary units
(e.g. 1024 bytes per KiB).
False
separator
str
A string used to split the value and unit. Defaults to an empty string ('').
Returns:
Type
Description
str
A human readable string representation of the byte size.
Source code in
pydantic/types.py
2139
2140
2141
2142
2143
2144
2145
2146
2147
2148
2149
2150
2151
2152
2153
2154
2155
2156
2157
2158
2159
2160
2161
2162
2163
2164
2165
2166
2167
2168
def
human_readable
self
decimal
bool
False
separator
str
str
"""Converts a byte size to a human readable string.
Args:
decimal: If True, use decimal units (e.g. 1000 bytes per KB). If False, use binary units
(e.g. 1024 bytes per KiB).
separator: A string used to split the value and unit. Defaults to an empty string ('').
Returns:
A human readable string representation of the byte size.
"""
decimal
divisor
1000
units
'B'
'KB'
'MB'
'GB'
'TB'
'PB'
final_unit
'EB'
else
divisor
1024
units
'B'
'KiB'
'MiB'
'GiB'
'TiB'
'PiB'
final_unit
'EiB'
num
float
self
for
unit
units
abs
num
divisor
unit
'B'
return
num
0.0f
separator
unit
else
return
num
0.1f
separator
unit
num
divisor
return
num
0.1f
separator
final_unit
unit
str
float
Converts a byte size to another unit, including both byte and bit units.
Parameters:
Name
Type
Description
Default
unit
str
The unit to convert to. Must be one of the following: B, KB, MB, GB, TB, PB, EB,
KiB, MiB, GiB, TiB, PiB, EiB (byte units) and
bit, kbit, mbit, gbit, tbit, pbit, ebit,
kibit, mibit, gibit, tibit, pibit, eibit (bit units).
required
Returns:
Type
Description
float
The byte size in the new unit.
Source code in
pydantic/types.py
2170
2171
2172
2173
2174
2175
2176
2177
2178
2179
2180
2181
2182
2183
2184
2185
2186
2187
def
self
unit
str
float
"""Converts a byte size to another unit, including both byte and bit units.
Args:
unit: The unit to convert to. Must be one of the following: B, KB, MB, GB, TB, PB, EB,
KiB, MiB, GiB, TiB, PiB, EiB (byte units) and
bit, kbit, mbit, gbit, tbit, pbit, ebit,
kibit, mibit, gibit, tibit, pibit, eibit (bit units).
Returns:
The byte size in the new unit.
"""
try
unit_div
self
byte_sizes
unit
lower
()]
except
KeyError
raise
PydanticCustomError
'byte_size_unit'
'Could not interpret byte unit:
{unit}
'unit'
unit
return
self
unit_div
PastDate
A date in the past.
Source code in
pydantic/types.py
2203
2204
2205
2206
2207
2208
2209
2210
2211
2212
2213
2214
2215
2216
2217
2218
2219
2220
class
PastDate
"""A date in the past."""
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
cls
source
# used directly as a type
return
core_schema
date_schema
now_op
'past'
else
schema
handler
source
_check_annotated_type
schema
'type'
'date'
cls
__name__
schema
'now_op'
'past'
return
schema
def
__repr__
self
str
return
'PastDate'
FutureDate
A date in the future.
Source code in
pydantic/types.py
2222
2223
2224
2225
2226
2227
2228
2229
2230
2231
2232
2233
2234
2235
2236
2237
2238
2239
class
FutureDate
"""A date in the future."""
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
cls
source
# used directly as a type
return
core_schema
date_schema
now_op
'future'
else
schema
handler
source
_check_annotated_type
schema
'type'
'date'
cls
__name__
schema
'now_op'
'future'
return
schema
def
__repr__
self
str
return
'FutureDate'
AwareDatetime
A datetime that requires timezone info.
Source code in
pydantic/types.py
2279
2280
2281
2282
2283
2284
2285
2286
2287
2288
2289
2290
2291
2292
2293
2294
2295
2296
class
AwareDatetime
"""A datetime that requires timezone info."""
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
cls
source
# used directly as a type
return
core_schema
datetime_schema
tz_constraint
'aware'
else
schema
handler
source
_check_annotated_type
schema
'type'
'datetime'
cls
__name__
schema
'tz_constraint'
'aware'
return
schema
def
__repr__
self
str
return
'AwareDatetime'
NaiveDatetime
A datetime that doesn't require timezone info.
Source code in
pydantic/types.py
2298
2299
2300
2301
2302
2303
2304
2305
2306
2307
2308
2309
2310
2311
2312
2313
2314
2315
class
NaiveDatetime
"""A datetime that doesn't require timezone info."""
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
cls
source
# used directly as a type
return
core_schema
datetime_schema
tz_constraint
'naive'
else
schema
handler
source
_check_annotated_type
schema
'type'
'datetime'
cls
__name__
schema
'tz_constraint'
'naive'
return
schema
def
__repr__
self
str
return
'NaiveDatetime'
PastDatetime
A datetime that must be in the past.
Source code in
pydantic/types.py
2317
2318
2319
2320
2321
2322
2323
2324
2325
2326
2327
2328
2329
2330
2331
2332
2333
2334
class
PastDatetime
"""A datetime that must be in the past."""
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
cls
source
# used directly as a type
return
core_schema
datetime_schema
now_op
'past'
else
schema
handler
source
_check_annotated_type
schema
'type'
'datetime'
cls
__name__
schema
'now_op'
'past'
return
schema
def
__repr__
self
str
return
'PastDatetime'
FutureDatetime
A datetime that must be in the future.
Source code in
pydantic/types.py
2336
2337
2338
2339
2340
2341
2342
2343
2344
2345
2346
2347
2348
2349
2350
2351
2352
2353
class
FutureDatetime
"""A datetime that must be in the future."""
@classmethod
def
__get_pydantic_core_schema__
cls
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
cls
source
# used directly as a type
return
core_schema
datetime_schema
now_op
'future'
else
schema
handler
source
_check_annotated_type
schema
'type'
'datetime'
cls
__name__
schema
'now_op'
'future'
return
schema
def
__repr__
self
str
return
'FutureDatetime'
EncoderProtocol
Bases:
Protocol
Protocol for encoding and decoding data to and from bytes.
Source code in
pydantic/types.py
2359
2360
2361
2362
2363
2364
2365
2366
2367
2368
2369
2370
2371
2372
2373
2374
2375
2376
2377
2378
2379
2380
2381
2382
2383
2384
2385
2386
2387
2388
2389
2390
2391
2392
2393
class
EncoderProtocol
Protocol
"""Protocol for encoding and decoding data to and from bytes."""
@classmethod
def
decode
cls
data
bytes
bytes
"""Decode the data using the encoder.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
...
@classmethod
def
encode
cls
value
bytes
bytes
"""Encode the data using the encoder.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
...
@classmethod
def
get_json_format
cls
str
"""Get the JSON format for the encoded data.
Returns:
The JSON format for the encoded data.
"""
...
decode
classmethod
decode
data
bytes
bytes
Decode the data using the encoder.
Parameters:
Name
Type
Description
Default
data
bytes
The data to decode.
required
Returns:
Type
Description
bytes
The decoded data.
Source code in
pydantic/types.py
2362
2363
2364
2365
2366
2367
2368
2369
2370
2371
2372
@classmethod
def
decode
cls
data
bytes
bytes
"""Decode the data using the encoder.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
...
encode
classmethod
encode
value
bytes
bytes
Encode the data using the encoder.
Parameters:
Name
Type
Description
Default
value
bytes
The data to encode.
required
Returns:
Type
Description
bytes
The encoded data.
Source code in
pydantic/types.py
2374
2375
2376
2377
2378
2379
2380
2381
2382
2383
2384
@classmethod
def
encode
cls
value
bytes
bytes
"""Encode the data using the encoder.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
...
get_json_format
classmethod
get_json_format
str
Get the JSON format for the encoded data.
Returns:
Type
Description
str
The JSON format for the encoded data.
Source code in
pydantic/types.py
2386
2387
2388
2389
2390
2391
2392
2393
@classmethod
def
get_json_format
cls
str
"""Get the JSON format for the encoded data.
Returns:
The JSON format for the encoded data.
"""
...
Base64Encoder
Bases:
EncoderProtocol
Standard (non-URL-safe) Base64 encoder.
Source code in
pydantic/types.py
2396
2397
2398
2399
2400
2401
2402
2403
2404
2405
2406
2407
2408
2409
2410
2411
2412
2413
2414
2415
2416
2417
2418
2419
2420
2421
2422
2423
2424
2425
2426
2427
2428
2429
2430
2431
2432
2433
class
Base64Encoder
EncoderProtocol
"""Standard (non-URL-safe) Base64 encoder."""
@classmethod
def
decode
cls
data
bytes
bytes
"""Decode the data from base64 encoded bytes to original bytes data.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
try
return
base64
b64decode
data
except
ValueError
raise
PydanticCustomError
'base64_decode'
"Base64 decoding error: '
{error}
'error'
str
)})
@classmethod
def
encode
cls
value
bytes
bytes
"""Encode the data from bytes to a base64 encoded bytes.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
base64
b64encode
value
@classmethod
def
get_json_format
cls
Literal
'base64'
"""Get the JSON format for the encoded data.
Returns:
The JSON format for the encoded data.
"""
return
'base64'
decode
classmethod
decode
data
bytes
bytes
Decode the data from base64 encoded bytes to original bytes data.
Parameters:
Name
Type
Description
Default
data
bytes
The data to decode.
required
Returns:
Type
Description
bytes
The decoded data.
Source code in
pydantic/types.py
2399
2400
2401
2402
2403
2404
2405
2406
2407
2408
2409
2410
2411
2412
@classmethod
def
decode
cls
data
bytes
bytes
"""Decode the data from base64 encoded bytes to original bytes data.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
try
return
base64
b64decode
data
except
ValueError
raise
PydanticCustomError
'base64_decode'
"Base64 decoding error: '
{error}
'error'
str
)})
encode
classmethod
encode
value
bytes
bytes
Encode the data from bytes to a base64 encoded bytes.
Parameters:
Name
Type
Description
Default
value
bytes
The data to encode.
required
Returns:
Type
Description
bytes
The encoded data.
Source code in
pydantic/types.py
2414
2415
2416
2417
2418
2419
2420
2421
2422
2423
2424
@classmethod
def
encode
cls
value
bytes
bytes
"""Encode the data from bytes to a base64 encoded bytes.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
base64
b64encode
value
get_json_format
classmethod
get_json_format
Literal
'base64'
Get the JSON format for the encoded data.
Returns:
Type
Description
Literal
['base64']
The JSON format for the encoded data.
Source code in
pydantic/types.py
2426
2427
2428
2429
2430
2431
2432
2433
@classmethod
def
get_json_format
cls
Literal
'base64'
"""Get the JSON format for the encoded data.
Returns:
The JSON format for the encoded data.
"""
return
'base64'
Base64UrlEncoder
Bases:
EncoderProtocol
URL-safe Base64 encoder.
Source code in
pydantic/types.py
2436
2437
2438
2439
2440
2441
2442
2443
2444
2445
2446
2447
2448
2449
2450
2451
2452
2453
2454
2455
2456
2457
2458
2459
2460
2461
2462
2463
2464
2465
2466
2467
2468
2469
2470
2471
2472
2473
class
Base64UrlEncoder
EncoderProtocol
"""URL-safe Base64 encoder."""
@classmethod
def
decode
cls
data
bytes
bytes
"""Decode the data from base64 encoded bytes to original bytes data.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
try
return
base64
urlsafe_b64decode
data
except
ValueError
raise
PydanticCustomError
'base64_decode'
"Base64 decoding error: '
{error}
'error'
str
)})
@classmethod
def
encode
cls
value
bytes
bytes
"""Encode the data from bytes to a base64 encoded bytes.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
base64
urlsafe_b64encode
value
@classmethod
def
get_json_format
cls
Literal
'base64url'
"""Get the JSON format for the encoded data.
Returns:
The JSON format for the encoded data.
"""
return
'base64url'
decode
classmethod
decode
data
bytes
bytes
Decode the data from base64 encoded bytes to original bytes data.
Parameters:
Name
Type
Description
Default
data
bytes
The data to decode.
required
Returns:
Type
Description
bytes
The decoded data.
Source code in
pydantic/types.py
2439
2440
2441
2442
2443
2444
2445
2446
2447
2448
2449
2450
2451
2452
@classmethod
def
decode
cls
data
bytes
bytes
"""Decode the data from base64 encoded bytes to original bytes data.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
try
return
base64
urlsafe_b64decode
data
except
ValueError
raise
PydanticCustomError
'base64_decode'
"Base64 decoding error: '
{error}
'error'
str
)})
encode
classmethod
encode
value
bytes
bytes
Encode the data from bytes to a base64 encoded bytes.
Parameters:
Name
Type
Description
Default
value
bytes
The data to encode.
required
Returns:
Type
Description
bytes
The encoded data.
Source code in
pydantic/types.py
2454
2455
2456
2457
2458
2459
2460
2461
2462
2463
2464
@classmethod
def
encode
cls
value
bytes
bytes
"""Encode the data from bytes to a base64 encoded bytes.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
base64
urlsafe_b64encode
value
get_json_format
classmethod
get_json_format
Literal
'base64url'
Get the JSON format for the encoded data.
Returns:
Type
Description
Literal
['base64url']
The JSON format for the encoded data.
Source code in
pydantic/types.py
2466
2467
2468
2469
2470
2471
2472
2473
@classmethod
def
get_json_format
cls
Literal
'base64url'
"""Get the JSON format for the encoded data.
Returns:
The JSON format for the encoded data.
"""
return
'base64url'
EncodedBytes
dataclass
A bytes type that is encoded and decoded using the specified encoder.
EncodedBytes
needs an encoder that implements
EncoderProtocol
to operate.
from
typing
import
Annotated
from
pydantic
import
BaseModel
EncodedBytes
EncoderProtocol
ValidationError
class
MyEncoder
EncoderProtocol
@classmethod
def
decode
cls
data
bytes
bytes
data
'**undecodable**'
raise
ValueError
'Cannot decode data'
return
data
@classmethod
def
encode
cls
value
bytes
bytes
return
'**encoded**: '
value
@classmethod
def
get_json_format
cls
str
return
'my-encoder'
MyEncodedBytes
Annotated
bytes
EncodedBytes
encoder
MyEncoder
class
Model
BaseModel
my_encoded_bytes
MyEncodedBytes
# Initialize the model with encoded data
Model
my_encoded_bytes
'**encoded**: some bytes'
# Access decoded value
print
my_encoded_bytes
#> b'some bytes'
# Serialize into the encoded form
print
model_dump
())
#> {'my_encoded_bytes': b'**encoded**: some bytes'}
# Validate encoded data
try
Model
my_encoded_bytes
'**undecodable**'
except
ValidationError
print
'''
1 validation error for Model
my_encoded_bytes
Value error, Cannot decode data [type=value_error, input_value=b'**undecodable**', input_type=bytes]
'''
Source code in
pydantic/types.py
2476
2477
2478
2479
2480
2481
2482
2483
2484
2485
2486
2487
2488
2489
2490
2491
2492
2493
2494
2495
2496
2497
2498
2499
2500
2501
2502
2503
2504
2505
2506
2507
2508
2509
2510
2511
2512
2513
2514
2515
2516
2517
2518
2519
2520
2521
2522
2523
2524
2525
2526
2527
2528
2529
2530
2531
2532
2533
2534
2535
2536
2537
2538
2539
2540
2541
2542
2543
2544
2545
2546
2547
2548
2549
2550
2551
2552
2553
2554
2555
2556
2557
2558
2559
2560
2561
2562
2563
2564
2565
2566
2567
2568
2569
2570
2571
2572
@_dataclasses
dataclass
_internal_dataclass
slots_true
class
EncodedBytes
"""A bytes type that is encoded and decoded using the specified encoder.
`EncodedBytes` needs an encoder that implements `EncoderProtocol` to operate.
```python
from typing import Annotated
from pydantic import BaseModel, EncodedBytes, EncoderProtocol, ValidationError
class MyEncoder(EncoderProtocol):
@classmethod
def decode(cls, data: bytes) -> bytes:
if data == b'**undecodable**':
raise ValueError('Cannot decode data')
return data[13:]
@classmethod
def encode(cls, value: bytes) -> bytes:
return b'**encoded**: ' + value
@classmethod
def get_json_format(cls) -> str:
return 'my-encoder'
MyEncodedBytes = Annotated[bytes, EncodedBytes(encoder=MyEncoder)]
class Model(BaseModel):
my_encoded_bytes: MyEncodedBytes
# Initialize the model with encoded data
m = Model(my_encoded_bytes=b'**encoded**: some bytes')
# Access decoded value
print(m.my_encoded_bytes)
#> b'some bytes'
# Serialize into the encoded form
print(m.model_dump())
#> {'my_encoded_bytes': b'**encoded**: some bytes'}
# Validate encoded data
try:
Model(my_encoded_bytes=b'**undecodable**')
except ValidationError as e:
print(e)
'''
1 validation error for Model
my_encoded_bytes
Value error, Cannot decode data [type=value_error, input_value=b'**undecodable**', input_type=bytes]
'''
```
"""
encoder
type
EncoderProtocol
def
__get_pydantic_json_schema__
self
core_schema
core_schema
CoreSchema
handler
GetJsonSchemaHandler
JsonSchemaValue
field_schema
handler
core_schema
field_schema
update
type
'string'
format
self
encoder
get_json_format
())
return
field_schema
def
__get_pydantic_core_schema__
self
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
schema
handler
source
_check_annotated_type
schema
'type'
'bytes'
self
__class__
__name__
return
core_schema
with_info_after_validator_function
function
self
decode
schema
schema
serialization
core_schema
plain_serializer_function_ser_schema
function
self
encode
def
decode
self
data
bytes
core_schema
ValidationInfo
bytes
"""Decode the data using the specified encoder.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
return
self
encoder
decode
data
def
encode
self
value
bytes
bytes
"""Encode the data using the specified encoder.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
self
encoder
encode
value
def
__hash__
self
int
return
hash
self
encoder
decode
decode
data
bytes
ValidationInfo
bytes
Decode the data using the specified encoder.
Parameters:
Name
Type
Description
Default
data
bytes
The data to decode.
required
Returns:
Type
Description
bytes
The decoded data.
Source code in
pydantic/types.py
2549
2550
2551
2552
2553
2554
2555
2556
2557
2558
def
decode
self
data
bytes
core_schema
ValidationInfo
bytes
"""Decode the data using the specified encoder.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
return
self
encoder
decode
data
encode
encode
value
bytes
bytes
Encode the data using the specified encoder.
Parameters:
Name
Type
Description
Default
value
bytes
The data to encode.
required
Returns:
Type
Description
bytes
The encoded data.
Source code in
pydantic/types.py
2560
2561
2562
2563
2564
2565
2566
2567
2568
2569
def
encode
self
value
bytes
bytes
"""Encode the data using the specified encoder.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
self
encoder
encode
value
EncodedStr
dataclass
A str type that is encoded and decoded using the specified encoder.
EncodedStr
needs an encoder that implements
EncoderProtocol
to operate.
from
typing
import
Annotated
from
pydantic
import
BaseModel
EncodedStr
EncoderProtocol
ValidationError
class
MyEncoder
EncoderProtocol
@classmethod
def
decode
cls
data
bytes
bytes
data
'**undecodable**'
raise
ValueError
'Cannot decode data'
return
data
@classmethod
def
encode
cls
value
bytes
bytes
return
'**encoded**: '
value
@classmethod
def
get_json_format
cls
str
return
'my-encoder'
MyEncodedStr
Annotated
str
EncodedStr
encoder
MyEncoder
class
Model
BaseModel
my_encoded_str
MyEncodedStr
# Initialize the model with encoded data
Model
my_encoded_str
'**encoded**: some str'
# Access decoded value
print
my_encoded_str
#> some str
# Serialize into the encoded form
print
model_dump
())
#> {'my_encoded_str': '**encoded**: some str'}
# Validate encoded data
try
Model
my_encoded_str
'**undecodable**'
except
ValidationError
print
'''
1 validation error for Model
my_encoded_str
Value error, Cannot decode data [type=value_error, input_value='**undecodable**', input_type=str]
'''
Source code in
pydantic/types.py
2575
2576
2577
2578
2579
2580
2581
2582
2583
2584
2585
2586
2587
2588
2589
2590
2591
2592
2593
2594
2595
2596
2597
2598
2599
2600
2601
2602
2603
2604
2605
2606
2607
2608
2609
2610
2611
2612
2613
2614
2615
2616
2617
2618
2619
2620
2621
2622
2623
2624
2625
2626
2627
2628
2629
2630
2631
2632
2633
2634
2635
2636
2637
2638
2639
2640
2641
2642
2643
2644
2645
2646
2647
2648
2649
2650
2651
2652
2653
2654
2655
2656
2657
2658
2659
2660
2661
2662
2663
2664
2665
2666
2667
2668
2669
2670
2671
@_dataclasses
dataclass
_internal_dataclass
slots_true
class
EncodedStr
"""A str type that is encoded and decoded using the specified encoder.
`EncodedStr` needs an encoder that implements `EncoderProtocol` to operate.
```python
from typing import Annotated
from pydantic import BaseModel, EncodedStr, EncoderProtocol, ValidationError
class MyEncoder(EncoderProtocol):
@classmethod
def decode(cls, data: bytes) -> bytes:
if data == b'**undecodable**':
raise ValueError('Cannot decode data')
return data[13:]
@classmethod
def encode(cls, value: bytes) -> bytes:
return b'**encoded**: ' + value
@classmethod
def get_json_format(cls) -> str:
return 'my-encoder'
MyEncodedStr = Annotated[str, EncodedStr(encoder=MyEncoder)]
class Model(BaseModel):
my_encoded_str: MyEncodedStr
# Initialize the model with encoded data
m = Model(my_encoded_str='**encoded**: some str')
# Access decoded value
print(m.my_encoded_str)
#> some str
# Serialize into the encoded form
print(m.model_dump())
#> {'my_encoded_str': '**encoded**: some str'}
# Validate encoded data
try:
Model(my_encoded_str='**undecodable**')
except ValidationError as e:
print(e)
'''
1 validation error for Model
my_encoded_str
Value error, Cannot decode data [type=value_error, input_value='**undecodable**', input_type=str]
'''
```
"""
encoder
type
EncoderProtocol
def
__get_pydantic_json_schema__
self
core_schema
core_schema
CoreSchema
handler
GetJsonSchemaHandler
JsonSchemaValue
field_schema
handler
core_schema
field_schema
update
type
'string'
format
self
encoder
get_json_format
())
return
field_schema
def
__get_pydantic_core_schema__
self
source
type
Any
handler
GetCoreSchemaHandler
core_schema
CoreSchema
schema
handler
source
_check_annotated_type
schema
'type'
'str'
self
__class__
__name__
return
core_schema
with_info_after_validator_function
function
self
decode_str
schema
schema
serialization
core_schema
plain_serializer_function_ser_schema
function
self
encode_str
def
decode_str
self
data
str
core_schema
ValidationInfo
str
"""Decode the data using the specified encoder.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
return
self
encoder
decode
data
encode
())
decode
def
encode_str
self
value
str
str
"""Encode the data using the specified encoder.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
self
encoder
encode
value
encode
())
decode
# noqa: UP008
def
__hash__
self
int
return
hash
self
encoder
decode_str
decode_str
data
str
ValidationInfo
str
Decode the data using the specified encoder.
Parameters:
Name
Type
Description
Default
data
str
The data to decode.
required
Returns:
Type
Description
str
The decoded data.
Source code in
pydantic/types.py
2648
2649
2650
2651
2652
2653
2654
2655
2656
2657
def
decode_str
self
data
str
core_schema
ValidationInfo
str
"""Decode the data using the specified encoder.
Args:
data: The data to decode.
Returns:
The decoded data.
"""
return
self
encoder
decode
data
encode
())
decode
encode_str
encode_str
value
str
str
Encode the data using the specified encoder.
Parameters:
Name
Type
Description
Default
value
str
The data to encode.
required
Returns:
Type
Description
str
The encoded data.
Source code in
pydantic/types.py
2659
2660
2661
2662
2663
2664
2665
2666
2667
2668
def
encode_str
self
value
str
str
"""Encode the data using the specified encoder.
Args:
value: The data to encode.
Returns:
The encoded data.
"""
return
self
encoder
encode
value
encode
())
decode
# noqa: UP008
GetPydanticSchema
dataclass
Usage Documentation
Using
GetPydanticSchema
to Reduce Boilerplate
A convenience class for creating an annotation that provides pydantic custom type hooks.
This class is intended to eliminate the need to create a custom "marker" which defines the
__get_pydantic_core_schema__
and
__get_pydantic_json_schema__
custom hook methods.
For example, to have a field treated by type checkers as
int
, but by pydantic as
Any
, you can do:
from
typing
import
Annotated
Any
from
pydantic
import
BaseModel
GetPydanticSchema
HandleAsAny
GetPydanticSchema
lambda
Any
class
Model
BaseModel
Annotated
int
HandleAsAny
# pydantic sees `x: Any`
print
repr
Model
'abc'
#> 'abc'
Source code in
pydantic/types.py
2847
2848
2849
2850
2851
2852
2853
2854
2855
2856
2857
2858
2859
2860
2861
2862
2863
2864
2865
2866
2867
2868
2869
2870
2871
2872
2873
2874
2875
2876
2877
2878
2879
2880
2881
2882
2883
2884
2885
2886
2887
2888
2889
2890
2891
@_dataclasses
dataclass
_internal_dataclass
slots_true
class
GetPydanticSchema
"""!!! abstract "Usage Documentation"
[Using `GetPydanticSchema` to Reduce Boilerplate](../concepts/types.md#using-getpydanticschema-to-reduce-boilerplate)
A convenience class for creating an annotation that provides pydantic custom type hooks.
This class is intended to eliminate the need to create a custom "marker" which defines the
`__get_pydantic_core_schema__` and `__get_pydantic_json_schema__` custom hook methods.
For example, to have a field treated by type checkers as `int`, but by pydantic as `Any`, you can do:
```python
from typing import Annotated, Any
from pydantic import BaseModel, GetPydanticSchema
HandleAsAny = GetPydanticSchema(lambda _s, h: h(Any))
class Model(BaseModel):
x: Annotated[int, HandleAsAny]  # pydantic sees `x: Any`
print(repr(Model(x='abc').x))
#> 'abc'
```
"""
get_pydantic_core_schema
Callable
Any
GetCoreSchemaHandler
CoreSchema
None
None
get_pydantic_json_schema
Callable
Any
GetJsonSchemaHandler
JsonSchemaValue
None
None
# Note: we may want to consider adding a convenience staticmethod `def for_type(type_: Any) -> GetPydanticSchema:`
# which returns `GetPydanticSchema(lambda _s, h: h(type_))`
not
TYPE_CHECKING
# We put `__getattr__` in a non-TYPE_CHECKING block because otherwise, mypy allows arbitrary attribute access
def
__getattr__
self
item
str
Any
"""Use this rather than defining `__get_pydantic_core_schema__` etc. to reduce the number of nested calls."""
item
'__get_pydantic_core_schema__'
and
self
get_pydantic_core_schema
return
self
get_pydantic_core_schema
elif
item
'__get_pydantic_json_schema__'
and
self
get_pydantic_json_schema
return
self
get_pydantic_json_schema
else
return
object
__getattribute__
self
item
__hash__
object
__hash__
Tag
dataclass
Provides a way to specify the expected tag to use for a case of a (callable) discriminated union.
Also provides a way to label a union case in error messages.
When using a callable
Discriminator
, attach a
Tag
to each case in the
Union
to specify the tag that
should be used to identify that case. For example, in the below example, the
Tag
is used to specify that
get_discriminator_value
returns
'apple'
, the input should be validated as an
ApplePie
, and if it
returns
'pumpkin'
, the input should be validated as a
PumpkinPie
The primary role of the
Tag
here is to map the return value from the callable
Discriminator
function to
the appropriate member of the
Union
in question.
from
typing
import
Annotated
Any
Literal
Union
from
pydantic
import
BaseModel
Discriminator
Tag
class
Pie
BaseModel
time_to_cook
int
num_ingredients
int
class
ApplePie
Pie
fruit
Literal
'apple'
'apple'
class
PumpkinPie
Pie
filling
Literal
'pumpkin'
'pumpkin'
def
get_discriminator_value
Any
str
isinstance
dict
return
get
'fruit'
get
'filling'
return
getattr
'fruit'
getattr
'filling'
None
class
ThanksgivingDinner
BaseModel
dessert
Annotated
Union
Annotated
ApplePie
Tag
'apple'
)],
Annotated
PumpkinPie
Tag
'pumpkin'
)],
Discriminator
get_discriminator_value
apple_variation
ThanksgivingDinner
model_validate
'dessert'
'fruit'
'apple'
'time_to_cook'
'num_ingredients'
print
repr
apple_variation
'''
ThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))
'''
pumpkin_variation
ThanksgivingDinner
model_validate
'dessert'
'filling'
'pumpkin'
'time_to_cook'
'num_ingredients'
print
repr
pumpkin_variation
'''
ThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))
'''
Note
You must specify a
Tag
for every case in a
Tag
that is associated with a
callable
Discriminator
. Failing to do so will result in a
PydanticUserError
with code
callable-discriminator-no-tag
See the
Discriminated Unions
concepts docs for more details on how to use
Tag
Source code in
pydantic/types.py
2894
2895
2896
2897
2898
2899
2900
2901
2902
2903
2904
2905
2906
2907
2908
2909
2910
2911
2912
2913
2914
2915
2916
2917
2918
2919
2920
2921
2922
2923
2924
2925
2926
2927
2928
2929
2930
2931
2932
2933
2934
2935
2936
2937
2938
2939
2940
2941
2942
2943
2944
2945
2946
2947
2948
2949
2950
2951
2952
2953
2954
2955
2956
2957
2958
2959
2960
2961
2962
2963
2964
2965
2966
2967
2968
2969
2970
2971
2972
2973
2974
2975
2976
@_dataclasses
dataclass
_internal_dataclass
slots_true
frozen
True
class
Tag
"""Provides a way to specify the expected tag to use for a case of a (callable) discriminated union.
Also provides a way to label a union case in error messages.
When using a callable `Discriminator`, attach a `Tag` to each case in the `Union` to specify the tag that
should be used to identify that case. For example, in the below example, the `Tag` is used to specify that
if `get_discriminator_value` returns `'apple'`, the input should be validated as an `ApplePie`, and if it
returns `'pumpkin'`, the input should be validated as a `PumpkinPie`.
The primary role of the `Tag` here is to map the return value from the callable `Discriminator` function to
the appropriate member of the `Union` in question.
```python
from typing import Annotated, Any, Literal, Union
from pydantic import BaseModel, Discriminator, Tag
class Pie(BaseModel):
time_to_cook: int
num_ingredients: int
class ApplePie(Pie):
fruit: Literal['apple'] = 'apple'
class PumpkinPie(Pie):
filling: Literal['pumpkin'] = 'pumpkin'
def get_discriminator_value(v: Any) -> str:
if isinstance(v, dict):
return v.get('fruit', v.get('filling'))
return getattr(v, 'fruit', getattr(v, 'filling', None))
class ThanksgivingDinner(BaseModel):
dessert: Annotated[
Union[
Annotated[ApplePie, Tag('apple')],
Annotated[PumpkinPie, Tag('pumpkin')],
Discriminator(get_discriminator_value),
apple_variation = ThanksgivingDinner.model_validate(
{'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}
print(repr(apple_variation))
'''
ThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))
'''
pumpkin_variation = ThanksgivingDinner.model_validate(
'dessert': {
'filling': 'pumpkin',
'time_to_cook': 40,
'num_ingredients': 6,
print(repr(pumpkin_variation))
'''
ThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))
'''
```
!!! note
You must specify a `Tag` for every case in a `Tag` that is associated with a
callable `Discriminator`. Failing to do so will result in a `PydanticUserError` with code
[`callable-discriminator-no-tag`](../errors/usage_errors.md#callable-discriminator-no-tag).
See the [Discriminated Unions] concepts docs for more details on how to use `Tag`s.
[Discriminated Unions]: ../concepts/unions.md#discriminated-unions
"""
tag
str
def
__get_pydantic_core_schema__
self
source_type
Any
handler
GetCoreSchemaHandler
CoreSchema
schema
handler
source_type
metadata
cast
'CoreMetadata'
schema
setdefault
'metadata'
{}))
metadata
'pydantic_internal_union_tag_key'
self
tag
return
schema
Discriminator
dataclass
Usage Documentation
Discriminated Unions with
Callable
Discriminator
Provides a way to use a custom callable as the way to extract the value of a union discriminator.
This allows you to get validation behavior like you'd get from
Field(discriminator=<field_name>)
but without needing to have a single shared field across all the union choices. This also makes it
possible to handle unions of models and primitive types with discriminated-union-style validation errors.
Finally, this allows you to use a custom callable as the way to identify which member of a union a value
belongs to, while still seeing all the performance benefits of a discriminated union.
Consider this example, which is much more performant with the use of
Discriminator
and thus a
TaggedUnion
than it would be as a normal
Union
from
typing
import
Annotated
Any
Literal
Union
from
pydantic
import
BaseModel
Discriminator
Tag
class
Pie
BaseModel
time_to_cook
int
num_ingredients
int
class
ApplePie
Pie
fruit
Literal
'apple'
'apple'
class
PumpkinPie
Pie
filling
Literal
'pumpkin'
'pumpkin'
def
get_discriminator_value
Any
str
isinstance
dict
return
get
'fruit'
get
'filling'
return
getattr
'fruit'
getattr
'filling'
None
class
ThanksgivingDinner
BaseModel
dessert
Annotated
Union
Annotated
ApplePie
Tag
'apple'
)],
Annotated
PumpkinPie
Tag
'pumpkin'
)],
Discriminator
get_discriminator_value
apple_variation
ThanksgivingDinner
model_validate
'dessert'
'fruit'
'apple'
'time_to_cook'
'num_ingredients'
print
repr
apple_variation
'''
ThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))
'''
pumpkin_variation
ThanksgivingDinner
model_validate
'dessert'
'filling'
'pumpkin'
'time_to_cook'
'num_ingredients'
print
repr
pumpkin_variation
'''
ThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))
'''
See the
Discriminated Unions
concepts docs for more details on how to use
Discriminator
Source code in
pydantic/types.py
2979
2980
2981
2982
2983
2984
2985
2986
2987
2988
2989
2990
2991
2992
2993
2994
2995
2996
2997
2998
2999
3000
3001
3002
3003
3004
3005
3006
3007
3008
3009
3010
3011
3012
3013
3014
3015
3016
3017
3018
3019
3020
3021
3022
3023
3024
3025
3026
3027
3028
3029
3030
3031
3032
3033
3034
3035
3036
3037
3038
3039
3040
3041
3042
3043
3044
3045
3046
3047
3048
3049
3050
3051
3052
3053
3054
3055
3056
3057
3058
3059
3060
3061
3062
3063
3064
3065
3066
3067
3068
3069
3070
3071
3072
3073
3074
3075
3076
3077
3078
3079
3080
3081
3082
3083
3084
3085
3086
3087
3088
3089
3090
3091
3092
3093
3094
3095
3096
3097
3098
3099
3100
3101
3102
3103
3104
3105
3106
3107
3108
3109
3110
3111
3112
3113
3114
3115
3116
3117
3118
3119
3120
3121
3122
3123
3124
3125
3126
@_dataclasses
dataclass
_internal_dataclass
slots_true
frozen
True
class
Discriminator
"""!!! abstract "Usage Documentation"
[Discriminated Unions with `Callable` `Discriminator`](../concepts/unions.md#discriminated-unions-with-callable-discriminator)
Provides a way to use a custom callable as the way to extract the value of a union discriminator.
This allows you to get validation behavior like you'd get from `Field(discriminator=<field_name>)`,
but without needing to have a single shared field across all the union choices. This also makes it
possible to handle unions of models and primitive types with discriminated-union-style validation errors.
Finally, this allows you to use a custom callable as the way to identify which member of a union a value
belongs to, while still seeing all the performance benefits of a discriminated union.
Consider this example, which is much more performant with the use of `Discriminator` and thus a `TaggedUnion`
than it would be as a normal `Union`.
```python
from typing import Annotated, Any, Literal, Union
from pydantic import BaseModel, Discriminator, Tag
class Pie(BaseModel):
time_to_cook: int
num_ingredients: int
class ApplePie(Pie):
fruit: Literal['apple'] = 'apple'
class PumpkinPie(Pie):
filling: Literal['pumpkin'] = 'pumpkin'
def get_discriminator_value(v: Any) -> str:
if isinstance(v, dict):
return v.get('fruit', v.get('filling'))
return getattr(v, 'fruit', getattr(v, 'filling', None))
class ThanksgivingDinner(BaseModel):
dessert: Annotated[
Union[
Annotated[ApplePie, Tag('apple')],
Annotated[PumpkinPie, Tag('pumpkin')],
Discriminator(get_discriminator_value),
apple_variation = ThanksgivingDinner.model_validate(
{'dessert': {'fruit': 'apple', 'time_to_cook': 60, 'num_ingredients': 8}}
print(repr(apple_variation))
'''
ThanksgivingDinner(dessert=ApplePie(time_to_cook=60, num_ingredients=8, fruit='apple'))
'''
pumpkin_variation = ThanksgivingDinner.model_validate(
'dessert': {
'filling': 'pumpkin',
'time_to_cook': 40,
'num_ingredients': 6,
print(repr(pumpkin_variation))
'''
ThanksgivingDinner(dessert=PumpkinPie(time_to_cook=40, num_ingredients=6, filling='pumpkin'))
'''
```
See the [Discriminated Unions] concepts docs for more details on how to use `Discriminator`s.
[Discriminated Unions]: ../concepts/unions.md#discriminated-unions
"""
discriminator
str
Callable
Any
Hashable
"""The callable or field name for discriminating the type in a tagged union.
A `Callable` discriminator must extract the value of the discriminator from the input.
A `str` discriminator must be the name of a field to discriminate against.
"""
custom_error_type
str
None
None
"""Type to use in [custom errors](../errors/errors.md) replacing the standard discriminated union
validation errors.
"""
custom_error_message
str
None
None
"""Message to use in custom errors."""
custom_error_context
dict
str
int
str
float
None
None
"""Context to use in custom errors."""
def
__get_pydantic_core_schema__
self
source_type
Any
handler
GetCoreSchemaHandler
CoreSchema
not
is_union_origin
get_origin
source_type
)):
raise
TypeError
type
self
__name__
must be used with a Union type, not
source_type
isinstance
self
discriminator
str
from
pydantic
import
Field
return
handler
Annotated
source_type
Field
discriminator
self
discriminator
)])
else
original_schema
handler
source_type
return
self
_convert_schema
original_schema
def
_convert_schema
self
original_schema
core_schema
CoreSchema
core_schema
TaggedUnionSchema
original_schema
'type'
'union'
# This likely indicates that the schema was a single-item union that was simplified.
# In this case, we do the same thing we do in
# `pydantic._internal._discriminated_union._ApplyInferredDiscriminator._apply_to_root`, namely,
# package the generated schema back into a single-item union.
original_schema
core_schema
union_schema
original_schema
tagged_union_choices
for
choice
original_schema
'choices'
tag
None
isinstance
choice
tuple
choice
tag
choice
metadata
cast
'CoreMetadata | None'
choice
get
'metadata'
metadata
not
None
tag
metadata
get
'pydantic_internal_union_tag_key'
tag
tag
None
raise
PydanticUserError
'`Tag` not provided for choice
choice
used with `Discriminator`'
code
'callable-discriminator-no-tag'
tagged_union_choices
tag
choice
# Have to do these verbose checks to ensure falsy values ('' and {}) don't get ignored
custom_error_type
self
custom_error_type
custom_error_type
None
custom_error_type
original_schema
get
'custom_error_type'
custom_error_message
self
custom_error_message
custom_error_message
None
custom_error_message
original_schema
get
'custom_error_message'
custom_error_context
self
custom_error_context
custom_error_context
None
custom_error_context
original_schema
get
'custom_error_context'
custom_error_type
original_schema
get
'custom_error_type'
custom_error_type
None
else
custom_error_type
return
core_schema
tagged_union_schema
tagged_union_choices
self
discriminator
custom_error_type
custom_error_type
custom_error_message
custom_error_message
custom_error_context
custom_error_context
strict
original_schema
get
'strict'
ref
original_schema
get
'ref'
metadata
original_schema
get
'metadata'
serialization
original_schema
get
'serialization'
discriminator
instance-attribute
discriminator
str
Callable
Any
Hashable
The callable or field name for discriminating the type in a tagged union.
Callable
discriminator must extract the value of the discriminator from the input.
str
discriminator must be the name of a field to discriminate against.
custom_error_type
class-attribute
instance-attribute
custom_error_type
str
None
None
Type to use in
custom errors
replacing the standard discriminated union
validation errors.
custom_error_message
class-attribute
instance-attribute
custom_error_message
str
None
None
Message to use in custom errors.
custom_error_context
class-attribute
instance-attribute
custom_error_context
dict
str
int
str
float
None
None
Context to use in custom errors.
FailFast
dataclass
Bases:
PydanticMetadata
BaseMetadata
FailFast
annotation can be used to specify that validation should stop at the first error.
This can be useful when you want to validate a large amount of data and you only need to know if it's valid or not.
You might want to enable this setting if you want to validate your data faster (basically, if you use this,
validation will be more performant with the caveat that you get less information).
from
typing
import
Annotated
from
pydantic
import
BaseModel
FailFast
ValidationError
class
Model
BaseModel
Annotated
list
int
FailFast
()]
# This will raise a single error for the first invalid value and stop validation
try
obj
Model
'a'
'b'
'c'
except
ValidationError
print
'''
1 validation error for Model
x.2
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]
'''
Source code in
pydantic/types.py
3255
3256
3257
3258
3259
3260
3261
3262
3263
3264
3265
3266
3267
3268
3269
3270
3271
3272
3273
3274
3275
3276
3277
3278
3279
3280
3281
3282
3283
3284
3285
@_dataclasses
dataclass
class
FailFast
_fields
PydanticMetadata
BaseMetadata
"""A `FailFast` annotation can be used to specify that validation should stop at the first error.
This can be useful when you want to validate a large amount of data and you only need to know if it's valid or not.
You might want to enable this setting if you want to validate your data faster (basically, if you use this,
validation will be more performant with the caveat that you get less information).
```python
from typing import Annotated
from pydantic import BaseModel, FailFast, ValidationError
class Model(BaseModel):
x: Annotated[list[int], FailFast()]
# This will raise a single error for the first invalid value and stop validation
try:
obj = Model(x=[1, 2, 'a', 4, 5, 'b', 7, 8, 9, 'c'])
except ValidationError as e:
print(e)
'''
1 validation error for Model
x.2
Input should be a valid integer, unable to parse string as an integer [type=int_parsing, input_value='a', input_type=str]
'''
```
"""
fail_fast
bool
True
conint
conint
strict
bool
None
None
int
None
None
int
None
None
int
None
None
int
None
None
multiple_of
int
None
None
type
int
Discouraged
This function is
discouraged
in favor of using
Annotated
with
Field
instead.
This function will be
deprecated
in Pydantic 3.0.
The reason is that
conint
returns a type, which doesn't play well with static analysis tools.
Don't do this
Do this
from
pydantic
import
BaseModel
conint
class
Foo
BaseModel
bar
conint
strict
True
from
typing
import
Annotated
from
pydantic
import
BaseModel
Field
class
Foo
BaseModel
bar
Annotated
int
Field
strict
True
A wrapper around
int
that allows for additional constraints.
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether to validate the integer in strict mode. Defaults to
None
None
int
| None
The value must be greater than this.
None
int
| None
The value must be greater than or equal to this.
None
int
| None
The value must be less than this.
None
int
| None
The value must be less than or equal to this.
None
multiple_of
int
| None
The value must be a multiple of this.
None
Returns:
Type
Description
type
int
The wrapped integer type.
from
pydantic
import
BaseModel
ValidationError
conint
class
ConstrainedExample
BaseModel
constrained_int
conint
ConstrainedExample
constrained_int
print
repr
#> ConstrainedExample(constrained_int=2)
try
ConstrainedExample
constrained_int
except
ValidationError
print
errors
())
'''
'type': 'greater_than',
'loc': ('constrained_int',),
'msg': 'Input should be greater than 1',
'input': 0,
'ctx': {'gt': 1},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
Source code in
pydantic/types.py
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
def
conint
strict
bool
None
None
int
None
None
int
None
None
int
None
None
int
None
None
multiple_of
int
None
None
type
int
"""
!!! warning "Discouraged"
This function is **discouraged** in favor of using
[`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with
[`Field`][pydantic.fields.Field] instead.
This function will be **deprecated** in Pydantic 3.0.
The reason is that `conint` returns a type, which doesn't play well with static analysis tools.
=== ":x: Don't do this"
```python
from pydantic import BaseModel, conint
class Foo(BaseModel):
bar: conint(strict=True, gt=0)
```
=== ":white_check_mark: Do this"
```python
from typing import Annotated
from pydantic import BaseModel, Field
class Foo(BaseModel):
bar: Annotated[int, Field(strict=True, gt=0)]
```
A wrapper around `int` that allows for additional constraints.
Args:
strict: Whether to validate the integer in strict mode. Defaults to `None`.
gt: The value must be greater than this.
ge: The value must be greater than or equal to this.
lt: The value must be less than this.
le: The value must be less than or equal to this.
multiple_of: The value must be a multiple of this.
Returns:
The wrapped integer type.
```python
from pydantic import BaseModel, ValidationError, conint
class ConstrainedExample(BaseModel):
constrained_int: conint(gt=1)
m = ConstrainedExample(constrained_int=2)
print(repr(m))
#> ConstrainedExample(constrained_int=2)
try:
ConstrainedExample(constrained_int=0)
except ValidationError as e:
print(e.errors())
'''
'type': 'greater_than',
'loc': ('constrained_int',),
'msg': 'Input should be greater than 1',
'input': 0,
'ctx': {'gt': 1},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
```
"""
# noqa: D212
return
Annotated
# pyright: ignore[reportReturnType]
int
Strict
strict
strict
not
None
else
None
annotated_types
Interval
annotated_types
MultipleOf
multiple_of
multiple_of
not
None
else
None
confloat
confloat
strict
bool
None
None
float
None
None
float
None
None
float
None
None
float
None
None
multiple_of
float
None
None
allow_inf_nan
bool
None
None
type
float
Discouraged
This function is
discouraged
in favor of using
Annotated
with
Field
instead.
This function will be
deprecated
in Pydantic 3.0.
The reason is that
confloat
returns a type, which doesn't play well with static analysis tools.
Don't do this
Do this
from
pydantic
import
BaseModel
confloat
class
Foo
BaseModel
bar
confloat
strict
True
from
typing
import
Annotated
from
pydantic
import
BaseModel
Field
class
Foo
BaseModel
bar
Annotated
float
Field
strict
True
A wrapper around
float
that allows for additional constraints.
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether to validate the float in strict mode.
None
float
| None
The value must be greater than this.
None
float
| None
The value must be greater than or equal to this.
None
float
| None
The value must be less than this.
None
float
| None
The value must be less than or equal to this.
None
multiple_of
float
| None
The value must be a multiple of this.
None
allow_inf_nan
bool
| None
Whether to allow
-inf
inf
, and
nan
None
Returns:
Type
Description
type
float
The wrapped float type.
from
pydantic
import
BaseModel
ValidationError
confloat
class
ConstrainedExample
BaseModel
constrained_float
confloat
1.0
ConstrainedExample
constrained_float
1.1
print
repr
#> ConstrainedExample(constrained_float=1.1)
try
ConstrainedExample
constrained_float
0.9
except
ValidationError
print
errors
())
'''
'type': 'greater_than',
'loc': ('constrained_float',),
'msg': 'Input should be greater than 1',
'input': 0.9,
'ctx': {'gt': 1.0},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
Source code in
pydantic/types.py
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
def
confloat
strict
bool
None
None
float
None
None
float
None
None
float
None
None
float
None
None
multiple_of
float
None
None
allow_inf_nan
bool
None
None
type
float
"""
!!! warning "Discouraged"
This function is **discouraged** in favor of using
[`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with
[`Field`][pydantic.fields.Field] instead.
This function will be **deprecated** in Pydantic 3.0.
The reason is that `confloat` returns a type, which doesn't play well with static analysis tools.
=== ":x: Don't do this"
```python
from pydantic import BaseModel, confloat
class Foo(BaseModel):
bar: confloat(strict=True, gt=0)
```
=== ":white_check_mark: Do this"
```python
from typing import Annotated
from pydantic import BaseModel, Field
class Foo(BaseModel):
bar: Annotated[float, Field(strict=True, gt=0)]
```
A wrapper around `float` that allows for additional constraints.
Args:
strict: Whether to validate the float in strict mode.
gt: The value must be greater than this.
ge: The value must be greater than or equal to this.
lt: The value must be less than this.
le: The value must be less than or equal to this.
multiple_of: The value must be a multiple of this.
allow_inf_nan: Whether to allow `-inf`, `inf`, and `nan`.
Returns:
The wrapped float type.
```python
from pydantic import BaseModel, ValidationError, confloat
class ConstrainedExample(BaseModel):
constrained_float: confloat(gt=1.0)
m = ConstrainedExample(constrained_float=1.1)
print(repr(m))
#> ConstrainedExample(constrained_float=1.1)
try:
ConstrainedExample(constrained_float=0.9)
except ValidationError as e:
print(e.errors())
'''
'type': 'greater_than',
'loc': ('constrained_float',),
'msg': 'Input should be greater than 1',
'input': 0.9,
'ctx': {'gt': 1.0},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
```
"""
# noqa: D212
return
Annotated
# pyright: ignore[reportReturnType]
float
Strict
strict
strict
not
None
else
None
annotated_types
Interval
annotated_types
MultipleOf
multiple_of
multiple_of
not
None
else
None
AllowInfNan
allow_inf_nan
allow_inf_nan
not
None
else
None
conbytes
conbytes
min_length
int
None
None
max_length
int
None
None
strict
bool
None
None
type
bytes
A wrapper around
bytes
that allows for additional constraints.
Parameters:
Name
Type
Description
Default
min_length
int
| None
The minimum length of the bytes.
None
max_length
int
| None
The maximum length of the bytes.
None
strict
bool
| None
Whether to validate the bytes in strict mode.
None
Returns:
Type
Description
type
bytes
The wrapped bytes type.
Source code in
pydantic/types.py
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
def
conbytes
min_length
int
None
None
max_length
int
None
None
strict
bool
None
None
type
bytes
"""A wrapper around `bytes` that allows for additional constraints.
Args:
min_length: The minimum length of the bytes.
max_length: The maximum length of the bytes.
strict: Whether to validate the bytes in strict mode.
Returns:
The wrapped bytes type.
"""
return
Annotated
# pyright: ignore[reportReturnType]
bytes
Strict
strict
strict
not
None
else
None
annotated_types
Len
min_length
max_length
constr
constr
strip_whitespace
bool
None
None
to_upper
bool
None
None
to_lower
bool
None
None
strict
bool
None
None
min_length
int
None
None
max_length
int
None
None
pattern
str
Pattern
str
None
None
type
str
Discouraged
This function is
discouraged
in favor of using
Annotated
with
StringConstraints
instead.
This function will be
deprecated
in Pydantic 3.0.
The reason is that
constr
returns a type, which doesn't play well with static analysis tools.
Don't do this
Do this
from
pydantic
import
BaseModel
constr
class
Foo
BaseModel
bar
constr
strip_whitespace
True
to_upper
True
pattern
'^[A-Z]+$'
from
typing
import
Annotated
from
pydantic
import
BaseModel
StringConstraints
class
Foo
BaseModel
bar
Annotated
str
StringConstraints
strip_whitespace
True
to_upper
True
pattern
'^[A-Z]+$'
A wrapper around
str
that allows for additional constraints.
from
pydantic
import
BaseModel
constr
class
Foo
BaseModel
bar
constr
strip_whitespace
True
to_upper
True
foo
Foo
bar
' hello '
print
foo
#> bar='HELLO'
Parameters:
Name
Type
Description
Default
strip_whitespace
bool
| None
Whether to remove leading and trailing whitespace.
None
to_upper
bool
| None
Whether to turn all characters to uppercase.
None
to_lower
bool
| None
Whether to turn all characters to lowercase.
None
strict
bool
| None
Whether to validate the string in strict mode.
None
min_length
int
| None
The minimum length of the string.
None
max_length
int
| None
The maximum length of the string.
None
pattern
str
Pattern
str
] | None
A regex pattern to validate the string against.
None
Returns:
Type
Description
type
str
The wrapped string type.
Source code in
pydantic/types.py
749
750
751
752
753
754
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
def
constr
strip_whitespace
bool
None
None
to_upper
bool
None
None
to_lower
bool
None
None
strict
bool
None
None
min_length
int
None
None
max_length
int
None
None
pattern
str
Pattern
str
None
None
type
str
"""
!!! warning "Discouraged"
This function is **discouraged** in favor of using
[`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with
[`StringConstraints`][pydantic.types.StringConstraints] instead.
This function will be **deprecated** in Pydantic 3.0.
The reason is that `constr` returns a type, which doesn't play well with static analysis tools.
=== ":x: Don't do this"
```python
from pydantic import BaseModel, constr
class Foo(BaseModel):
bar: constr(strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$')
```
=== ":white_check_mark: Do this"
```python
from typing import Annotated
from pydantic import BaseModel, StringConstraints
class Foo(BaseModel):
bar: Annotated[
str,
StringConstraints(
strip_whitespace=True, to_upper=True, pattern=r'^[A-Z]+$'
```
A wrapper around `str` that allows for additional constraints.
```python
from pydantic import BaseModel, constr
class Foo(BaseModel):
bar: constr(strip_whitespace=True, to_upper=True)
foo = Foo(bar='  hello  ')
print(foo)
#> bar='HELLO'
```
Args:
strip_whitespace: Whether to remove leading and trailing whitespace.
to_upper: Whether to turn all characters to uppercase.
to_lower: Whether to turn all characters to lowercase.
strict: Whether to validate the string in strict mode.
min_length: The minimum length of the string.
max_length: The maximum length of the string.
pattern: A regex pattern to validate the string against.
Returns:
The wrapped string type.
"""
# noqa: D212
return
Annotated
# pyright: ignore[reportReturnType]
str
StringConstraints
strip_whitespace
strip_whitespace
to_upper
to_upper
to_lower
to_lower
strict
strict
min_length
min_length
max_length
max_length
pattern
pattern
conset
conset
item_type
type
HashableItemType
min_length
int
None
None
max_length
int
None
None
type
set
HashableItemType
A wrapper around
typing.Set
that allows for additional constraints.
Parameters:
Name
Type
Description
Default
item_type
type
HashableItemType
The type of the items in the set.
required
min_length
int
| None
The minimum length of the set.
None
max_length
int
| None
The maximum length of the set.
None
Returns:
Type
Description
type
set
HashableItemType
The wrapped set type.
Source code in
pydantic/types.py
839
840
841
842
843
844
845
846
847
848
849
850
851
852
def
conset
item_type
type
HashableItemType
min_length
int
None
None
max_length
int
None
None
type
set
HashableItemType
]]:
"""A wrapper around `typing.Set` that allows for additional constraints.
Args:
item_type: The type of the items in the set.
min_length: The minimum length of the set.
max_length: The maximum length of the set.
Returns:
The wrapped set type.
"""
return
Annotated
set
item_type
annotated_types
Len
min_length
max_length
# pyright: ignore[reportReturnType]
confrozenset
confrozenset
item_type
type
HashableItemType
min_length
int
None
None
max_length
int
None
None
type
frozenset
HashableItemType
A wrapper around
typing.FrozenSet
that allows for additional constraints.
Parameters:
Name
Type
Description
Default
item_type
type
HashableItemType
The type of the items in the frozenset.
required
min_length
int
| None
The minimum length of the frozenset.
None
max_length
int
| None
The maximum length of the frozenset.
None
Returns:
Type
Description
type
frozenset
HashableItemType
The wrapped frozenset type.
Source code in
pydantic/types.py
855
856
857
858
859
860
861
862
863
864
865
866
867
868
def
confrozenset
item_type
type
HashableItemType
min_length
int
None
None
max_length
int
None
None
type
frozenset
HashableItemType
]]:
"""A wrapper around `typing.FrozenSet` that allows for additional constraints.
Args:
item_type: The type of the items in the frozenset.
min_length: The minimum length of the frozenset.
max_length: The maximum length of the frozenset.
Returns:
The wrapped frozenset type.
"""
return
Annotated
frozenset
item_type
annotated_types
Len
min_length
max_length
# pyright: ignore[reportReturnType]
conlist
conlist
item_type
type
AnyItemType
min_length
int
None
None
max_length
int
None
None
unique_items
bool
None
None
type
list
AnyItemType
A wrapper around
list
that adds validation.
Parameters:
Name
Type
Description
Default
item_type
type
AnyItemType
The type of the items in the list.
required
min_length
int
| None
The minimum length of the list. Defaults to None.
None
max_length
int
| None
The maximum length of the list. Defaults to None.
None
unique_items
bool
| None
Whether the items in the list must be unique. Defaults to None.
Warning
The
unique_items
parameter is deprecated, use
Set
instead.
See
this issue
for more details.
None
Returns:
Type
Description
type
list
AnyItemType
The wrapped list type.
Source code in
pydantic/types.py
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899
900
901
902
903
def
conlist
item_type
type
AnyItemType
min_length
int
None
None
max_length
int
None
None
unique_items
bool
None
None
type
list
AnyItemType
]]:
"""A wrapper around [`list`][] that adds validation.
Args:
item_type: The type of the items in the list.
min_length: The minimum length of the list. Defaults to None.
max_length: The maximum length of the list. Defaults to None.
unique_items: Whether the items in the list must be unique. Defaults to None.
!!! warning Deprecated
The `unique_items` parameter is deprecated, use `Set` instead.
See [this issue](https://github.com/pydantic/pydantic-core/issues/296) for more details.
Returns:
The wrapped list type.
"""
unique_items
not
None
raise
PydanticUserError
'`unique_items` is removed, use `Set` instead'
'(this feature is discussed in https://github.com/pydantic/pydantic-core/issues/296)'
code
'removed-kwargs'
return
Annotated
list
item_type
annotated_types
Len
min_length
max_length
# pyright: ignore[reportReturnType]
condecimal
condecimal
strict
bool
None
None
int
Decimal
None
None
int
Decimal
None
None
int
Decimal
None
None
int
Decimal
None
None
multiple_of
int
Decimal
None
None
max_digits
int
None
None
decimal_places
int
None
None
allow_inf_nan
bool
None
None
type
Decimal
Discouraged
This function is
discouraged
in favor of using
Annotated
with
Field
instead.
This function will be
deprecated
in Pydantic 3.0.
The reason is that
condecimal
returns a type, which doesn't play well with static analysis tools.
Don't do this
Do this
from
pydantic
import
BaseModel
condecimal
class
Foo
BaseModel
bar
condecimal
strict
True
allow_inf_nan
True
from
decimal
import
Decimal
from
typing
import
Annotated
from
pydantic
import
BaseModel
Field
class
Foo
BaseModel
bar
Annotated
Decimal
Field
strict
True
allow_inf_nan
True
A wrapper around Decimal that adds validation.
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether to validate the value in strict mode. Defaults to
None
None
int
Decimal
| None
The value must be greater than this. Defaults to
None
None
int
Decimal
| None
The value must be greater than or equal to this. Defaults to
None
None
int
Decimal
| None
The value must be less than this. Defaults to
None
None
int
Decimal
| None
The value must be less than or equal to this. Defaults to
None
None
multiple_of
int
Decimal
| None
The value must be a multiple of this. Defaults to
None
None
max_digits
int
| None
The maximum number of digits. Defaults to
None
None
decimal_places
int
| None
The number of decimal places. Defaults to
None
None
allow_inf_nan
bool
| None
Whether to allow infinity and NaN. Defaults to
None
None
from
decimal
import
Decimal
from
pydantic
import
BaseModel
ValidationError
condecimal
class
ConstrainedExample
BaseModel
constrained_decimal
condecimal
Decimal
'1.0'
ConstrainedExample
constrained_decimal
Decimal
'1.1'
print
repr
#> ConstrainedExample(constrained_decimal=Decimal('1.1'))
try
ConstrainedExample
constrained_decimal
Decimal
'0.9'
except
ValidationError
print
errors
())
'''
'type': 'greater_than',
'loc': ('constrained_decimal',),
'msg': 'Input should be greater than 1.0',
'input': Decimal('0.9'),
'ctx': {'gt': Decimal('1.0')},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
Source code in
pydantic/types.py
1041
1042
1043
1044
1045
1046
1047
1048
1049
1050
1051
1052
1053
1054
1055
1056
1057
1058
1059
1060
1061
1062
1063
1064
1065
1066
1067
1068
1069
1070
1071
1072
1073
1074
1075
1076
1077
1078
1079
1080
1081
1082
1083
1084
1085
1086
1087
1088
1089
1090
1091
1092
1093
1094
1095
1096
1097
1098
1099
1100
1101
1102
1103
1104
1105
1106
1107
1108
1109
1110
1111
1112
1113
1114
1115
1116
1117
1118
1119
1120
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
def
condecimal
strict
bool
None
None
int
Decimal
None
None
int
Decimal
None
None
int
Decimal
None
None
int
Decimal
None
None
multiple_of
int
Decimal
None
None
max_digits
int
None
None
decimal_places
int
None
None
allow_inf_nan
bool
None
None
type
Decimal
"""
!!! warning "Discouraged"
This function is **discouraged** in favor of using
[`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) with
[`Field`][pydantic.fields.Field] instead.
This function will be **deprecated** in Pydantic 3.0.
The reason is that `condecimal` returns a type, which doesn't play well with static analysis tools.
=== ":x: Don't do this"
```python
from pydantic import BaseModel, condecimal
class Foo(BaseModel):
bar: condecimal(strict=True, allow_inf_nan=True)
```
=== ":white_check_mark: Do this"
```python
from decimal import Decimal
from typing import Annotated
from pydantic import BaseModel, Field
class Foo(BaseModel):
bar: Annotated[Decimal, Field(strict=True, allow_inf_nan=True)]
```
A wrapper around Decimal that adds validation.
Args:
strict: Whether to validate the value in strict mode. Defaults to `None`.
gt: The value must be greater than this. Defaults to `None`.
ge: The value must be greater than or equal to this. Defaults to `None`.
lt: The value must be less than this. Defaults to `None`.
le: The value must be less than or equal to this. Defaults to `None`.
multiple_of: The value must be a multiple of this. Defaults to `None`.
max_digits: The maximum number of digits. Defaults to `None`.
decimal_places: The number of decimal places. Defaults to `None`.
allow_inf_nan: Whether to allow infinity and NaN. Defaults to `None`.
```python
from decimal import Decimal
from pydantic import BaseModel, ValidationError, condecimal
class ConstrainedExample(BaseModel):
constrained_decimal: condecimal(gt=Decimal('1.0'))
m = ConstrainedExample(constrained_decimal=Decimal('1.1'))
print(repr(m))
#> ConstrainedExample(constrained_decimal=Decimal('1.1'))
try:
ConstrainedExample(constrained_decimal=Decimal('0.9'))
except ValidationError as e:
print(e.errors())
'''
'type': 'greater_than',
'loc': ('constrained_decimal',),
'msg': 'Input should be greater than 1.0',
'input': Decimal('0.9'),
'ctx': {'gt': Decimal('1.0')},
'url': 'https://errors.pydantic.dev/2/v/greater_than',
'''
```
"""
# noqa: D212
return
Annotated
# pyright: ignore[reportReturnType]
Decimal
Strict
strict
strict
not
None
else
None
annotated_types
Interval
annotated_types
MultipleOf
multiple_of
multiple_of
not
None
else
None
_fields
pydantic_general_metadata
max_digits
max_digits
decimal_places
decimal_places
AllowInfNan
allow_inf_nan
allow_inf_nan
not
None
else
None
condate
condate
strict
bool
None
None
date
None
None
date
None
None
date
None
None
date
None
None
type
date
A wrapper for date that adds constraints.
Parameters:
Name
Type
Description
Default
strict
bool
| None
Whether to validate the date value in strict mode. Defaults to
None
None
date
| None
The value must be greater than this. Defaults to
None
None
date
| None
The value must be greater than or equal to this. Defaults to
None
None
date
| None
The value must be less than this. Defaults to
None
None
date
| None
The value must be less than or equal to this. Defaults to
None
None
Returns:
Type
Description
type
date
A date type with the specified constraints.
Source code in
pydantic/types.py
2242
2243
2244
2245
2246
2247
2248
2249
2250
2251
2252
2253
2254
2255
2256
2257
2258
2259
2260
2261
2262
2263
2264
2265
2266
def
condate
strict
bool
None
None
date
None
None
date
None
None
date
None
None
date
None
None
type
date
"""A wrapper for date that adds constraints.
Args:
strict: Whether to validate the date value in strict mode. Defaults to `None`.
gt: The value must be greater than this. Defaults to `None`.
ge: The value must be greater than or equal to this. Defaults to `None`.
lt: The value must be less than this. Defaults to `None`.
le: The value must be less than or equal to this. Defaults to `None`.
Returns:
A date type with the specified constraints.
"""
return
Annotated
# pyright: ignore[reportReturnType]
date
Strict
strict
strict
not
None
else
None
annotated_types
Interval
Was this page helpful?
Thanks for your feedback!
Thanks for your feedback!